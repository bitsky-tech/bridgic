{"config":{"lang":["en"],"separator":"[\\s\\-,:!=\\[\\]()\"`/]+|\\.(?!\\d)|&[lg]t;|(?!\\b)(?=[A-Z][a-z])","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to the world of Bridgic!","text":"<p>Bridgic is an innovative programming framework designed to create agentic systems, from simple workflows to fully autonomous agents. Its APIs are thoughtfully crafted to be both simple and powerful.</p>"},{"location":"#what-does-the-word-bridgic-mean","title":"What does the word \"Bridgic\" mean?","text":"<p>The name Bridgic is inspired by the idea of \u201cbridging logic and magic\u201d \u2014 bringing together the precision of logic and the creativity of intelligence.</p> <ul> <li>Logic stands for deterministic execution flows.</li> <li>Magic stands for highly autonomous AI agents.</li> </ul> <p>Refer to the relevant sections of the documentation for details:</p> <ul> <li>Installation.</li> <li>Tutorials.</li> <li>Understanding.</li> <li>API Reference.</li> </ul>"},{"location":"extras/llms/","title":"llms","text":""},{"location":"extras/llms/#llm_indexmd","title":"llm_index.md","text":"<p>TODO:</p>"},{"location":"home/concepts/","title":"Concepts","text":"<p>Bridgic has two core concepts:</p> <ul> <li>Worker: the basic execution unit in Bridgic.</li> <li>Automa: an entity that manages and orchestrates a group of workers. An Automa itself is also a Worker, which enables the nesting of Automa instances within each other.</li> </ul>"},{"location":"home/concepts/#worker","title":"Worker","text":"<p>A worker is the basic execution unit in the Bridgic framework, representing a specific task node.</p> <p>A worker typically corresponds to a function (which can be synchronous or asynchronous) that performs an independent business logic or processing step. Workers are automatically linked through <code>dependencies</code> to form a complete workflow. The design of Workers makes task decomposition, reuse, and composition simple and flexible, serving as the core foundation for implementing automated processing and complex business orchestration.</p> <p>There are two ways to define worker:</p> <ol> <li>Use the <code>@worker</code> decorator to decorate member methods (regular or asynchronous) of a <code>GraphAutoma</code> class, thereby registering them as Workers.</li> <li>Inherit from the <code>Worker</code> base class and implement its <code>run</code> or <code>arun</code> method.</li> </ol>"},{"location":"home/concepts/#graphautoma","title":"GraphAutoma","text":"<p><code>GraphAutoma</code> is an implementation of <code>Automa</code> based on Dynamic Directed Graph (abbreviated as DDG). It is the core class in Bridgic. It is not just a simple task scheduler, but a highly abstracted workflow engine that helps developers organize, manage, and run complex asynchronous or synchronous task flows in a declarative manner. Compared to traditional flow control or manual orchestration, <code>GraphAutoma</code> offers the following significant advantages:</p> <ol> <li> <p>Declarative Dependency Modeling    With the <code>@worker</code> decorator, developers can define each node (worker) and its dependencies as if building blocks, without writing tedious scheduling logic. Each worker can be a synchronous or asynchronous function, and GraphAutoma will automatically recognize dependencies and construct the complete task graph.</p> </li> <li> <p>Call-based Branch Management    In addition to automatic scheduling based on dependencies, GraphAutoma also supports \"active jump\" or \"call-based branching\" control via the <code>ferry_to</code> method. Developers can call <code>ferry_to(worker_key, *args, **kwargs)</code> inside any worker to directly switch the control flow to the specified worker and pass parameters. This mechanism is similar to \"goto\" or \"event-driven jump\", but operates at the granularity of complete workers, while still maintaining asynchronous safety and context consistency. This feature is suitable for complex scenarios requiring dynamic branching, making it easy for developers to create logic orchestration programs that depend on runtime conditions. The call to <code>ferry_to</code> is not constrained by dependencies; the target worker will be scheduled for asynchronous execution in the next dynamic step, greatly enhancing the flexibility and controllability of the workflow.</p> </li> <li> <p>Automatic Driving and Scheduling    Once dependencies are defined, GraphAutoma will automatically drive the execution of each worker according to the dependency topology. As long as dependencies are satisfied, workers will be automatically scheduled, eliminating the need for developers to manually manage execution order or state transitions, thus greatly reducing error rates and maintenance costs.</p> </li> <li> <p>Asynchronous and Concurrent Support    GraphAutoma natively supports asynchronous workers, fully leveraging Python's async features for efficient concurrent execution. For I/O-intensive tasks, it can also be combined with thread pools to achieve true parallel processing and improve overall throughput.</p> </li> <li> <p>Human Interaction and Event Mechanism    In complex business scenarios, some nodes may need to wait for manual input or external events. GraphAutoma has a built-in human interaction mechanism, supporting task pausing and resuming after receiving user feedback. At the same time, it integrates Bridgic's event system, enabling real-time communication between workers and the application layer, greatly enhancing the system's interactivity and flexibility.</p> </li> <li> <p>Serialization and Persistence    GraphAutoma supports complete serialization and deserialization, allowing the current workflow state to be persisted to disk or a database, enabling advanced features such as checkpointing and fault recovery. This is especially important for long-running or highly reliable business processes.</p> </li> <li> <p>Dynamic Topology Changes    Supports dynamically adding or removing workers at runtime, flexibly adapting to changes and expansion needs in business processes. There is no need to restart or refactor the entire workflow, greatly improving system maintainability and scalability.</p> </li> <li> <p>Layerizable and Editable    GraphAutoma supports deep customization through inheritance and secondary development. Developers can flexibly add new worker nodes based on existing GraphAutoma subclasses to extend their behavior and functionality, enabling personalized customization of business processes. By inheriting, you can reuse existing dependencies and scheduling logic, and then stack new business nodes on top, quickly building more complex automated workflows. This layered and editable design allows GraphAutoma to be continuously expanded and evolved like Lego blocks, greatly enhancing the system's maintainability and evolutionary capability.</p> </li> <li> <p>Nestable and Reusable    GraphAutoma is not just a top-level workflow engine; it can itself act as a \"super worker\" and be nested within another, larger GraphAutoma. Each GraphAutoma instance can be scheduled, passed parameters, and reused just like a regular worker, enabling recursive composition and hierarchical management of workflows. This nesting mechanism allows developers to use the composition pattern to orchestrate even larger and more complex business processes.</p> </li> </ol> <p>In summary, <code>GraphAutoma</code> enables developers to quickly build robust, flexible, interactive, and persistent complex workflow systems with minimal mental overhead, making it a powerful cornerstone for modern automation and intelligent application development.</p>"},{"location":"home/introduction/","title":"Introduction","text":"<p>Bridgic is an innovative programming framework designed to create agentic systems, from simple workflows to fully autonomous agents. Its APIs are thoughtfully crafted to be both simple and powerful.</p>"},{"location":"home/introduction/#core-features","title":"Core Features","text":"<ul> <li>Orchestration: Bridgic helps to manage the control flow of your AI applications asynchronously.</li> <li>Dynamic Control Flow: Bridgic supports dynamic routing based on input data, and even allows workers to be added or removed at runtime.</li> <li>Modularity: In Bridgic, a complex agentic system can be composed by reusing components through hierarchical nesting.</li> <li>Human-in-the-Loop: A workflow or an agent built with Bridgic can request feedback from humans whenever needed.</li> <li>Serialization: Bridgic includes serialization, deserialization, and resuming capabilities to support human-in-the-loop.</li> <li>Parameter Binding: There are three ways to pass data among workers, including Arguments Mapping, Arguments Injection, and Inputs Propagation.</li> <li>Systematic Integration: A wide range of tools and LLMs can be seamlessly integrated into the Bridgic world, in a systematic way.</li> <li>Customization: What Bridgic provides is not a \"black box\" approach. You have full control over every aspect of your AI applications, such as prompts, context windows, the control flow, and more.</li> </ul>"},{"location":"reference/bridgic-core/bridgic/core/","title":"core","text":""},{"location":"reference/bridgic-core/bridgic/core/agentic/","title":"agentic","text":"<p>The Agentic module provides core components for building intelligent agent systems.</p> <p>This module contains various Automa implementations for orchestrating and executing  LLM-based intelligent agent workflows. These Automa implementations are typically  composed together to build complex intelligent agents with advanced capabilities.</p>"},{"location":"reference/bridgic-core/bridgic/core/agentic/#bridgic.core.agentic.ConcurrentAutoma","title":"ConcurrentAutoma","text":"<p>               Bases: <code>GraphAutoma</code></p> <p>This class is to provide concurrent execution of multiple workers.</p> <p>In accordance with the defined \"Concurrency Model of Worker\", each worker within  a ConcurrentAutoma can be configured to operate in one of two concurrency modes:</p> <ol> <li>Async Mode: Workers execute concurrently in an asynchronous fashion, driven  by the event loop of the main thread. This execution mode corresponds to the <code>arun()</code>  method of the Worker.</li> <li>Parallel Mode: Workers execute synchronously, each running in a dedicated  thread within a thread pool managed by the ConcurrentAutoma. This execution mode  corresponds to the <code>run()</code> method of the Worker.</li> </ol> <p>Upon completion of all worker tasks, the concurrent automa instance aggregates  the result outputs from each worker into a single list, which is then returned  to the caller.</p> Source code in <code>bridgic/core/agentic/_concurrent_automa.py</code> <pre><code>class ConcurrentAutoma(GraphAutoma):\n    \"\"\"\n    This class is to provide concurrent execution of multiple workers.\n\n    In accordance with the defined \"Concurrency Model of Worker\", each worker within \n    a ConcurrentAutoma can be configured to operate in one of two concurrency modes:\n\n    1. **Async Mode**: Workers execute concurrently in an asynchronous fashion, driven \n    by the event loop of the main thread. This execution mode corresponds to the `arun()` \n    method of the Worker.\n    2. **Parallel Mode**: Workers execute synchronously, each running in a dedicated \n    thread within a thread pool managed by the ConcurrentAutoma. This execution mode \n    corresponds to the `run()` method of the Worker.\n\n    Upon completion of all worker tasks, the concurrent automa instance aggregates \n    the result outputs from each worker into a single list, which is then returned \n    to the caller.\n    \"\"\"\n\n    # Automa type.\n    AUTOMA_TYPE: ClassVar[AutomaType] = AutomaType.Concurrent\n\n    _MERGER_WORKER_KEY: Final[str] = \"__merger__\"\n\n    def __init__(\n        self,\n        name: Optional[str] = None,\n        thread_pool: Optional[ThreadPoolExecutor] = None,\n    ):\n        super().__init__(name=name, thread_pool=thread_pool)\n\n        # Implementation notes:\n        # There are two types of workers in the concurrent automa:\n        # 1. Concurrent workers: These workers will be concurrently executed with each other.\n        # 2. The Merger worker: This worker will merge the results of all the concurrent workers.\n\n        cls = type(self)\n        if cls.AUTOMA_TYPE == AutomaType.Concurrent:\n            # The _registered_worker_funcs data are from @worker decorators.\n            # Initialize the decorated concurrent workers.\n            for worker_key, worker_func in self._registered_worker_funcs.items():\n                super().add_func_as_worker(\n                    key=worker_key,\n                    func=worker_func,\n                    is_start=True,\n                )\n\n        # Add a hidden worker as the merger worker, which will merge the results of all the start workers.\n        super().add_func_as_worker(\n            key=self._MERGER_WORKER_KEY,\n            func=self._merge_workers_results,\n            dependencies=super().all_workers(),\n            is_output=True,\n            args_mapping_rule=ArgsMappingRule.MERGE,\n        )\n\n    def _merge_workers_results(self, results: List[Any]) -&gt; List[Any]:\n        return results\n\n    @override\n    def add_worker(\n        self,\n        key: str,\n        worker: Worker,\n    ) -&gt; None:\n        \"\"\"\n        Add a concurrent worker to the concurrent automa. This worker will be concurrently executed with other concurrent workers.\n\n        Parameters\n        ----------\n        key : str\n            The key of the worker.\n        worker : Worker\n            The worker instance to be registered.\n        \"\"\"\n        if key == self._MERGER_WORKER_KEY:\n            raise AutomaRuntimeError(f\"the reserved key `{key}` is not allowed to be used by `add_worker()`\")\n        # Implementation notes:\n        # Concurrent workers are implemented as start workers in the underlying graph automa.\n        super().add_worker(key=key, worker=worker, is_start=True)\n        super().add_dependency(self._MERGER_WORKER_KEY, key)\n\n    @override\n    def add_func_as_worker(\n        self,\n        key: str,\n        func: Callable,\n    ) -&gt; None:\n        \"\"\"\n        Add a function or method as a concurrent worker to the concurrent automa. This worker will be concurrently executed with other concurrent workers.\n\n        Parameters\n        ----------\n        key : str\n            The key of the function worker.\n        func : Callable\n            The function to be added as a concurrent worker to the automa.\n        \"\"\"\n        if key == self._MERGER_WORKER_KEY:\n            raise AutomaRuntimeError(f\"the reserved key `{key}` is not allowed to be used by `add_func_as_worker()`\")\n        # Implementation notes:\n        # Concurrent workers are implemented as start workers in the underlying graph automa.\n        super().add_func_as_worker(key=key, func=func, is_start=True)\n        super().add_dependency(self._MERGER_WORKER_KEY, key)\n\n    @override\n    def worker(\n        self,\n        *,\n        key: Optional[str] = None,\n    ) -&gt; Callable:\n        \"\"\"\n        This is a decorator to mark a function or method as a concurrent worker of the concurrent automa. This worker will be concurrently executed with other concurrent workers.\n\n        Parameters\n        ----------\n        key : str\n            The key of the worker. If not provided, the name of the decorated callable will be used.\n        \"\"\"\n        if key == self._MERGER_WORKER_KEY:\n            raise AutomaRuntimeError(f\"the reserved key `{key}` is not allowed to be used by `automa.worker()`\")\n\n        super_automa = super()\n        def wrapper(func: Callable):\n            super_automa.add_func_as_worker(key=key, func=func, is_start=True)\n            super_automa.add_dependency(self._MERGER_WORKER_KEY, key)\n\n        return wrapper\n\n    @override\n    def remove_worker(self, key: str) -&gt; None:\n        \"\"\"\n        Remove a concurrent worker from the concurrent automa.\n\n        Parameters\n        ----------\n        key : str\n            The key of the worker to be removed.\n        \"\"\"\n        if key == self._MERGER_WORKER_KEY:\n            raise AutomaRuntimeError(f\"the merge worker is not allowed to be removed from the concurrent automa\")\n        super().remove_worker(key=key)\n\n    @override\n    def add_dependency(\n        self,\n        key: str,\n        dependency: str,\n    ) -&gt; None:\n        raise AutomaRuntimeError(f\"add_dependency() is not allowed to be called on a concurrent automa\")\n\n    def all_workers(self) -&gt; List[str]:\n        \"\"\"\n        Gets a list containing the keys of all concurrent workers registered in this concurrent automa.\n\n        Returns\n        -------\n        List[str]\n            A list of concurrent worker keys.\n        \"\"\"\n        keys_list = super().all_workers()\n        # Implementation notes:\n        # Hide the merger worker from the list of concurrent workers.\n        return list(filter(lambda key: key != self._MERGER_WORKER_KEY, keys_list))\n\n    def ferry_to(self, worker_key: str, /, *args, **kwargs):\n        raise AutomaRuntimeError(f\"ferry_to() is not allowed to be called on a concurrent automa\")\n\n    async def arun(\n        self, \n        *args: Tuple[Any, ...],\n        interaction_feedback: Optional[InteractionFeedback] = None,\n        interaction_feedbacks: Optional[List[InteractionFeedback]] = None,\n        **kwargs: Dict[str, Any]\n    ) -&gt; List[Any]:\n        result = await super().arun(\n            *args,\n            interaction_feedback=interaction_feedback,\n            interaction_feedbacks=interaction_feedbacks,\n            **kwargs\n        )\n        return cast(List[Any], result)\n</code></pre>"},{"location":"reference/bridgic-core/bridgic/core/agentic/#bridgic.core.agentic.ConcurrentAutoma.add_worker","title":"add_worker","text":"<pre><code>add_worker(key: str, worker: Worker) -&gt; None\n</code></pre> <p>Add a concurrent worker to the concurrent automa. This worker will be concurrently executed with other concurrent workers.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>The key of the worker.</p> required <code>worker</code> <code>Worker</code> <p>The worker instance to be registered.</p> required Source code in <code>bridgic/core/agentic/_concurrent_automa.py</code> <pre><code>@override\ndef add_worker(\n    self,\n    key: str,\n    worker: Worker,\n) -&gt; None:\n    \"\"\"\n    Add a concurrent worker to the concurrent automa. This worker will be concurrently executed with other concurrent workers.\n\n    Parameters\n    ----------\n    key : str\n        The key of the worker.\n    worker : Worker\n        The worker instance to be registered.\n    \"\"\"\n    if key == self._MERGER_WORKER_KEY:\n        raise AutomaRuntimeError(f\"the reserved key `{key}` is not allowed to be used by `add_worker()`\")\n    # Implementation notes:\n    # Concurrent workers are implemented as start workers in the underlying graph automa.\n    super().add_worker(key=key, worker=worker, is_start=True)\n    super().add_dependency(self._MERGER_WORKER_KEY, key)\n</code></pre>"},{"location":"reference/bridgic-core/bridgic/core/agentic/#bridgic.core.agentic.ConcurrentAutoma.add_func_as_worker","title":"add_func_as_worker","text":"<pre><code>add_func_as_worker(key: str, func: Callable) -&gt; None\n</code></pre> <p>Add a function or method as a concurrent worker to the concurrent automa. This worker will be concurrently executed with other concurrent workers.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>The key of the function worker.</p> required <code>func</code> <code>Callable</code> <p>The function to be added as a concurrent worker to the automa.</p> required Source code in <code>bridgic/core/agentic/_concurrent_automa.py</code> <pre><code>@override\ndef add_func_as_worker(\n    self,\n    key: str,\n    func: Callable,\n) -&gt; None:\n    \"\"\"\n    Add a function or method as a concurrent worker to the concurrent automa. This worker will be concurrently executed with other concurrent workers.\n\n    Parameters\n    ----------\n    key : str\n        The key of the function worker.\n    func : Callable\n        The function to be added as a concurrent worker to the automa.\n    \"\"\"\n    if key == self._MERGER_WORKER_KEY:\n        raise AutomaRuntimeError(f\"the reserved key `{key}` is not allowed to be used by `add_func_as_worker()`\")\n    # Implementation notes:\n    # Concurrent workers are implemented as start workers in the underlying graph automa.\n    super().add_func_as_worker(key=key, func=func, is_start=True)\n    super().add_dependency(self._MERGER_WORKER_KEY, key)\n</code></pre>"},{"location":"reference/bridgic-core/bridgic/core/agentic/#bridgic.core.agentic.ConcurrentAutoma.worker","title":"worker","text":"<pre><code>worker(*, key: Optional[str] = None) -&gt; Callable\n</code></pre> <p>This is a decorator to mark a function or method as a concurrent worker of the concurrent automa. This worker will be concurrently executed with other concurrent workers.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>The key of the worker. If not provided, the name of the decorated callable will be used.</p> <code>None</code> Source code in <code>bridgic/core/agentic/_concurrent_automa.py</code> <pre><code>@override\ndef worker(\n    self,\n    *,\n    key: Optional[str] = None,\n) -&gt; Callable:\n    \"\"\"\n    This is a decorator to mark a function or method as a concurrent worker of the concurrent automa. This worker will be concurrently executed with other concurrent workers.\n\n    Parameters\n    ----------\n    key : str\n        The key of the worker. If not provided, the name of the decorated callable will be used.\n    \"\"\"\n    if key == self._MERGER_WORKER_KEY:\n        raise AutomaRuntimeError(f\"the reserved key `{key}` is not allowed to be used by `automa.worker()`\")\n\n    super_automa = super()\n    def wrapper(func: Callable):\n        super_automa.add_func_as_worker(key=key, func=func, is_start=True)\n        super_automa.add_dependency(self._MERGER_WORKER_KEY, key)\n\n    return wrapper\n</code></pre>"},{"location":"reference/bridgic-core/bridgic/core/agentic/#bridgic.core.agentic.ConcurrentAutoma.remove_worker","title":"remove_worker","text":"<pre><code>remove_worker(key: str) -&gt; None\n</code></pre> <p>Remove a concurrent worker from the concurrent automa.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>The key of the worker to be removed.</p> required Source code in <code>bridgic/core/agentic/_concurrent_automa.py</code> <pre><code>@override\ndef remove_worker(self, key: str) -&gt; None:\n    \"\"\"\n    Remove a concurrent worker from the concurrent automa.\n\n    Parameters\n    ----------\n    key : str\n        The key of the worker to be removed.\n    \"\"\"\n    if key == self._MERGER_WORKER_KEY:\n        raise AutomaRuntimeError(f\"the merge worker is not allowed to be removed from the concurrent automa\")\n    super().remove_worker(key=key)\n</code></pre>"},{"location":"reference/bridgic-core/bridgic/core/agentic/#bridgic.core.agentic.ConcurrentAutoma.all_workers","title":"all_workers","text":"<pre><code>all_workers() -&gt; List[str]\n</code></pre> <p>Gets a list containing the keys of all concurrent workers registered in this concurrent automa.</p> <p>Returns:</p> Type Description <code>List[str]</code> <p>A list of concurrent worker keys.</p> Source code in <code>bridgic/core/agentic/_concurrent_automa.py</code> <pre><code>def all_workers(self) -&gt; List[str]:\n    \"\"\"\n    Gets a list containing the keys of all concurrent workers registered in this concurrent automa.\n\n    Returns\n    -------\n    List[str]\n        A list of concurrent worker keys.\n    \"\"\"\n    keys_list = super().all_workers()\n    # Implementation notes:\n    # Hide the merger worker from the list of concurrent workers.\n    return list(filter(lambda key: key != self._MERGER_WORKER_KEY, keys_list))\n</code></pre>"},{"location":"reference/bridgic-core/bridgic/core/agentic/#bridgic.core.agentic.SequentialAutoma","title":"SequentialAutoma","text":"<p>               Bases: <code>GraphAutoma</code></p> <p>This class is to provide an easy way to orchestrate workers in a strictly  sequential manner.</p> <p>Each worker within the SequentialAutoma is invoked in the precise order determined  by their positional index, ensuring a linear workflow where the output of one worker  can serve as the input to the next.</p> <p>Upon the completion of all registered workers, the SequentialAutoma returns the output  produced by the final worker in the sequence as the overall result to the caller. This  design enforces ordered, step-wise processing, making the SequentialAutoma particularly  suitable for use cases that require strict procedural dependencies among constituent tasks.</p> Source code in <code>bridgic/core/agentic/_sequential_automa.py</code> <pre><code>class SequentialAutoma(GraphAutoma):\n    \"\"\"\n    This class is to provide an easy way to orchestrate workers in a strictly \n    sequential manner.\n\n    Each worker within the SequentialAutoma is invoked in the precise order determined \n    by their positional index, ensuring a linear workflow where the output of one worker \n    can serve as the input to the next.\n\n    Upon the completion of all registered workers, the SequentialAutoma returns the output \n    produced by the final worker in the sequence as the overall result to the caller. This \n    design enforces ordered, step-wise processing, making the SequentialAutoma particularly \n    suitable for use cases that require strict procedural dependencies among constituent tasks.\n    \"\"\"\n\n    # Automa type.\n    AUTOMA_TYPE: ClassVar[AutomaType] = AutomaType.Sequential\n\n    _TAIL_WORKER_KEY: Final[str] = \"__tail__\"\n    _last_worker_key: Optional[str]\n\n    def __init__(\n        self,\n        name: Optional[str] = None,\n        thread_pool: Optional[ThreadPoolExecutor] = None,\n    ):\n        super().__init__(name=name, thread_pool=thread_pool)\n\n        cls = type(self)\n        self._last_worker_key = None\n        if cls.AUTOMA_TYPE == AutomaType.Sequential:\n            # The _registered_worker_funcs data are from @worker decorators.\n            # Initialize the decorated sequential workers.\n            for worker_key, worker_func in self._registered_worker_funcs.items():\n                is_start = self._last_worker_key is None\n                dependencies = [] if self._last_worker_key is None else [self._last_worker_key]\n                super().add_func_as_worker(\n                    key=worker_key,\n                    func=worker_func,\n                    dependencies=dependencies,\n                    is_start=is_start,\n                    args_mapping_rule=worker_func.__args_mapping_rule__,\n                )\n                self._last_worker_key = worker_key\n\n        if self._last_worker_key is not None:\n            # Add a hidden worker as the tail worker.\n            super().add_func_as_worker(\n                key=self._TAIL_WORKER_KEY,\n                func=self._tail_worker,\n                dependencies=[self._last_worker_key],\n                is_output=True,\n                args_mapping_rule=ArgsMappingRule.AS_IS,\n            )\n\n    def _tail_worker(self, result: Any) -&gt; Any:\n        # Return the result of the last worker without any modification.\n        return result\n\n    @override\n    def dump_to_dict(self) -&gt; Dict[str, Any]:\n        state_dict = super().dump_to_dict()\n        state_dict[\"last_worker_key\"] = self._last_worker_key\n        return state_dict\n\n    @override\n    def load_from_dict(self, state_dict: Dict[str, Any]) -&gt; None:\n        super().load_from_dict(state_dict)\n        self._last_worker_key = state_dict[\"last_worker_key\"]\n\n    def __add_worker_internal(\n        self,\n        key: str,\n        func_or_worker: Union[Callable, Worker],\n        *,\n        args_mapping_rule: ArgsMappingRule = ArgsMappingRule.AS_IS,\n    ) -&gt; None:\n        is_start = self._last_worker_key is None\n        dependencies = [] if self._last_worker_key is None else [self._last_worker_key]\n        if isinstance(func_or_worker, Callable):\n            super().add_func_as_worker(\n                key=key, \n                func=func_or_worker,\n                dependencies=dependencies,\n                is_start=is_start,\n                args_mapping_rule=args_mapping_rule,\n            )\n        else:\n            super().add_worker(\n                key=key, \n                worker=func_or_worker,\n                dependencies=dependencies,\n                is_start=is_start,\n                args_mapping_rule=args_mapping_rule,\n            )\n        if self._last_worker_key is not None:\n            # Remove the old hidden tail worker.\n            super().remove_worker(self._TAIL_WORKER_KEY)\n\n        # Add a new hidden tail worker.\n        self._last_worker_key = key\n        super().add_func_as_worker(\n            key=self._TAIL_WORKER_KEY,\n            func=self._tail_worker,\n            dependencies=[self._last_worker_key],\n            is_output=True,\n            args_mapping_rule=ArgsMappingRule.AS_IS,\n        )\n\n    @override\n    def add_worker(\n        self,\n        key: str,\n        worker: Worker,\n        *,\n        args_mapping_rule: ArgsMappingRule = ArgsMappingRule.AS_IS,\n    ) -&gt; None:\n        \"\"\"\n        Add a sequential worker to the sequential automa at the end of the automa.\n\n        Parameters\n        ----------\n        key : str\n            The key of the worker.\n        worker : Worker\n            The worker instance to be registered.\n        args_mapping_rule : ArgsMappingRule\n            The rule of arguments mapping.\n        \"\"\"\n        if key == self._TAIL_WORKER_KEY:\n            raise AutomaRuntimeError(f\"the reserved key `{key}` is not allowed to be used by `add_worker()`\")\n\n        self.__add_worker_internal(\n            key, \n            worker, \n            args_mapping_rule=args_mapping_rule\n        )\n\n    @override\n    def add_func_as_worker(\n        self,\n        key: str,\n        func: Callable,\n        *,\n        args_mapping_rule: ArgsMappingRule = ArgsMappingRule.AS_IS,\n    ) -&gt; None:\n        \"\"\"\n        Add a function or method as a sequential worker to the sequential automa at the end of the automa.\n\n        Parameters\n        ----------\n        key : str\n            The key of the worker.\n        func : Callable\n            The function to be added as a sequential worker to the automa.\n        args_mapping_rule : ArgsMappingRule\n            The rule of arguments mapping.\n        \"\"\"\n        if key == self._TAIL_WORKER_KEY:\n            raise AutomaRuntimeError(f\"the reserved key `{key}` is not allowed to be used by `add_func_as_worker()`\")\n\n        self.__add_worker_internal(\n            key, \n            func, \n            args_mapping_rule=args_mapping_rule\n        )\n\n    @override\n    def worker(\n        self,\n        *,\n        key: Optional[str] = None,\n        args_mapping_rule: ArgsMappingRule = ArgsMappingRule.AS_IS,\n    ) -&gt; Callable:\n        \"\"\"\n        This is a decorator to mark a function or method as a sequential worker of the sequential automa, at the end of the automa.\n\n        Parameters\n        ----------\n        key : str\n            The key of the worker. If not provided, the name of the decorated callable will be used.\n        args_mapping_rule: ArgsMappingRule\n            The rule of arguments mapping.\n        \"\"\"\n        if key == self._TAIL_WORKER_KEY:\n            raise AutomaRuntimeError(f\"the reserved key `{key}` is not allowed to be used by `automa.worker()`\")\n\n        def wrapper(func: Callable):\n            self.__add_worker_internal(\n                key, \n                func, \n                args_mapping_rule=args_mapping_rule\n            )\n\n        return wrapper\n\n    @override\n    def remove_worker(self, key: str) -&gt; None:\n        raise AutomaRuntimeError(f\"remove_worker() is not allowed to be called on a sequential automa\")\n\n    @override\n    def add_dependency(\n        self,\n        key: str,\n        depends: str,\n    ) -&gt; None:\n        raise AutomaRuntimeError(f\"add_dependency() is not allowed to be called on a sequential automa\")\n\n    def ferry_to(self, worker_key: str, /, *args, **kwargs):\n        raise AutomaRuntimeError(f\"ferry_to() is not allowed to be called on a sequential automa\")\n</code></pre>"},{"location":"reference/bridgic-core/bridgic/core/agentic/#bridgic.core.agentic.SequentialAutoma.add_worker","title":"add_worker","text":"<pre><code>add_worker(\n    key: str,\n    worker: Worker,\n    *,\n    args_mapping_rule: ArgsMappingRule = AS_IS\n) -&gt; None\n</code></pre> <p>Add a sequential worker to the sequential automa at the end of the automa.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>The key of the worker.</p> required <code>worker</code> <code>Worker</code> <p>The worker instance to be registered.</p> required <code>args_mapping_rule</code> <code>ArgsMappingRule</code> <p>The rule of arguments mapping.</p> <code>AS_IS</code> Source code in <code>bridgic/core/agentic/_sequential_automa.py</code> <pre><code>@override\ndef add_worker(\n    self,\n    key: str,\n    worker: Worker,\n    *,\n    args_mapping_rule: ArgsMappingRule = ArgsMappingRule.AS_IS,\n) -&gt; None:\n    \"\"\"\n    Add a sequential worker to the sequential automa at the end of the automa.\n\n    Parameters\n    ----------\n    key : str\n        The key of the worker.\n    worker : Worker\n        The worker instance to be registered.\n    args_mapping_rule : ArgsMappingRule\n        The rule of arguments mapping.\n    \"\"\"\n    if key == self._TAIL_WORKER_KEY:\n        raise AutomaRuntimeError(f\"the reserved key `{key}` is not allowed to be used by `add_worker()`\")\n\n    self.__add_worker_internal(\n        key, \n        worker, \n        args_mapping_rule=args_mapping_rule\n    )\n</code></pre>"},{"location":"reference/bridgic-core/bridgic/core/agentic/#bridgic.core.agentic.SequentialAutoma.add_func_as_worker","title":"add_func_as_worker","text":"<pre><code>add_func_as_worker(\n    key: str,\n    func: Callable,\n    *,\n    args_mapping_rule: ArgsMappingRule = AS_IS\n) -&gt; None\n</code></pre> <p>Add a function or method as a sequential worker to the sequential automa at the end of the automa.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>The key of the worker.</p> required <code>func</code> <code>Callable</code> <p>The function to be added as a sequential worker to the automa.</p> required <code>args_mapping_rule</code> <code>ArgsMappingRule</code> <p>The rule of arguments mapping.</p> <code>AS_IS</code> Source code in <code>bridgic/core/agentic/_sequential_automa.py</code> <pre><code>@override\ndef add_func_as_worker(\n    self,\n    key: str,\n    func: Callable,\n    *,\n    args_mapping_rule: ArgsMappingRule = ArgsMappingRule.AS_IS,\n) -&gt; None:\n    \"\"\"\n    Add a function or method as a sequential worker to the sequential automa at the end of the automa.\n\n    Parameters\n    ----------\n    key : str\n        The key of the worker.\n    func : Callable\n        The function to be added as a sequential worker to the automa.\n    args_mapping_rule : ArgsMappingRule\n        The rule of arguments mapping.\n    \"\"\"\n    if key == self._TAIL_WORKER_KEY:\n        raise AutomaRuntimeError(f\"the reserved key `{key}` is not allowed to be used by `add_func_as_worker()`\")\n\n    self.__add_worker_internal(\n        key, \n        func, \n        args_mapping_rule=args_mapping_rule\n    )\n</code></pre>"},{"location":"reference/bridgic-core/bridgic/core/agentic/#bridgic.core.agentic.SequentialAutoma.worker","title":"worker","text":"<pre><code>worker(\n    *,\n    key: Optional[str] = None,\n    args_mapping_rule: ArgsMappingRule = AS_IS\n) -&gt; Callable\n</code></pre> <p>This is a decorator to mark a function or method as a sequential worker of the sequential automa, at the end of the automa.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>The key of the worker. If not provided, the name of the decorated callable will be used.</p> <code>None</code> <code>args_mapping_rule</code> <code>ArgsMappingRule</code> <p>The rule of arguments mapping.</p> <code>AS_IS</code> Source code in <code>bridgic/core/agentic/_sequential_automa.py</code> <pre><code>@override\ndef worker(\n    self,\n    *,\n    key: Optional[str] = None,\n    args_mapping_rule: ArgsMappingRule = ArgsMappingRule.AS_IS,\n) -&gt; Callable:\n    \"\"\"\n    This is a decorator to mark a function or method as a sequential worker of the sequential automa, at the end of the automa.\n\n    Parameters\n    ----------\n    key : str\n        The key of the worker. If not provided, the name of the decorated callable will be used.\n    args_mapping_rule: ArgsMappingRule\n        The rule of arguments mapping.\n    \"\"\"\n    if key == self._TAIL_WORKER_KEY:\n        raise AutomaRuntimeError(f\"the reserved key `{key}` is not allowed to be used by `automa.worker()`\")\n\n    def wrapper(func: Callable):\n        self.__add_worker_internal(\n            key, \n            func, \n            args_mapping_rule=args_mapping_rule\n        )\n\n    return wrapper\n</code></pre>"},{"location":"reference/bridgic-core/bridgic/core/agentic/#bridgic.core.agentic.ReActAutoma","title":"ReActAutoma","text":"<p>               Bases: <code>GraphAutoma</code></p> <p>A react automa is a subclass of graph automa that implements the ReAct prompting framework.</p> Source code in <code>bridgic/core/agentic/react/_react_automa.py</code> <pre><code>class ReActAutoma(GraphAutoma):\n    \"\"\"\n    A react automa is a subclass of graph automa that implements the [ReAct](https://arxiv.org/abs/2210.03629) prompting framework.\n    \"\"\"\n\n    _llm: ToolSelection\n    \"\"\" The LLM to be used by the react automa. \"\"\"\n    _tools: Optional[List[ToolSpec]]\n    \"\"\" The candidate tools to be used by the react automa. \"\"\"\n    _system_prompt: Optional[SystemMessage]\n    \"\"\" The system prompt to be used by the react automa. \"\"\"\n    _max_iterations: int\n    \"\"\" The maximum number of iterations for the react automa. \"\"\"\n    _prompt_template: str\n    \"\"\" The template file for the react automa. \"\"\"\n    _jinja_env: Environment\n    \"\"\" The Jinja environment to be used by the react automa. \"\"\"\n    _jinja_template: Template\n    \"\"\" The Jinja template to be used by the react automa. \"\"\"\n\n    def __init__(\n        self,\n        llm: ToolSelection,\n        system_prompt: Optional[Union[str, SystemMessage]] = None,\n        tools: Optional[List[Union[Callable, Automa, ToolSpec]]] = None,\n        name: Optional[str] = None,\n        thread_pool: Optional[ThreadPoolExecutor] = None,\n        max_iterations: int = DEFAULT_MAX_ITERATIONS,\n        prompt_template: str = DEFAULT_TEMPLATE_FILE,\n    ):\n        super().__init__(name=name, thread_pool=thread_pool)\n\n        self._llm = llm\n        if system_prompt:\n            # Validate SystemMessage...\n            if isinstance(system_prompt, str):\n                system_prompt = SystemMessage(role=\"system\", content=system_prompt)\n            elif (\"role\" not in system_prompt) or (system_prompt[\"role\"] != \"system\"):\n                raise ValueError(f\"Invalid `system_prompt` value received: {system_prompt}. It should contain `role`=`system`.\")\n\n        self._system_prompt = system_prompt\n        if tools:\n            self._tools = [self._ensure_tool_spec(tool) for tool in tools]\n        else:\n            self._tools = None\n        self._max_iterations = max_iterations\n        self._prompt_template = prompt_template\n        self._jinja_env = Environment(loader=PackageLoader(\"bridgic.core.agentic.react\"))\n        self._jinja_template = self._jinja_env.get_template(prompt_template)\n\n        self.add_worker(\n            key=\"tool_selector\",\n            worker=ToolSelectionWorker(tool_selection_llm=llm),\n            dependencies=[\"assemble_context\"],\n            args_mapping_rule=ArgsMappingRule.UNPACK,\n        )\n\n    @override\n    def dump_to_dict(self) -&gt; Dict[str, Any]:\n        state_dict = super().dump_to_dict()\n        # TODO: tools\n        state_dict[\"max_iterations\"] = self._max_iterations\n        state_dict[\"prompt_template\"] = self._prompt_template\n        return state_dict\n\n    @override\n    def load_from_dict(self, state_dict: Dict[str, Any]) -&gt; None:\n        super().load_from_dict(state_dict)\n        self._max_iterations = state_dict[\"max_iterations\"]\n        self._prompt_template = state_dict[\"prompt_template\"]\n\n    @property\n    def max_iterations(self) -&gt; int:\n        return self._max_iterations\n\n    @max_iterations.setter\n    def max_iterations(self, max_iterations: int) -&gt; None:\n        self._max_iterations = max_iterations\n\n    @property\n    def prompt_template(self) -&gt; str:\n        return self._prompt_template\n\n    @prompt_template.setter\n    def prompt_template(self, prompt_template: str) -&gt; None:\n        self._prompt_template = prompt_template\n\n    @override\n    async def arun(\n        self,\n        user_msg: Optional[Union[str, UserTextMessage]] = None,\n        *,\n        chat_history: Optional[List[Union[UserTextMessage, AssistantTextMessage, ToolMessage]]] = None,\n        messages: Optional[List[ChatMessage]] = None,\n        tools: Optional[List[Union[Callable, Automa, ToolSpec]]] = None,\n        interaction_feedback: Optional[InteractionFeedback] = None,\n        interaction_feedbacks: Optional[List[InteractionFeedback]] = None,\n    ) -&gt; Any:\n        return await super().arun(\n            user_msg=user_msg,\n            chat_history=chat_history,\n            messages=messages,\n            tools=tools,\n            interaction_feedback=interaction_feedback,\n            interaction_feedbacks=interaction_feedbacks,\n        )\n\n    @worker(is_start=True)\n    async def validate_and_transform(\n        self,\n        user_msg: Optional[Union[str, UserTextMessage]] = None,\n        *,\n        chat_history: Optional[List[Union[UserTextMessage, AssistantTextMessage, ToolMessage]]] = None,\n        messages: Optional[List[ChatMessage]] = None,\n        tools: Optional[List[Union[Callable, Automa, ToolSpec]]] = None,\n    ) -&gt; Dict[str, Any]:\n        \"\"\"\n        Validate and transform the input messages and tools to the canonical format.\n        \"\"\"\n\n        # Part One: validate and transform the input messages.\n        # Unify input messages of various types to the `ChatMessage` format.\n        chat_messages: List[ChatMessage] = []\n        if messages:\n            # If `messages` is provided, use it directly.\n            chat_messages = messages\n        elif user_msg:\n            # Since `messages` is not provided, join the system prompt + `chat_history` + `user_msg`\n            # First, append the `system_prompt`\n            if self._system_prompt:\n                chat_messages.append(self._system_prompt)\n\n            # Second, append the `chat_history`\n            if chat_history:\n                for history_msg in chat_history:\n                    # Validate the history messages...\n                    role = history_msg[\"role\"]\n                    if role == \"user\" or role == \"assistant\" or role == \"tool\":\n                        chat_messages.append(history_msg)\n                    else:\n                        raise ValueError(f\"Invalid role: `{role}` received in history message: `{history_msg}`, expected `user`, `assistant`, or `tool`.\")\n\n            # Third, append the `user_msg`\n            if isinstance(user_msg, str):\n                chat_messages.append(UserTextMessage(role=\"user\", content=user_msg))\n            elif isinstance(user_msg, dict):\n                if \"role\" in user_msg and user_msg[\"role\"] == \"user\":\n                    chat_messages.append(user_msg)\n                else:\n                    raise ValueError(f\"`role` must be `user` in user message: `{user_msg}`.\")\n        else:\n            raise ValueError(f\"Either `messages` or `user_msg` must be provided.\")\n\n        # Part Two: validate and transform the intput tools.\n        # Unify input tools of various types to the `ToolSpec` format.\n        if self._tools:\n            tool_spec_list = self._tools\n        elif tools:\n            tool_spec_list = [self._ensure_tool_spec(tool) for tool in tools]\n        else:\n            # TODO: whether to support empty tool list?\n            tool_spec_list = []\n\n        return {\n            \"initial_messages\": chat_messages,\n            \"candidate_tools\": tool_spec_list,\n        }\n\n    @worker(dependencies=[\"validate_and_transform\"], args_mapping_rule=ArgsMappingRule.UNPACK)\n    async def assemble_context(\n        self,\n        *,\n        initial_messages: Optional[List[ChatMessage]] = None,\n        candidate_tools: Optional[List[ToolSpec]] = None,\n        tool_selection_outputs: Tuple[List[ToolCall], Optional[str]] = From(\"tool_selector\", default=None),\n        tool_result_messages: Optional[List[ToolMessage]] = None,\n        rtx = System(\"runtime_context\"),\n    ) -&gt; Dict[str, Any]:\n        # print(f\"\\n******* ReActAutoma.assemble_context *******\\n\")\n        # print(f\"initial_messages: {initial_messages}\")\n        # print(f\"candidate_tools: {candidate_tools}\")\n        # print(f\"tool_selection_outputs: {tool_selection_outputs}\")\n        # print(f\"tool_result_messages: {tool_result_messages}\")\n\n        local_space = self.get_local_space(rtx)\n        # Build messages memory with help of local space.\n        messages_memory: List[ChatMessage] = []\n        if initial_messages:\n            # If `messages` is provided, use it to re-initialize the messages memory.\n            messages_memory = initial_messages.copy()\n        else:\n            messages_memory = local_space.get(\"messages_memory\", [])\n        if tool_selection_outputs:\n            # Transform tools_calls format:\n            tool_calls = tool_selection_outputs[0]\n            tool_calls_list = [\n                FunctionToolCall(\n                    id=tool_call.id,\n                    type=\"function\",\n                    function=Function(\n                        name=tool_call.name,\n                        arguments=tool_call.arguments,\n                    ),\n                ) for tool_call in tool_calls\n            ]\n            llm_response = tool_selection_outputs[1]\n            assistant_message = AssistantTextMessage(\n                role=\"assistant\",\n                # TOD: name?\n                content=llm_response,\n                tool_calls=tool_calls_list,\n            )\n            messages_memory.append(assistant_message)\n        if tool_result_messages:\n            messages_memory.extend(tool_result_messages)\n        local_space[\"messages_memory\"] = messages_memory\n        # print(\"--------------------------------\")\n        # print(f\"messages_memory: {messages_memory}\")\n\n        # Save &amp; retrieve tools with help of local space.\n        if candidate_tools:\n            local_space[\"tools\"] = candidate_tools\n        else:\n            candidate_tools = local_space.get(\"tools\", [])\n\n        # Note: here 'messages' and `tools` are injected into the template as variables.\n        raw_prompt = self._jinja_template.render(messages=messages_memory, tools=candidate_tools)\n        # print(f\"\\n ##### raw_prompt ##### \\n{raw_prompt}\")\n\n        # Note: the jinjia template must conform to the TypedDict `ChatMessage` format (in json).\n        llm_messages = cast(List[ChatMessage], json.loads(raw_prompt))\n        llm_tools: List[Tool] = [tool.to_tool() for tool in candidate_tools]\n\n        return {\n            \"messages\": llm_messages,\n            \"tools\": llm_tools,\n        }\n\n    @worker(dependencies=[\"tool_selector\"], args_mapping_rule=ArgsMappingRule.UNPACK)\n    async def plan_next_step(\n        self,\n        tool_calls: List[ToolCall],\n        llm_response: Optional[str] = None,\n        messages_and_tools: dict = From(\"validate_and_transform\"),\n        rtx = System(\"runtime_context\"),\n    ) -&gt; None:\n        local_space = self.get_local_space(rtx)\n        iterations_count = local_space.get(\"iterations_count\", 0)\n        iterations_count += 1\n        local_space[\"iterations_count\"] = iterations_count\n        if iterations_count &gt; self._max_iterations:\n            # TODO: how to report this to users?\n            self.ferry_to(\n                \"finally_summarize\", \n                final_answer=f\"Sorry, I am unable to answer your question after {self._max_iterations} iterations. Please try again later.\"\n            )\n            return\n\n        # TODO: maybe hand over the control flow to users?\n        # print(f\"\\n******* ReActAutoma.plan_next_step *******\\n\")\n        # print(f\"tool_calls: {tool_calls}\")\n        # print(f\"llm_response: {llm_response}\")\n        if tool_calls:\n            tool_spec_list = messages_and_tools[\"candidate_tools\"]\n            matched_list = self._match_tool_calls_and_tool_specs(tool_calls, tool_spec_list)\n            if matched_list:\n                matched_tool_calls = []\n                tool_worker_keys = []\n                for tool_call, tool_spec in matched_list:\n                    matched_tool_calls.append(tool_call)\n                    tool_worker = tool_spec.create_worker()\n                    worker_key = f\"tool_{tool_call.name}_{tool_call.id}\"\n                    self.add_worker(\n                        key=worker_key,\n                        worker=tool_worker,\n                    )\n                    # TODO: convert tool_call.arguments to the tool parameters types\n                    # TODO: validate the arguments against the tool parameters / json schema\n                    self.ferry_to(worker_key, **tool_call.arguments)\n                    tool_worker_keys.append(worker_key)\n                self.add_func_as_worker(\n                    key=\"merge_tools_results\",\n                    func=self.merge_tools_results,\n                    dependencies=tool_worker_keys,\n                    args_mapping_rule=ArgsMappingRule.MERGE,\n                )\n                return matched_tool_calls\n            else:\n                # TODO\n                ...\n        else:\n            # Got final answer from the LLM.\n            self.ferry_to(\"finally_summarize\", final_answer=llm_response)\n\n    async def merge_tools_results(\n        self, \n        tool_results: List[Any],\n        tool_calls: List[ToolCall] = From(\"plan_next_step\"),\n    ) -&gt; List[ToolMessage]:\n        \"\"\"\n        Merge the results of the tools.\n        \"\"\"\n        # print(f\"\\n******* ReActAutoma.merge_tools_results *******\\n\")\n        # print(f\"tool_results: {tool_results}\")\n        # print(f\"tool_calls: {tool_calls}\")\n        assert len(tool_results) == len(tool_calls)\n        tool_messages = []\n        for tool_result, tool_call in zip(tool_results, tool_calls):\n            tool_messages.append(ToolMessage(\n                role=\"tool\", \n                # Note: Convert the tool result to string, since a tool can return any type of data.\n                # TODO: maybe we can use a better way to serialize the tool result?\n                content=str(tool_result), \n                tool_call_id=tool_call.id\n            ))\n            # Remove the tool workers\n            self.remove_worker(f\"tool_{tool_call.name}_{tool_call.id}\")\n        # Remove self...\n        self.remove_worker(\"merge_tools_results\")\n        self.ferry_to(\"assemble_context\", tool_result_messages=tool_messages)\n        return tool_messages\n\n    @worker(is_output=True)\n    async def finally_summarize(self, final_answer: str) -&gt; str:\n        return final_answer\n\n    def _ensure_tool_spec(self, tool: Union[Callable, Automa, ToolSpec]) -&gt; ToolSpec:\n        if isinstance(tool, ToolSpec):\n            return tool\n        elif isinstance(tool, type) and issubclass(tool, Automa):\n            return AutomaToolSpec.from_raw(tool)\n        elif isinstance(tool, Callable):\n            # Note: this test against `Callable` should be placed at last.\n            return FunctionToolSpec.from_raw(tool)\n        else:\n            raise TypeError(f\"Invalid tool type: {type(tool)} detected, expected `Callable`, `Automa`, or `ToolSpec`.\")\n\n    def _match_tool_calls_and_tool_specs(\n        self,\n        tool_calls: List[ToolCall],\n        tool_spec_list: List[ToolSpec],\n    ) -&gt; List[Tuple[ToolCall, ToolSpec]]:\n        \"\"\"\n        This function is used to match the tool calls and the tool specs based on the tool name.\n\n        Parameters\n        ----------\n        tool_calls : List[ToolCall]\n            The tool calls to match.\n        tool_spec_list : List[ToolSpec]\n            The tool specs to match.\n\n        Returns\n        -------\n        List[(ToolCall, ToolSpec)]\n            The matched tool calls and tool specs.\n        \"\"\"\n        matched_list: List[Tuple[ToolCall, ToolSpec]] = []\n        for tool_call in tool_calls:\n            for tool_spec in tool_spec_list:\n                if tool_call.name == tool_spec.tool_name:\n                    matched_list.append((tool_call, tool_spec))\n        return matched_list\n</code></pre>"},{"location":"reference/bridgic-core/bridgic/core/agentic/#bridgic.core.agentic.ReActAutoma.validate_and_transform","title":"validate_and_transform","text":"<code>async</code> <pre><code>validate_and_transform(\n    user_msg: Optional[Union[str, UserTextMessage]] = None,\n    *,\n    chat_history: Optional[\n        List[\n            Union[\n                UserTextMessage,\n                AssistantTextMessage,\n                ToolMessage,\n            ]\n        ]\n    ] = None,\n    messages: Optional[List[ChatMessage]] = None,\n    tools: Optional[\n        List[Union[Callable, Automa, ToolSpec]]\n    ] = None\n) -&gt; Dict[str, Any]\n</code></pre> <p>Validate and transform the input messages and tools to the canonical format.</p> Source code in <code>bridgic/core/agentic/react/_react_automa.py</code> <pre><code>@worker(is_start=True)\nasync def validate_and_transform(\n    self,\n    user_msg: Optional[Union[str, UserTextMessage]] = None,\n    *,\n    chat_history: Optional[List[Union[UserTextMessage, AssistantTextMessage, ToolMessage]]] = None,\n    messages: Optional[List[ChatMessage]] = None,\n    tools: Optional[List[Union[Callable, Automa, ToolSpec]]] = None,\n) -&gt; Dict[str, Any]:\n    \"\"\"\n    Validate and transform the input messages and tools to the canonical format.\n    \"\"\"\n\n    # Part One: validate and transform the input messages.\n    # Unify input messages of various types to the `ChatMessage` format.\n    chat_messages: List[ChatMessage] = []\n    if messages:\n        # If `messages` is provided, use it directly.\n        chat_messages = messages\n    elif user_msg:\n        # Since `messages` is not provided, join the system prompt + `chat_history` + `user_msg`\n        # First, append the `system_prompt`\n        if self._system_prompt:\n            chat_messages.append(self._system_prompt)\n\n        # Second, append the `chat_history`\n        if chat_history:\n            for history_msg in chat_history:\n                # Validate the history messages...\n                role = history_msg[\"role\"]\n                if role == \"user\" or role == \"assistant\" or role == \"tool\":\n                    chat_messages.append(history_msg)\n                else:\n                    raise ValueError(f\"Invalid role: `{role}` received in history message: `{history_msg}`, expected `user`, `assistant`, or `tool`.\")\n\n        # Third, append the `user_msg`\n        if isinstance(user_msg, str):\n            chat_messages.append(UserTextMessage(role=\"user\", content=user_msg))\n        elif isinstance(user_msg, dict):\n            if \"role\" in user_msg and user_msg[\"role\"] == \"user\":\n                chat_messages.append(user_msg)\n            else:\n                raise ValueError(f\"`role` must be `user` in user message: `{user_msg}`.\")\n    else:\n        raise ValueError(f\"Either `messages` or `user_msg` must be provided.\")\n\n    # Part Two: validate and transform the intput tools.\n    # Unify input tools of various types to the `ToolSpec` format.\n    if self._tools:\n        tool_spec_list = self._tools\n    elif tools:\n        tool_spec_list = [self._ensure_tool_spec(tool) for tool in tools]\n    else:\n        # TODO: whether to support empty tool list?\n        tool_spec_list = []\n\n    return {\n        \"initial_messages\": chat_messages,\n        \"candidate_tools\": tool_spec_list,\n    }\n</code></pre>"},{"location":"reference/bridgic-core/bridgic/core/agentic/#bridgic.core.agentic.ReActAutoma.merge_tools_results","title":"merge_tools_results","text":"<code>async</code> <pre><code>merge_tools_results(\n    tool_results: List[Any],\n    tool_calls: List[ToolCall] = From(\"plan_next_step\"),\n) -&gt; List[ToolMessage]\n</code></pre> <p>Merge the results of the tools.</p> Source code in <code>bridgic/core/agentic/react/_react_automa.py</code> <pre><code>async def merge_tools_results(\n    self, \n    tool_results: List[Any],\n    tool_calls: List[ToolCall] = From(\"plan_next_step\"),\n) -&gt; List[ToolMessage]:\n    \"\"\"\n    Merge the results of the tools.\n    \"\"\"\n    # print(f\"\\n******* ReActAutoma.merge_tools_results *******\\n\")\n    # print(f\"tool_results: {tool_results}\")\n    # print(f\"tool_calls: {tool_calls}\")\n    assert len(tool_results) == len(tool_calls)\n    tool_messages = []\n    for tool_result, tool_call in zip(tool_results, tool_calls):\n        tool_messages.append(ToolMessage(\n            role=\"tool\", \n            # Note: Convert the tool result to string, since a tool can return any type of data.\n            # TODO: maybe we can use a better way to serialize the tool result?\n            content=str(tool_result), \n            tool_call_id=tool_call.id\n        ))\n        # Remove the tool workers\n        self.remove_worker(f\"tool_{tool_call.name}_{tool_call.id}\")\n    # Remove self...\n    self.remove_worker(\"merge_tools_results\")\n    self.ferry_to(\"assemble_context\", tool_result_messages=tool_messages)\n    return tool_messages\n</code></pre>"},{"location":"reference/bridgic-core/bridgic/core/agentic/tool_specs/","title":"tool_specs","text":"<p>The Tool Specs module provides definitions and implementations of tool specifications.</p> <p>This module contains various tool specification classes that support transforming  \"tool ingredients\" such as Python functions and Automa workflows into LLM-callable tools,  enabling callable objects to be seamlessly used in agentic systems.</p>"},{"location":"reference/bridgic-core/bridgic/core/agentic/tool_specs/#bridgic.core.agentic.tool_specs.ToolSpec","title":"ToolSpec","text":"<p>               Bases: <code>Serializable</code></p> <p>ToolSpec is an abstract class that represents a tool specification that describes all necessary information about a tool used by the LLM. </p> <p>ToolSpec and its subclasses are responsible for providing four categories of interfaces: 1. Transformations to LLM Tool: <code>to_tool</code>. 2. Worker Creation: <code>create_worker</code>. 3. Serialization and Deserialization. 4. ToolSpec initialization from raw resources: <code>from_raw</code>.</p> Source code in <code>bridgic/core/agentic/tool_specs/_base_tool_spec.py</code> <pre><code>class ToolSpec(Serializable):\n    \"\"\"\n    ToolSpec is an abstract class that represents a tool specification that describes all necessary information about a tool used by the LLM. \n\n    ToolSpec and its subclasses are responsible for providing four categories of interfaces:\n    1. Transformations to LLM Tool: `to_tool`.\n    2. Worker Creation: `create_worker`.\n    3. Serialization and Deserialization.\n    4. ToolSpec initialization from raw resources: `from_raw`.\n    \"\"\"\n    _tool_id: Optional[Union[str, int]]\n    \"\"\"The unique ID of the tool, used to uniquely identify a tool across the entire system. This tool can be of various types.\"\"\"\n    _tool_name: Optional[str]\n    \"\"\"The name of the tool to be called\"\"\"\n    _tool_description: Optional[str]\n    \"\"\"A description of what the tool does, used by the model to choose when and how to call the tool.\"\"\"\n    _tool_parameters: Optional[Dict[str, Any]]\n    \"\"\"The JSON schema of the tool's parameters\"\"\"\n\n    def __init__(\n        self,\n        tool_name: Optional[str] = None,\n        tool_description: Optional[str] = None,\n        tool_parameters: Optional[Dict[str, Any]] = None,\n    ):\n        self._tool_id = None\n        self._tool_name = tool_name\n        self._tool_description = tool_description\n        self._tool_parameters = tool_parameters\n\n    @property\n    def tool_name(self) -&gt; Optional[str]:\n        return self._tool_name\n\n    @property\n    def tool_description(self) -&gt; Optional[str]:\n        return self._tool_description\n\n    @property\n    def tool_parameters(self) -&gt; Optional[Dict[str, Any]]:\n        return self._tool_parameters\n\n    def __str__(self) -&gt; str:\n        return f\"{self.__class__.__name__}(tool_name={self._tool_name}, tool_description={self._tool_description}, tool_parameters={self._tool_parameters})\"\n\n    def __repr__(self) -&gt; str:\n        return f\"&lt;{self.__class__.__name__}(tool_name={self._tool_name}, tool_description={self._tool_description}, tool_parameters={self._tool_parameters})&gt;\"\n\n    ###############################################################\n    ######## Part One of interfaces: Transformations to Tool ######\n    ###############################################################\n\n    @abstractmethod\n    def to_tool(self) -&gt; Tool:\n        \"\"\"\n        Transform this ToolSpec to a `Tool` object used by LLM.\n\n        Returns\n        -------\n        Tool\n            A `Tool` object that can be used by LLM.\n        \"\"\"\n        ...\n\n    ###############################################################\n    ######## Part Two of interfaces: Worker Creation ##############\n    ###############################################################\n\n    @abstractmethod\n    def create_worker(self) -&gt; Worker:\n        \"\"\"\n        Create a Worker from the information included in this ToolSpec.\n\n        Returns\n        -------\n        Worker\n            A new `Worker` object that can be added to an Automa to execute the tool.\n        \"\"\"\n        ...\n\n    ###############################################################\n    ######## Part Three of interfaces: \n    ######## Serialization and Deserialization ####################\n    ###############################################################\n\n    @override\n    def dump_to_dict(self) -&gt; Dict[str, Any]:\n        state_dict = {}\n        if self._tool_id:\n            state_dict[\"tool_id\"] = self._tool_id\n        if self._tool_name:\n            state_dict[\"tool_name\"] = self._tool_name\n        if self._tool_description:\n            state_dict[\"tool_description\"] = self._tool_description\n        if self._tool_parameters:\n            state_dict[\"tool_parameters\"] = self._tool_parameters\n        return state_dict\n\n    @override\n    def load_from_dict(self, state_dict: Dict[str, Any]) -&gt; None:\n        self._tool_id = state_dict.get(\"tool_id\")\n        self._tool_name = state_dict.get(\"tool_name\")\n        self._tool_description = state_dict.get(\"tool_description\")\n        self._tool_parameters = state_dict.get(\"tool_parameters\")\n</code></pre>"},{"location":"reference/bridgic-core/bridgic/core/agentic/tool_specs/#bridgic.core.agentic.tool_specs.ToolSpec.to_tool","title":"to_tool","text":"<code>abstractmethod</code> <pre><code>to_tool() -&gt; Tool\n</code></pre> <p>Transform this ToolSpec to a <code>Tool</code> object used by LLM.</p> <p>Returns:</p> Type Description <code>Tool</code> <p>A <code>Tool</code> object that can be used by LLM.</p> Source code in <code>bridgic/core/agentic/tool_specs/_base_tool_spec.py</code> <pre><code>@abstractmethod\ndef to_tool(self) -&gt; Tool:\n    \"\"\"\n    Transform this ToolSpec to a `Tool` object used by LLM.\n\n    Returns\n    -------\n    Tool\n        A `Tool` object that can be used by LLM.\n    \"\"\"\n    ...\n</code></pre>"},{"location":"reference/bridgic-core/bridgic/core/agentic/tool_specs/#bridgic.core.agentic.tool_specs.ToolSpec.create_worker","title":"create_worker","text":"<code>abstractmethod</code> <pre><code>create_worker() -&gt; Worker\n</code></pre> <p>Create a Worker from the information included in this ToolSpec.</p> <p>Returns:</p> Type Description <code>Worker</code> <p>A new <code>Worker</code> object that can be added to an Automa to execute the tool.</p> Source code in <code>bridgic/core/agentic/tool_specs/_base_tool_spec.py</code> <pre><code>@abstractmethod\ndef create_worker(self) -&gt; Worker:\n    \"\"\"\n    Create a Worker from the information included in this ToolSpec.\n\n    Returns\n    -------\n    Worker\n        A new `Worker` object that can be added to an Automa to execute the tool.\n    \"\"\"\n    ...\n</code></pre>"},{"location":"reference/bridgic-core/bridgic/core/agentic/tool_specs/#bridgic.core.agentic.tool_specs.FunctionToolSpec","title":"FunctionToolSpec","text":"<p>               Bases: <code>ToolSpec</code></p> Source code in <code>bridgic/core/agentic/tool_specs/_function_tool_spec.py</code> <pre><code>class FunctionToolSpec(ToolSpec):\n    _func: Callable\n    \"\"\"The python function to be used as a tool\"\"\"\n\n    def __init__(\n        self,\n        func: Callable,\n        tool_name: Optional[str] = None,\n        tool_description: Optional[str] = None,\n        tool_parameters: Optional[Dict[str, Any]] = None,\n    ):\n        super().__init__(\n            tool_name=tool_name,\n            tool_description=tool_description,\n            tool_parameters=tool_parameters\n        )\n        self._func = func\n\n    @classmethod\n    def from_raw(\n        cls,\n        func: Callable,\n        tool_name: Optional[str] = None,\n        tool_description: Optional[str] = None,\n        tool_parameters: Optional[Dict[str, Any]] = None,\n    ) -&gt; \"FunctionToolSpec\":\n        \"\"\"\n        Create a FunctionToolSpec from a python function. By default, the tool name, description and parameters' json schema will be extracted from the function's docstring and the parameters' type and description. However, these values can be customized by passing in the corresponding arguments.\n\n        Parameters\n        ----------\n        func : Callable\n            The python function to create a FunctionToolSpec from.\n        tool_name : Optional[str]\n            The name of the tool. If not provided, the function name will be used.\n        tool_description : Optional[str]\n            The description of the tool. If not provided, the function docstring will be used.\n        tool_parameters : Optional[Dict[str, Any]]\n            The JSON schema of the tool's parameters. If not provided, the JSON schema will be constructed properly from the parameters' annotations, the function's signature and/or docstring.\n\n        Returns\n        -------\n        FunctionToolSpec\n            A new `FunctionToolSpec` object.\n        \"\"\"\n        if isinstance(func, MethodType):\n            raise ValueError(f\"`func` is not allowed to be a bound method: {func}.\")\n\n        if not tool_name:\n            tool_name = func.__name__\n\n        if not tool_description:\n            tool_description = get_tool_description_from(func, tool_name)\n\n        if not tool_parameters:\n            tool_parameters = create_func_params_json_schema(func)\n            # TODO: whether to remove the `title` field of the params_schema?\n\n        return cls(\n            func=func,\n            tool_name=tool_name,\n            tool_description=tool_description,\n            tool_parameters=tool_parameters\n        )\n\n    @override\n    def to_tool(self) -&gt; Tool:\n        \"\"\"\n        Transform this FunctionToolSpec to a `Tool` object used by LLM.\n\n        Returns\n        -------\n        Tool\n            A `Tool` object that can be used by LLM.\n        \"\"\"\n        return Tool(\n            name=self._tool_name,\n            description=self._tool_description,\n            parameters=self._tool_parameters\n        )\n\n    @override\n    def create_worker(self) -&gt; Worker:\n        \"\"\"\n        Create a Worker from the information included in this FunctionToolSpec.\n\n        Returns\n        -------\n        Worker\n            A new `Worker` object that can be added to an Automa to execute the tool.\n        \"\"\"\n        # TODO: some initialization arguments may be needed in future, e.g., `bound_needed`.\n        return CallableWorker(self._func)\n\n    @override\n    def dump_to_dict(self) -&gt; Dict[str, Any]:\n        state_dict = super().dump_to_dict()\n        state_dict[\"func\"] = self._func.__module__ + \".\" + self._func.__qualname__\n        return state_dict\n\n    @override\n    def load_from_dict(self, state_dict: Dict[str, Any]) -&gt; None:\n        super().load_from_dict(state_dict)\n        self._func = load_qualified_class_or_func(state_dict[\"func\"])\n</code></pre>"},{"location":"reference/bridgic-core/bridgic/core/agentic/tool_specs/#bridgic.core.agentic.tool_specs.FunctionToolSpec.from_raw","title":"from_raw","text":"<code>classmethod</code> <pre><code>from_raw(\n    func: Callable,\n    tool_name: Optional[str] = None,\n    tool_description: Optional[str] = None,\n    tool_parameters: Optional[Dict[str, Any]] = None,\n) -&gt; FunctionToolSpec\n</code></pre> <p>Create a FunctionToolSpec from a python function. By default, the tool name, description and parameters' json schema will be extracted from the function's docstring and the parameters' type and description. However, these values can be customized by passing in the corresponding arguments.</p> <p>Parameters:</p> Name Type Description Default <code>func</code> <code>Callable</code> <p>The python function to create a FunctionToolSpec from.</p> required <code>tool_name</code> <code>Optional[str]</code> <p>The name of the tool. If not provided, the function name will be used.</p> <code>None</code> <code>tool_description</code> <code>Optional[str]</code> <p>The description of the tool. If not provided, the function docstring will be used.</p> <code>None</code> <code>tool_parameters</code> <code>Optional[Dict[str, Any]]</code> <p>The JSON schema of the tool's parameters. If not provided, the JSON schema will be constructed properly from the parameters' annotations, the function's signature and/or docstring.</p> <code>None</code> <p>Returns:</p> Type Description <code>FunctionToolSpec</code> <p>A new <code>FunctionToolSpec</code> object.</p> Source code in <code>bridgic/core/agentic/tool_specs/_function_tool_spec.py</code> <pre><code>@classmethod\ndef from_raw(\n    cls,\n    func: Callable,\n    tool_name: Optional[str] = None,\n    tool_description: Optional[str] = None,\n    tool_parameters: Optional[Dict[str, Any]] = None,\n) -&gt; \"FunctionToolSpec\":\n    \"\"\"\n    Create a FunctionToolSpec from a python function. By default, the tool name, description and parameters' json schema will be extracted from the function's docstring and the parameters' type and description. However, these values can be customized by passing in the corresponding arguments.\n\n    Parameters\n    ----------\n    func : Callable\n        The python function to create a FunctionToolSpec from.\n    tool_name : Optional[str]\n        The name of the tool. If not provided, the function name will be used.\n    tool_description : Optional[str]\n        The description of the tool. If not provided, the function docstring will be used.\n    tool_parameters : Optional[Dict[str, Any]]\n        The JSON schema of the tool's parameters. If not provided, the JSON schema will be constructed properly from the parameters' annotations, the function's signature and/or docstring.\n\n    Returns\n    -------\n    FunctionToolSpec\n        A new `FunctionToolSpec` object.\n    \"\"\"\n    if isinstance(func, MethodType):\n        raise ValueError(f\"`func` is not allowed to be a bound method: {func}.\")\n\n    if not tool_name:\n        tool_name = func.__name__\n\n    if not tool_description:\n        tool_description = get_tool_description_from(func, tool_name)\n\n    if not tool_parameters:\n        tool_parameters = create_func_params_json_schema(func)\n        # TODO: whether to remove the `title` field of the params_schema?\n\n    return cls(\n        func=func,\n        tool_name=tool_name,\n        tool_description=tool_description,\n        tool_parameters=tool_parameters\n    )\n</code></pre>"},{"location":"reference/bridgic-core/bridgic/core/agentic/tool_specs/#bridgic.core.agentic.tool_specs.FunctionToolSpec.to_tool","title":"to_tool","text":"<pre><code>to_tool() -&gt; Tool\n</code></pre> <p>Transform this FunctionToolSpec to a <code>Tool</code> object used by LLM.</p> <p>Returns:</p> Type Description <code>Tool</code> <p>A <code>Tool</code> object that can be used by LLM.</p> Source code in <code>bridgic/core/agentic/tool_specs/_function_tool_spec.py</code> <pre><code>@override\ndef to_tool(self) -&gt; Tool:\n    \"\"\"\n    Transform this FunctionToolSpec to a `Tool` object used by LLM.\n\n    Returns\n    -------\n    Tool\n        A `Tool` object that can be used by LLM.\n    \"\"\"\n    return Tool(\n        name=self._tool_name,\n        description=self._tool_description,\n        parameters=self._tool_parameters\n    )\n</code></pre>"},{"location":"reference/bridgic-core/bridgic/core/agentic/tool_specs/#bridgic.core.agentic.tool_specs.FunctionToolSpec.create_worker","title":"create_worker","text":"<pre><code>create_worker() -&gt; Worker\n</code></pre> <p>Create a Worker from the information included in this FunctionToolSpec.</p> <p>Returns:</p> Type Description <code>Worker</code> <p>A new <code>Worker</code> object that can be added to an Automa to execute the tool.</p> Source code in <code>bridgic/core/agentic/tool_specs/_function_tool_spec.py</code> <pre><code>@override\ndef create_worker(self) -&gt; Worker:\n    \"\"\"\n    Create a Worker from the information included in this FunctionToolSpec.\n\n    Returns\n    -------\n    Worker\n        A new `Worker` object that can be added to an Automa to execute the tool.\n    \"\"\"\n    # TODO: some initialization arguments may be needed in future, e.g., `bound_needed`.\n    return CallableWorker(self._func)\n</code></pre>"},{"location":"reference/bridgic-core/bridgic/core/agentic/tool_specs/#bridgic.core.agentic.tool_specs.AutomaToolSpec","title":"AutomaToolSpec","text":"<p>               Bases: <code>ToolSpec</code></p> Source code in <code>bridgic/core/agentic/tool_specs/_automa_tool_spec.py</code> <pre><code>class AutomaToolSpec(ToolSpec):\n    _automa_cls: Type[Automa]\n    \"\"\"The Automa class to be used as a tool\"\"\"\n    _automa_init_kwargs: Dict[str, Any]\n    \"\"\"The initialization arguments for the Automa\"\"\"\n\n    def __init__(\n        self,\n        automa_cls: Type[Automa],\n        tool_name: Optional[str] = None,\n        tool_description: Optional[str] = None,\n        tool_parameters: Optional[Dict[str, Any]] = None,\n        **automa_init_kwargs: Dict[str, Any],\n    ):\n        super().__init__(\n            tool_name=tool_name,\n            tool_description=tool_description,\n            tool_parameters=tool_parameters\n        )\n        self._automa_cls = automa_cls\n        self._automa_init_kwargs = automa_init_kwargs\n\n    @classmethod\n    def from_raw(\n        cls,\n        automa_cls: Type[Automa],\n        tool_name: Optional[str] = None,\n        tool_description: Optional[str] = None,\n        tool_parameters: Optional[Dict[str, Any]] = None,\n        **automa_init_kwargs: Dict[str, Any],\n    ) -&gt; \"AutomaToolSpec\":\n        \"\"\"\n        Create an AutomaToolSpec from an Automa class.\n        \"\"\"\n\n        def check_spec_func(automa_cls):\n            if hasattr(automa_cls, \"spec_func\") and isinstance(automa_cls.spec_func, Callable):\n                return\n            raise ValueError(f\"The Automa class {automa_cls} must be decorated with `@as_tool` in order to be used as a tool.\")\n\n        if (not tool_name) or (not tool_description) or (not tool_parameters):\n            check_spec_func(automa_cls)\n\n        if not tool_name:\n            tool_name = automa_cls.spec_func.__name__\n\n        if not tool_description:\n            tool_description = get_tool_description_from(automa_cls.spec_func, tool_name)\n\n        if not tool_parameters:\n            tool_parameters = create_func_params_json_schema(automa_cls.spec_func)\n            # TODO: whether to remove the `title` field of the params_schema?\n\n        return cls(\n            automa_cls=automa_cls,\n            tool_name=tool_name,\n            tool_description=tool_description,\n            tool_parameters=tool_parameters,\n            **automa_init_kwargs\n        )\n\n    @override\n    def to_tool(self) -&gt; Tool:\n        \"\"\"\n        Transform this AutomaToolSpec to a `Tool` object used by LLM.\n\n        Returns\n        -------\n        Tool\n            A `Tool` object that can be used by LLM.\n        \"\"\"\n        return Tool(\n            name=self._tool_name,\n            description=self._tool_description,\n            parameters=self._tool_parameters\n        )\n\n    @override\n    def create_worker(self) -&gt; Worker:\n        \"\"\"\n        Create a Worker from the information included in this AutomaToolSpec.\n\n        Returns\n        -------\n        Worker\n            A new `Worker` object that can be added to an Automa to execute the tool.\n        \"\"\"\n        return self._automa_cls(**self._automa_init_kwargs)\n\n    @override\n    def dump_to_dict(self) -&gt; Dict[str, Any]:\n        state_dict = super().dump_to_dict()\n        state_dict[\"automa_cls\"] = self._automa_cls.__module__ + \".\" + self._automa_cls.__qualname__\n        if self._automa_init_kwargs:\n            state_dict[\"automa_init_kwargs\"] = self._automa_init_kwargs\n        return state_dict\n\n    @override\n    def load_from_dict(self, state_dict: Dict[str, Any]) -&gt; None:\n        super().load_from_dict(state_dict)\n        self._automa_cls = load_qualified_class_or_func(state_dict[\"automa_cls\"])\n        self._automa_init_kwargs = state_dict.get(\"automa_init_kwargs\") or {}\n</code></pre>"},{"location":"reference/bridgic-core/bridgic/core/agentic/tool_specs/#bridgic.core.agentic.tool_specs.AutomaToolSpec.from_raw","title":"from_raw","text":"<code>classmethod</code> <pre><code>from_raw(\n    automa_cls: Type[Automa],\n    tool_name: Optional[str] = None,\n    tool_description: Optional[str] = None,\n    tool_parameters: Optional[Dict[str, Any]] = None,\n    **automa_init_kwargs: Dict[str, Any]\n) -&gt; AutomaToolSpec\n</code></pre> <p>Create an AutomaToolSpec from an Automa class.</p> Source code in <code>bridgic/core/agentic/tool_specs/_automa_tool_spec.py</code> <pre><code>@classmethod\ndef from_raw(\n    cls,\n    automa_cls: Type[Automa],\n    tool_name: Optional[str] = None,\n    tool_description: Optional[str] = None,\n    tool_parameters: Optional[Dict[str, Any]] = None,\n    **automa_init_kwargs: Dict[str, Any],\n) -&gt; \"AutomaToolSpec\":\n    \"\"\"\n    Create an AutomaToolSpec from an Automa class.\n    \"\"\"\n\n    def check_spec_func(automa_cls):\n        if hasattr(automa_cls, \"spec_func\") and isinstance(automa_cls.spec_func, Callable):\n            return\n        raise ValueError(f\"The Automa class {automa_cls} must be decorated with `@as_tool` in order to be used as a tool.\")\n\n    if (not tool_name) or (not tool_description) or (not tool_parameters):\n        check_spec_func(automa_cls)\n\n    if not tool_name:\n        tool_name = automa_cls.spec_func.__name__\n\n    if not tool_description:\n        tool_description = get_tool_description_from(automa_cls.spec_func, tool_name)\n\n    if not tool_parameters:\n        tool_parameters = create_func_params_json_schema(automa_cls.spec_func)\n        # TODO: whether to remove the `title` field of the params_schema?\n\n    return cls(\n        automa_cls=automa_cls,\n        tool_name=tool_name,\n        tool_description=tool_description,\n        tool_parameters=tool_parameters,\n        **automa_init_kwargs\n    )\n</code></pre>"},{"location":"reference/bridgic-core/bridgic/core/agentic/tool_specs/#bridgic.core.agentic.tool_specs.AutomaToolSpec.to_tool","title":"to_tool","text":"<pre><code>to_tool() -&gt; Tool\n</code></pre> <p>Transform this AutomaToolSpec to a <code>Tool</code> object used by LLM.</p> <p>Returns:</p> Type Description <code>Tool</code> <p>A <code>Tool</code> object that can be used by LLM.</p> Source code in <code>bridgic/core/agentic/tool_specs/_automa_tool_spec.py</code> <pre><code>@override\ndef to_tool(self) -&gt; Tool:\n    \"\"\"\n    Transform this AutomaToolSpec to a `Tool` object used by LLM.\n\n    Returns\n    -------\n    Tool\n        A `Tool` object that can be used by LLM.\n    \"\"\"\n    return Tool(\n        name=self._tool_name,\n        description=self._tool_description,\n        parameters=self._tool_parameters\n    )\n</code></pre>"},{"location":"reference/bridgic-core/bridgic/core/agentic/tool_specs/#bridgic.core.agentic.tool_specs.AutomaToolSpec.create_worker","title":"create_worker","text":"<pre><code>create_worker() -&gt; Worker\n</code></pre> <p>Create a Worker from the information included in this AutomaToolSpec.</p> <p>Returns:</p> Type Description <code>Worker</code> <p>A new <code>Worker</code> object that can be added to an Automa to execute the tool.</p> Source code in <code>bridgic/core/agentic/tool_specs/_automa_tool_spec.py</code> <pre><code>@override\ndef create_worker(self) -&gt; Worker:\n    \"\"\"\n    Create a Worker from the information included in this AutomaToolSpec.\n\n    Returns\n    -------\n    Worker\n        A new `Worker` object that can be added to an Automa to execute the tool.\n    \"\"\"\n    return self._automa_cls(**self._automa_init_kwargs)\n</code></pre>"},{"location":"reference/bridgic-core/bridgic/core/agentic/tool_specs/#bridgic.core.agentic.tool_specs.as_tool","title":"as_tool","text":"<pre><code>as_tool(spec_func: Callable) -&gt; Callable\n</code></pre> <p>A decorator that transforms a class to a tool that may be used by LLM.</p> <p>Parameters:</p> Name Type Description Default <code>spec_func</code> <code>Callable</code> <p>The function used to declare the tool spec. Note that this function is not intended to be called directly.</p> required Source code in <code>bridgic/core/agentic/tool_specs/_automa_tool_spec.py</code> <pre><code>def as_tool(spec_func: Callable) -&gt; Callable:\n    \"\"\"\n    A decorator that transforms a class to a tool that may be used by LLM.\n\n    Parameters\n    ----------\n    spec_func : Callable\n        The function used to declare the tool spec. Note that this function is not intended to be called directly.\n    \"\"\"\n    def decorator(cls):\n        if not isinstance(spec_func, Callable):\n            raise ValueError(f\"A function argument is expected, but got {type(spec_func)}.\")\n        if isinstance(spec_func, MethodType):\n            raise ValueError(f\"`spec_func` is not allowed to be a bound method: {spec_func}.\")\n        cls.spec_func = spec_func\n        return cls\n    return decorator\n</code></pre>"},{"location":"reference/bridgic-core/bridgic/core/agentic/types/","title":"types","text":"<p>The Agentic Types module defines foundational data structures for agentic systems.</p> <p>This module defines several important type definitions, such as <code>ToolSpec</code> and  <code>ChatMessage</code>, which are designed to be \"model-neutral\" as much as possible,  allowing developers to build agentic systems using different models.</p>"},{"location":"reference/bridgic-core/bridgic/core/agentic/types/#bridgic.core.agentic.types.Function","title":"Function","text":"<p>               Bases: <code>TypedDict</code></p> <p>The function that the model called.</p> Source code in <code>bridgic/core/agentic/types/_chat_message.py</code> <pre><code>class Function(TypedDict, total=True):\n    \"\"\"The function that the model called.\"\"\"\n\n    arguments: Required[str]\n    \"\"\"\n    The arguments to call the function with, as generated by the model in JSON\n    format. Note that the model does not always generate valid JSON, and may\n    hallucinate parameters not defined by your function schema. Validate the\n    arguments in your code before calling your function.\n    \"\"\"\n    name: Required[str]\n    \"\"\"The name of the function to call.\"\"\"\n</code></pre>"},{"location":"reference/bridgic-core/bridgic/core/agentic/types/#bridgic.core.agentic.types.Function.arguments","title":"arguments  <code>instance-attribute</code>","text":"<pre><code>arguments: Required[str]\n</code></pre> <p>The arguments to call the function with, as generated by the model in JSON format. Note that the model does not always generate valid JSON, and may hallucinate parameters not defined by your function schema. Validate the arguments in your code before calling your function.</p>"},{"location":"reference/bridgic-core/bridgic/core/agentic/types/#bridgic.core.agentic.types.Function.name","title":"name  <code>instance-attribute</code>","text":"<pre><code>name: Required[str]\n</code></pre> <p>The name of the function to call.</p>"},{"location":"reference/bridgic-core/bridgic/core/agentic/types/#bridgic.core.agentic.types.FunctionToolCall","title":"FunctionToolCall","text":"<p>               Bases: <code>TypedDict</code></p> <p>A call to a function tool created by the model.</p> Source code in <code>bridgic/core/agentic/types/_chat_message.py</code> <pre><code>class FunctionToolCall(TypedDict, total=True):\n    \"\"\"A call to a function tool created by the model.\"\"\"\n\n    id: Required[str]\n    \"\"\"The ID of the tool call.\"\"\"\n    function: Required[Function]\n    \"\"\"The function that the model called.\"\"\"\n    type: Required[Literal[\"function\"]]\n    \"\"\"The type of the tool. Currently, only `function` is supported.\"\"\"\n</code></pre>"},{"location":"reference/bridgic-core/bridgic/core/agentic/types/#bridgic.core.agentic.types.FunctionToolCall.id","title":"id  <code>instance-attribute</code>","text":"<pre><code>id: Required[str]\n</code></pre> <p>The ID of the tool call.</p>"},{"location":"reference/bridgic-core/bridgic/core/agentic/types/#bridgic.core.agentic.types.FunctionToolCall.function","title":"function  <code>instance-attribute</code>","text":"<pre><code>function: Required[Function]\n</code></pre> <p>The function that the model called.</p>"},{"location":"reference/bridgic-core/bridgic/core/agentic/types/#bridgic.core.agentic.types.FunctionToolCall.type","title":"type  <code>instance-attribute</code>","text":"<pre><code>type: Required[Literal['function']]\n</code></pre> <p>The type of the tool. Currently, only <code>function</code> is supported.</p>"},{"location":"reference/bridgic-core/bridgic/core/agentic/types/#bridgic.core.agentic.types.SystemMessage","title":"SystemMessage","text":"<p>               Bases: <code>TypedDict</code></p> <p>Developer-provided instructions that the model should follow, regardless of messages sent by the user.</p> Source code in <code>bridgic/core/agentic/types/_chat_message.py</code> <pre><code>class SystemMessage(TypedDict, total=False):\n    \"\"\"Developer-provided instructions that the model should follow, regardless of messages sent by the user.\"\"\"\n\n    role: Required[Literal[\"system\"]]\n    \"\"\"The role of the messages author, in this case `system`.\"\"\"\n    content: Required[str]\n    \"\"\"The contents of the system message, which is a text.\"\"\"\n</code></pre>"},{"location":"reference/bridgic-core/bridgic/core/agentic/types/#bridgic.core.agentic.types.SystemMessage.role","title":"role  <code>instance-attribute</code>","text":"<pre><code>role: Required[Literal['system']]\n</code></pre> <p>The role of the messages author, in this case <code>system</code>.</p>"},{"location":"reference/bridgic-core/bridgic/core/agentic/types/#bridgic.core.agentic.types.SystemMessage.content","title":"content  <code>instance-attribute</code>","text":"<pre><code>content: Required[str]\n</code></pre> <p>The contents of the system message, which is a text.</p>"},{"location":"reference/bridgic-core/bridgic/core/agentic/types/#bridgic.core.agentic.types.UserTextMessage","title":"UserTextMessage","text":"<p>               Bases: <code>TypedDict</code></p> <p>Messages sent by an end user, containing prompts.</p> Source code in <code>bridgic/core/agentic/types/_chat_message.py</code> <pre><code>class UserTextMessage(TypedDict, total=False):\n    \"\"\"Messages sent by an end user, containing prompts.\"\"\"\n\n    role: Required[Literal[\"user\"]]\n    \"\"\"The role of the messages author, in this case `user`.\"\"\"\n    content: Required[str]\n    \"\"\"The content of the user message, which is a text.\"\"\"\n</code></pre>"},{"location":"reference/bridgic-core/bridgic/core/agentic/types/#bridgic.core.agentic.types.UserTextMessage.role","title":"role  <code>instance-attribute</code>","text":"<pre><code>role: Required[Literal['user']]\n</code></pre> <p>The role of the messages author, in this case <code>user</code>.</p>"},{"location":"reference/bridgic-core/bridgic/core/agentic/types/#bridgic.core.agentic.types.UserTextMessage.content","title":"content  <code>instance-attribute</code>","text":"<pre><code>content: Required[str]\n</code></pre> <p>The content of the user message, which is a text.</p>"},{"location":"reference/bridgic-core/bridgic/core/agentic/types/#bridgic.core.agentic.types.AssistantTextMessage","title":"AssistantTextMessage","text":"<p>               Bases: <code>TypedDict</code></p> <p>Messages sent by the model in response to user messages.</p> Source code in <code>bridgic/core/agentic/types/_chat_message.py</code> <pre><code>class AssistantTextMessage(TypedDict, total=False):\n    \"\"\"Messages sent by the model in response to user messages.\"\"\"\n\n    role: Required[Literal[\"assistant\"]]\n    \"\"\"The role of the messages author, in this case `assistant`.\"\"\"\n    content: Optional[str]\n    \"\"\"The content of the assistant message, which is a text. Required unless `tool_calls` is specified.\"\"\"\n    tool_calls: Optional[Iterable[FunctionToolCall]]\n    \"\"\"The tool calls generated by the model, such as function calls.\"\"\"\n</code></pre>"},{"location":"reference/bridgic-core/bridgic/core/agentic/types/#bridgic.core.agentic.types.AssistantTextMessage.role","title":"role  <code>instance-attribute</code>","text":"<pre><code>role: Required[Literal['assistant']]\n</code></pre> <p>The role of the messages author, in this case <code>assistant</code>.</p>"},{"location":"reference/bridgic-core/bridgic/core/agentic/types/#bridgic.core.agentic.types.AssistantTextMessage.content","title":"content  <code>instance-attribute</code>","text":"<pre><code>content: Optional[str]\n</code></pre> <p>The content of the assistant message, which is a text. Required unless <code>tool_calls</code> is specified.</p>"},{"location":"reference/bridgic-core/bridgic/core/agentic/types/#bridgic.core.agentic.types.AssistantTextMessage.tool_calls","title":"tool_calls  <code>instance-attribute</code>","text":"<pre><code>tool_calls: Optional[Iterable[FunctionToolCall]]\n</code></pre> <p>The tool calls generated by the model, such as function calls.</p>"},{"location":"reference/bridgic-core/bridgic/core/agentic/types/#bridgic.core.agentic.types.ToolMessage","title":"ToolMessage","text":"<p>               Bases: <code>TypedDict</code></p> <p>Messages generated by tools.</p> Source code in <code>bridgic/core/agentic/types/_chat_message.py</code> <pre><code>class ToolMessage(TypedDict, total=False):\n    \"\"\"Messages generated by tools.\"\"\"\n\n    role: Required[Literal[\"tool\"]]\n    \"\"\"The role of the messages author, in this case `tool`.\"\"\"\n    content: Required[str]\n    \"\"\"The contents of the tool message.\"\"\"\n    tool_call_id: Required[str]\n    \"\"\"Tool call that this message is responding to.\"\"\"\n</code></pre>"},{"location":"reference/bridgic-core/bridgic/core/agentic/types/#bridgic.core.agentic.types.ToolMessage.role","title":"role  <code>instance-attribute</code>","text":"<pre><code>role: Required[Literal['tool']]\n</code></pre> <p>The role of the messages author, in this case <code>tool</code>.</p>"},{"location":"reference/bridgic-core/bridgic/core/agentic/types/#bridgic.core.agentic.types.ToolMessage.content","title":"content  <code>instance-attribute</code>","text":"<pre><code>content: Required[str]\n</code></pre> <p>The contents of the tool message.</p>"},{"location":"reference/bridgic-core/bridgic/core/agentic/types/#bridgic.core.agentic.types.ToolMessage.tool_call_id","title":"tool_call_id  <code>instance-attribute</code>","text":"<pre><code>tool_call_id: Required[str]\n</code></pre> <p>Tool call that this message is responding to.</p>"},{"location":"reference/bridgic-core/bridgic/core/agentic/workers/","title":"workers","text":"<p>The Agentic Workers module provides specialized implementation of Worker for agentic systems.</p> <p>This module provides specialized Worker implementations for specific functions to support  building Agentic systems with complex capabilities.</p>"},{"location":"reference/bridgic-core/bridgic/core/agentic/workers/#bridgic.core.agentic.workers.ToolSelectionWorker","title":"ToolSelectionWorker","text":"<p>               Bases: <code>Worker</code></p> <p>A worker that calls an LLM to select tools and/or generate a response.</p> Source code in <code>bridgic/core/agentic/workers/_tool_selection.py</code> <pre><code>class ToolSelectionWorker(Worker):\n    \"\"\"\n    A worker that calls an LLM to select tools and/or generate a response.\n    \"\"\"\n\n    # Note: the ToolSelection LLM instance need support serialization and deserialization.\n    _tool_selection_llm: ToolSelection\n    \"\"\"The LLM to be used for tool selection.\"\"\"\n\n    def __init__(self, tool_selection_llm: ToolSelection):\n        \"\"\"\n        Parameters\n        ----------\n        tool_selection_llm: ToolSelect\n            The LLM to be used for tool selection.\n        \"\"\"\n        super().__init__()\n        self._tool_selection_llm = tool_selection_llm\n\n    async def arun(\n        self,\n        messages: List[ChatMessage],\n        tools: List[Tool],\n    ) -&gt; Tuple[List[ToolCall], Optional[str]]:\n        \"\"\"\n        Run the worker.\n\n        Parameters\n        ----------\n        messages: List[ChatMessage]\n            The messages to send to the LLM.\n        tools: List[Tool]\n            The tool list for the LLM to select from.\n\n        Returns\n        -------\n        Tuple[List[ToolCall], Optional[str]]\n            * The first element is a list of `ToolCall` that the LLM selected.\n            * The second element is the text response from the LLM.\n        \"\"\"\n        # Validate and transform the input messages and tools to the format expected by the LLM.\n        llm_messages: List[Message] = []\n        for message in messages:\n            llm_messages.append(transform_chat_message_to_llm_message(message))\n        # print(f\"\\n******* ToolSelectionWorker.arun *******\\n\")\n        # print(f\"messages: {llm_messages}\")\n        # print(f\"tools: {tools}\")\n        tool_calls, llm_response = await self._tool_selection_llm.aselect_tool(\n            messages=llm_messages, \n            tools=tools, \n        )\n        return tool_calls, llm_response\n</code></pre>"},{"location":"reference/bridgic-core/bridgic/core/agentic/workers/#bridgic.core.agentic.workers.ToolSelectionWorker.arun","title":"arun","text":"<code>async</code> <pre><code>arun(\n    messages: List[ChatMessage], tools: List[Tool]\n) -&gt; Tuple[List[ToolCall], Optional[str]]\n</code></pre> <p>Run the worker.</p> <p>Parameters:</p> Name Type Description Default <code>messages</code> <code>List[ChatMessage]</code> <p>The messages to send to the LLM.</p> required <code>tools</code> <code>List[Tool]</code> <p>The tool list for the LLM to select from.</p> required <p>Returns:</p> Type Description <code>Tuple[List[ToolCall], Optional[str]]</code> <ul> <li>The first element is a list of <code>ToolCall</code> that the LLM selected.</li> <li>The second element is the text response from the LLM.</li> </ul> Source code in <code>bridgic/core/agentic/workers/_tool_selection.py</code> <pre><code>async def arun(\n    self,\n    messages: List[ChatMessage],\n    tools: List[Tool],\n) -&gt; Tuple[List[ToolCall], Optional[str]]:\n    \"\"\"\n    Run the worker.\n\n    Parameters\n    ----------\n    messages: List[ChatMessage]\n        The messages to send to the LLM.\n    tools: List[Tool]\n        The tool list for the LLM to select from.\n\n    Returns\n    -------\n    Tuple[List[ToolCall], Optional[str]]\n        * The first element is a list of `ToolCall` that the LLM selected.\n        * The second element is the text response from the LLM.\n    \"\"\"\n    # Validate and transform the input messages and tools to the format expected by the LLM.\n    llm_messages: List[Message] = []\n    for message in messages:\n        llm_messages.append(transform_chat_message_to_llm_message(message))\n    # print(f\"\\n******* ToolSelectionWorker.arun *******\\n\")\n    # print(f\"messages: {llm_messages}\")\n    # print(f\"tools: {tools}\")\n    tool_calls, llm_response = await self._tool_selection_llm.aselect_tool(\n        messages=llm_messages, \n        tools=tools, \n    )\n    return tool_calls, llm_response\n</code></pre>"},{"location":"reference/bridgic-core/bridgic/core/automa/","title":"automa","text":"<p>This module contains the core Automa classes and functions.</p>"},{"location":"reference/bridgic-core/bridgic/core/automa/#bridgic.core.automa.worker._worker_decorator.worker","title":"worker","text":"<pre><code>worker(**kwargs) -&gt; Callable\n</code></pre> <p>Decorator for marking a method inside an Automa class as a worker.</p> <p>To cover the need to declare workers in various Automa classes, this decorator actually  accepts a variable kwargs parameter. Through overloading, it further supports specifying  the parameters that need to be passed in when registering a worker under a specific Automa,  such as <code>ConcurrentAutoma</code>, <code>SequentialAutoma</code> and so on.</p> <p>Parameters:</p> Name Type Description Default <code>kwargs</code> <code>Dict[str, Any]</code> <p>The keyword arguments for the worker decorator.</p> <code>{}</code> Source code in <code>bridgic/core/automa/worker/_worker_decorator.py</code> <pre><code>def worker(**kwargs) -&gt; Callable:\n    \"\"\"\n    Decorator for marking a method inside an Automa class as a worker.\n\n    To cover the need to declare workers in various Automa classes, this decorator actually \n    accepts a variable kwargs parameter. Through overloading, it further supports specifying \n    the parameters that need to be passed in when registering a worker under a specific Automa, \n    such as `ConcurrentAutoma`, `SequentialAutoma` and so on.\n\n    Parameters\n    ----------\n    kwargs : Dict[str, Any]\n        The keyword arguments for the worker decorator.\n    \"\"\"\n    def wrapper(func: Callable):\n        setattr(func, \"__worker_kwargs__\", kwargs)\n        return func\n    return wrapper\n</code></pre>"},{"location":"reference/bridgic-core/bridgic/core/automa/#bridgic.core.automa.Automa","title":"Automa","text":"<p>               Bases: <code>Worker</code></p> <p>Base class for an Automa.</p> <p>In Bridgic, an Automa is an entity that manages and orchestrates a group of workers. An Automa itself is also a Worker, which enables the nesting of Automa instances within each other.</p> Source code in <code>bridgic/core/automa/_automa.py</code> <pre><code>class Automa(Worker):\n    \"\"\"\n    Base class for an Automa.\n\n    In Bridgic, an Automa is an entity that manages and orchestrates a group of workers. An Automa itself is also a Worker, which enables the nesting of Automa instances within each other.\n    \"\"\"\n    _running_options: RunningOptions\n\n    # For event handling.\n    _event_handlers: Dict[str, EventHandlerType]\n    _default_event_handler: EventHandlerType\n\n    # For human interaction.\n    _worker_interaction_indices: Dict[str, int]\n\n    # Ongoing human interactions triggered by the `interact_with_human()` call from workers of the current Automa.\n    # worker_key -&gt; list of interactions.\n    _ongoing_interactions: Dict[str, List[_InteractionAndFeedback]]\n\n    _thread_pool: ThreadPoolExecutor\n    _main_thread_id: int\n    _main_loop: asyncio.AbstractEventLoop\n\n    def __init__(\n        self,\n        name: str = None,\n        thread_pool: Optional[ThreadPoolExecutor] = None,\n    ):\n        super().__init__()\n\n        # Set the name of the Automa instance.\n        self.name = name or f\"automa-{uuid.uuid4().hex[:8]}\"\n\n        # Initialize the shared running options.\n        self._running_options = RunningOptions()\n\n        # Initialize data structures for event handling and human interactions\n        self._event_handlers = {}\n        self._default_event_handler = None\n        self._worker_interaction_indices = {}\n        self._ongoing_interactions = {}\n\n        self._thread_pool = thread_pool\n\n    @override\n    def dump_to_dict(self) -&gt; Dict[str, Any]:\n        state_dict = super().dump_to_dict()\n        state_dict[\"name\"] = self.name\n        state_dict[\"running_options\"] = self._running_options\n        state_dict[\"ongoing_interactions\"] = self._ongoing_interactions\n        return state_dict\n\n    @override\n    def load_from_dict(self, state_dict: Dict[str, Any]) -&gt; None:\n        super().load_from_dict(state_dict)\n        self.name = state_dict[\"name\"]\n        self._running_options = state_dict[\"running_options\"]\n\n        self._event_handlers = {}\n        self._default_event_handler = None\n        self._worker_interaction_indices = {}\n        self._ongoing_interactions = state_dict[\"ongoing_interactions\"]\n        self._thread_pool = None\n\n    @classmethod\n    def load_from_snapshot(\n        cls, \n        snapshot: Snapshot,\n        thread_pool: Optional[ThreadPoolExecutor] = None,\n    ) -&gt; \"Automa\":\n        # Here you can compare snapshot.serialization_version with SERIALIZATION_VERSION, and handle any necessary version compatibility issues if needed.\n        automa = load_bytes(snapshot.serialized_bytes)\n        if thread_pool:\n            automa.thread_pool = thread_pool\n        return automa\n\n    @property\n    def thread_pool(self) -&gt; Optional[ThreadPoolExecutor]:\n        return self._thread_pool\n\n    @thread_pool.setter\n    def thread_pool(self, executor: ThreadPoolExecutor) -&gt; None:\n        \"\"\"\n        Set the thread pool for parallel running of I/O-bound tasks.\n\n        If an Automa is nested within another Automa, the thread pool of the top-level Automa will be used, rather than the thread pool of the nested Automa.\n        \"\"\"\n        self._thread_pool = executor\n\n    @abstractmethod\n    def _locate_interacting_worker(self) -&gt; Optional[str]:\n        \"\"\"\n        Locate the worker that is currently interacting with human.\n\n        Returns\n        -------\n        Optional[str]\n            The necessary identifier of the worker that is currently interacting with human.\n        \"\"\"\n        ...\n\n    @abstractmethod\n    def _get_worker_key(self, worker: Worker) -&gt; Optional[str]:\n        \"\"\"\n        Identify the worker key by the worker instance.\n        \"\"\"\n        ...\n\n    @abstractmethod\n    def _get_worker_instance(self, worker_key: str) -&gt; Worker:\n        \"\"\"\n        Get the worker instance by the worker key.\n        \"\"\"\n        ...\n\n    def is_top_level(self) -&gt; bool:\n        \"\"\"\n        Check if the current automa is the top-level automa.\n\n        Returns\n        -------\n        bool\n            True if the current automa is the top-level automa, False otherwise.\n        \"\"\"\n        return self.parent is None\n\n    def set_running_options(self, debug: bool = None):\n        \"\"\"\n        Set running options for this Automa instance, and ensure these options propagate through all nested Automa instances.\n\n        When different options are set on different nested Automa instances, the setting from the outermost \n        (top-level) Automa will override the settings of all inner (nested) Automa instances.\n\n        For example, if the top-level Automa instance sets `debug = True` and the nested instances sets `debug = False`, \n        then the nested Automa instance will run in debug mode, when the top-level Automa instance is executed.\n\n        Parameters\n        ----------\n        debug : bool, optional\n            Whether to enable debug mode. If not set, the effect is the same as setting `debug = False` by default.\n        \"\"\"\n        if debug is not None:\n            self._running_options.debug = debug\n\n    def _get_top_running_options(self) -&gt; RunningOptions:\n        if self.parent is None:\n            # Here we are at the top-level automa.\n            return self._running_options\n        return self.parent._get_top_running_options()\n\n    ###############################################################\n    ########## [Bridgic Event Handling Mechanism] starts ##########\n    ###############################################################\n\n    def register_event_handler(self, event_type: Optional[str], event_handler: EventHandlerType) -&gt; None:\n        \"\"\"\n        Register an event handler for the specified event type. If `event_type` is set to None, the event handler will be registered as the default handler that will handle all event types.\n\n        Note: Only event handlers registered on the top-level Automa will be invoked to handle events.\n\n        Parameters\n        ----------\n        event_type: Optional[str]\n            The type of event to be handled. If set to None, the event handler will be registered as the default handler and will be used to handle all event types.\n        event_handler: EventHandlerType\n            The event handler to be registered.\n        \"\"\"\n        if event_type is None:\n            self._default_event_handler = event_handler\n        else:\n            self._event_handlers[event_type] = event_handler\n\n    def unregister_event_handler(self, event_type: Optional[str]) -&gt; None:\n        \"\"\"\n        Unregister an event handler for the specified event type.\n\n        Parameters\n        ----------\n        event_type: Optional[str]\n            The type of event to be unregistered. If set to None, the default event handler will be unregistered.\n        \"\"\"\n        if event_type in self._event_handlers:\n            del self._event_handlers[event_type]\n        if event_type is None:\n            self._default_event_handler = None\n\n    def unregister_all_event_handlers(self) -&gt; None:\n        \"\"\"\n        Unregister all event handlers.\n        \"\"\"\n        self._event_handlers.clear()\n        self._default_event_handler = None\n\n    class _FeedbackSender(FeedbackSender):\n        def __init__(\n                self, \n                future: asyncio.Future[Feedback],\n                post_loop: asyncio.AbstractEventLoop,\n                ):\n            self._future = future\n            self._post_loop = post_loop\n\n        def send(self, feedback: Feedback) -&gt; None:\n            try:\n                current_loop = asyncio.get_running_loop()\n            except Exception:\n                current_loop = None\n            try:\n                if current_loop is self._post_loop:\n                    self._future.set_result(feedback)\n                else:\n                    self._post_loop.call_soon_threadsafe(self._future.set_result, feedback)\n            except asyncio.InvalidStateError:\n                # Suppress the InvalidStateError to be raised, maybe due to timeout.\n                import warnings\n                warnings.warn(f\"Feedback future already set. feedback: {feedback}\", FutureWarning)\n\n    @override\n    def post_event(self, event: Event) -&gt; None:\n        \"\"\"\n        Post an event to the application layer outside the Automa.\n\n        The event handler implemented by the application layer will be called in the same thread as the worker (maybe the main thread or a new thread from the thread pool).\n\n        Note that `post_event` can be called in a non-async method or an async method.\n\n        The event will be bubbled up to the top-level Automa, where it will be processed by the event handler registered with the event type.\n\n        Parameters\n        ----------\n        event: Event\n            The event to be posted.\n        \"\"\"\n        def _handler_need_feedback_sender(handler: EventHandlerType):\n            positional_param_names = get_param_names_by_kind(handler, Parameter.POSITIONAL_ONLY) + get_param_names_by_kind(handler, Parameter.POSITIONAL_OR_KEYWORD)\n            var_positional_param_names = get_param_names_by_kind(handler, Parameter.VAR_POSITIONAL)\n            return len(var_positional_param_names) &gt; 0 or len(positional_param_names) &gt; 1\n\n        if self.parent is not None:\n            # Bubble up the event to the top-level Automa.\n            return self.parent.post_event(event)\n\n        # Here is the top-level Automa.\n        # Call event handlers\n        if event.event_type in self._event_handlers:\n            if _handler_need_feedback_sender(self._event_handlers[event.event_type]):\n                self._event_handlers[event.event_type](event, feedback_sender=None)\n            else:\n                self._event_handlers[event.event_type](event)\n        if self._default_event_handler is not None:\n            if _handler_need_feedback_sender(self._default_event_handler):\n                self._default_event_handler(event, feedback_sender=None)\n            else:\n                self._default_event_handler(event)\n\n    def request_feedback(\n        self, \n        event: Event,\n        timeout: Optional[float] = None\n    ) -&gt; Feedback:\n        \"\"\"\n        Request feedback for the specified event from the application layer outside the Automa. This method blocks the caller until the feedback is received.\n\n        Note that `post_event` should only be called from within a non-async method running in the new thread of the Automa thread pool.\n\n        Parameters\n        ----------\n        event: Event\n            The event to be posted to the event handler implemented by the application layer.\n        timeout: Optional[float]\n            A float or int number of seconds to wait for if the feedback is not received. If None, then there is no limit on the wait time.\n\n        Returns\n        -------\n        Feedback\n            The feedback received from the application layer.\n\n        Raises\n        ------\n        TimeoutError\n            If the feedback is not received before the timeout. Note that the raised exception is the built-in `TimeoutError` exception, instead of asyncio.TimeoutError or concurrent.futures.TimeoutError!\n        \"\"\"\n        if threading.get_ident() == self._main_thread_id:\n            raise AutomaRuntimeError(\n                f\"`request_feedback` should only be called in a different thread from the main thread of the {self.name}. \"\n            )\n        return asyncio.run_coroutine_threadsafe(\n            self.request_feedback_async(event, timeout),\n            self._main_loop\n        ).result()\n\n    async def request_feedback_async(\n        self, \n        event: Event,\n        timeout: Optional[float] = None\n    ) -&gt; Feedback:\n        \"\"\"\n        Request feedback for the specified event from the application layer outside the Automa. This method blocks the caller until the feedback is received.\n\n        The event handler implemented by the application layer will be called in the next event loop, in the main thread.\n\n        Parameters\n        ----------\n        event: Event\n            The event to be posted to the event handler implemented by the application layer.\n        timeout: Optional[float]\n            A float or int number of seconds to wait for if the feedback is not received. If None, then there is no limit on the wait time.\n\n        Returns\n        -------\n        Feedback\n            The feedback received from the application layer.\n\n        Raises\n        ------\n        TimeoutError\n            If the feedback is not received before the timeout. Note that the raised exception is the built-in `TimeoutError` exception, instead of asyncio.TimeoutError!\n        \"\"\"\n        if self.parent is not None:\n            # Bubble up the event to the top-level Automa.\n            return await self.parent.request_feedback_async(event, timeout)\n\n        # Here is the top-level Automa.\n        event_loop = asyncio.get_running_loop()\n        future = event_loop.create_future()\n        feedback_sender = self._FeedbackSender(future, event_loop)\n        # Call event handlers\n        if event.event_type in self._event_handlers:\n            self._event_handlers[event.event_type](event, feedback_sender)\n        if self._default_event_handler is not None:\n            self._default_event_handler(event, feedback_sender)\n\n        try:\n            return await asyncio.wait_for(future, timeout)\n        except TimeoutError as e:\n            # When python &gt;= 3.11 here.\n            raise TimeoutError(f\"No feedback is received before timeout in Automa[{self.name}]\") from e\n        except asyncio.TimeoutError as e:\n            # Version compatibility resolution: asyncio.wait_for raises asyncio.TimeoutError before python 3.11.\n            # https://docs.python.org/3/library/asyncio-task.html#asyncio.wait_for\n            raise TimeoutError(f\"No feedback is received before timeout in Automa[{self.name}]\") from e\n\n    ###############################################################\n    ########### [Bridgic Event Handling Mechanism] ends ###########\n    ###############################################################\n\n    ###############################################################\n    ######## [Bridgic Human Interaction Mechanism] starts #########\n    ###############################################################\n\n    def interact_with_human(\n        self,\n        event: Event,\n        interacting_worker: Optional[Worker] = None,\n    ) -&gt; InteractionFeedback:\n        \"\"\"\n        Trigger an interruption in the \"human-computer interaction\" during the execution of Automa.\n\n        Parameters\n        ----------\n        event: Event\n            The event that triggered the interaction.\n        interacting_worker: Optional[Worker]\n            The worker that is currently interacting with human. If not provided, the worker will be located automatically.\n\n        Returns\n        -------\n        InteractionFeedback\n            The feedback received from the application layer.\n\n        Raises\n        ------\n        _InteractionEventException\n            If the Automa is not the top-level Automa and the `interact_with_human()` method is called by \n            one or more workers, this exception will be raised to the upper level Automa.\n        \"\"\"\n        if not interacting_worker:\n            kickoff_worker_key: str = self._locate_interacting_worker()\n        else:\n            kickoff_worker_key = self._get_worker_key(interacting_worker)\n\n        if kickoff_worker_key:\n            return self.interact_with_human_from_worker_key(event, kickoff_worker_key)\n        raise AutomaRuntimeError(\n            f\"Get kickoff worker failed in Automa[{self.name}] \"\n            f\"when trying to interact with human with event: {event}\"\n        )\n\n    def interact_with_human_from_worker_key(\n        self,\n        event: Event,\n        worker_key: str\n    ) -&gt; InteractionFeedback:\n        # Match interaction_feedback to see if it matches\n        matched_feedback: _InteractionAndFeedback = None\n        cur_interact_index = self._get_and_increment_interaction_index(worker_key)\n        if worker_key in self._ongoing_interactions:\n            interaction_and_feedbacks = self._ongoing_interactions[worker_key]\n            if cur_interact_index &lt; len(interaction_and_feedbacks):\n                matched_feedback = interaction_and_feedbacks[cur_interact_index]\n                # Check the event type\n                if event.event_type != matched_feedback.interaction.event.event_type:\n                    raise AutomaRuntimeError(\n                        f\"Event type mismatch! Automa[{self.name}-worker[{worker_key}]]. \"\n                        f\"interact_with_human passed-in event: {event}\\n\"\n                        f\"ongoing interaction &amp;&amp; feedback: {matched_feedback}\\n\"\n                    )\n        if matched_feedback is None or matched_feedback.feedback is None:\n            # Important: The interaction_id should be unique for each human interaction.\n            interaction_id = uuid.uuid4().hex if matched_feedback is None else matched_feedback.interaction.interaction_id\n            # Match interaction_feedback failed, raise an exception to go into the human interactioin process.\n            raise _InteractionEventException(Interaction(\n                interaction_id=interaction_id,\n                event=event,\n            ))\n        else:\n            # Match interaction_feedback succeeded, return it.\n            return matched_feedback.feedback\n\n    def _get_and_increment_interaction_index(self, worker_key: str) -&gt; int:\n        if worker_key not in self._worker_interaction_indices:\n            cur_index = 0\n            self._worker_interaction_indices[worker_key] = 0\n        else:\n            cur_index = self._worker_interaction_indices[worker_key]\n        self._worker_interaction_indices[worker_key] += 1\n        return cur_index\n\n    ###############################################################\n    ######### [Bridgic Human Interaction Mechanism] ends ##########\n    ###############################################################\n\n    def get_local_space(self, runtime_context: RuntimeContext) -&gt; Dict[str, Any]:\n        \"\"\"\n        Retrieve the local execution context (local space) associated with the current worker. \n        If you require the local space to be cleared after the completion of `automa.arun()`, \n        you may customize this behavior by overriding the `should_reset_local_space()` method.\n\n        Parameters\n        ----------\n        runtime_context : RuntimeContext\n            The runtime context.\n\n        Returns\n        -------\n        Dict[str, Any]\n            The local space.\n        \"\"\"\n        worker_key = runtime_context.worker_key\n        worker_obj = self._get_worker_instance(worker_key)\n        return worker_obj.local_space\n\n    def should_reset_local_space(self) -&gt; bool:\n        \"\"\"\n        This method indicates whether to reset the local space at the end of the arun method of Automa. \n        By default, it returns True, standing for resetting. Otherwise, it means doing nothing.\n\n        Examples:\n        --------\n        ```python\n        class MyAutoma(Automa):\n            def should_reset_local_space(self) -&gt; bool:\n                return False\n        ```\n        \"\"\"\n        return True\n</code></pre>"},{"location":"reference/bridgic-core/bridgic/core/automa/#bridgic.core.automa.Automa.is_top_level","title":"is_top_level","text":"<pre><code>is_top_level() -&gt; bool\n</code></pre> <p>Check if the current automa is the top-level automa.</p> <p>Returns:</p> Type Description <code>bool</code> <p>True if the current automa is the top-level automa, False otherwise.</p> Source code in <code>bridgic/core/automa/_automa.py</code> <pre><code>def is_top_level(self) -&gt; bool:\n    \"\"\"\n    Check if the current automa is the top-level automa.\n\n    Returns\n    -------\n    bool\n        True if the current automa is the top-level automa, False otherwise.\n    \"\"\"\n    return self.parent is None\n</code></pre>"},{"location":"reference/bridgic-core/bridgic/core/automa/#bridgic.core.automa.Automa.set_running_options","title":"set_running_options","text":"<pre><code>set_running_options(debug: bool = None)\n</code></pre> <p>Set running options for this Automa instance, and ensure these options propagate through all nested Automa instances.</p> <p>When different options are set on different nested Automa instances, the setting from the outermost  (top-level) Automa will override the settings of all inner (nested) Automa instances.</p> <p>For example, if the top-level Automa instance sets <code>debug = True</code> and the nested instances sets <code>debug = False</code>,  then the nested Automa instance will run in debug mode, when the top-level Automa instance is executed.</p> <p>Parameters:</p> Name Type Description Default <code>debug</code> <code>bool</code> <p>Whether to enable debug mode. If not set, the effect is the same as setting <code>debug = False</code> by default.</p> <code>None</code> Source code in <code>bridgic/core/automa/_automa.py</code> <pre><code>def set_running_options(self, debug: bool = None):\n    \"\"\"\n    Set running options for this Automa instance, and ensure these options propagate through all nested Automa instances.\n\n    When different options are set on different nested Automa instances, the setting from the outermost \n    (top-level) Automa will override the settings of all inner (nested) Automa instances.\n\n    For example, if the top-level Automa instance sets `debug = True` and the nested instances sets `debug = False`, \n    then the nested Automa instance will run in debug mode, when the top-level Automa instance is executed.\n\n    Parameters\n    ----------\n    debug : bool, optional\n        Whether to enable debug mode. If not set, the effect is the same as setting `debug = False` by default.\n    \"\"\"\n    if debug is not None:\n        self._running_options.debug = debug\n</code></pre>"},{"location":"reference/bridgic-core/bridgic/core/automa/#bridgic.core.automa.Automa.register_event_handler","title":"register_event_handler","text":"<pre><code>register_event_handler(\n    event_type: Optional[str],\n    event_handler: EventHandlerType,\n) -&gt; None\n</code></pre> <p>Register an event handler for the specified event type. If <code>event_type</code> is set to None, the event handler will be registered as the default handler that will handle all event types.</p> <p>Note: Only event handlers registered on the top-level Automa will be invoked to handle events.</p> <p>Parameters:</p> Name Type Description Default <code>event_type</code> <code>Optional[str]</code> <p>The type of event to be handled. If set to None, the event handler will be registered as the default handler and will be used to handle all event types.</p> required <code>event_handler</code> <code>EventHandlerType</code> <p>The event handler to be registered.</p> required Source code in <code>bridgic/core/automa/_automa.py</code> <pre><code>def register_event_handler(self, event_type: Optional[str], event_handler: EventHandlerType) -&gt; None:\n    \"\"\"\n    Register an event handler for the specified event type. If `event_type` is set to None, the event handler will be registered as the default handler that will handle all event types.\n\n    Note: Only event handlers registered on the top-level Automa will be invoked to handle events.\n\n    Parameters\n    ----------\n    event_type: Optional[str]\n        The type of event to be handled. If set to None, the event handler will be registered as the default handler and will be used to handle all event types.\n    event_handler: EventHandlerType\n        The event handler to be registered.\n    \"\"\"\n    if event_type is None:\n        self._default_event_handler = event_handler\n    else:\n        self._event_handlers[event_type] = event_handler\n</code></pre>"},{"location":"reference/bridgic-core/bridgic/core/automa/#bridgic.core.automa.Automa.unregister_event_handler","title":"unregister_event_handler","text":"<pre><code>unregister_event_handler(event_type: Optional[str]) -&gt; None\n</code></pre> <p>Unregister an event handler for the specified event type.</p> <p>Parameters:</p> Name Type Description Default <code>event_type</code> <code>Optional[str]</code> <p>The type of event to be unregistered. If set to None, the default event handler will be unregistered.</p> required Source code in <code>bridgic/core/automa/_automa.py</code> <pre><code>def unregister_event_handler(self, event_type: Optional[str]) -&gt; None:\n    \"\"\"\n    Unregister an event handler for the specified event type.\n\n    Parameters\n    ----------\n    event_type: Optional[str]\n        The type of event to be unregistered. If set to None, the default event handler will be unregistered.\n    \"\"\"\n    if event_type in self._event_handlers:\n        del self._event_handlers[event_type]\n    if event_type is None:\n        self._default_event_handler = None\n</code></pre>"},{"location":"reference/bridgic-core/bridgic/core/automa/#bridgic.core.automa.Automa.unregister_all_event_handlers","title":"unregister_all_event_handlers","text":"<pre><code>unregister_all_event_handlers() -&gt; None\n</code></pre> <p>Unregister all event handlers.</p> Source code in <code>bridgic/core/automa/_automa.py</code> <pre><code>def unregister_all_event_handlers(self) -&gt; None:\n    \"\"\"\n    Unregister all event handlers.\n    \"\"\"\n    self._event_handlers.clear()\n    self._default_event_handler = None\n</code></pre>"},{"location":"reference/bridgic-core/bridgic/core/automa/#bridgic.core.automa.Automa.post_event","title":"post_event","text":"<pre><code>post_event(event: Event) -&gt; None\n</code></pre> <p>Post an event to the application layer outside the Automa.</p> <p>The event handler implemented by the application layer will be called in the same thread as the worker (maybe the main thread or a new thread from the thread pool).</p> <p>Note that <code>post_event</code> can be called in a non-async method or an async method.</p> <p>The event will be bubbled up to the top-level Automa, where it will be processed by the event handler registered with the event type.</p> <p>Parameters:</p> Name Type Description Default <code>event</code> <code>Event</code> <p>The event to be posted.</p> required Source code in <code>bridgic/core/automa/_automa.py</code> <pre><code>@override\ndef post_event(self, event: Event) -&gt; None:\n    \"\"\"\n    Post an event to the application layer outside the Automa.\n\n    The event handler implemented by the application layer will be called in the same thread as the worker (maybe the main thread or a new thread from the thread pool).\n\n    Note that `post_event` can be called in a non-async method or an async method.\n\n    The event will be bubbled up to the top-level Automa, where it will be processed by the event handler registered with the event type.\n\n    Parameters\n    ----------\n    event: Event\n        The event to be posted.\n    \"\"\"\n    def _handler_need_feedback_sender(handler: EventHandlerType):\n        positional_param_names = get_param_names_by_kind(handler, Parameter.POSITIONAL_ONLY) + get_param_names_by_kind(handler, Parameter.POSITIONAL_OR_KEYWORD)\n        var_positional_param_names = get_param_names_by_kind(handler, Parameter.VAR_POSITIONAL)\n        return len(var_positional_param_names) &gt; 0 or len(positional_param_names) &gt; 1\n\n    if self.parent is not None:\n        # Bubble up the event to the top-level Automa.\n        return self.parent.post_event(event)\n\n    # Here is the top-level Automa.\n    # Call event handlers\n    if event.event_type in self._event_handlers:\n        if _handler_need_feedback_sender(self._event_handlers[event.event_type]):\n            self._event_handlers[event.event_type](event, feedback_sender=None)\n        else:\n            self._event_handlers[event.event_type](event)\n    if self._default_event_handler is not None:\n        if _handler_need_feedback_sender(self._default_event_handler):\n            self._default_event_handler(event, feedback_sender=None)\n        else:\n            self._default_event_handler(event)\n</code></pre>"},{"location":"reference/bridgic-core/bridgic/core/automa/#bridgic.core.automa.Automa.request_feedback","title":"request_feedback","text":"<pre><code>request_feedback(\n    event: Event, timeout: Optional[float] = None\n) -&gt; Feedback\n</code></pre> <p>Request feedback for the specified event from the application layer outside the Automa. This method blocks the caller until the feedback is received.</p> <p>Note that <code>post_event</code> should only be called from within a non-async method running in the new thread of the Automa thread pool.</p> <p>Parameters:</p> Name Type Description Default <code>event</code> <code>Event</code> <p>The event to be posted to the event handler implemented by the application layer.</p> required <code>timeout</code> <code>Optional[float]</code> <p>A float or int number of seconds to wait for if the feedback is not received. If None, then there is no limit on the wait time.</p> <code>None</code> <p>Returns:</p> Type Description <code>Feedback</code> <p>The feedback received from the application layer.</p> <p>Raises:</p> Type Description <code>TimeoutError</code> <p>If the feedback is not received before the timeout. Note that the raised exception is the built-in <code>TimeoutError</code> exception, instead of asyncio.TimeoutError or concurrent.futures.TimeoutError!</p> Source code in <code>bridgic/core/automa/_automa.py</code> <pre><code>def request_feedback(\n    self, \n    event: Event,\n    timeout: Optional[float] = None\n) -&gt; Feedback:\n    \"\"\"\n    Request feedback for the specified event from the application layer outside the Automa. This method blocks the caller until the feedback is received.\n\n    Note that `post_event` should only be called from within a non-async method running in the new thread of the Automa thread pool.\n\n    Parameters\n    ----------\n    event: Event\n        The event to be posted to the event handler implemented by the application layer.\n    timeout: Optional[float]\n        A float or int number of seconds to wait for if the feedback is not received. If None, then there is no limit on the wait time.\n\n    Returns\n    -------\n    Feedback\n        The feedback received from the application layer.\n\n    Raises\n    ------\n    TimeoutError\n        If the feedback is not received before the timeout. Note that the raised exception is the built-in `TimeoutError` exception, instead of asyncio.TimeoutError or concurrent.futures.TimeoutError!\n    \"\"\"\n    if threading.get_ident() == self._main_thread_id:\n        raise AutomaRuntimeError(\n            f\"`request_feedback` should only be called in a different thread from the main thread of the {self.name}. \"\n        )\n    return asyncio.run_coroutine_threadsafe(\n        self.request_feedback_async(event, timeout),\n        self._main_loop\n    ).result()\n</code></pre>"},{"location":"reference/bridgic-core/bridgic/core/automa/#bridgic.core.automa.Automa.request_feedback_async","title":"request_feedback_async","text":"<code>async</code> <pre><code>request_feedback_async(\n    event: Event, timeout: Optional[float] = None\n) -&gt; Feedback\n</code></pre> <p>Request feedback for the specified event from the application layer outside the Automa. This method blocks the caller until the feedback is received.</p> <p>The event handler implemented by the application layer will be called in the next event loop, in the main thread.</p> <p>Parameters:</p> Name Type Description Default <code>event</code> <code>Event</code> <p>The event to be posted to the event handler implemented by the application layer.</p> required <code>timeout</code> <code>Optional[float]</code> <p>A float or int number of seconds to wait for if the feedback is not received. If None, then there is no limit on the wait time.</p> <code>None</code> <p>Returns:</p> Type Description <code>Feedback</code> <p>The feedback received from the application layer.</p> <p>Raises:</p> Type Description <code>TimeoutError</code> <p>If the feedback is not received before the timeout. Note that the raised exception is the built-in <code>TimeoutError</code> exception, instead of asyncio.TimeoutError!</p> Source code in <code>bridgic/core/automa/_automa.py</code> <pre><code>async def request_feedback_async(\n    self, \n    event: Event,\n    timeout: Optional[float] = None\n) -&gt; Feedback:\n    \"\"\"\n    Request feedback for the specified event from the application layer outside the Automa. This method blocks the caller until the feedback is received.\n\n    The event handler implemented by the application layer will be called in the next event loop, in the main thread.\n\n    Parameters\n    ----------\n    event: Event\n        The event to be posted to the event handler implemented by the application layer.\n    timeout: Optional[float]\n        A float or int number of seconds to wait for if the feedback is not received. If None, then there is no limit on the wait time.\n\n    Returns\n    -------\n    Feedback\n        The feedback received from the application layer.\n\n    Raises\n    ------\n    TimeoutError\n        If the feedback is not received before the timeout. Note that the raised exception is the built-in `TimeoutError` exception, instead of asyncio.TimeoutError!\n    \"\"\"\n    if self.parent is not None:\n        # Bubble up the event to the top-level Automa.\n        return await self.parent.request_feedback_async(event, timeout)\n\n    # Here is the top-level Automa.\n    event_loop = asyncio.get_running_loop()\n    future = event_loop.create_future()\n    feedback_sender = self._FeedbackSender(future, event_loop)\n    # Call event handlers\n    if event.event_type in self._event_handlers:\n        self._event_handlers[event.event_type](event, feedback_sender)\n    if self._default_event_handler is not None:\n        self._default_event_handler(event, feedback_sender)\n\n    try:\n        return await asyncio.wait_for(future, timeout)\n    except TimeoutError as e:\n        # When python &gt;= 3.11 here.\n        raise TimeoutError(f\"No feedback is received before timeout in Automa[{self.name}]\") from e\n    except asyncio.TimeoutError as e:\n        # Version compatibility resolution: asyncio.wait_for raises asyncio.TimeoutError before python 3.11.\n        # https://docs.python.org/3/library/asyncio-task.html#asyncio.wait_for\n        raise TimeoutError(f\"No feedback is received before timeout in Automa[{self.name}]\") from e\n</code></pre>"},{"location":"reference/bridgic-core/bridgic/core/automa/#bridgic.core.automa.Automa.interact_with_human","title":"interact_with_human","text":"<pre><code>interact_with_human(\n    event: Event,\n    interacting_worker: Optional[Worker] = None,\n) -&gt; InteractionFeedback\n</code></pre> <p>Trigger an interruption in the \"human-computer interaction\" during the execution of Automa.</p> <p>Parameters:</p> Name Type Description Default <code>event</code> <code>Event</code> <p>The event that triggered the interaction.</p> required <code>interacting_worker</code> <code>Optional[Worker]</code> <p>The worker that is currently interacting with human. If not provided, the worker will be located automatically.</p> <code>None</code> <p>Returns:</p> Type Description <code>InteractionFeedback</code> <p>The feedback received from the application layer.</p> <p>Raises:</p> Type Description <code>_InteractionEventException</code> <p>If the Automa is not the top-level Automa and the <code>interact_with_human()</code> method is called by  one or more workers, this exception will be raised to the upper level Automa.</p> Source code in <code>bridgic/core/automa/_automa.py</code> <pre><code>def interact_with_human(\n    self,\n    event: Event,\n    interacting_worker: Optional[Worker] = None,\n) -&gt; InteractionFeedback:\n    \"\"\"\n    Trigger an interruption in the \"human-computer interaction\" during the execution of Automa.\n\n    Parameters\n    ----------\n    event: Event\n        The event that triggered the interaction.\n    interacting_worker: Optional[Worker]\n        The worker that is currently interacting with human. If not provided, the worker will be located automatically.\n\n    Returns\n    -------\n    InteractionFeedback\n        The feedback received from the application layer.\n\n    Raises\n    ------\n    _InteractionEventException\n        If the Automa is not the top-level Automa and the `interact_with_human()` method is called by \n        one or more workers, this exception will be raised to the upper level Automa.\n    \"\"\"\n    if not interacting_worker:\n        kickoff_worker_key: str = self._locate_interacting_worker()\n    else:\n        kickoff_worker_key = self._get_worker_key(interacting_worker)\n\n    if kickoff_worker_key:\n        return self.interact_with_human_from_worker_key(event, kickoff_worker_key)\n    raise AutomaRuntimeError(\n        f\"Get kickoff worker failed in Automa[{self.name}] \"\n        f\"when trying to interact with human with event: {event}\"\n    )\n</code></pre>"},{"location":"reference/bridgic-core/bridgic/core/automa/#bridgic.core.automa.Automa.get_local_space","title":"get_local_space","text":"<pre><code>get_local_space(\n    runtime_context: RuntimeContext,\n) -&gt; Dict[str, Any]\n</code></pre> <p>Retrieve the local execution context (local space) associated with the current worker.  If you require the local space to be cleared after the completion of <code>automa.arun()</code>,  you may customize this behavior by overriding the <code>should_reset_local_space()</code> method.</p> <p>Parameters:</p> Name Type Description Default <code>runtime_context</code> <code>RuntimeContext</code> <p>The runtime context.</p> required <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>The local space.</p> Source code in <code>bridgic/core/automa/_automa.py</code> <pre><code>def get_local_space(self, runtime_context: RuntimeContext) -&gt; Dict[str, Any]:\n    \"\"\"\n    Retrieve the local execution context (local space) associated with the current worker. \n    If you require the local space to be cleared after the completion of `automa.arun()`, \n    you may customize this behavior by overriding the `should_reset_local_space()` method.\n\n    Parameters\n    ----------\n    runtime_context : RuntimeContext\n        The runtime context.\n\n    Returns\n    -------\n    Dict[str, Any]\n        The local space.\n    \"\"\"\n    worker_key = runtime_context.worker_key\n    worker_obj = self._get_worker_instance(worker_key)\n    return worker_obj.local_space\n</code></pre>"},{"location":"reference/bridgic-core/bridgic/core/automa/#bridgic.core.automa.Automa.should_reset_local_space","title":"should_reset_local_space","text":"<pre><code>should_reset_local_space() -&gt; bool\n</code></pre> <p>This method indicates whether to reset the local space at the end of the arun method of Automa.  By default, it returns True, standing for resetting. Otherwise, it means doing nothing.</p> Examples: <pre><code>class MyAutoma(Automa):\n    def should_reset_local_space(self) -&gt; bool:\n        return False\n</code></pre> Source code in <code>bridgic/core/automa/_automa.py</code> <pre><code>def should_reset_local_space(self) -&gt; bool:\n    \"\"\"\n    This method indicates whether to reset the local space at the end of the arun method of Automa. \n    By default, it returns True, standing for resetting. Otherwise, it means doing nothing.\n\n    Examples:\n    --------\n    ```python\n    class MyAutoma(Automa):\n        def should_reset_local_space(self) -&gt; bool:\n            return False\n    ```\n    \"\"\"\n    return True\n</code></pre>"},{"location":"reference/bridgic-core/bridgic/core/automa/#bridgic.core.automa.Snapshot","title":"Snapshot","text":"<p>               Bases: <code>BaseModel</code></p> <p>A snapshot that represents the current state of an Automa. It is used when an Automa resumes after a human interaction.</p> Source code in <code>bridgic/core/automa/_automa.py</code> <pre><code>class Snapshot(BaseModel):\n    \"\"\"\n    A snapshot that represents the current state of an Automa. It is used when an Automa resumes after a human interaction.\n    \"\"\"\n    serialized_bytes: bytes\n    \"\"\"\n    The serialized bytes that represents the snapshot of the Automa.\n    \"\"\"\n    serialization_version: str\n    \"\"\"\n    The serialization version.\n    \"\"\"\n</code></pre>"},{"location":"reference/bridgic-core/bridgic/core/automa/#bridgic.core.automa.Snapshot.serialized_bytes","title":"serialized_bytes  <code>instance-attribute</code>","text":"<pre><code>serialized_bytes: bytes\n</code></pre> <p>The serialized bytes that represents the snapshot of the Automa.</p>"},{"location":"reference/bridgic-core/bridgic/core/automa/#bridgic.core.automa.Snapshot.serialization_version","title":"serialization_version  <code>instance-attribute</code>","text":"<pre><code>serialization_version: str\n</code></pre> <p>The serialization version.</p>"},{"location":"reference/bridgic-core/bridgic/core/automa/#bridgic.core.automa.GraphAutoma","title":"GraphAutoma","text":"<p>               Bases: <code>Automa</code></p> <p>Dynamic Directed Graph (abbreviated as DDG) implementation of Automa. <code>GraphAutoma</code> manages  the running control flow between workers automatically, via <code>dependencies</code> and <code>ferry_to</code>. Outputs of workers can be mapped and passed to their successor workers in the runtime,  following <code>args_mapping_rule</code>.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>Optional[str]</code> <p>The name of the automa.</p> <code>None</code> <code>thread_pool</code> <code>Optional[ThreadPoolExecutor]</code> <p>The thread pool for parallel running of I/O-bound tasks.</p> <ul> <li> <p>If not provided, a default thread pool will be used. The maximum number of threads in the default thread pool dependends on the number of CPU cores. Please refer to  the ThreadPoolExecutor for detail.</p> </li> <li> <p>If provided, all workers (including all nested Automa instances) will be run in it. In this case, the  application layer code is responsible to create it and shut it down.</p> </li> </ul> <code>None</code> <p>Examples:</p> <p>The following example shows how to use <code>GraphAutoma</code> to create a simple graph automa that prints \"Hello, Bridgic\".</p> <pre><code>import asyncio\nfrom bridgic.core.automa import GraphAutoma, worker, ArgsMappingRule\n\nclass MyGraphAutoma(GraphAutoma):\n    @worker(is_start=True)\n    async def greet(self) -&gt; list[str]:\n        return [\"Hello\", \"Bridgic\"]\n\n    @worker(dependencies=[\"greet\"], args_mapping_rule=ArgsMappingRule.AS_IS, is_output=True)\n    async def output(self, message: list[str]):\n        print(\"Echo: \" + \" \".join(message))\n\nasync def main():\n    automa_obj = MyGraphAutoma(name=\"my_graph_automa\")\n    await automa_obj.arun()\n\nasyncio.run(main())\n</code></pre> Source code in <code>bridgic/core/automa/_graph_automa.py</code> <pre><code>class GraphAutoma(Automa, metaclass=GraphMeta):\n    \"\"\"\n    Dynamic Directed Graph (abbreviated as DDG) implementation of Automa. `GraphAutoma` manages \n    the running control flow between workers automatically, via `dependencies` and `ferry_to`.\n    Outputs of workers can be mapped and passed to their successor workers in the runtime, \n    following `args_mapping_rule`.\n\n    Parameters\n    ----------\n    name : Optional[str]\n        The name of the automa.\n\n    thread_pool : Optional[ThreadPoolExecutor]\n        The thread pool for parallel running of I/O-bound tasks.\n\n        - If not provided, a default thread pool will be used.\n        The maximum number of threads in the default thread pool dependends on the number of CPU cores. Please refer to \n        the [ThreadPoolExecutor](https://docs.python.org/3/library/concurrent.futures.html#threadpoolexecutor) for detail.\n\n        - If provided, all workers (including all nested Automa instances) will be run in it. In this case, the \n        application layer code is responsible to create it and shut it down.\n\n    Examples\n    --------\n\n    The following example shows how to use `GraphAutoma` to create a simple graph automa that prints \"Hello, Bridgic\".\n\n    ```python\n    import asyncio\n    from bridgic.core.automa import GraphAutoma, worker, ArgsMappingRule\n\n    class MyGraphAutoma(GraphAutoma):\n        @worker(is_start=True)\n        async def greet(self) -&gt; list[str]:\n            return [\"Hello\", \"Bridgic\"]\n\n        @worker(dependencies=[\"greet\"], args_mapping_rule=ArgsMappingRule.AS_IS, is_output=True)\n        async def output(self, message: list[str]):\n            print(\"Echo: \" + \" \".join(message))\n\n    async def main():\n        automa_obj = MyGraphAutoma(name=\"my_graph_automa\")\n        await automa_obj.arun()\n\n    asyncio.run(main())\n    ```\n    \"\"\"\n\n    # Automa type.\n    AUTOMA_TYPE: ClassVar[AutomaType] = AutomaType.Graph\n\n    # The initial topology defined by @worker functions.\n    _registered_worker_funcs: ClassVar[Dict[str, Callable]] = {}\n\n    # IMPORTANT: The entire states of a GraphAutoma instance include 2 part:\n    # \n    # Part-1 (for the states of topology structure):\n    #   1. Inner worker instances: self._workers\n    #   2. Relations between worker: self._worker_forwards\n    #   3. Dynamic states that serve as trigger of execution of workers: self._workers_dynamic_states\n    #   4. Execution result of inner workers: self._worker_output\n    #   5. Configurations of this automa instance: self._output_worker_key\n    # \n    # Part-2 (for the states of running states):\n    #   1. Records of Workers that are going to be kicked off: self._current_kickoff_workers\n    #   2. Records of running or deferred tasks:\n    #      - self._running_tasks\n    #      - self._topology_change_deferred_tasks\n    #      - self._ferry_deferred_tasks\n    #      - self._set_output_worker_deferred_task\n    #   3. Buffers of automa inputs: self._input_buffer\n    #   4. Ongoing human interactions: self._ongoing_interactions\n    #   ...\n\n    _workers: Dict[str, _GraphAdaptedWorker]\n    _worker_output: Dict[str, Any]\n    _worker_forwards: Dict[str, List[str]]\n\n    _current_kickoff_workers: List[_KickoffInfo]\n    _input_buffer: _AutomaInputBuffer\n    _workers_dynamic_states: Dict[str, _WorkerDynamicState]\n\n    # The whole running process of the DDG is divided into two main phases:\n    # 1. [Initialization Phase] The first phase (when _automa_running is False): the initial topology of DDG was constructed.\n    # 2. [Running Phase] The second phase (when _automa_running is True): the DDG is running, and the workers are executed in a dynamic step-by-step manner (DS loop).\n    _automa_running: bool\n\n    #########################################################\n    #### The following fields need not to be serialized. ####\n    #########################################################\n    _running_tasks: List[_RunnningTask]\n\n    # TODO: The following deferred task structures need to be thread-safe.\n    # TODO: Need to be refactored when parallelization features are added.\n    _topology_change_deferred_tasks: List[Union[_AddWorkerDeferredTask, _RemoveWorkerDeferredTask]]\n    _ferry_deferred_tasks: List[_FerryDeferredTask]\n\n    def __init__(\n        self,\n        name: Optional[str] = None,\n        thread_pool: Optional[ThreadPoolExecutor] = None,\n    ):\n        \"\"\"\n        Parameters\n        ----------\n        name : Optional[str]\n            The name of the automa.\n\n        thread_pool : Optional[ThreadPoolExecutor]\n            The thread pool for parallel running of I/O-bound tasks.\n\n            - If not provided, a default thread pool will be used.\n            The maximum number of threads in the default thread pool dependends on the number of CPU cores. Please refer to \n            the [ThreadPoolExecutor](https://docs.python.org/3/library/concurrent.futures.html#threadpoolexecutor) for detail.\n\n            - If provided, all workers (including all nested Automa instances) will be run in it. In this case, the \n            application layer code is responsible to create it and shut it down.\n\n        state_dict : Optional[Dict[str, Any]]\n            A dictionary for initializing the automa's runtime states. This parameter is designed for framework use only.\n        \"\"\"\n        super().__init__(name=name, thread_pool=thread_pool)\n\n        self._workers = {}\n        self._worker_outputs = {}\n        self._automa_running = False\n\n        # Initialize the states that need to be serialized.\n        self._normal_init()\n\n        # The list of the tasks that are currently being executed.\n        self._running_tasks = []\n        # deferred tasks\n        self._topology_change_deferred_tasks = []\n        self._ferry_deferred_tasks = []\n\n    def _normal_init(self):\n        ###############################################################################\n        # Initialization of [Part One: Topology-Related Runtime States] #### Strat ####\n        ###############################################################################\n\n        cls = type(self)\n\n        # _workers, _worker_forwards and _workers_dynamic_states will be initialized incrementally by add_worker()...\n        self._worker_forwards = {}\n        self._worker_output = {}\n        self._workers_dynamic_states = {}\n\n        if cls.AUTOMA_TYPE == AutomaType.Graph:\n            # The _registered_worker_funcs data are from @worker decorators.\n            for worker_key, worker_func in cls._registered_worker_funcs.items():\n                # The decorator based mechanism (i.e. @worker) is based on the add_worker() interface.\n                # Parameters check and other implementation details can be unified.\n                self._add_func_as_worker_internal(\n                    key=worker_key,\n                    func=worker_func,\n                    dependencies=worker_func.__dependencies__,\n                    is_start=worker_func.__is_start__,\n                    is_output=worker_func.__is_output__,\n                    args_mapping_rule=worker_func.__args_mapping_rule__,\n                )\n\n        ###############################################################################\n        # Initialization of [Part One: Topology-Related Runtime States] ##### End #####\n        ###############################################################################\n\n        ###############################################################################\n        # Initialization of [Part Two: Task-Related Runtime States] ###### Strat ######\n        ###############################################################################\n\n        # -- Current kickoff workers list.\n        # The key list of the workers that are ready to be immediately executed in the next DS (Dynamic Step). It will be lazily initialized in _compile_graph_and_detect_risks().\n        self._current_kickoff_workers = []\n        # -- Automa input buffer.\n        self._input_buffer = _AutomaInputBuffer()\n\n\n        ###############################################################################\n        # Initialization of [Part Two: Task-Related Runtime States] ####### End #######\n        ###############################################################################\n\n    ###############################################################\n    ########## [Bridgic Serialization Mechanism] starts ###########\n    ###############################################################\n\n    # The version of the serialization format.\n    SERIALIZATION_VERSION: str = \"1.0\"\n\n    @override\n    def dump_to_dict(self) -&gt; Dict[str, Any]:\n        state_dict = super().dump_to_dict()\n\n        state_dict[\"name\"] = self.name\n        state_dict[\"automa_running\"] = self._automa_running\n\n        # States related to workers.\n        state_dict[\"workers\"] = self._workers\n        state_dict[\"worker_forwards\"] = self._worker_forwards\n        state_dict[\"workers_dynamic_states\"] = self._workers_dynamic_states\n        state_dict[\"worker_output\"] = self._worker_output\n\n        # States related to interruption recovery.\n        state_dict[\"current_kickoff_workers\"] = self._current_kickoff_workers\n        state_dict[\"input_buffer\"] = self._input_buffer\n\n        return state_dict\n\n    @override\n    def load_from_dict(self, state_dict: Dict[str, Any]) -&gt; None:\n        super().load_from_dict(state_dict)\n\n        self.name = state_dict[\"name\"]\n        self._automa_running = state_dict[\"automa_running\"]\n\n        # States related to workers.\n        self._workers = state_dict[\"workers\"]\n        for worker in self._workers.values():\n            worker.parent = self\n        self._worker_forwards = state_dict[\"worker_forwards\"]\n        self._workers_dynamic_states = state_dict[\"workers_dynamic_states\"]\n        self._worker_output = state_dict[\"worker_output\"]\n\n        # States related to interruption recovery.\n        self._current_kickoff_workers = state_dict[\"current_kickoff_workers\"]\n        self._input_buffer = state_dict[\"input_buffer\"]\n\n        # The list of the tasks that are currently being executed.\n        self._running_tasks = []\n        # Deferred tasks\n        self._topology_change_deferred_tasks = []\n        self._set_output_worker_deferred_task = None\n        self._ferry_deferred_tasks = []\n\n    ###############################################################\n    ########### [Bridgic Serialization Mechanism] ends ############\n    ###############################################################\n\n    def _add_worker_incrementally(\n        self,\n        key: str,\n        worker: Worker,\n        *,\n        dependencies: List[str] = [],\n        is_start: bool = False,\n        is_output: bool = False,\n        args_mapping_rule: ArgsMappingRule = ArgsMappingRule.AS_IS,\n    ) -&gt; None:\n        \"\"\"\n        Incrementally add a worker into the automa. For internal use only.\n        This method is one of the very basic primitives of DDG for dynamic topology changes. \n        \"\"\"\n        if key in self._workers:\n            raise AutomaRuntimeError(\n                f\"duplicate workers with the same key '{key}' are not allowed to be added!\"\n            )\n\n        # Note: the dependencies argument must be a new copy of the list, created with list(dependencies).\n        # Refer to the Python documentation for more details:\n        # 1. https://docs.python.org/3/reference/compound_stmts.html#function-definitions\n        # \"Default parameter values are evaluated from left to right when the function definition is executed\"\n        # 2. https://docs.python.org/3/tutorial/controlflow.html#default-argument-values\n        # \"The default values are evaluated at the point of function definition in the defining scope\"\n        # \"Important warning: The default value is evaluated only once.\"\n        new_worker_obj = _GraphAdaptedWorker(\n            key=key,\n            worker=worker,\n            dependencies=list(dependencies),\n            is_start=is_start,\n            is_output=is_output,\n            args_mapping_rule=args_mapping_rule,\n        )\n\n        # Register the worker_obj.\n        new_worker_obj.parent = self\n        self._workers[new_worker_obj.key] = new_worker_obj\n\n        # Incrementally update the dynamic states of added workers.\n        self._workers_dynamic_states[key] = _WorkerDynamicState(\n            dependency_triggers=set(dependencies)\n        )\n\n        # Incrementally update the forwards table.\n        for trigger in dependencies:\n            if trigger not in self._worker_forwards:\n                self._worker_forwards[trigger] = []\n            self._worker_forwards[trigger].append(key)\n\n        # TODO : Validation may be needed in appropriate time later, because we are not able to guarantee \n        # the existence of the trigger-workers in self._workers of the final automa graph after dynamically changing.\n\n    def _remove_worker_incrementally(\n        self,\n        key: str\n    ) -&gt; None:\n        \"\"\"\n        Incrementally remove a worker from the automa. For internal use only.\n        This method is one of the very basic primitives of DDG for dynamic topology changes.\n        \"\"\"\n        if key not in self._workers:\n            raise AutomaRuntimeError(\n                f\"fail to remove worker '{key}' that does not exist!\"\n            )\n\n        worker_to_remove = self._workers[key]\n\n        # Remove the worker.\n        del self._workers[key]\n        # Incrementally update the dynamic states of removed workers.\n        del self._workers_dynamic_states[key]\n\n        if key in self._worker_forwards:\n            # Update the dependencies of the successor workers, if needed.\n            for successor in self._worker_forwards[key]:\n                self._workers[successor].dependencies.remove(key)\n                # Note this detail here: use discard() instead of remove() to avoid KeyError.\n                # This case occurs when a worker call remove_worker() to remove its predecessor worker.\n                self._workers_dynamic_states[successor].dependency_triggers.discard(key)\n            # Incrementally update the forwards table.\n            del self._worker_forwards[key]\n\n        # Remove from the forwards list of all dependencies worker.\n        for trigger in worker_to_remove.dependencies:\n            self._worker_forwards[trigger].remove(key)\n        if key in self._worker_interaction_indices:\n            del self._worker_interaction_indices[key]\n        if key in self._ongoing_interactions:\n            del self._ongoing_interactions[key]\n\n    def _add_dependency_incrementally(\n        self,\n        key: str,\n        dependency: str,\n    ) -&gt; None:\n        \"\"\"\n        Incrementally add a dependency from `key` to `depends`. For internal use only.\n        This method is one of the very basic primitives of DDG for dynamic topology changes.\n        \"\"\"\n        if key not in self._workers:\n            raise AutomaRuntimeError(\n                f\"fail to add dependency from a worker that does not exist: `{key}`!\"\n            )\n        if dependency not in self._workers:\n            raise AutomaRuntimeError(\n                f\"fail to add dependency to a worker that does not exist: `{dependency}`!\"\n            )\n        if dependency in self._workers[key].dependencies:\n            raise AutomaRuntimeError(\n                f\"dependency from '{key}' to '{dependency}' already exists!\"\n            )\n\n        self._workers[key].dependencies.append(dependency)\n        # Note this detail here for dynamic states change:\n        # The new dependency added here may be removed right away if the dependency is just the next kickoff worker. This is a valid behavior.\n        self._workers_dynamic_states[key].dependency_triggers.add(dependency)\n\n        if dependency not in self._worker_forwards:\n            self._worker_forwards[dependency] = []\n        self._worker_forwards[dependency].append(key)\n\n    def _add_worker_internal(\n        self,\n        key: str,\n        worker: Worker,\n        *,\n        dependencies: List[str] = [],\n        is_start: bool = False,\n        is_output: bool = False,\n        args_mapping_rule: ArgsMappingRule = ArgsMappingRule.AS_IS,\n    ) -&gt; None:\n        \"\"\"\n        The private version of the method `add_worker()`.\n        \"\"\"\n\n        def _basic_worker_params_check(key: str, worker_obj: Worker):\n            if not isinstance(worker_obj, Worker):\n                raise TypeError(\n                    f\"worker_obj to be registered must be a Worker, \"\n                    f\"but got {type(worker_obj)} for worker '{key}'\"\n                )\n\n            if not asyncio.iscoroutinefunction(worker_obj.arun):\n                raise WorkerSignatureError(\n                    f\"arun of Worker must be an async method, \"\n                    f\"but got {type(worker_obj.arun)} for worker '{key}'\"\n                )\n\n            if not isinstance(dependencies, list):\n                raise TypeError(\n                    f\"dependencies must be a list, \"\n                    f\"but got {type(dependencies)} for worker '{key}'\"\n                )\n            if not all([isinstance(d, str) for d in dependencies]):\n                raise ValueError(\n                    f\"dependencies must be a List of str, \"\n                    f\"but got {dependencies} for worker {key}\"\n                )\n\n            if args_mapping_rule not in ArgsMappingRule:\n                raise ValueError(\n                    f\"args_mapping_rule must be one of the following: {[e for e in ArgsMappingRule]}, \"\n                    f\"but got {args_mapping_rule} for worker {key}\"\n                )\n\n        # Ensure the parameters are valid.\n        _basic_worker_params_check(key, worker)\n\n        if not self._automa_running:\n            # Add worker during the [Initialization Phase].\n            self._add_worker_incrementally(\n                key=key,\n                worker=worker,\n                dependencies=dependencies,\n                is_start=is_start,\n                is_output=is_output,\n                args_mapping_rule=args_mapping_rule,\n            )\n        else:\n            # Add worker during the [Running Phase].\n            deferred_task = _AddWorkerDeferredTask(\n                worker_key=key,\n                worker_obj=worker,\n                dependencies=dependencies,\n                is_start=is_start,\n                is_output=is_output,\n                args_mapping_rule=args_mapping_rule,\n            )\n            # Note1: the execution order of topology change deferred tasks is important and is determined by the order of the calls of add_worker(), remove_worker() and add_dependency() in one DS.\n            # Note2: add_worker() and remove_worker() may be called in a new thread. But _topology_change_deferred_tasks is not necessary to be thread-safe due to Visibility Guarantees of the Bridgic Concurrency Model.\n            self._topology_change_deferred_tasks.append(deferred_task)\n\n    def _add_func_as_worker_internal(\n        self,\n        key: str,\n        func: Callable,\n        *,\n        dependencies: List[str] = [],\n        is_start: bool = False,\n        is_output: bool = False,\n        args_mapping_rule: ArgsMappingRule = ArgsMappingRule.AS_IS,\n    ) -&gt; None:\n        \"\"\"\n        The private version of the method `add_func_as_worker()`.\n        \"\"\"\n        if not isinstance(func, MethodType) and key in self._registered_worker_funcs:\n            func = MethodType(func, self)\n\n        # Validate: if func is a method, its bounded __self__ must be self when add_func_as_worker() is called.\n        if hasattr(func, \"__self__\") and func.__self__ is not self:\n            raise AutomaRuntimeError(\n                f\"the bounded instance of `func` must be the same as the instance of the GraphAutoma, \"\n                f\"but got {func.__self__}\"\n            )\n\n        # Register func as an instance of CallableWorker.\n        func_worker = CallableWorker(func)\n\n        self._add_worker_internal(\n            key=key,\n            worker=func_worker,\n            dependencies=dependencies,\n            is_start=is_start,\n            is_output=is_output,\n            args_mapping_rule=args_mapping_rule,\n        )\n\n    def all_workers(self) -&gt; List[str]:\n        \"\"\"\n        Gets a list containing the keys of all workers registered in this Automa.\n\n        Returns\n        -------\n        List[str]\n            A list of worker keys.\n        \"\"\"\n        return list(self._workers.keys())\n\n    def add_worker(\n        self,\n        key: str,\n        worker: Worker,\n        *,\n        dependencies: List[str] = [],\n        is_start: bool = False,\n        is_output: bool = False,\n        args_mapping_rule: ArgsMappingRule = ArgsMappingRule.AS_IS,\n    ) -&gt; None:\n        \"\"\"\n        This method is used to add a worker dynamically into the automa.\n\n        If this method is called during the [Initialization Phase], the worker will be added immediately. If this method is called during the [Running Phase], the worker will be added as a deferred task which will be executed in the next DS.\n\n        The dependencies can be added together with a worker. However, you can add a worker without any dependencies.\n\n        Note: The args_mapping_rule can only be set together with adding a worker, even if the worker has no any dependencies.\n\n        Parameters\n        ----------\n        key : str\n            The key of the worker.\n        worker : Worker\n            The worker instance to be registered.\n        dependencies : List[str]\n            A list of worker keys that the worker depends on.\n        is_start : bool\n            Whether the worker is a start worker.\n        is_output : bool\n            Whether the worker is an output worker.\n        args_mapping_rule : ArgsMappingRule\n            The rule of arguments mapping.\n        \"\"\"\n        self._add_worker_internal(\n            key=key,\n            worker=worker,\n            dependencies=dependencies,\n            is_start=is_start,\n            is_output=is_output,\n            args_mapping_rule=args_mapping_rule,\n        )\n\n    def add_func_as_worker(\n        self,\n        key: str,\n        func: Callable,\n        *,\n        dependencies: List[str] = [],\n        is_start: bool = False,\n        is_output: bool = False,\n        args_mapping_rule: ArgsMappingRule = ArgsMappingRule.AS_IS,\n    ) -&gt; None:\n        \"\"\"\n        This method is used to add a function as a worker into the automa.\n\n        The format of the parameters will follow that of the decorator @worker(...), so that the \n        behavior of the decorated function is consistent with that of normal CallableLandableWorker objects.\n\n        Parameters\n        ----------\n        key : str\n            The key of the function worker.\n        func : Callable\n            The function to be added as a worker to the automa.\n        dependencies : List[str]\n            A list of worker names that the decorated callable depends on.\n        is_start : bool\n            Whether the decorated callable is a start worker. True means it is, while False means it is not.\n        is_output : bool\n            Whether the decorated callable is an output worker. True means it is, while False means it is not.\n        args_mapping_rule : ArgsMappingRule\n            The rule of arguments mapping.\n        \"\"\"\n        self._add_func_as_worker_internal(\n            key=key,\n            func=func,\n            dependencies=dependencies,\n            is_start=is_start,\n            is_output=is_output,\n            args_mapping_rule=args_mapping_rule,\n        )\n\n    def worker(\n        self,\n        *,\n        key: Optional[str] = None,\n        dependencies: List[str] = [],\n        is_start: bool = False,\n        is_output: bool = False,\n        args_mapping_rule: ArgsMappingRule = ArgsMappingRule.AS_IS,\n    ) -&gt; Callable:\n        \"\"\"\n        This is a decorator used to mark a function as an GraphAutoma detectable Worker. Dislike the \n        global decorator @worker(...), it is usally used after an GraphAutoma instance is initialized.\n\n        The format of the parameters will follow that of the decorator @worker(...), so that the \n        behavior of the decorated function is consistent with that of normal CallableLandableWorker objects.\n\n        Parameters\n        ----------\n        key : str\n            The key of the worker. If not provided, the name of the decorated callable will be used.\n        dependencies : List[str]\n            A list of worker names that the decorated callable depends on.\n        is_start : bool\n            Whether the decorated callable is a start worker. True means it is, while False means it is not.\n        is_output : bool\n            Whether the decorated callable is an output worker. True means it is, while False means it is not.\n        args_mapping_rule : str\n            The rule of arguments mapping. The options are: \"auto\", \"as_list\", \"as_dict\", \"suppressed\".\n        \"\"\"\n        def wrapper(func: Callable):\n            self._add_func_as_worker_internal(\n                key=(key or func.__name__),\n                func=func,\n                dependencies=dependencies,\n                is_start=is_start,\n                is_output=is_output,\n                args_mapping_rule=args_mapping_rule,\n            )\n\n        return wrapper\n\n    def remove_worker(self, key: str) -&gt; None:\n        \"\"\"\n        Remove a worker from the Automa. This method can be called at any time to remove a worker from the Automa.\n\n        When a worker is removed, all dependencies related to this worker, including all the dependencies of the worker itself and the dependencies between the worker and its successor workers, will be also removed.\n\n        Parameters\n        ----------\n        key : str\n            The key of the worker to be removed.\n\n        Returns\n        -------\n        None\n\n        Raises\n        ------\n        AutomaDeclarationError\n            If the worker specified by key does not exist in the Automa, this exception will be raised.\n        \"\"\"\n        if not self._automa_running:\n            # remove immediately\n            self._remove_worker_incrementally(key)\n        else:\n            deferred_task = _RemoveWorkerDeferredTask(\n                worker_key=key,\n            )\n            # Note: the execution order of topology change deferred tasks is important and is determined by the order of the calls of add_worker(), remove_worker() and add_dependency() in one DS.\n            self._topology_change_deferred_tasks.append(deferred_task)\n\n    def add_dependency(\n        self,\n        key: str,\n        dependency: str,\n    ) -&gt; None:\n        \"\"\"\n        This method is used to dynamically add a dependency from `key` to `dependency`.\n\n        Note: args_mapping_rule is not allowed to be set by this method, instead it should be set together with add_worker() or add_func_as_worker().\n\n        Parameters\n        ----------\n        key : str\n            The key of the worker that will depend on the worker with key `dependency`.\n        dependency : str\n            The key of the worker on which the worker with key `key` will depend.\n        \"\"\"\n        ...\n        if not self._automa_running:\n            # add the dependency immediately\n            self._add_dependency_incrementally(key, dependency)\n        else:\n            deferred_task = _AddDependencyDeferredTask(\n                worker_key=key,\n                dependency=dependency,\n            )\n            # Note: the execution order of topology change deferred tasks is important and is determined by the order of the calls of add_worker(), remove_worker() and add_dependency() in one DS.\n            self._topology_change_deferred_tasks.append(deferred_task)\n\n    def _validate_canonical_graph(self):\n        \"\"\"\n        This method is used to validate that DDG graph is canonical.\n        \"\"\"\n        for worker_key, worker_obj in self._workers.items():\n            for dependency_key in worker_obj.dependencies:\n                if dependency_key not in self._workers:\n                    raise AutomaCompilationError(\n                        f\"the dependency `{dependency_key}` of worker `{worker_key}` does not exist\"\n                    )\n        assert set(self._workers.keys()) == set(self._workers_dynamic_states.keys())\n        for worker_key, worker_dynamic_state in self._workers_dynamic_states.items():\n            for dependency_key in worker_dynamic_state.dependency_triggers:\n                assert dependency_key in self._workers[worker_key].dependencies\n\n        for worker_key, worker_obj in self._workers.items():\n            for dependency_key in worker_obj.dependencies:\n                assert worker_key in self._worker_forwards[dependency_key]\n        for worker_key, successor_keys in self._worker_forwards.items():\n            for successor_key in successor_keys:\n                assert worker_key in self._workers[successor_key].dependencies\n\n    def _compile_graph_and_detect_risks(self):\n        \"\"\"\n        This method should be called at the very beginning of self.run() to ensure that:\n        1. The whole graph is built out of all of the following worker sources:\n            - Pre-defined workers, such as:\n                - Methods decorated with @worker(...)\n            - Post-added workers, such as:\n                - Functions decorated with @automa_obj.worker(...)\n                - Workers added via automa_obj.add_func_as_worker(...)\n                - Workers added via automa_obj.add_worker(...)\n        2. The dependencies of each worker are confirmed to satisfy the DAG constraints.\n        \"\"\"\n\n        # Validate the canonical graph.\n        self._validate_canonical_graph()\n        # Validate the DAG constraints.\n        GraphMeta.validate_dag_constraints(self._worker_forwards)\n        # TODO: More validations can be added here...\n\n        # Find all connected components of the whole automa graph.\n        self._find_connected_components()\n\n    def ferry_to(self, key: str, /, *args, **kwargs):\n        \"\"\"\n        Defer the invocation to the specified worker, passing any provided arguments. This creates a \n        delayed call, ensuring the worker will be scheduled to run asynchronously in the next event loop, \n        independent of its dependencies.\n\n        This primitive is commonly used for:\n\n        1. Implementing dynamic branching based on runtime conditions.\n        2. Creating logic that forms cyclic graphs.\n\n        Parameters\n        ----------\n        key : str\n            The key of the worker to run.\n        args : optional\n            Positional arguments to be passed.\n        kwargs : optional\n            Keyword arguments to be passed.\n\n        Examples\n        --------\n        ```python\n        class MyGraphAutoma(GraphAutoma):\n            @worker(is_start=True)\n            def start_worker(self):\n                number = random.randint(0, 1)\n                if number == 0:\n                    self.ferry_to(\"cond_1_worker\", number=number)\n                else:\n                    self.ferry_to(\"cond_2_worker\")\n\n            @worker()\n            def cond_1_worker(self, number: int):\n                print(f'Got {{number}}!')\n\n            @worker()\n            def cond_2_worker(self):\n                self.ferry_to(\"start_worker\")\n\n        automa = MyGraphAutoma()\n        await automa.arun()\n\n        # Output: Got 0!\n        ```\n        \"\"\"\n        # TODO: check worker_key is valid, maybe deferred check...\n        running_options = self._get_top_running_options()\n        # if debug is enabled, trace back the kickoff worker key from stacktrace.\n        kickoff_worker_key: str = self._trace_back_kickoff_worker_key_from_stack() if running_options.debug else None\n        deferred_task = _FerryDeferredTask(\n            ferry_to_worker_key=key,\n            kickoff_worker_key=kickoff_worker_key,\n            args=args,\n            kwargs=kwargs,\n        )\n        # Note: ferry_to() may be called in a new thread.\n        # But _ferry_deferred_tasks is not necessary to be thread-safe due to Visibility Guarantees of the Bridgic Concurrency Model.\n        self._ferry_deferred_tasks.append(deferred_task)\n\n    def _clean_all_worker_local_space(self):\n        \"\"\"\n        Clean the local space of all workers.\n        \"\"\"\n        for worker_obj in self._workers.values():\n            worker_obj.local_space = {}\n\n    async def arun(\n        self, \n        *args: Tuple[Any, ...],\n        interaction_feedback: Optional[InteractionFeedback] = None,\n        interaction_feedbacks: Optional[List[InteractionFeedback]] = None,\n        **kwargs: Dict[str, Any]\n    ) -&gt; Any:\n        \"\"\"\n        The entry point for running the constructed `GraphAutoma` instance.\n\n        Parameters\n        ----------\n        args : optional\n            Positional arguments to be passed.\n\n        interaction_feedback : Optional[InteractionFeedback]\n            Feedback that is received from a human interaction. Only one of interaction_feedback or \n            interaction_feedbacks should be provided at a time.\n\n        interaction_feedbacks : Optional[List[InteractionFeedback]]\n            Feedbacks that are received from multiple interactions occurred simultaneously before the Automa \n            was paused. Only one of interaction_feedback or interaction_feedbacks should be provided at a time.\n\n        kwargs : optional\n            Keyword arguments to be passed.\n\n        Returns\n        -------\n        Any\n            The execution result of the output-worker that has the setting `is_output=True`, otherwise None.\n\n        Raises\n        ------\n        InteractionException\n            If the Automa is the top-level Automa and the `interact_with_human()` method is called by \n            one or more workers, this exception will be raised to the application layer.\n\n        _InteractionEventException\n            If the Automa is not the top-level Automa and the `interact_with_human()` method is called by \n            one or more workers, this exception will be raised to the upper level Automa.\n        \"\"\"\n\n        def _reinit_current_kickoff_workers_if_needed():\n            # Note: After deserialization, the _current_kickoff_workers must not be empty!\n            # Therefore, _current_kickoff_workers will only be reinitialized when the Automa is run for the first time or rerun.\n            # It is guaranteed that _current_kickoff_workers will not be reinitialized when the Automa is resumed after deserialization.\n            if not self._current_kickoff_workers:\n                self._current_kickoff_workers = [\n                    _KickoffInfo(\n                        worker_key=worker_key,\n                        last_kickoff=\"__automa__\"\n                    ) for worker_key, worker_obj in self._workers.items()\n                    if getattr(worker_obj, \"is_start\", False)\n                ]\n                # Each time the Automa re-runs, buffer the input arguments here.\n                self._input_buffer.args = args\n                self._input_buffer.kwargs = kwargs\n\n        def _execute_topology_change_deferred_tasks(tc_tasks: List[Union[_AddWorkerDeferredTask, _RemoveWorkerDeferredTask, _AddDependencyDeferredTask]]):\n            for topology_task in tc_tasks:\n                if topology_task.task_type == \"add_worker\":\n                    self._add_worker_incrementally(\n                        key=topology_task.worker_key,\n                        worker=topology_task.worker_obj,\n                        dependencies=topology_task.dependencies,\n                        is_start=topology_task.is_start,\n                        is_output=topology_task.is_output,\n                        args_mapping_rule=topology_task.args_mapping_rule,\n                    )\n                elif topology_task.task_type == \"remove_worker\":\n                    self._remove_worker_incrementally(topology_task.worker_key)\n                elif topology_task.task_type == \"add_dependency\":\n                    self._add_dependency_incrementally(topology_task.worker_key, topology_task.dependency)\n\n        def _set_worker_run_finished(worker_key: str):\n            for kickoff_info in self._current_kickoff_workers:\n                if kickoff_info.worker_key == worker_key:\n                    kickoff_info.run_finished = True\n                    break\n\n        def _check_and_normalize_interaction_params(\n            interaction_feedback: Optional[InteractionFeedback] = None,\n            interaction_feedbacks: Optional[List[InteractionFeedback]] = None,\n        ):\n            if interaction_feedback and interaction_feedbacks:\n                raise AutomaRuntimeError(\n                    f\"Only one of interaction_feedback or interaction_feedbacks can be used. \"\n                    f\"But received interaction_feedback={interaction_feedback} and \\n\"\n                    f\"interaction_feedbacks={interaction_feedbacks}\"\n                )\n            if interaction_feedback:\n                rx_feedbacks = [interaction_feedback]\n            else:\n                rx_feedbacks = interaction_feedbacks\n            return rx_feedbacks\n\n        def _match_ongoing_interaction_and_feedbacks(rx_feedbacks:List[InteractionFeedback]):\n            match_left_feedbacks = []\n            for feedback in rx_feedbacks:\n                matched = False\n                for interaction_and_feedbacks in self._ongoing_interactions.values():\n                    for interaction_and_feedback in interaction_and_feedbacks:\n                        if interaction_and_feedback.interaction.interaction_id == feedback.interaction_id:\n                            matched = True\n                            # Note: Only one feedback is allowed for each interaction. Here we assume that only the first feedback is valid, which is a choice of implementation.\n                            if interaction_and_feedback.feedback is None:\n                                # Set feedback to self._ongoing_interactions\n                                interaction_and_feedback.feedback = feedback\n                            break\n                    if matched:\n                        break\n                if not matched:\n                    match_left_feedbacks.append(feedback)\n            return match_left_feedbacks\n\n        running_options = self._get_top_running_options()\n\n        self._main_loop = asyncio.get_running_loop()\n        self._main_thread_id = threading.get_ident()\n\n        if self.thread_pool is None:\n            self.thread_pool = ThreadPoolExecutor(thread_name_prefix=\"bridgic-thread\")\n\n        if not self._automa_running:\n            # Here is the last chance to compile and check the DDG in the end of the [Initialization Phase] (phase 1 just before the first DS).\n            self._compile_graph_and_detect_risks()\n            self._automa_running = True\n\n        # An Automa needs to be re-run with _current_kickoff_workers reinitialized.\n        _reinit_current_kickoff_workers_if_needed()\n\n        rx_feedbacks = _check_and_normalize_interaction_params(interaction_feedback, interaction_feedbacks)\n        if rx_feedbacks:\n            rx_feedbacks = _match_ongoing_interaction_and_feedbacks(rx_feedbacks)\n\n        if running_options.debug:\n            printer.print(f\"\\n{type(self).__name__}-[{self.name}] is getting started.\", color=\"green\")\n\n        # Task loop divided into many dynamic steps (DS).\n        is_output_worker_keys = set()\n        while self._current_kickoff_workers:\n            # A new DS started.\n            if running_options.debug:\n                kickoff_worker_keys = [kickoff_info.worker_key for kickoff_info in self._current_kickoff_workers]\n                printer.print(f\"[DS][Before Tasks Started] kickoff workers: {kickoff_worker_keys}\", color=\"purple\")\n\n            for kickoff_info in self._current_kickoff_workers:\n                if kickoff_info.run_finished:\n                    # Skip finished workers. Here is the case that the Automa is resumed after a human interaction.\n                    if running_options.debug:\n                        printer.print(f\"[{kickoff_info.worker_key}] will be skipped - run finished\", color=\"blue\")\n                    continue\n\n                if running_options.debug:\n                    kickoff_name = kickoff_info.last_kickoff\n                    if kickoff_name == \"__automa__\":\n                        kickoff_name = f\"{kickoff_name}:({self.name})\"\n                    printer.print(f\"[{kickoff_name}] will kick off [{kickoff_info.worker_key}]\", color=\"cyan\")\n\n                # First, Arguments Mapping:\n                if kickoff_info.last_kickoff == \"__automa__\":\n                    next_args, next_kwargs = self._input_buffer.args, {}\n                elif kickoff_info.from_ferry:\n                    next_args, next_kwargs = kickoff_info.args, kickoff_info.kwargs\n                else:\n                    next_args, next_kwargs = self._mapping_args(\n                        kickoff_worker_key=kickoff_info.last_kickoff,\n                        current_worker_key=kickoff_info.worker_key,\n                    )\n                # Second, Inputs Propagation:\n                next_args, next_kwargs = self._propagate_inputs(\n                    current_worker_key=kickoff_info.worker_key,\n                    next_args=next_args,\n                    next_kwargs=next_kwargs,\n                    input_kwargs=self._input_buffer.kwargs,\n                )\n                # Third, Resolve data injection.\n                worker_obj = self._workers[kickoff_info.worker_key]\n                next_args, next_kwargs = injector.inject(\n                    current_worker_key=kickoff_info.worker_key, \n                    current_worker_sig=worker_obj.get_input_param_names(), \n                    current_automa=self,\n                    next_args=next_args, \n                    next_kwargs=next_kwargs\n                )\n\n                # Collect the output worker keys.\n                if self._workers[kickoff_info.worker_key].is_output:\n                    is_output_worker_keys.add(kickoff_info.worker_key)\n                    if len(is_output_worker_keys) &gt; 1:\n                        raise AutomaRuntimeError(\n                            f\"It is not allowed to have more than one worker with `is_output=True` and \"\n                            f\"they are all considered as output-worker when the automa terminates and returns.\"\n                            f\"The current output-worker keys are: {is_output_worker_keys}.\"\n                            f\"If you want to collect the results of multiple workers simultaneously, \"\n                            f\"it is recommended that you add one worker to gather them.\"\n                        )\n\n                # Schedule task for each kickoff worker.\n                if worker_obj.is_automa():\n                    coro = worker_obj.arun(\n                        *next_args, \n                        interaction_feedbacks=rx_feedbacks, \n                        **next_kwargs\n                    )\n                else:\n                    coro = worker_obj.arun(*next_args, **next_kwargs)\n\n                task = asyncio.create_task(\n                    # TODO1: arun() may need to be wrapped to support better interrupt...\n                    coro,\n                    name=f\"Task-{kickoff_info.worker_key}\"\n                )\n                self._running_tasks.append(_RunnningTask(\n                    worker_key=kickoff_info.worker_key,\n                    task=task,\n                ))\n\n            # Wait until all of the tasks are finished.\n            while True:\n                undone_tasks = [t.task for t in self._running_tasks if not t.task.done()]\n                if not undone_tasks:\n                    break\n                try:\n                    await undone_tasks[0]\n                except Exception as e:\n                    ...\n                    # The same exception will be raised again in the following task.result().\n                    # Note: A Task is done when the wrapped coroutine either returned a value, raised an exception, or the Task was cancelled.\n                    # Refer to: https://docs.python.org/3/library/asyncio-task.html#task-object\n\n            # Process graph topology change deferred tasks triggered by add_worker() and remove_worker().\n            _execute_topology_change_deferred_tasks(self._topology_change_deferred_tasks)\n\n            # Perform post-task follow-up operations.\n            interaction_exceptions: List[_InteractionEventException] = []\n            for task in self._running_tasks:\n                # task.task.result must be called here! It will raise an exception if task failed.\n                try:\n                    task_result = task.task.result()\n                    _set_worker_run_finished(task.worker_key)\n                    if task.worker_key in self._workers:\n                        # The current running worker may be removed.\n                        worker_obj = self._workers[task.worker_key]\n                        # Collect results of the finished tasks.\n                        self._worker_output[task.worker_key] = task_result\n                        # reset dynamic states of finished workers.\n                        self._workers_dynamic_states[task.worker_key].dependency_triggers = set(getattr(worker_obj, \"dependencies\", []))\n                        # Update the dynamic states of successor workers.\n                        for successor_key in self._worker_forwards.get(task.worker_key, []):\n                            self._workers_dynamic_states[successor_key].dependency_triggers.remove(task.worker_key)\n                        # Each time a worker is finished running, the ongoing interaction states should be cleared. Once it is re-run, the human interactions in the worker can be triggered again.\n                        if task.worker_key in self._worker_interaction_indices:\n                            del self._worker_interaction_indices[task.worker_key]\n                        if task.worker_key in self._ongoing_interactions:\n                            del self._ongoing_interactions[task.worker_key]\n\n                except _InteractionEventException as e:\n                    interaction_exceptions.append(e)\n                    if task.worker_key in self._workers and not self._workers[task.worker_key].is_automa():\n                        if task.worker_key not in self._ongoing_interactions:\n                            self._ongoing_interactions[task.worker_key] = []\n                        interaction=e.args[0]\n                        # Make sure the interaction_id is unique for each human interaction.\n                        found = False\n                        for iaf in self._ongoing_interactions[task.worker_key]:\n                            if iaf.interaction.interaction_id == interaction.interaction_id:\n                                found = True\n                                break\n                        if not found:\n                            self._ongoing_interactions[task.worker_key].append(_InteractionAndFeedback(\n                                interaction=interaction,\n                            ))\n\n            if len(self._topology_change_deferred_tasks) &gt; 0:\n                # Graph topology validation and risk detection. Only needed when topology changes.\n                # Guarantee the graph topology is valid and consistent after each DS.\n                # 1. Validate the canonical graph.\n                self._validate_canonical_graph()\n                # 2. Validate the DAG constraints.\n                GraphMeta.validate_dag_constraints(self._worker_forwards)\n                # TODO: more validations can be added here...\n\n            # TODO: Ferry-related risk detection may be added here...\n\n            # Just after post-task operations (several deferred tasks) and before finding next kickoff workers, collect and process the interaction exceptions.\n            if len(interaction_exceptions) &gt; 0:\n                all_interactions: List[Interaction] = [interaction for e in interaction_exceptions for interaction in e.args]\n                if self.parent is None:\n                    # This is the top-level Automa. Serialize the Automa and raise InteractionException to the application layer.\n                    serialized_automa = dump_bytes(self)\n                    snapshot = Snapshot(\n                        serialized_bytes=serialized_automa,\n                        serialization_version=GraphAutoma.SERIALIZATION_VERSION,\n                    )\n                    raise InteractionException(\n                        interactions=all_interactions,\n                        snapshot=snapshot,\n                    )\n                else:\n                    # Continue raise exception to the upper level Automa.\n                    raise _InteractionEventException(*all_interactions)\n\n            # Find next kickoff workers and rebuild _current_kickoff_workers\n            run_finished_worker_keys: List[str] = [kickoff_info.worker_key for kickoff_info in self._current_kickoff_workers if kickoff_info.run_finished]\n            assert len(run_finished_worker_keys) == len(self._current_kickoff_workers)\n            self._current_kickoff_workers = []\n            # New kickoff workers can be triggered by two ways:\n            # 1. The ferry_to() operation is called during current worker execution.\n            # 2. The dependencies are eliminated after all predecessor workers are finished.\n            # So,\n            # First add kickoff workers triggered by ferry_to();\n            for ferry_task in self._ferry_deferred_tasks:\n                self._current_kickoff_workers.append(_KickoffInfo(\n                    worker_key=ferry_task.ferry_to_worker_key,\n                    last_kickoff=ferry_task.kickoff_worker_key,\n                    from_ferry=True,\n                    args=ferry_task.args,\n                    kwargs=ferry_task.kwargs,\n                ))\n            # Then add kickoff workers triggered by dependencies elimination.\n            # Merge successor keys of all finished tasks.\n            successor_keys = set()\n            for worker_key in run_finished_worker_keys:\n                # Note: The `worker_key` worker may have been removed from the Automa.\n                for successor_key in self._worker_forwards.get(worker_key, []):\n                    if successor_key not in successor_keys:\n                        dependency_triggers = self._workers_dynamic_states[successor_key].dependency_triggers\n                        if not dependency_triggers:\n                            self._current_kickoff_workers.append(_KickoffInfo(\n                                worker_key=successor_key,\n                                last_kickoff=worker_key,\n                            ))\n                        successor_keys.add(successor_key)\n            if running_options.debug:\n                deferred_ferrys = [ferry_task.ferry_to_worker_key for ferry_task in self._ferry_deferred_tasks]\n                printer.print(f\"[DS][After Tasks Finished] successor workers: {successor_keys}, deferred ferrys: {deferred_ferrys}\", color=\"purple\")\n\n            # Clear running tasks after all finished.\n            self._running_tasks.clear()\n            self._ferry_deferred_tasks.clear()\n            self._topology_change_deferred_tasks.clear()\n\n        if running_options.debug:\n            printer.print(f\"{type(self).__name__}-[{self.name}] is finished.\", color=\"green\")\n\n        # After a complete run, reset all necessary states to allow the automa to re-run.\n        self._input_buffer = _AutomaInputBuffer()\n        if self.should_reset_local_space():\n            self._clean_all_worker_local_space()\n        self._ongoing_interactions.clear()\n        self._worker_interaction_indices.clear()\n        self._automa_running = False\n\n        if is_output_worker_keys:\n            return self._worker_output.get(list(is_output_worker_keys)[0], None)\n        else:\n            return None\n\n    def _get_worker_dependencies(self, worker_key: str) -&gt; List[str]:\n        \"\"\"\n        Get the worker keys of all dependencies of the worker.\n        \"\"\"\n        deps = self._workers[worker_key].dependencies\n        return [] if deps is None else deps\n\n    def _find_connected_components(self):\n        \"\"\"\n        Find all of the connected components in the whole automa graph described by self._workers.\n        \"\"\"\n        visited = set()\n        component_list = []\n        component_idx = {}\n\n        def dfs(worker: str, component: List[str]):\n            visited.add(worker)\n            component.append(worker)\n            for target in self._worker_forwards.get(worker, []):\n                if target not in visited:\n                    dfs(target, component)\n\n        for worker in self._workers.keys():\n            if worker not in visited:\n                component_list.append([])\n                current_idx = len(component_list) - 1\n                current_component = component_list[current_idx]\n\n                dfs(worker, current_component)\n\n                for worker in current_component:\n                    component_idx[worker] = current_idx\n\n        # self._component_list, self._component_idx = component_list, component_idx\n        # TODO: check how to use _component_list and _component_idx...\n\n    @override\n    def _get_worker_key(self, worker: Worker) -&gt; Optional[str]:\n        for worker_key, worker_obj in self._workers.items():\n            if worker_obj == worker:\n                # Note: _GraphAdaptedWorker.__eq__() is overridden to support the '==' operator.\n                return worker_key\n        return None\n\n    @override\n    def _get_worker_instance(self, worker_key: str) -&gt; Worker:\n        return self._workers[worker_key]\n\n    @override\n    def _locate_interacting_worker(self) -&gt; Optional[str]:\n        return self._trace_back_kickoff_worker_key_from_stack()\n\n    def _trace_back_kickoff_worker_key_from_stack(self) -&gt; Optional[str]:\n        worker = self._get_current_running_worker_instance_by_stacktrace()\n        if worker:\n            return self._get_worker_key(worker)\n        return None\n\n    def _get_current_running_worker_instance_by_stacktrace(self) -&gt; Optional[Worker]:\n        for frame_info in inspect.stack():\n            frame = frame_info.frame\n            if 'self' in frame.f_locals:\n                self_obj = frame.f_locals['self']\n                if isinstance(self_obj, Worker) and (not isinstance(self_obj, Automa)) and (frame_info.function == \"arun\" or frame_info.function == \"run\"):\n                    return self_obj\n        return None\n\n    def _mapping_args(\n        self, \n        kickoff_worker_key: str,\n        current_worker_key: str,\n    ) -&gt; Tuple[Tuple[Any, ...], Dict[str, Any]]:\n        \"\"\"\n        Resolve arguments mapping between workers that have dependency relationships.\n\n        Parameters\n        ----------\n        kickoff_worker_key : str\n            The key of the kickoff worker.\n        current_worker_key : str\n            The key of the current worker.\n\n        Returns\n        -------\n        Tuple[Tuple[Any, ...], Dict[str, Any]]\n            The mapped positional arguments and keyword arguments.\n        \"\"\"\n        current_worker_obj = self._workers[current_worker_key]\n        args_mapping_rule = current_worker_obj.args_mapping_rule\n        dep_workers_keys = self._get_worker_dependencies(current_worker_key)\n        assert kickoff_worker_key in dep_workers_keys\n\n        def as_is_return_values(results: List[Any]) -&gt; Tuple[Tuple, Dict[str, Any]]:\n            next_args, next_kwargs = tuple(results), {}\n            return next_args, next_kwargs\n\n        def unpack_return_value(result: Any) -&gt; Tuple[Tuple, Dict[str, Any]]:\n            if dep_workers_keys != [kickoff_worker_key]:\n                raise WorkerArgsMappingError(\n                    f\"The worker \\\"{current_worker_key}\\\" must has exactly one dependency for the args_mapping_rule=\\\"{ArgsMappingRule.UNPACK}\\\", \"\n                    f\"but got {len(dep_workers_keys)} dependencies: {dep_workers_keys}\"\n                )\n            # result is not allowed to be None, since None can not be unpacked.\n            if isinstance(result, (List, Tuple)):\n                # Similar args mapping logic to as_is_return_values()\n                next_args, next_kwargs = tuple(result), {}\n            elif isinstance(result, Mapping):\n                next_args, next_kwargs = (), {**result}\n\n            else:\n                # Other types, including None, are not unpackable.\n                raise WorkerArgsMappingError(\n                    f\"args_mapping_rule=\\\"{ArgsMappingRule.UNPACK}\\\" is only valid for \"\n                    f\"tuple/list, or dict. But the worker \\\"{current_worker_key}\\\" got type \\\"{type(result)}\\\" from the kickoff worker \\\"{kickoff_worker_key}\\\".\"\n                )\n            return next_args, next_kwargs\n\n        def merge_return_values(results: List[Any]) -&gt; Tuple[Tuple, Dict[str, Any]]:\n            next_args, next_kwargs = tuple([results]), {}\n            return next_args, next_kwargs\n\n        if args_mapping_rule == ArgsMappingRule.AS_IS:\n            next_args, next_kwargs = as_is_return_values([self._worker_output[dep_worker_key] for dep_worker_key in dep_workers_keys])\n        elif args_mapping_rule == ArgsMappingRule.UNPACK:\n            next_args, next_kwargs = unpack_return_value(self._worker_output[kickoff_worker_key])\n        elif args_mapping_rule == ArgsMappingRule.MERGE:\n            next_args, next_kwargs = merge_return_values([self._worker_output[dep_worker_key] for dep_worker_key in dep_workers_keys])\n        elif args_mapping_rule == ArgsMappingRule.SUPPRESSED:\n            next_args, next_kwargs = (), {}\n\n        return next_args, next_kwargs\n\n    def _propagate_inputs(\n        self, \n        current_worker_key: str,\n        next_args: Tuple[Any, ...],\n        next_kwargs: Dict[str, Any],\n        input_kwargs: Dict[str, Any],\n    ) -&gt; Tuple[Tuple[Any, ...], Dict[str, Any]]:\n        \"\"\"\n        Resolve inputs propagation from the input buffer of the container Automa to every worker within the Automa.\n\n        Parameters\n        ----------\n        current_worker_key : str\n            The key of the current worker.\n        next_args : Tuple[Any, ...]\n            The positional arguments to be mapped.\n        next_kwargs : Dict[str, Any]\n            The keyword arguments to be mapped.\n        input_kwargs : Dict[str, Any]\n            The keyword arguments to be propagated from the input buffer of the container Automa.\n\n        Returns\n        -------\n        Tuple[Tuple[Any, ...], Dict[str, Any]]\n            The mapped positional arguments and keyword arguments.\n        \"\"\"\n        current_worker_obj = self._workers[current_worker_key]\n        input_kwargs = {k:v for k,v in input_kwargs.items() if k not in next_kwargs}\n        next_kwargs = {**next_kwargs, **input_kwargs}\n        rx_param_names_dict = current_worker_obj.get_input_param_names()\n        next_args, next_kwargs = safely_map_args(next_args, next_kwargs, rx_param_names_dict)\n        return next_args, next_kwargs\n\n    def __repr__(self) -&gt; str:\n        # TODO : It's good to make __repr__() of Automa compatible with eval().\n        # This feature depends on the implementation of __repr__() of workers.\n        class_name = self.__class__.__name__\n        workers_str = self._workers.__repr__()\n        return f\"{class_name}(workers={workers_str})\"\n\n    def __str__(self) -&gt; str:\n        d = {}\n        for k, v in self._workers.items():\n            d[k] = f\"{v} depends on {getattr(v, 'dependencies', [])}\"\n        return json.dumps(d, ensure_ascii=False, indent=4)\n</code></pre>"},{"location":"reference/bridgic-core/bridgic/core/automa/#bridgic.core.automa.GraphAutoma.all_workers","title":"all_workers","text":"<pre><code>all_workers() -&gt; List[str]\n</code></pre> <p>Gets a list containing the keys of all workers registered in this Automa.</p> <p>Returns:</p> Type Description <code>List[str]</code> <p>A list of worker keys.</p> Source code in <code>bridgic/core/automa/_graph_automa.py</code> <pre><code>def all_workers(self) -&gt; List[str]:\n    \"\"\"\n    Gets a list containing the keys of all workers registered in this Automa.\n\n    Returns\n    -------\n    List[str]\n        A list of worker keys.\n    \"\"\"\n    return list(self._workers.keys())\n</code></pre>"},{"location":"reference/bridgic-core/bridgic/core/automa/#bridgic.core.automa.GraphAutoma.add_worker","title":"add_worker","text":"<pre><code>add_worker(\n    key: str,\n    worker: Worker,\n    *,\n    dependencies: List[str] = [],\n    is_start: bool = False,\n    is_output: bool = False,\n    args_mapping_rule: ArgsMappingRule = AS_IS\n) -&gt; None\n</code></pre> <p>This method is used to add a worker dynamically into the automa.</p> <p>If this method is called during the [Initialization Phase], the worker will be added immediately. If this method is called during the [Running Phase], the worker will be added as a deferred task which will be executed in the next DS.</p> <p>The dependencies can be added together with a worker. However, you can add a worker without any dependencies.</p> <p>Note: The args_mapping_rule can only be set together with adding a worker, even if the worker has no any dependencies.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>The key of the worker.</p> required <code>worker</code> <code>Worker</code> <p>The worker instance to be registered.</p> required <code>dependencies</code> <code>List[str]</code> <p>A list of worker keys that the worker depends on.</p> <code>[]</code> <code>is_start</code> <code>bool</code> <p>Whether the worker is a start worker.</p> <code>False</code> <code>is_output</code> <code>bool</code> <p>Whether the worker is an output worker.</p> <code>False</code> <code>args_mapping_rule</code> <code>ArgsMappingRule</code> <p>The rule of arguments mapping.</p> <code>AS_IS</code> Source code in <code>bridgic/core/automa/_graph_automa.py</code> <pre><code>def add_worker(\n    self,\n    key: str,\n    worker: Worker,\n    *,\n    dependencies: List[str] = [],\n    is_start: bool = False,\n    is_output: bool = False,\n    args_mapping_rule: ArgsMappingRule = ArgsMappingRule.AS_IS,\n) -&gt; None:\n    \"\"\"\n    This method is used to add a worker dynamically into the automa.\n\n    If this method is called during the [Initialization Phase], the worker will be added immediately. If this method is called during the [Running Phase], the worker will be added as a deferred task which will be executed in the next DS.\n\n    The dependencies can be added together with a worker. However, you can add a worker without any dependencies.\n\n    Note: The args_mapping_rule can only be set together with adding a worker, even if the worker has no any dependencies.\n\n    Parameters\n    ----------\n    key : str\n        The key of the worker.\n    worker : Worker\n        The worker instance to be registered.\n    dependencies : List[str]\n        A list of worker keys that the worker depends on.\n    is_start : bool\n        Whether the worker is a start worker.\n    is_output : bool\n        Whether the worker is an output worker.\n    args_mapping_rule : ArgsMappingRule\n        The rule of arguments mapping.\n    \"\"\"\n    self._add_worker_internal(\n        key=key,\n        worker=worker,\n        dependencies=dependencies,\n        is_start=is_start,\n        is_output=is_output,\n        args_mapping_rule=args_mapping_rule,\n    )\n</code></pre>"},{"location":"reference/bridgic-core/bridgic/core/automa/#bridgic.core.automa.GraphAutoma.add_func_as_worker","title":"add_func_as_worker","text":"<pre><code>add_func_as_worker(\n    key: str,\n    func: Callable,\n    *,\n    dependencies: List[str] = [],\n    is_start: bool = False,\n    is_output: bool = False,\n    args_mapping_rule: ArgsMappingRule = AS_IS\n) -&gt; None\n</code></pre> <p>This method is used to add a function as a worker into the automa.</p> <p>The format of the parameters will follow that of the decorator @worker(...), so that the  behavior of the decorated function is consistent with that of normal CallableLandableWorker objects.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>The key of the function worker.</p> required <code>func</code> <code>Callable</code> <p>The function to be added as a worker to the automa.</p> required <code>dependencies</code> <code>List[str]</code> <p>A list of worker names that the decorated callable depends on.</p> <code>[]</code> <code>is_start</code> <code>bool</code> <p>Whether the decorated callable is a start worker. True means it is, while False means it is not.</p> <code>False</code> <code>is_output</code> <code>bool</code> <p>Whether the decorated callable is an output worker. True means it is, while False means it is not.</p> <code>False</code> <code>args_mapping_rule</code> <code>ArgsMappingRule</code> <p>The rule of arguments mapping.</p> <code>AS_IS</code> Source code in <code>bridgic/core/automa/_graph_automa.py</code> <pre><code>def add_func_as_worker(\n    self,\n    key: str,\n    func: Callable,\n    *,\n    dependencies: List[str] = [],\n    is_start: bool = False,\n    is_output: bool = False,\n    args_mapping_rule: ArgsMappingRule = ArgsMappingRule.AS_IS,\n) -&gt; None:\n    \"\"\"\n    This method is used to add a function as a worker into the automa.\n\n    The format of the parameters will follow that of the decorator @worker(...), so that the \n    behavior of the decorated function is consistent with that of normal CallableLandableWorker objects.\n\n    Parameters\n    ----------\n    key : str\n        The key of the function worker.\n    func : Callable\n        The function to be added as a worker to the automa.\n    dependencies : List[str]\n        A list of worker names that the decorated callable depends on.\n    is_start : bool\n        Whether the decorated callable is a start worker. True means it is, while False means it is not.\n    is_output : bool\n        Whether the decorated callable is an output worker. True means it is, while False means it is not.\n    args_mapping_rule : ArgsMappingRule\n        The rule of arguments mapping.\n    \"\"\"\n    self._add_func_as_worker_internal(\n        key=key,\n        func=func,\n        dependencies=dependencies,\n        is_start=is_start,\n        is_output=is_output,\n        args_mapping_rule=args_mapping_rule,\n    )\n</code></pre>"},{"location":"reference/bridgic-core/bridgic/core/automa/#bridgic.core.automa.GraphAutoma.worker","title":"worker","text":"<pre><code>worker(\n    *,\n    key: Optional[str] = None,\n    dependencies: List[str] = [],\n    is_start: bool = False,\n    is_output: bool = False,\n    args_mapping_rule: ArgsMappingRule = AS_IS\n) -&gt; Callable\n</code></pre> <p>This is a decorator used to mark a function as an GraphAutoma detectable Worker. Dislike the  global decorator @worker(...), it is usally used after an GraphAutoma instance is initialized.</p> <p>The format of the parameters will follow that of the decorator @worker(...), so that the  behavior of the decorated function is consistent with that of normal CallableLandableWorker objects.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>The key of the worker. If not provided, the name of the decorated callable will be used.</p> <code>None</code> <code>dependencies</code> <code>List[str]</code> <p>A list of worker names that the decorated callable depends on.</p> <code>[]</code> <code>is_start</code> <code>bool</code> <p>Whether the decorated callable is a start worker. True means it is, while False means it is not.</p> <code>False</code> <code>is_output</code> <code>bool</code> <p>Whether the decorated callable is an output worker. True means it is, while False means it is not.</p> <code>False</code> <code>args_mapping_rule</code> <code>str</code> <p>The rule of arguments mapping. The options are: \"auto\", \"as_list\", \"as_dict\", \"suppressed\".</p> <code>AS_IS</code> Source code in <code>bridgic/core/automa/_graph_automa.py</code> <pre><code>def worker(\n    self,\n    *,\n    key: Optional[str] = None,\n    dependencies: List[str] = [],\n    is_start: bool = False,\n    is_output: bool = False,\n    args_mapping_rule: ArgsMappingRule = ArgsMappingRule.AS_IS,\n) -&gt; Callable:\n    \"\"\"\n    This is a decorator used to mark a function as an GraphAutoma detectable Worker. Dislike the \n    global decorator @worker(...), it is usally used after an GraphAutoma instance is initialized.\n\n    The format of the parameters will follow that of the decorator @worker(...), so that the \n    behavior of the decorated function is consistent with that of normal CallableLandableWorker objects.\n\n    Parameters\n    ----------\n    key : str\n        The key of the worker. If not provided, the name of the decorated callable will be used.\n    dependencies : List[str]\n        A list of worker names that the decorated callable depends on.\n    is_start : bool\n        Whether the decorated callable is a start worker. True means it is, while False means it is not.\n    is_output : bool\n        Whether the decorated callable is an output worker. True means it is, while False means it is not.\n    args_mapping_rule : str\n        The rule of arguments mapping. The options are: \"auto\", \"as_list\", \"as_dict\", \"suppressed\".\n    \"\"\"\n    def wrapper(func: Callable):\n        self._add_func_as_worker_internal(\n            key=(key or func.__name__),\n            func=func,\n            dependencies=dependencies,\n            is_start=is_start,\n            is_output=is_output,\n            args_mapping_rule=args_mapping_rule,\n        )\n\n    return wrapper\n</code></pre>"},{"location":"reference/bridgic-core/bridgic/core/automa/#bridgic.core.automa.GraphAutoma.remove_worker","title":"remove_worker","text":"<pre><code>remove_worker(key: str) -&gt; None\n</code></pre> <p>Remove a worker from the Automa. This method can be called at any time to remove a worker from the Automa.</p> <p>When a worker is removed, all dependencies related to this worker, including all the dependencies of the worker itself and the dependencies between the worker and its successor workers, will be also removed.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>The key of the worker to be removed.</p> required <p>Returns:</p> Type Description <code>None</code> <p>Raises:</p> Type Description <code>AutomaDeclarationError</code> <p>If the worker specified by key does not exist in the Automa, this exception will be raised.</p> Source code in <code>bridgic/core/automa/_graph_automa.py</code> <pre><code>def remove_worker(self, key: str) -&gt; None:\n    \"\"\"\n    Remove a worker from the Automa. This method can be called at any time to remove a worker from the Automa.\n\n    When a worker is removed, all dependencies related to this worker, including all the dependencies of the worker itself and the dependencies between the worker and its successor workers, will be also removed.\n\n    Parameters\n    ----------\n    key : str\n        The key of the worker to be removed.\n\n    Returns\n    -------\n    None\n\n    Raises\n    ------\n    AutomaDeclarationError\n        If the worker specified by key does not exist in the Automa, this exception will be raised.\n    \"\"\"\n    if not self._automa_running:\n        # remove immediately\n        self._remove_worker_incrementally(key)\n    else:\n        deferred_task = _RemoveWorkerDeferredTask(\n            worker_key=key,\n        )\n        # Note: the execution order of topology change deferred tasks is important and is determined by the order of the calls of add_worker(), remove_worker() and add_dependency() in one DS.\n        self._topology_change_deferred_tasks.append(deferred_task)\n</code></pre>"},{"location":"reference/bridgic-core/bridgic/core/automa/#bridgic.core.automa.GraphAutoma.add_dependency","title":"add_dependency","text":"<pre><code>add_dependency(key: str, dependency: str) -&gt; None\n</code></pre> <p>This method is used to dynamically add a dependency from <code>key</code> to <code>dependency</code>.</p> <p>Note: args_mapping_rule is not allowed to be set by this method, instead it should be set together with add_worker() or add_func_as_worker().</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>The key of the worker that will depend on the worker with key <code>dependency</code>.</p> required <code>dependency</code> <code>str</code> <p>The key of the worker on which the worker with key <code>key</code> will depend.</p> required Source code in <code>bridgic/core/automa/_graph_automa.py</code> <pre><code>def add_dependency(\n    self,\n    key: str,\n    dependency: str,\n) -&gt; None:\n    \"\"\"\n    This method is used to dynamically add a dependency from `key` to `dependency`.\n\n    Note: args_mapping_rule is not allowed to be set by this method, instead it should be set together with add_worker() or add_func_as_worker().\n\n    Parameters\n    ----------\n    key : str\n        The key of the worker that will depend on the worker with key `dependency`.\n    dependency : str\n        The key of the worker on which the worker with key `key` will depend.\n    \"\"\"\n    ...\n    if not self._automa_running:\n        # add the dependency immediately\n        self._add_dependency_incrementally(key, dependency)\n    else:\n        deferred_task = _AddDependencyDeferredTask(\n            worker_key=key,\n            dependency=dependency,\n        )\n        # Note: the execution order of topology change deferred tasks is important and is determined by the order of the calls of add_worker(), remove_worker() and add_dependency() in one DS.\n        self._topology_change_deferred_tasks.append(deferred_task)\n</code></pre>"},{"location":"reference/bridgic-core/bridgic/core/automa/#bridgic.core.automa.GraphAutoma.ferry_to","title":"ferry_to","text":"<pre><code>ferry_to(key: str, /, *args, **kwargs)\n</code></pre> <p>Defer the invocation to the specified worker, passing any provided arguments. This creates a  delayed call, ensuring the worker will be scheduled to run asynchronously in the next event loop,  independent of its dependencies.</p> <p>This primitive is commonly used for:</p> <ol> <li>Implementing dynamic branching based on runtime conditions.</li> <li>Creating logic that forms cyclic graphs.</li> </ol> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>The key of the worker to run.</p> required <code>args</code> <code>optional</code> <p>Positional arguments to be passed.</p> <code>()</code> <code>kwargs</code> <code>optional</code> <p>Keyword arguments to be passed.</p> <code>{}</code> <p>Examples:</p> <pre><code>class MyGraphAutoma(GraphAutoma):\n    @worker(is_start=True)\n    def start_worker(self):\n        number = random.randint(0, 1)\n        if number == 0:\n            self.ferry_to(\"cond_1_worker\", number=number)\n        else:\n            self.ferry_to(\"cond_2_worker\")\n\n    @worker()\n    def cond_1_worker(self, number: int):\n        print(f'Got {{number}}!')\n\n    @worker()\n    def cond_2_worker(self):\n        self.ferry_to(\"start_worker\")\n\nautoma = MyGraphAutoma()\nawait automa.arun()\n\n# Output: Got 0!\n</code></pre> Source code in <code>bridgic/core/automa/_graph_automa.py</code> <pre><code>def ferry_to(self, key: str, /, *args, **kwargs):\n    \"\"\"\n    Defer the invocation to the specified worker, passing any provided arguments. This creates a \n    delayed call, ensuring the worker will be scheduled to run asynchronously in the next event loop, \n    independent of its dependencies.\n\n    This primitive is commonly used for:\n\n    1. Implementing dynamic branching based on runtime conditions.\n    2. Creating logic that forms cyclic graphs.\n\n    Parameters\n    ----------\n    key : str\n        The key of the worker to run.\n    args : optional\n        Positional arguments to be passed.\n    kwargs : optional\n        Keyword arguments to be passed.\n\n    Examples\n    --------\n    ```python\n    class MyGraphAutoma(GraphAutoma):\n        @worker(is_start=True)\n        def start_worker(self):\n            number = random.randint(0, 1)\n            if number == 0:\n                self.ferry_to(\"cond_1_worker\", number=number)\n            else:\n                self.ferry_to(\"cond_2_worker\")\n\n        @worker()\n        def cond_1_worker(self, number: int):\n            print(f'Got {{number}}!')\n\n        @worker()\n        def cond_2_worker(self):\n            self.ferry_to(\"start_worker\")\n\n    automa = MyGraphAutoma()\n    await automa.arun()\n\n    # Output: Got 0!\n    ```\n    \"\"\"\n    # TODO: check worker_key is valid, maybe deferred check...\n    running_options = self._get_top_running_options()\n    # if debug is enabled, trace back the kickoff worker key from stacktrace.\n    kickoff_worker_key: str = self._trace_back_kickoff_worker_key_from_stack() if running_options.debug else None\n    deferred_task = _FerryDeferredTask(\n        ferry_to_worker_key=key,\n        kickoff_worker_key=kickoff_worker_key,\n        args=args,\n        kwargs=kwargs,\n    )\n    # Note: ferry_to() may be called in a new thread.\n    # But _ferry_deferred_tasks is not necessary to be thread-safe due to Visibility Guarantees of the Bridgic Concurrency Model.\n    self._ferry_deferred_tasks.append(deferred_task)\n</code></pre>"},{"location":"reference/bridgic-core/bridgic/core/automa/#bridgic.core.automa.GraphAutoma.arun","title":"arun","text":"<code>async</code> <pre><code>arun(\n    *args: Tuple[Any, ...],\n    interaction_feedback: Optional[\n        InteractionFeedback\n    ] = None,\n    interaction_feedbacks: Optional[\n        List[InteractionFeedback]\n    ] = None,\n    **kwargs: Dict[str, Any]\n) -&gt; Any\n</code></pre> <p>The entry point for running the constructed <code>GraphAutoma</code> instance.</p> <p>Parameters:</p> Name Type Description Default <code>args</code> <code>optional</code> <p>Positional arguments to be passed.</p> <code>()</code> <code>interaction_feedback</code> <code>Optional[InteractionFeedback]</code> <p>Feedback that is received from a human interaction. Only one of interaction_feedback or  interaction_feedbacks should be provided at a time.</p> <code>None</code> <code>interaction_feedbacks</code> <code>Optional[List[InteractionFeedback]]</code> <p>Feedbacks that are received from multiple interactions occurred simultaneously before the Automa  was paused. Only one of interaction_feedback or interaction_feedbacks should be provided at a time.</p> <code>None</code> <code>kwargs</code> <code>optional</code> <p>Keyword arguments to be passed.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Any</code> <p>The execution result of the output-worker that has the setting <code>is_output=True</code>, otherwise None.</p> <p>Raises:</p> Type Description <code>InteractionException</code> <p>If the Automa is the top-level Automa and the <code>interact_with_human()</code> method is called by  one or more workers, this exception will be raised to the application layer.</p> <code>_InteractionEventException</code> <p>If the Automa is not the top-level Automa and the <code>interact_with_human()</code> method is called by  one or more workers, this exception will be raised to the upper level Automa.</p> Source code in <code>bridgic/core/automa/_graph_automa.py</code> <pre><code>async def arun(\n    self, \n    *args: Tuple[Any, ...],\n    interaction_feedback: Optional[InteractionFeedback] = None,\n    interaction_feedbacks: Optional[List[InteractionFeedback]] = None,\n    **kwargs: Dict[str, Any]\n) -&gt; Any:\n    \"\"\"\n    The entry point for running the constructed `GraphAutoma` instance.\n\n    Parameters\n    ----------\n    args : optional\n        Positional arguments to be passed.\n\n    interaction_feedback : Optional[InteractionFeedback]\n        Feedback that is received from a human interaction. Only one of interaction_feedback or \n        interaction_feedbacks should be provided at a time.\n\n    interaction_feedbacks : Optional[List[InteractionFeedback]]\n        Feedbacks that are received from multiple interactions occurred simultaneously before the Automa \n        was paused. Only one of interaction_feedback or interaction_feedbacks should be provided at a time.\n\n    kwargs : optional\n        Keyword arguments to be passed.\n\n    Returns\n    -------\n    Any\n        The execution result of the output-worker that has the setting `is_output=True`, otherwise None.\n\n    Raises\n    ------\n    InteractionException\n        If the Automa is the top-level Automa and the `interact_with_human()` method is called by \n        one or more workers, this exception will be raised to the application layer.\n\n    _InteractionEventException\n        If the Automa is not the top-level Automa and the `interact_with_human()` method is called by \n        one or more workers, this exception will be raised to the upper level Automa.\n    \"\"\"\n\n    def _reinit_current_kickoff_workers_if_needed():\n        # Note: After deserialization, the _current_kickoff_workers must not be empty!\n        # Therefore, _current_kickoff_workers will only be reinitialized when the Automa is run for the first time or rerun.\n        # It is guaranteed that _current_kickoff_workers will not be reinitialized when the Automa is resumed after deserialization.\n        if not self._current_kickoff_workers:\n            self._current_kickoff_workers = [\n                _KickoffInfo(\n                    worker_key=worker_key,\n                    last_kickoff=\"__automa__\"\n                ) for worker_key, worker_obj in self._workers.items()\n                if getattr(worker_obj, \"is_start\", False)\n            ]\n            # Each time the Automa re-runs, buffer the input arguments here.\n            self._input_buffer.args = args\n            self._input_buffer.kwargs = kwargs\n\n    def _execute_topology_change_deferred_tasks(tc_tasks: List[Union[_AddWorkerDeferredTask, _RemoveWorkerDeferredTask, _AddDependencyDeferredTask]]):\n        for topology_task in tc_tasks:\n            if topology_task.task_type == \"add_worker\":\n                self._add_worker_incrementally(\n                    key=topology_task.worker_key,\n                    worker=topology_task.worker_obj,\n                    dependencies=topology_task.dependencies,\n                    is_start=topology_task.is_start,\n                    is_output=topology_task.is_output,\n                    args_mapping_rule=topology_task.args_mapping_rule,\n                )\n            elif topology_task.task_type == \"remove_worker\":\n                self._remove_worker_incrementally(topology_task.worker_key)\n            elif topology_task.task_type == \"add_dependency\":\n                self._add_dependency_incrementally(topology_task.worker_key, topology_task.dependency)\n\n    def _set_worker_run_finished(worker_key: str):\n        for kickoff_info in self._current_kickoff_workers:\n            if kickoff_info.worker_key == worker_key:\n                kickoff_info.run_finished = True\n                break\n\n    def _check_and_normalize_interaction_params(\n        interaction_feedback: Optional[InteractionFeedback] = None,\n        interaction_feedbacks: Optional[List[InteractionFeedback]] = None,\n    ):\n        if interaction_feedback and interaction_feedbacks:\n            raise AutomaRuntimeError(\n                f\"Only one of interaction_feedback or interaction_feedbacks can be used. \"\n                f\"But received interaction_feedback={interaction_feedback} and \\n\"\n                f\"interaction_feedbacks={interaction_feedbacks}\"\n            )\n        if interaction_feedback:\n            rx_feedbacks = [interaction_feedback]\n        else:\n            rx_feedbacks = interaction_feedbacks\n        return rx_feedbacks\n\n    def _match_ongoing_interaction_and_feedbacks(rx_feedbacks:List[InteractionFeedback]):\n        match_left_feedbacks = []\n        for feedback in rx_feedbacks:\n            matched = False\n            for interaction_and_feedbacks in self._ongoing_interactions.values():\n                for interaction_and_feedback in interaction_and_feedbacks:\n                    if interaction_and_feedback.interaction.interaction_id == feedback.interaction_id:\n                        matched = True\n                        # Note: Only one feedback is allowed for each interaction. Here we assume that only the first feedback is valid, which is a choice of implementation.\n                        if interaction_and_feedback.feedback is None:\n                            # Set feedback to self._ongoing_interactions\n                            interaction_and_feedback.feedback = feedback\n                        break\n                if matched:\n                    break\n            if not matched:\n                match_left_feedbacks.append(feedback)\n        return match_left_feedbacks\n\n    running_options = self._get_top_running_options()\n\n    self._main_loop = asyncio.get_running_loop()\n    self._main_thread_id = threading.get_ident()\n\n    if self.thread_pool is None:\n        self.thread_pool = ThreadPoolExecutor(thread_name_prefix=\"bridgic-thread\")\n\n    if not self._automa_running:\n        # Here is the last chance to compile and check the DDG in the end of the [Initialization Phase] (phase 1 just before the first DS).\n        self._compile_graph_and_detect_risks()\n        self._automa_running = True\n\n    # An Automa needs to be re-run with _current_kickoff_workers reinitialized.\n    _reinit_current_kickoff_workers_if_needed()\n\n    rx_feedbacks = _check_and_normalize_interaction_params(interaction_feedback, interaction_feedbacks)\n    if rx_feedbacks:\n        rx_feedbacks = _match_ongoing_interaction_and_feedbacks(rx_feedbacks)\n\n    if running_options.debug:\n        printer.print(f\"\\n{type(self).__name__}-[{self.name}] is getting started.\", color=\"green\")\n\n    # Task loop divided into many dynamic steps (DS).\n    is_output_worker_keys = set()\n    while self._current_kickoff_workers:\n        # A new DS started.\n        if running_options.debug:\n            kickoff_worker_keys = [kickoff_info.worker_key for kickoff_info in self._current_kickoff_workers]\n            printer.print(f\"[DS][Before Tasks Started] kickoff workers: {kickoff_worker_keys}\", color=\"purple\")\n\n        for kickoff_info in self._current_kickoff_workers:\n            if kickoff_info.run_finished:\n                # Skip finished workers. Here is the case that the Automa is resumed after a human interaction.\n                if running_options.debug:\n                    printer.print(f\"[{kickoff_info.worker_key}] will be skipped - run finished\", color=\"blue\")\n                continue\n\n            if running_options.debug:\n                kickoff_name = kickoff_info.last_kickoff\n                if kickoff_name == \"__automa__\":\n                    kickoff_name = f\"{kickoff_name}:({self.name})\"\n                printer.print(f\"[{kickoff_name}] will kick off [{kickoff_info.worker_key}]\", color=\"cyan\")\n\n            # First, Arguments Mapping:\n            if kickoff_info.last_kickoff == \"__automa__\":\n                next_args, next_kwargs = self._input_buffer.args, {}\n            elif kickoff_info.from_ferry:\n                next_args, next_kwargs = kickoff_info.args, kickoff_info.kwargs\n            else:\n                next_args, next_kwargs = self._mapping_args(\n                    kickoff_worker_key=kickoff_info.last_kickoff,\n                    current_worker_key=kickoff_info.worker_key,\n                )\n            # Second, Inputs Propagation:\n            next_args, next_kwargs = self._propagate_inputs(\n                current_worker_key=kickoff_info.worker_key,\n                next_args=next_args,\n                next_kwargs=next_kwargs,\n                input_kwargs=self._input_buffer.kwargs,\n            )\n            # Third, Resolve data injection.\n            worker_obj = self._workers[kickoff_info.worker_key]\n            next_args, next_kwargs = injector.inject(\n                current_worker_key=kickoff_info.worker_key, \n                current_worker_sig=worker_obj.get_input_param_names(), \n                current_automa=self,\n                next_args=next_args, \n                next_kwargs=next_kwargs\n            )\n\n            # Collect the output worker keys.\n            if self._workers[kickoff_info.worker_key].is_output:\n                is_output_worker_keys.add(kickoff_info.worker_key)\n                if len(is_output_worker_keys) &gt; 1:\n                    raise AutomaRuntimeError(\n                        f\"It is not allowed to have more than one worker with `is_output=True` and \"\n                        f\"they are all considered as output-worker when the automa terminates and returns.\"\n                        f\"The current output-worker keys are: {is_output_worker_keys}.\"\n                        f\"If you want to collect the results of multiple workers simultaneously, \"\n                        f\"it is recommended that you add one worker to gather them.\"\n                    )\n\n            # Schedule task for each kickoff worker.\n            if worker_obj.is_automa():\n                coro = worker_obj.arun(\n                    *next_args, \n                    interaction_feedbacks=rx_feedbacks, \n                    **next_kwargs\n                )\n            else:\n                coro = worker_obj.arun(*next_args, **next_kwargs)\n\n            task = asyncio.create_task(\n                # TODO1: arun() may need to be wrapped to support better interrupt...\n                coro,\n                name=f\"Task-{kickoff_info.worker_key}\"\n            )\n            self._running_tasks.append(_RunnningTask(\n                worker_key=kickoff_info.worker_key,\n                task=task,\n            ))\n\n        # Wait until all of the tasks are finished.\n        while True:\n            undone_tasks = [t.task for t in self._running_tasks if not t.task.done()]\n            if not undone_tasks:\n                break\n            try:\n                await undone_tasks[0]\n            except Exception as e:\n                ...\n                # The same exception will be raised again in the following task.result().\n                # Note: A Task is done when the wrapped coroutine either returned a value, raised an exception, or the Task was cancelled.\n                # Refer to: https://docs.python.org/3/library/asyncio-task.html#task-object\n\n        # Process graph topology change deferred tasks triggered by add_worker() and remove_worker().\n        _execute_topology_change_deferred_tasks(self._topology_change_deferred_tasks)\n\n        # Perform post-task follow-up operations.\n        interaction_exceptions: List[_InteractionEventException] = []\n        for task in self._running_tasks:\n            # task.task.result must be called here! It will raise an exception if task failed.\n            try:\n                task_result = task.task.result()\n                _set_worker_run_finished(task.worker_key)\n                if task.worker_key in self._workers:\n                    # The current running worker may be removed.\n                    worker_obj = self._workers[task.worker_key]\n                    # Collect results of the finished tasks.\n                    self._worker_output[task.worker_key] = task_result\n                    # reset dynamic states of finished workers.\n                    self._workers_dynamic_states[task.worker_key].dependency_triggers = set(getattr(worker_obj, \"dependencies\", []))\n                    # Update the dynamic states of successor workers.\n                    for successor_key in self._worker_forwards.get(task.worker_key, []):\n                        self._workers_dynamic_states[successor_key].dependency_triggers.remove(task.worker_key)\n                    # Each time a worker is finished running, the ongoing interaction states should be cleared. Once it is re-run, the human interactions in the worker can be triggered again.\n                    if task.worker_key in self._worker_interaction_indices:\n                        del self._worker_interaction_indices[task.worker_key]\n                    if task.worker_key in self._ongoing_interactions:\n                        del self._ongoing_interactions[task.worker_key]\n\n            except _InteractionEventException as e:\n                interaction_exceptions.append(e)\n                if task.worker_key in self._workers and not self._workers[task.worker_key].is_automa():\n                    if task.worker_key not in self._ongoing_interactions:\n                        self._ongoing_interactions[task.worker_key] = []\n                    interaction=e.args[0]\n                    # Make sure the interaction_id is unique for each human interaction.\n                    found = False\n                    for iaf in self._ongoing_interactions[task.worker_key]:\n                        if iaf.interaction.interaction_id == interaction.interaction_id:\n                            found = True\n                            break\n                    if not found:\n                        self._ongoing_interactions[task.worker_key].append(_InteractionAndFeedback(\n                            interaction=interaction,\n                        ))\n\n        if len(self._topology_change_deferred_tasks) &gt; 0:\n            # Graph topology validation and risk detection. Only needed when topology changes.\n            # Guarantee the graph topology is valid and consistent after each DS.\n            # 1. Validate the canonical graph.\n            self._validate_canonical_graph()\n            # 2. Validate the DAG constraints.\n            GraphMeta.validate_dag_constraints(self._worker_forwards)\n            # TODO: more validations can be added here...\n\n        # TODO: Ferry-related risk detection may be added here...\n\n        # Just after post-task operations (several deferred tasks) and before finding next kickoff workers, collect and process the interaction exceptions.\n        if len(interaction_exceptions) &gt; 0:\n            all_interactions: List[Interaction] = [interaction for e in interaction_exceptions for interaction in e.args]\n            if self.parent is None:\n                # This is the top-level Automa. Serialize the Automa and raise InteractionException to the application layer.\n                serialized_automa = dump_bytes(self)\n                snapshot = Snapshot(\n                    serialized_bytes=serialized_automa,\n                    serialization_version=GraphAutoma.SERIALIZATION_VERSION,\n                )\n                raise InteractionException(\n                    interactions=all_interactions,\n                    snapshot=snapshot,\n                )\n            else:\n                # Continue raise exception to the upper level Automa.\n                raise _InteractionEventException(*all_interactions)\n\n        # Find next kickoff workers and rebuild _current_kickoff_workers\n        run_finished_worker_keys: List[str] = [kickoff_info.worker_key for kickoff_info in self._current_kickoff_workers if kickoff_info.run_finished]\n        assert len(run_finished_worker_keys) == len(self._current_kickoff_workers)\n        self._current_kickoff_workers = []\n        # New kickoff workers can be triggered by two ways:\n        # 1. The ferry_to() operation is called during current worker execution.\n        # 2. The dependencies are eliminated after all predecessor workers are finished.\n        # So,\n        # First add kickoff workers triggered by ferry_to();\n        for ferry_task in self._ferry_deferred_tasks:\n            self._current_kickoff_workers.append(_KickoffInfo(\n                worker_key=ferry_task.ferry_to_worker_key,\n                last_kickoff=ferry_task.kickoff_worker_key,\n                from_ferry=True,\n                args=ferry_task.args,\n                kwargs=ferry_task.kwargs,\n            ))\n        # Then add kickoff workers triggered by dependencies elimination.\n        # Merge successor keys of all finished tasks.\n        successor_keys = set()\n        for worker_key in run_finished_worker_keys:\n            # Note: The `worker_key` worker may have been removed from the Automa.\n            for successor_key in self._worker_forwards.get(worker_key, []):\n                if successor_key not in successor_keys:\n                    dependency_triggers = self._workers_dynamic_states[successor_key].dependency_triggers\n                    if not dependency_triggers:\n                        self._current_kickoff_workers.append(_KickoffInfo(\n                            worker_key=successor_key,\n                            last_kickoff=worker_key,\n                        ))\n                    successor_keys.add(successor_key)\n        if running_options.debug:\n            deferred_ferrys = [ferry_task.ferry_to_worker_key for ferry_task in self._ferry_deferred_tasks]\n            printer.print(f\"[DS][After Tasks Finished] successor workers: {successor_keys}, deferred ferrys: {deferred_ferrys}\", color=\"purple\")\n\n        # Clear running tasks after all finished.\n        self._running_tasks.clear()\n        self._ferry_deferred_tasks.clear()\n        self._topology_change_deferred_tasks.clear()\n\n    if running_options.debug:\n        printer.print(f\"{type(self).__name__}-[{self.name}] is finished.\", color=\"green\")\n\n    # After a complete run, reset all necessary states to allow the automa to re-run.\n    self._input_buffer = _AutomaInputBuffer()\n    if self.should_reset_local_space():\n        self._clean_all_worker_local_space()\n    self._ongoing_interactions.clear()\n    self._worker_interaction_indices.clear()\n    self._automa_running = False\n\n    if is_output_worker_keys:\n        return self._worker_output.get(list(is_output_worker_keys)[0], None)\n    else:\n        return None\n</code></pre>"},{"location":"reference/bridgic-core/bridgic/core/automa/#bridgic.core.automa.WorkerSignatureError","title":"WorkerSignatureError","text":"<p>               Bases: <code>Exception</code></p> <p>Raised when invalid signature is detected in the case of defining a worker.</p> Source code in <code>bridgic/core/types/_error.py</code> <pre><code>class WorkerSignatureError(Exception):\n    \"\"\"\n    Raised when invalid signature is detected in the case of defining a worker.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/bridgic-core/bridgic/core/automa/#bridgic.core.automa.WorkerArgsMappingError","title":"WorkerArgsMappingError","text":"<p>               Bases: <code>Exception</code></p> <p>Raised when the parameters declaration of a worker does not meet the requirements of the arguments mapping rule.</p> Source code in <code>bridgic/core/types/_error.py</code> <pre><code>class WorkerArgsMappingError(Exception):\n    \"\"\"\n    Raised when the parameters declaration of a worker does not meet the requirements of the arguments mapping rule.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/bridgic-core/bridgic/core/automa/#bridgic.core.automa.WorkerArgsInjectionError","title":"WorkerArgsInjectionError","text":"<p>               Bases: <code>Exception</code></p> <p>Raised when the arguments injection mechanism encountered an error during operation.</p> Source code in <code>bridgic/core/types/_error.py</code> <pre><code>class WorkerArgsInjectionError(Exception):\n    \"\"\"\n    Raised when the arguments injection mechanism encountered an error during operation.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/bridgic-core/bridgic/core/automa/#bridgic.core.automa.WorkerRuntimeError","title":"WorkerRuntimeError","text":"<p>               Bases: <code>RuntimeError</code></p> <p>Raised when the worker encounters an unexpected error during runtime.</p> Source code in <code>bridgic/core/types/_error.py</code> <pre><code>class WorkerRuntimeError(RuntimeError):\n    \"\"\"\n    Raised when the worker encounters an unexpected error during runtime.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/bridgic-core/bridgic/core/automa/#bridgic.core.automa.AutomaDeclarationError","title":"AutomaDeclarationError","text":"<p>               Bases: <code>Exception</code></p> <p>Raised when the declaration of workers within an Automa is not valid.</p> Source code in <code>bridgic/core/types/_error.py</code> <pre><code>class AutomaDeclarationError(Exception):\n    \"\"\"\n    Raised when the declaration of workers within an Automa is not valid.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/bridgic-core/bridgic/core/automa/#bridgic.core.automa.AutomaCompilationError","title":"AutomaCompilationError","text":"<p>               Bases: <code>Exception</code></p> <p>Raised when the compilation or validation of an Automa fails.</p> Source code in <code>bridgic/core/types/_error.py</code> <pre><code>class AutomaCompilationError(Exception):\n    \"\"\"\n    Raised when the compilation or validation of an Automa fails.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/bridgic-core/bridgic/core/automa/#bridgic.core.automa.AutomaRuntimeError","title":"AutomaRuntimeError","text":"<p>               Bases: <code>RuntimeError</code></p> <p>Raised when the execution of an Automa encounters an unexpected error.</p> Source code in <code>bridgic/core/types/_error.py</code> <pre><code>class AutomaRuntimeError(RuntimeError):\n    \"\"\"\n    Raised when the execution of an Automa encounters an unexpected error.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/bridgic-core/bridgic/core/automa/args/","title":"args","text":"<p>The Args module provides Arguments Mapping and Arguments Injection mechanisms in Bridgic.</p>"},{"location":"reference/bridgic-core/bridgic/core/automa/args/#bridgic.core.automa.args.ArgsMappingRule","title":"ArgsMappingRule","text":"<p>               Bases: <code>Enum</code></p> <p>Enumeration of Arguments Mapping rules for worker parameter passing.</p> <p>ArgsMappingRule defines how the return values from predecessor workers are mapped  to the parameters of the current worker. This controls the data flow between workers  in an automa execution graph.</p> <p>Attributes:</p> Name Type Description <code>AS_IS</code> <code>Enum</code> <p>Preserves the exact order and types of return values from predecessor workers. No unpacking or merging is performed.</p> <code>UNPACK</code> <code>Enum</code> <p>Unpacks the return value from the predecessor worker and passes as individual  arguments. Only valid when the current worker has exactly one dependency and  the return value is a list/tuple or dict.</p> <code>MERGE</code> <code>Enum</code> <p>Merges all return values from predecessor workers into a single tuple as the  only argument of the current worker.</p> <code>SUPPRESSED</code> <code>Enum</code> <p>Suppresses all return values from predecessor workers. No arguments are passed  to the current worker from its dependencies.</p> <p>Examples:</p> <pre><code>class MyAutoma(GraphAutoma):\n    @worker(is_start=True)\n    def worker_0(self, user_input: int) -&gt; int:\n        return user_input + 1\n\n    @worker(dependencies=[\"worker_0\"], args_mapping_rule=ArgsMappingRule.AS_IS)\n    def worker_1(self, worker_0_output: int) -&gt; int:\n        # Receives the exact return value from worker_0\n        return worker_0_output + 1\n\n    @worker(dependencies=[\"worker_0\"], args_mapping_rule=ArgsMappingRule.UNPACK)\n    def worker_2(self, user_input: int, result: int) -&gt; int:\n        # Unpacks the return value from worker_0 (assuming it returns a tuple)\n        return user_input + result\n\n    @worker(dependencies=[\"worker_0\", \"worker_1\"], args_mapping_rule=ArgsMappingRule.MERGE)\n    def worker_3(self, all_results: tuple) -&gt; int:\n        # Receives all results as a single tuple\n        return sum(all_results)\n\n    @worker(dependencies=[\"worker_3\"], args_mapping_rule=ArgsMappingRule.SUPPRESSED)\n    def worker_4(self, custom_input: int = 10) -&gt; int:\n        # Ignores return value from worker_3, uses custom input\n        return custom_input + 1\n</code></pre> Note <ol> <li>AS_IS is the default mapping rule when not specified</li> <li>UNPACK requires exactly one dependency and a list/tuple/dict return value</li> <li>MERGE combines all predecessor outputs into a single tuple argument</li> <li>SUPPRESSED allows workers to ignore dependency outputs completely</li> </ol> Source code in <code>bridgic/core/types/_common.py</code> <pre><code>class ArgsMappingRule(Enum):\n    \"\"\"\n    Enumeration of Arguments Mapping rules for worker parameter passing.\n\n    ArgsMappingRule defines how the return values from predecessor workers are mapped \n    to the parameters of the current worker. This controls the data flow between workers \n    in an automa execution graph.\n\n    Attributes\n    ----------\n    AS_IS: Enum\n        Preserves the exact order and types of return values from predecessor workers.\n        No unpacking or merging is performed.\n    UNPACK: Enum\n        Unpacks the return value from the predecessor worker and passes as individual \n        arguments. Only valid when the current worker has exactly one dependency and \n        the return value is a list/tuple or dict.\n    MERGE: Enum\n        Merges all return values from predecessor workers into a single tuple as the \n        only argument of the current worker.\n    SUPPRESSED: Enum\n        Suppresses all return values from predecessor workers. No arguments are passed \n        to the current worker from its dependencies.\n\n    Examples\n    --------\n    ```python\n    class MyAutoma(GraphAutoma):\n        @worker(is_start=True)\n        def worker_0(self, user_input: int) -&gt; int:\n            return user_input + 1\n\n        @worker(dependencies=[\"worker_0\"], args_mapping_rule=ArgsMappingRule.AS_IS)\n        def worker_1(self, worker_0_output: int) -&gt; int:\n            # Receives the exact return value from worker_0\n            return worker_0_output + 1\n\n        @worker(dependencies=[\"worker_0\"], args_mapping_rule=ArgsMappingRule.UNPACK)\n        def worker_2(self, user_input: int, result: int) -&gt; int:\n            # Unpacks the return value from worker_0 (assuming it returns a tuple)\n            return user_input + result\n\n        @worker(dependencies=[\"worker_0\", \"worker_1\"], args_mapping_rule=ArgsMappingRule.MERGE)\n        def worker_3(self, all_results: tuple) -&gt; int:\n            # Receives all results as a single tuple\n            return sum(all_results)\n\n        @worker(dependencies=[\"worker_3\"], args_mapping_rule=ArgsMappingRule.SUPPRESSED)\n        def worker_4(self, custom_input: int = 10) -&gt; int:\n            # Ignores return value from worker_3, uses custom input\n            return custom_input + 1\n    ```\n\n    Note\n    ----\n    1. AS_IS is the default mapping rule when not specified\n    2. UNPACK requires exactly one dependency and a list/tuple/dict return value\n    3. MERGE combines all predecessor outputs into a single tuple argument\n    4. SUPPRESSED allows workers to ignore dependency outputs completely\n    \"\"\"\n    AS_IS = \"as_is\"\n    UNPACK = \"unpack\"\n    MERGE = \"merge\"\n    SUPPRESSED = \"suppressed\"\n</code></pre>"},{"location":"reference/bridgic-core/bridgic/core/automa/args/#bridgic.core.automa.args.From","title":"From  <code>dataclass</code>","text":"<p>               Bases: <code>ArgsDescriptor</code></p> <p>Implementing arguments injection for worker parameters with default value.</p> <p>When a worker needs the output of another worker but does not directly depend on  it in execution, you can use <code>From</code> to declare an arguments injection in  its parameters.</p> <p>Attributes:</p> Name Type Description <code>key</code> <code>str</code> <p>The key of the worker to inject arguments from.</p> <code>default</code> <code>Optional[Any]</code> <p>The default value of the arguments.</p> <p>Examples:</p> <pre><code>class MyAutoma(GraphAutoma):\n    @worker(is_start=True)\n    def worker_0(self, user_input: int) -&gt; int:\n        return user_input + 1\n\n    @worker(dependencies=[\"worker_0\"])\n    def worker_1(self, worker_0_output: int) -&gt; int:\n        return worker_0_output + 1\n\n    @worker(dependencies=[\"worker_1\"], is_output=True)\n    def worker_2(self, worker_1_output: int, worker_0_output: int = From(\"worker_0\", 1)) -&gt; int:\n        # needs the output of worker_0 but does not directly depend on it in execution\n        print(f'worker_0_output: {worker_0_output}')\n        return worker_1_output + 1\n</code></pre> <p>Returns:</p> Type Description <code>Any</code> <p>The output of the worker specified by the key.</p> <p>Raises:</p> Type Description <code>WorkerArgsInjectionError</code> <p>If the worker specified by the key does not exist and no default value is set.</p> Note: <ol> <li>Can set a default value for a <code>From</code> declaration, which will be returned when the specified worker does not exist.</li> <li>Will raise <code>WorkerArgsInjectionError</code> if the worker specified by the key does not exist and no default value is set.</li> </ol> Source code in <code>bridgic/core/automa/args/_args_descriptor.py</code> <pre><code>@dataclass\nclass From(ArgsDescriptor):\n    \"\"\"\n    Implementing arguments injection for worker parameters with default value.\n\n    When a worker needs the output of another worker but does not directly depend on \n    it in execution, you can use `From` to declare an arguments injection in \n    its parameters.\n\n    Attributes\n    ----------\n    key : str\n        The key of the worker to inject arguments from.\n    default : Optional[Any]\n        The default value of the arguments.\n\n    Examples\n    --------\n    ```python\n    class MyAutoma(GraphAutoma):\n        @worker(is_start=True)\n        def worker_0(self, user_input: int) -&gt; int:\n            return user_input + 1\n\n        @worker(dependencies=[\"worker_0\"])\n        def worker_1(self, worker_0_output: int) -&gt; int:\n            return worker_0_output + 1\n\n        @worker(dependencies=[\"worker_1\"], is_output=True)\n        def worker_2(self, worker_1_output: int, worker_0_output: int = From(\"worker_0\", 1)) -&gt; int:\n            # needs the output of worker_0 but does not directly depend on it in execution\n            print(f'worker_0_output: {worker_0_output}')\n            return worker_1_output + 1\n    ```\n\n    Returns\n    -------\n    Any\n        The output of the worker specified by the key.\n\n    Raises\n    ------\n    WorkerArgsInjectionError\n        If the worker specified by the key does not exist and no default value is set.\n\n    Note:\n    ------\n    1. Can set a default value for a `From` declaration, which will be returned when the specified worker does not exist.\n    2. Will raise `WorkerArgsInjectionError` if the worker specified by the key does not exist and no default value is set.\n    \"\"\"\n    key: str\n    default: Optional[Any] = InjectorNone()\n</code></pre>"},{"location":"reference/bridgic-core/bridgic/core/automa/args/#bridgic.core.automa.args.System","title":"System  <code>dataclass</code>","text":"<p>               Bases: <code>ArgsDescriptor</code></p> <p>Implementing system-level arguments injection for worker parameters.</p> <p>System provides access to automa-level resources and context through arguments  injection. It supports pattern matching for different types of system resources.</p> <p>Attributes:</p> Name Type Description <code>key</code> <code>str</code> <p>The system resource key to inject. Supported keys: - \"runtime_context\": Runtime context for data persistence across worker executions. - \"automa\": Current automa instance. - \"automa:worker_key\": Sub-automa instance in current automa.</p> <p>Examples:</p> <pre><code>def worker_1(x: int, current_automa = System(\"automa\")) -&gt; int:\n    # Access current automa instance\n    current_automa.add_worker(\n        key=\"sub_automa\",\n        worker=SubAutoma(),\n        dependencies=[\"worker_1\"]\n    )\n    return x + 1\n\nclass SubAutoma(GraphAutoma):\n    @worker(is_start=True)\n    def worker_0(self, user_input: int) -&gt; int:\n        return user_input + 1\n\nclass MyAutoma(GraphAutoma):\n    @worker(is_start=True)\n    def worker_0(self, user_input: int, rtx = System(\"runtime_context\")) -&gt; int:\n        # Access runtime context for data persistence\n        local_space = self.get_local_space(rtx)\n        count = local_space.get(\"count\", 0)\n        local_space[\"count\"] = count + 1\n\n        self.add_func_as_worker(\n            key=\"worker_1\",\n            func=worker_1,\n            dependencies=[\"worker_0\"]\n        )\n\n        return user_input + count\n\n    @worker(dependencies=[\"worker_1\"])\n    def worker_2(self, worker_1_output: int, sub_automa = System(\"automa:sub_automa\")) -&gt; int:\n        # Access sub-automa from worker_1\n        sub_automa.add_worker(\n            key=\"worker_3\",\n            worker=SubAutoma(),\n            dependencies=[\"worker_2\"],\n            is_output=True,\n        )\n        return worker_1_output + 1\n</code></pre> <p>Returns:</p> Type Description <code>Any</code> <p>The system resource specified by the key: - RuntimeContext: For \"runtime_context\" - AutomaInstance: For current automa instance or a sub-automa instance from the current automa.</p> <p>Raises:</p> Type Description <code>WorkerArgsInjectionError</code> <ul> <li>If the key pattern is not supported.</li> <li>If the specified resource does not exist.</li> <li>If the specified resource is not an Automa.</li> </ul> Note <ol> <li>\"runtime_context\" provides a <code>RuntimeContext</code> instance for data persistence</li> <li>\"automa\" provides access to the current automa instance</li> <li>\"automa:worker_key\" provides access to a sub-automa from the specified worker key</li> </ol> Source code in <code>bridgic/core/automa/args/_args_descriptor.py</code> <pre><code>@dataclass\nclass System(ArgsDescriptor):\n    \"\"\"\n    Implementing system-level arguments injection for worker parameters.\n\n    System provides access to automa-level resources and context through arguments \n    injection. It supports pattern matching for different types of system resources.\n\n    Attributes\n    ----------\n    key : str\n        The system resource key to inject. Supported keys:\n        - \"runtime_context\": Runtime context for data persistence across worker executions.\n        - \"automa\": Current automa instance.\n        - \"automa:worker_key\": Sub-automa instance in current automa.\n\n    Examples\n    --------\n    ```python\n    def worker_1(x: int, current_automa = System(\"automa\")) -&gt; int:\n        # Access current automa instance\n        current_automa.add_worker(\n            key=\"sub_automa\",\n            worker=SubAutoma(),\n            dependencies=[\"worker_1\"]\n        )\n        return x + 1\n\n    class SubAutoma(GraphAutoma):\n        @worker(is_start=True)\n        def worker_0(self, user_input: int) -&gt; int:\n            return user_input + 1\n\n    class MyAutoma(GraphAutoma):\n        @worker(is_start=True)\n        def worker_0(self, user_input: int, rtx = System(\"runtime_context\")) -&gt; int:\n            # Access runtime context for data persistence\n            local_space = self.get_local_space(rtx)\n            count = local_space.get(\"count\", 0)\n            local_space[\"count\"] = count + 1\n\n            self.add_func_as_worker(\n                key=\"worker_1\",\n                func=worker_1,\n                dependencies=[\"worker_0\"]\n            )\n\n            return user_input + count\n\n        @worker(dependencies=[\"worker_1\"])\n        def worker_2(self, worker_1_output: int, sub_automa = System(\"automa:sub_automa\")) -&gt; int:\n            # Access sub-automa from worker_1\n            sub_automa.add_worker(\n                key=\"worker_3\",\n                worker=SubAutoma(),\n                dependencies=[\"worker_2\"],\n                is_output=True,\n            )\n            return worker_1_output + 1\n    ```\n\n    Returns\n    -------\n    Any\n        The system resource specified by the key:\n        - RuntimeContext: For \"runtime_context\"\n        - AutomaInstance: For current automa instance or a sub-automa instance from the current automa.\n\n    Raises\n    ------\n    WorkerArgsInjectionError\n        - If the key pattern is not supported.\n        - If the specified resource does not exist.\n        - If the specified resource is not an Automa.\n\n    Note\n    ----\n    1. \"runtime_context\" provides a `RuntimeContext` instance for data persistence\n    2. \"automa\" provides access to the current automa instance\n    3. \"automa:worker_key\" provides access to a sub-automa from the specified worker key\n    \"\"\"\n    key: str\n\n    def __post_init__(self):\n        allowed_patterns = [\n            r\"^runtime_context$\",\n            r\"^automa:.*$\",\n            r\"^automa$\",\n        ]\n\n        if not any(re.match(pattern, self.key) for pattern in allowed_patterns):\n            raise WorkerArgsInjectionError(\n                f\"Key '{self.key}' is not supported. Supported keys: \"\n                f\"`runtime_context`: a context for data persistence of the current worker.\"\n                f\"`automa:.*`: a sub-automa in current automa.\"\n            )\n</code></pre>"},{"location":"reference/bridgic-core/bridgic/core/automa/interaction/","title":"interaction","text":"<p>The Interaction module provides human-machine interaction mechanisms for Automa.</p> <p>This module contains several important interface definitions for implementing event  listening, feedback collection, and interactive control during Automa execution.</p> <p>There are two fundamental mechanisms for human-machine interaction in Automa: - [Event and Feedback Mechanism]: For simple interaction scenarios during Automa execution. - [Human Interaction Mechanism]: For longer-running interaction scenarios that require    interruption and resumption during Automa execution.</p>"},{"location":"reference/bridgic-core/bridgic/core/automa/interaction/#bridgic.core.automa.interaction.Event","title":"Event","text":"<p>               Bases: <code>BaseModel</code></p> <p>An event is a message that is sent from one worker inside the Automa to the application layer outside the Automa.</p> Fields <p>event_type: Optional[str]     The type of the event. The type of the event is used to identify the event handler registered to handle the event. timestamp: datetime     The timestamp of the event. data: Optional[Any]     The data attached to the event.</p> Source code in <code>bridgic/core/automa/interaction/_event_handling.py</code> <pre><code>class Event(BaseModel):\n    \"\"\"\n    An event is a message that is sent from one worker inside the Automa to the application layer outside the Automa.\n\n    Fields\n    ------\n    event_type: Optional[str]\n        The type of the event. The type of the event is used to identify the event handler registered to handle the event.\n    timestamp: datetime\n        The timestamp of the event.\n    data: Optional[Any]\n        The data attached to the event.\n    \"\"\"\n    event_type: Optional[str] = None\n    timestamp: datetime = datetime.now()\n    data: Optional[Any] = None\n</code></pre>"},{"location":"reference/bridgic-core/bridgic/core/automa/interaction/#bridgic.core.automa.interaction.Feedback","title":"Feedback","text":"<p>               Bases: <code>BaseModel</code></p> <p>A feedback is a message that is sent from the application layer outside the Automa to a worker inside the Automa.</p> Fields <p>data: Any     The data attached to the feedback.</p> Source code in <code>bridgic/core/automa/interaction/_event_handling.py</code> <pre><code>class Feedback(BaseModel):\n    \"\"\"\n    A feedback is a message that is sent from the application layer outside the Automa to a worker inside the Automa.\n\n    Fields\n    ------\n    data: Any\n        The data attached to the feedback.\n    \"\"\"\n    data: Any\n</code></pre>"},{"location":"reference/bridgic-core/bridgic/core/automa/interaction/#bridgic.core.automa.interaction.FeedbackSender","title":"FeedbackSender","text":"<p>               Bases: <code>ABC</code></p> <p>The appliction layer must use <code>FeedbackSender</code> to send back feedback to the worker inside the Automa.</p> Source code in <code>bridgic/core/automa/interaction/_event_handling.py</code> <pre><code>class FeedbackSender(ABC):\n    \"\"\"\n    The appliction layer must use `FeedbackSender` to send back feedback to the worker inside the Automa.\n    \"\"\"\n    @abstractmethod\n    def send(self, feedback: Feedback) -&gt; None:\n        \"\"\"\n        Send feedback to the Automa.\n        This method can be called only once for each event.\n\n        This `send` method can be safely called in several different scenarios:\n        - In the same asyncio Task of the same event loop as the event handler.\n        - In a different asyncio Task of the same event loop as the event handler.\n        - In a different thread from the event handler.\n\n        Parameters\n        ----------\n        feedback: Feedback\n            The feedback to be sent.\n        \"\"\"\n        ...\n</code></pre>"},{"location":"reference/bridgic-core/bridgic/core/automa/interaction/#bridgic.core.automa.interaction.FeedbackSender.send","title":"send","text":"<code>abstractmethod</code> <pre><code>send(feedback: Feedback) -&gt; None\n</code></pre> <p>Send feedback to the Automa. This method can be called only once for each event.</p> <p>This <code>send</code> method can be safely called in several different scenarios: - In the same asyncio Task of the same event loop as the event handler. - In a different asyncio Task of the same event loop as the event handler. - In a different thread from the event handler.</p> <p>Parameters:</p> Name Type Description Default <code>feedback</code> <code>Feedback</code> <p>The feedback to be sent.</p> required Source code in <code>bridgic/core/automa/interaction/_event_handling.py</code> <pre><code>@abstractmethod\ndef send(self, feedback: Feedback) -&gt; None:\n    \"\"\"\n    Send feedback to the Automa.\n    This method can be called only once for each event.\n\n    This `send` method can be safely called in several different scenarios:\n    - In the same asyncio Task of the same event loop as the event handler.\n    - In a different asyncio Task of the same event loop as the event handler.\n    - In a different thread from the event handler.\n\n    Parameters\n    ----------\n    feedback: Feedback\n        The feedback to be sent.\n    \"\"\"\n    ...\n</code></pre>"},{"location":"reference/bridgic-core/bridgic/core/automa/interaction/#bridgic.core.automa.interaction.InteractionFeedback","title":"InteractionFeedback","text":"<p>               Bases: <code>Feedback</code></p> <p>A feedback object that contains both the data provided by the user and the <code>interaction_id</code>, which uniquely identifies the corresponding interaction.</p> Source code in <code>bridgic/core/automa/interaction/_human_interaction.py</code> <pre><code>class InteractionFeedback(Feedback):\n    \"\"\"\n    A feedback object that contains both the data provided by the user and the `interaction_id`, which uniquely identifies the corresponding interaction.\n    \"\"\"\n    interaction_id: str\n    timestamp: datetime = datetime.now()\n</code></pre>"},{"location":"reference/bridgic-core/bridgic/core/automa/interaction/#bridgic.core.automa.interaction.InteractionException","title":"InteractionException","text":"<p>               Bases: <code>Exception</code></p> <p>Exception raised when the <code>interact_with_human</code> method is called and a human interaction is triggered.</p> Source code in <code>bridgic/core/automa/interaction/_human_interaction.py</code> <pre><code>class InteractionException(Exception):\n    \"\"\"\n    Exception raised when the `interact_with_human` method is called and a human interaction is triggered.\n    \"\"\"\n    _interactions: List[Interaction]\n    _snapshot: \"Snapshot\"\n\n    def __init__(self, interactions: List[Interaction], snapshot: \"Snapshot\"):\n        self._interactions = interactions\n        self._snapshot = snapshot\n\n    @property\n    def interactions(self) -&gt; List[Interaction]:\n        \"\"\"\n        Returns a list of `Interaction` objects that occurred during the most recent event loop.\n\n        Multiple `Interaction` objects may be generated because, within the same event loop, several workers calling the `interact_with_human` method might be running concurrently in parallel branches of the graph.\n        \"\"\"\n        return self._interactions\n\n    @property\n    def snapshot(self) -&gt; \"Snapshot\":\n        \"\"\"\n        Returns a `Snapshot` of the Automa's current state.\n        The serialization is automatically triggered by the `interact_with_human` method.\n        \"\"\"\n        return self._snapshot\n</code></pre>"},{"location":"reference/bridgic-core/bridgic/core/automa/interaction/#bridgic.core.automa.interaction.InteractionException.interactions","title":"interactions  <code>property</code>","text":"<pre><code>interactions: List[Interaction]\n</code></pre> <p>Returns a list of <code>Interaction</code> objects that occurred during the most recent event loop.</p> <p>Multiple <code>Interaction</code> objects may be generated because, within the same event loop, several workers calling the <code>interact_with_human</code> method might be running concurrently in parallel branches of the graph.</p>"},{"location":"reference/bridgic-core/bridgic/core/automa/interaction/#bridgic.core.automa.interaction.InteractionException.snapshot","title":"snapshot  <code>property</code>","text":"<pre><code>snapshot: Snapshot\n</code></pre> <p>Returns a <code>Snapshot</code> of the Automa's current state. The serialization is automatically triggered by the <code>interact_with_human</code> method.</p>"},{"location":"reference/bridgic-core/bridgic/core/automa/interaction/#bridgic.core.automa.interaction.Interaction","title":"Interaction","text":"<p>               Bases: <code>BaseModel</code></p> <p>Represents a single interaction between the Automa and a human.  Each call to <code>interact_with_human</code> will generate an <code>Interaction</code> object.</p> Fields <p>interaction_id: str     The unique identifier for the interaction. event: Event     The event that triggered the interaction.</p> Source code in <code>bridgic/core/automa/interaction/_human_interaction.py</code> <pre><code>class Interaction(BaseModel):\n    \"\"\"\n    Represents a single interaction between the Automa and a human. \n    Each call to `interact_with_human` will generate an `Interaction` object.\n\n    Fields\n    ------\n    interaction_id: str\n        The unique identifier for the interaction.\n    event: Event\n        The event that triggered the interaction.\n    \"\"\"\n    interaction_id: str\n    event: Event\n</code></pre>"},{"location":"reference/bridgic-core/bridgic/core/automa/worker/","title":"worker","text":"<p>The Worker module defines work nodes in Automa.</p> <p>This module contains the base classes of workers, which will be used within Automa. Workers are the basic execution units in Automa, with each work node typically  corresponding to a function (which can be synchronous or asynchronous) responsible  for executing specific business logic.</p>"},{"location":"reference/bridgic-core/bridgic/core/automa/worker/#bridgic.core.automa.worker.Worker","title":"Worker","text":"<p>               Bases: <code>Serializable</code></p> <p>This class is the base class for all workers.</p> <p><code>Worker</code> has two methods that may be overridden by the subclass:</p> <ol> <li> <p><code>arun()</code>: This asynchronous method should be implemented when your worker  does not require almost immediately scheduling after all its task dependencies  are fulfilled, and when overall workflow is not sensitive to the fair sharing  of CPU resources between workers. If workers can afford to retain and occupy  execution resources for their entire execution duration, and there is no  explicit need for fair CPU time-sharing or timely scheduling, you should  implement <code>arun()</code> and allow workers to run to completion as cooperative tasks  within the event loop.</p> </li> <li> <p><code>run()</code>: This synchronous method should be implemented when either of the  following holds:</p> <ul> <li> <p>a. The automa includes other workers that require timely access to CPU  resources (for example, workers that must respond quickly or are sensitive  to scheduling latency).</p> </li> <li> <p>b. The current worker itself should be scheduled as soon as all its task  dependencies are met, to maintain overall workflow responsiveness. In these  cases, <code>run()</code> enables the framework to offload your worker to a thread pool,  ensuring that CPU time is shared fairly among all such workers and the event  loop remains responsive.</p> </li> </ul> </li> </ol> <p>In summary, if you are unsure whether your task require quickly scheduling or not,  it is recommended to implement the <code>arun()</code> method. Otherwise, implement the  <code>run()</code> ONLY if you are certain that you agree to share CPU time slices  with other workers.</p> Source code in <code>bridgic/core/automa/worker/_worker.py</code> <pre><code>class Worker(Serializable):\n    \"\"\"\n    This class is the base class for all workers.\n\n    `Worker` has two methods that may be overridden by the subclass:\n\n    1. `arun()`: This asynchronous method should be implemented when your worker \n    does not require almost immediately scheduling after all its task dependencies \n    are fulfilled, and when overall workflow is not sensitive to the fair sharing \n    of CPU resources between workers. If workers can afford to retain and occupy \n    execution resources for their entire execution duration, and there is no \n    explicit need for fair CPU time-sharing or timely scheduling, you should \n    implement `arun()` and allow workers to run to completion as cooperative tasks \n    within the event loop.\n\n    2. `run()`: This synchronous method should be implemented when either of the \n    following holds:\n\n        - a. The automa includes other workers that require timely access to CPU \n        resources (for example, workers that must respond quickly or are sensitive \n        to scheduling latency).\n\n        - b. The current worker itself should be scheduled as soon as all its task \n        dependencies are met, to maintain overall workflow responsiveness. In these \n        cases, `run()` enables the framework to offload your worker to a thread pool, \n        ensuring that CPU time is shared fairly among all such workers and the event \n        loop remains responsive.\n\n    In summary, if you are unsure whether your task require quickly scheduling or not, \n    it is recommended to implement the `arun()` method. Otherwise, implement the \n    `run()` **ONLY** if you are certain that you agree to share CPU time slices \n    with other workers.\n    \"\"\"\n\n    # TODO : Maybe process pool of the Automa is needed.\n\n    __parent: \"Automa\"\n    __local_space: Dict[str, Any]\n\n    # Cached method signatures, with no need for serialization.\n    __cached_param_names_of_arun: Dict[_ParameterKind, List[Tuple[str, Any]]]\n    __cached_param_names_of_run: Dict[_ParameterKind, List[Tuple[str, Any]]]\n\n    def __init__(self):\n        self.__parent = None\n        self.__local_space = {}\n\n        # Cached method signatures, with no need for serialization.\n        self.__cached_param_names_of_arun = None\n        self.__cached_param_names_of_run = None\n\n    async def arun(self, *args: Tuple[Any, ...], **kwargs: Dict[str, Any]) -&gt; Any:\n        loop = asyncio.get_running_loop()\n        topest_automa = self._get_top_level_automa()\n        if topest_automa:\n            thread_pool = topest_automa.thread_pool\n            if thread_pool:\n                rx_param_names_dict = self.get_input_param_names()\n                rx_args, rx_kwargs = safely_map_args(args, kwargs, rx_param_names_dict)\n                # kwargs can only be passed by functools.partial.\n                return await loop.run_in_executor(thread_pool, partial(self.run, *rx_args, **rx_kwargs))\n\n        # Unexpected: No thread pool is available.\n        # Case 1: the worker is not inside an Automa (uncommon case).\n        # Case 2: no thread pool is setup by the top-level automa.\n        raise WorkerRuntimeError(f\"No thread pool is available for the worker {type(self)}\")\n\n    def run(self, *args: Tuple[Any, ...], **kwargs: Dict[str, Any]) -&gt; Any:\n        raise NotImplementedError(f\"run() is not implemented in {type(self)}\")\n\n    def _get_top_level_automa(self) -&gt; Optional[\"Automa\"]:\n        \"\"\"\n        Get the top-level automa instance reference.\n        \"\"\"\n        top_level_automa = self.parent\n        while top_level_automa and (not top_level_automa.is_top_level()):\n            top_level_automa = top_level_automa.parent\n        return top_level_automa\n\n    def get_input_param_names(self) -&gt; Dict[_ParameterKind, List[Tuple[str, Any]]]:\n        \"\"\"\n        Get the names of input parameters of the worker.\n        Use cached result if available in order to improve performance.\n\n        This method intelligently detects whether the user has overridden the `run` method\n        or is using the default `arun` method, and returns the appropriate parameter signature.\n\n        Returns\n        -------\n        Dict[_ParameterKind, List[str]]\n            A dictionary of input parameter names by the kind of the parameter.\n            The key is the kind of the parameter, which is one of five possible values:\n\n            - inspect.Parameter.POSITIONAL_ONLY\n            - inspect.Parameter.POSITIONAL_OR_KEYWORD\n            - inspect.Parameter.VAR_POSITIONAL\n            - inspect.Parameter.KEYWORD_ONLY\n            - inspect.Parameter.VAR_KEYWORD\n        \"\"\"\n        # Check if user has overridden the arun method\n        if self._is_arun_overridden():\n            # User overrode arun method, return arun method parameters\n            if self.__cached_param_names_of_arun is None:\n                self.__cached_param_names_of_arun = get_param_names_all_kinds(self.arun)\n            return self.__cached_param_names_of_arun\n        else:\n            # User is using run method, return run method parameters\n            if self.__cached_param_names_of_run is None:\n                self.__cached_param_names_of_run = get_param_names_all_kinds(self.run)\n            return self.__cached_param_names_of_run\n\n    def _is_arun_overridden(self) -&gt; bool:\n        \"\"\"\n        Check if the user has overridden the arun method.\n        \"\"\"\n        # Compare method references - much faster than inspect.getsource()\n        return self.arun.__func__ is not Worker.arun\n\n    @property\n    def parent(self) -&gt; \"Automa\":\n        return self.__parent\n\n    @parent.setter\n    def parent(self, value: \"Automa\"):\n        self.__parent = value\n\n    @property\n    def local_space(self) -&gt; Dict[str, Any]:\n        return self.__local_space\n\n    @local_space.setter\n    def local_space(self, value: Dict[str, Any]):\n        self.__local_space = value\n\n    @override\n    def dump_to_dict(self) -&gt; Dict[str, Any]:\n        state_dict = {}\n        state_dict[\"local_space\"] = self.__local_space\n        return state_dict\n\n    @override\n    def load_from_dict(self, state_dict: Dict[str, Any]) -&gt; None:\n        # Initialize parent to None - it will be set by the containing Automa\n        self.__parent = None\n        self.__local_space = state_dict[\"local_space\"]\n\n        # Cached method signatures, with no need for serialization.\n        self.__cached_param_names_of_arun = None\n        self.__cached_param_names_of_run = None\n\n    def ferry_to(self, key: str, /, *args, **kwargs):\n        \"\"\"\n        Handoff control flow to the specified worker, passing along any arguments as needed.\n        The specified worker will always start to run asynchronously in the next event loop, regardless of its dependencies.\n\n        Parameters\n        ----------\n        key : str\n            The key of the worker to run.\n        args : optional\n            Positional arguments to be passed.\n        kwargs : optional\n            Keyword arguments to be passed.\n        \"\"\"\n        if self.parent is None:\n            raise WorkerRuntimeError(f\"`ferry_to` method can only be called by a worker inside an Automa\")\n        self.parent.ferry_to(key, *args, **kwargs)\n\n    def post_event(self, event: Event) -&gt; None:\n        \"\"\"\n        Post an event to the application layer outside the Automa.\n\n        The event handler implemented by the application layer will be called in the same thread as the worker (maybe the main thread or a new thread from the thread pool).\n\n        Note that `post_event` can be called in a non-async method or an async method.\n\n        The event will be bubbled up to the top-level Automa, where it will be processed by the event handler registered with the event type.\n\n        Parameters\n        ----------\n        event: Event\n            The event to be posted.\n        \"\"\"\n        if self.parent is None:\n            raise WorkerRuntimeError(f\"`post_event` method can only be called by a worker inside an Automa\")\n        self.parent.post_event(event)\n\n    def request_feedback(\n        self, \n        event: Event,\n        timeout: Optional[float] = None\n    ) -&gt; Feedback:\n        \"\"\"\n        Request feedback for the specified event from the application layer outside the Automa. This method blocks the caller until the feedback is received.\n\n        Note that `post_event` should only be called from within a non-async method running in the new thread of the Automa thread pool.\n\n        Parameters\n        ----------\n        event: Event\n            The event to be posted to the event handler implemented by the application layer.\n        timeout: Optional[float]\n            A float or int number of seconds to wait for if the feedback is not received. If None, then there is no limit on the wait time.\n\n        Returns\n        -------\n        Feedback\n            The feedback received from the application layer.\n\n        Raises\n        ------\n        TimeoutError\n            If the feedback is not received before the timeout. Note that the raised exception is the built-in `TimeoutError` exception, instead of asyncio.TimeoutError or concurrent.futures.TimeoutError!\n        \"\"\"\n        if self.parent is None:\n            raise WorkerRuntimeError(f\"`request_feedback` method can only be called by a worker inside an Automa\")\n        return self.parent.request_feedback(event, timeout)\n\n    async def request_feedback_async(\n        self, \n        event: Event,\n        timeout: Optional[float] = None\n    ) -&gt; Feedback:\n        \"\"\"\n        Request feedback for the specified event from the application layer outside the Automa. This method blocks the caller until the feedback is received.\n\n        The event handler implemented by the application layer will be called in the next event loop, in the main thread.\n\n        Note that `post_event` should only be called from within an asynchronous method running in the main event loop of the top-level Automa.\n\n        Parameters\n        ----------\n        event: Event\n            The event to be posted to the event handler implemented by the application layer.\n        timeout: Optional[float]\n            A float or int number of seconds to wait for if the feedback is not received. If None, then there is no limit on the wait time.\n\n        Returns\n        -------\n        Feedback\n            The feedback received from the application layer.\n\n        Raises\n        ------\n        TimeoutError\n            If the feedback is not received before the timeout. Note that the raised exception is the built-in `TimeoutError` exception, instead of asyncio.TimeoutError!\n        \"\"\"\n        if self.parent is None:\n            raise WorkerRuntimeError(f\"`request_feedback_async` method can only be called by a worker inside an Automa\")\n        return await self.parent.request_feedback_async(event, timeout)\n\n    def interact_with_human(self, event: Event) -&gt; InteractionFeedback:\n        if self.parent is None:\n            raise WorkerRuntimeError(f\"`interact_with_human` method can only be called by a worker inside an Automa\")\n        return self.parent.interact_with_human(event, self)\n</code></pre>"},{"location":"reference/bridgic-core/bridgic/core/automa/worker/#bridgic.core.automa.worker.Worker.get_input_param_names","title":"get_input_param_names","text":"<pre><code>get_input_param_names() -&gt; (\n    Dict[_ParameterKind, List[Tuple[str, Any]]]\n)\n</code></pre> <p>Get the names of input parameters of the worker. Use cached result if available in order to improve performance.</p> <p>This method intelligently detects whether the user has overridden the <code>run</code> method or is using the default <code>arun</code> method, and returns the appropriate parameter signature.</p> <p>Returns:</p> Type Description <code>Dict[_ParameterKind, List[str]]</code> <p>A dictionary of input parameter names by the kind of the parameter. The key is the kind of the parameter, which is one of five possible values:</p> <ul> <li>inspect.Parameter.POSITIONAL_ONLY</li> <li>inspect.Parameter.POSITIONAL_OR_KEYWORD</li> <li>inspect.Parameter.VAR_POSITIONAL</li> <li>inspect.Parameter.KEYWORD_ONLY</li> <li>inspect.Parameter.VAR_KEYWORD</li> </ul> Source code in <code>bridgic/core/automa/worker/_worker.py</code> <pre><code>def get_input_param_names(self) -&gt; Dict[_ParameterKind, List[Tuple[str, Any]]]:\n    \"\"\"\n    Get the names of input parameters of the worker.\n    Use cached result if available in order to improve performance.\n\n    This method intelligently detects whether the user has overridden the `run` method\n    or is using the default `arun` method, and returns the appropriate parameter signature.\n\n    Returns\n    -------\n    Dict[_ParameterKind, List[str]]\n        A dictionary of input parameter names by the kind of the parameter.\n        The key is the kind of the parameter, which is one of five possible values:\n\n        - inspect.Parameter.POSITIONAL_ONLY\n        - inspect.Parameter.POSITIONAL_OR_KEYWORD\n        - inspect.Parameter.VAR_POSITIONAL\n        - inspect.Parameter.KEYWORD_ONLY\n        - inspect.Parameter.VAR_KEYWORD\n    \"\"\"\n    # Check if user has overridden the arun method\n    if self._is_arun_overridden():\n        # User overrode arun method, return arun method parameters\n        if self.__cached_param_names_of_arun is None:\n            self.__cached_param_names_of_arun = get_param_names_all_kinds(self.arun)\n        return self.__cached_param_names_of_arun\n    else:\n        # User is using run method, return run method parameters\n        if self.__cached_param_names_of_run is None:\n            self.__cached_param_names_of_run = get_param_names_all_kinds(self.run)\n        return self.__cached_param_names_of_run\n</code></pre>"},{"location":"reference/bridgic-core/bridgic/core/automa/worker/#bridgic.core.automa.worker.Worker.ferry_to","title":"ferry_to","text":"<pre><code>ferry_to(key: str, /, *args, **kwargs)\n</code></pre> <p>Handoff control flow to the specified worker, passing along any arguments as needed. The specified worker will always start to run asynchronously in the next event loop, regardless of its dependencies.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>The key of the worker to run.</p> required <code>args</code> <code>optional</code> <p>Positional arguments to be passed.</p> <code>()</code> <code>kwargs</code> <code>optional</code> <p>Keyword arguments to be passed.</p> <code>{}</code> Source code in <code>bridgic/core/automa/worker/_worker.py</code> <pre><code>def ferry_to(self, key: str, /, *args, **kwargs):\n    \"\"\"\n    Handoff control flow to the specified worker, passing along any arguments as needed.\n    The specified worker will always start to run asynchronously in the next event loop, regardless of its dependencies.\n\n    Parameters\n    ----------\n    key : str\n        The key of the worker to run.\n    args : optional\n        Positional arguments to be passed.\n    kwargs : optional\n        Keyword arguments to be passed.\n    \"\"\"\n    if self.parent is None:\n        raise WorkerRuntimeError(f\"`ferry_to` method can only be called by a worker inside an Automa\")\n    self.parent.ferry_to(key, *args, **kwargs)\n</code></pre>"},{"location":"reference/bridgic-core/bridgic/core/automa/worker/#bridgic.core.automa.worker.Worker.post_event","title":"post_event","text":"<pre><code>post_event(event: Event) -&gt; None\n</code></pre> <p>Post an event to the application layer outside the Automa.</p> <p>The event handler implemented by the application layer will be called in the same thread as the worker (maybe the main thread or a new thread from the thread pool).</p> <p>Note that <code>post_event</code> can be called in a non-async method or an async method.</p> <p>The event will be bubbled up to the top-level Automa, where it will be processed by the event handler registered with the event type.</p> <p>Parameters:</p> Name Type Description Default <code>event</code> <code>Event</code> <p>The event to be posted.</p> required Source code in <code>bridgic/core/automa/worker/_worker.py</code> <pre><code>def post_event(self, event: Event) -&gt; None:\n    \"\"\"\n    Post an event to the application layer outside the Automa.\n\n    The event handler implemented by the application layer will be called in the same thread as the worker (maybe the main thread or a new thread from the thread pool).\n\n    Note that `post_event` can be called in a non-async method or an async method.\n\n    The event will be bubbled up to the top-level Automa, where it will be processed by the event handler registered with the event type.\n\n    Parameters\n    ----------\n    event: Event\n        The event to be posted.\n    \"\"\"\n    if self.parent is None:\n        raise WorkerRuntimeError(f\"`post_event` method can only be called by a worker inside an Automa\")\n    self.parent.post_event(event)\n</code></pre>"},{"location":"reference/bridgic-core/bridgic/core/automa/worker/#bridgic.core.automa.worker.Worker.request_feedback","title":"request_feedback","text":"<pre><code>request_feedback(\n    event: Event, timeout: Optional[float] = None\n) -&gt; Feedback\n</code></pre> <p>Request feedback for the specified event from the application layer outside the Automa. This method blocks the caller until the feedback is received.</p> <p>Note that <code>post_event</code> should only be called from within a non-async method running in the new thread of the Automa thread pool.</p> <p>Parameters:</p> Name Type Description Default <code>event</code> <code>Event</code> <p>The event to be posted to the event handler implemented by the application layer.</p> required <code>timeout</code> <code>Optional[float]</code> <p>A float or int number of seconds to wait for if the feedback is not received. If None, then there is no limit on the wait time.</p> <code>None</code> <p>Returns:</p> Type Description <code>Feedback</code> <p>The feedback received from the application layer.</p> <p>Raises:</p> Type Description <code>TimeoutError</code> <p>If the feedback is not received before the timeout. Note that the raised exception is the built-in <code>TimeoutError</code> exception, instead of asyncio.TimeoutError or concurrent.futures.TimeoutError!</p> Source code in <code>bridgic/core/automa/worker/_worker.py</code> <pre><code>def request_feedback(\n    self, \n    event: Event,\n    timeout: Optional[float] = None\n) -&gt; Feedback:\n    \"\"\"\n    Request feedback for the specified event from the application layer outside the Automa. This method blocks the caller until the feedback is received.\n\n    Note that `post_event` should only be called from within a non-async method running in the new thread of the Automa thread pool.\n\n    Parameters\n    ----------\n    event: Event\n        The event to be posted to the event handler implemented by the application layer.\n    timeout: Optional[float]\n        A float or int number of seconds to wait for if the feedback is not received. If None, then there is no limit on the wait time.\n\n    Returns\n    -------\n    Feedback\n        The feedback received from the application layer.\n\n    Raises\n    ------\n    TimeoutError\n        If the feedback is not received before the timeout. Note that the raised exception is the built-in `TimeoutError` exception, instead of asyncio.TimeoutError or concurrent.futures.TimeoutError!\n    \"\"\"\n    if self.parent is None:\n        raise WorkerRuntimeError(f\"`request_feedback` method can only be called by a worker inside an Automa\")\n    return self.parent.request_feedback(event, timeout)\n</code></pre>"},{"location":"reference/bridgic-core/bridgic/core/automa/worker/#bridgic.core.automa.worker.Worker.request_feedback_async","title":"request_feedback_async","text":"<code>async</code> <pre><code>request_feedback_async(\n    event: Event, timeout: Optional[float] = None\n) -&gt; Feedback\n</code></pre> <p>Request feedback for the specified event from the application layer outside the Automa. This method blocks the caller until the feedback is received.</p> <p>The event handler implemented by the application layer will be called in the next event loop, in the main thread.</p> <p>Note that <code>post_event</code> should only be called from within an asynchronous method running in the main event loop of the top-level Automa.</p> <p>Parameters:</p> Name Type Description Default <code>event</code> <code>Event</code> <p>The event to be posted to the event handler implemented by the application layer.</p> required <code>timeout</code> <code>Optional[float]</code> <p>A float or int number of seconds to wait for if the feedback is not received. If None, then there is no limit on the wait time.</p> <code>None</code> <p>Returns:</p> Type Description <code>Feedback</code> <p>The feedback received from the application layer.</p> <p>Raises:</p> Type Description <code>TimeoutError</code> <p>If the feedback is not received before the timeout. Note that the raised exception is the built-in <code>TimeoutError</code> exception, instead of asyncio.TimeoutError!</p> Source code in <code>bridgic/core/automa/worker/_worker.py</code> <pre><code>async def request_feedback_async(\n    self, \n    event: Event,\n    timeout: Optional[float] = None\n) -&gt; Feedback:\n    \"\"\"\n    Request feedback for the specified event from the application layer outside the Automa. This method blocks the caller until the feedback is received.\n\n    The event handler implemented by the application layer will be called in the next event loop, in the main thread.\n\n    Note that `post_event` should only be called from within an asynchronous method running in the main event loop of the top-level Automa.\n\n    Parameters\n    ----------\n    event: Event\n        The event to be posted to the event handler implemented by the application layer.\n    timeout: Optional[float]\n        A float or int number of seconds to wait for if the feedback is not received. If None, then there is no limit on the wait time.\n\n    Returns\n    -------\n    Feedback\n        The feedback received from the application layer.\n\n    Raises\n    ------\n    TimeoutError\n        If the feedback is not received before the timeout. Note that the raised exception is the built-in `TimeoutError` exception, instead of asyncio.TimeoutError!\n    \"\"\"\n    if self.parent is None:\n        raise WorkerRuntimeError(f\"`request_feedback_async` method can only be called by a worker inside an Automa\")\n    return await self.parent.request_feedback_async(event, timeout)\n</code></pre>"},{"location":"reference/bridgic-core/bridgic/core/automa/worker/#bridgic.core.automa.worker.CallableWorker","title":"CallableWorker","text":"<p>               Bases: <code>Worker</code></p> <p>This class is a worker that wraps a callable object, such as functions or methods.</p> <p>Parameters:</p> Name Type Description Default <code>func_or_method</code> <code>Optional[Callable]</code> <p>The callable to be wrapped by the worker. If <code>func_or_method</code> is None,  <code>state_dict</code> must be provided.</p> <code>None</code> Source code in <code>bridgic/core/automa/worker/_callable_worker.py</code> <pre><code>class CallableWorker(Worker):\n    \"\"\"\n    This class is a worker that wraps a callable object, such as functions or methods.\n\n    Parameters\n    ----------\n    func_or_method : Optional[Callable]\n        The callable to be wrapped by the worker. If `func_or_method` is None, \n        `state_dict` must be provided.\n    \"\"\"\n    _is_async: bool\n    _callable: Callable\n    # Used to deserialization.\n    _expected_bound_parent: bool\n\n    # Cached method signatures, with no need for serialization.\n    __cached_param_names_of_callable: Dict[_ParameterKind, List[str]]\n\n    def __init__(\n        self, \n        func_or_method: Optional[Callable] = None,\n    ):\n        \"\"\"\n        Parameters\n        ----------\n        func_or_method : Optional[Callable]\n            The callable to be wrapped by the worker. If `func_or_method` is None, \n            `state_dict` must be provided.\n        \"\"\"\n        super().__init__()\n        self._is_async = inspect.iscoroutinefunction(func_or_method)\n        self._callable = func_or_method\n        self._expected_bound_parent = False\n\n        # Cached method signatures, with no need for serialization.\n        self.__cached_param_names_of_callable = None\n\n    async def arun(self, *args: Tuple[Any, ...], **kwargs: Dict[str, Any]) -&gt; Any:\n        if self._expected_bound_parent:\n            raise WorkerRuntimeError(\n                f\"The callable is expected to be bound to the parent, \"\n                f\"but not bounded yet: {self._callable}\"\n            )\n        if self._is_async:\n            return await self._callable(*args, **kwargs)\n        return await super().arun(*args, **kwargs)\n\n    def run(self, *args: Tuple[Any, ...], **kwargs: Dict[str, Any]) -&gt; Any:\n        assert self._is_async is False\n        return self._callable(*args, **kwargs)\n\n    @override\n    def get_input_param_names(self) -&gt; Dict[_ParameterKind, List[str]]:\n        \"\"\"\n        Get the names of input parameters of this callable worker.\n        Use cached result if available in order to improve performance.\n\n        Returns\n        -------\n        Dict[_ParameterKind, List[str]]\n            A dictionary of input parameter names by the kind of the parameter.\n            The key is the kind of the parameter, which is one of five possible values:\n\n            - inspect.Parameter.POSITIONAL_ONLY\n            - inspect.Parameter.POSITIONAL_OR_KEYWORD\n            - inspect.Parameter.VAR_POSITIONAL\n            - inspect.Parameter.KEYWORD_ONLY\n            - inspect.Parameter.VAR_KEYWORD\n        \"\"\"\n        if self.__cached_param_names_of_callable is None:\n            self.__cached_param_names_of_callable = get_param_names_all_kinds(self._callable)\n        return self.__cached_param_names_of_callable\n\n    @override\n    def dump_to_dict(self) -&gt; Dict[str, Any]:\n        state_dict = super().dump_to_dict()\n        state_dict[\"is_async\"] = self._is_async\n        # Note: Not to use pickle to serialize the callable here.\n        # We customize the serialization method of the callable to avoid creating instance multiple times and to minimize side effects.\n        bounded = isinstance(self._callable, MethodType)\n        state_dict[\"bounded\"] = bounded\n        if bounded:\n            if self._callable.__self__ is self.parent:\n                state_dict[\"callable_name\"] = self._callable.__module__ + \".\" + self._callable.__qualname__\n            else:\n                state_dict[\"pickled_callable\"] = pickle.dumps(self._callable)\n        else:\n            state_dict[\"callable_name\"] = self._callable.__module__ + \".\" + self._callable.__qualname__\n        return state_dict\n\n    @override\n    def load_from_dict(self, state_dict: Dict[str, Any]) -&gt; None:\n        super().load_from_dict(state_dict)\n        # Deserialize from the state_dict.\n        self._is_async = state_dict[\"is_async\"]\n        bounded = state_dict[\"bounded\"]\n        if bounded:\n            pickled_callable = state_dict.get(\"pickled_callable\", None)\n            if pickled_callable is None:\n                self._callable = load_qualified_class_or_func(state_dict[\"callable_name\"])\n                # Partially deserialized, need to be bound to the parent.\n                self._expected_bound_parent = True\n            else:\n                self._callable = pickle.loads(pickled_callable)\n                self._expected_bound_parent = False\n        else:\n            self._callable = load_qualified_class_or_func(state_dict[\"callable_name\"])\n            self._expected_bound_parent = False\n\n        # Cached method signatures, with no need for serialization.\n        self.__cached_param_names_of_callable = None\n\n    @property\n    def callable(self):\n        return self._callable\n\n    @property\n    def parent(self) -&gt; \"Automa\":\n        return super().parent\n\n    @parent.setter\n    def parent(self, value: \"Automa\"):\n        if self._expected_bound_parent:\n            self._callable = MethodType(self._callable, value)\n            self._expected_bound_parent = False\n        Worker.parent.fset(self, value)\n\n    @override\n    def __str__(self) -&gt; str:\n        return f\"CallableWorker(callable={self._callable.__name__})\"\n</code></pre>"},{"location":"reference/bridgic-core/bridgic/core/automa/worker/#bridgic.core.automa.worker.CallableWorker.get_input_param_names","title":"get_input_param_names","text":"<pre><code>get_input_param_names() -&gt; Dict[_ParameterKind, List[str]]\n</code></pre> <p>Get the names of input parameters of this callable worker. Use cached result if available in order to improve performance.</p> <p>Returns:</p> Type Description <code>Dict[_ParameterKind, List[str]]</code> <p>A dictionary of input parameter names by the kind of the parameter. The key is the kind of the parameter, which is one of five possible values:</p> <ul> <li>inspect.Parameter.POSITIONAL_ONLY</li> <li>inspect.Parameter.POSITIONAL_OR_KEYWORD</li> <li>inspect.Parameter.VAR_POSITIONAL</li> <li>inspect.Parameter.KEYWORD_ONLY</li> <li>inspect.Parameter.VAR_KEYWORD</li> </ul> Source code in <code>bridgic/core/automa/worker/_callable_worker.py</code> <pre><code>@override\ndef get_input_param_names(self) -&gt; Dict[_ParameterKind, List[str]]:\n    \"\"\"\n    Get the names of input parameters of this callable worker.\n    Use cached result if available in order to improve performance.\n\n    Returns\n    -------\n    Dict[_ParameterKind, List[str]]\n        A dictionary of input parameter names by the kind of the parameter.\n        The key is the kind of the parameter, which is one of five possible values:\n\n        - inspect.Parameter.POSITIONAL_ONLY\n        - inspect.Parameter.POSITIONAL_OR_KEYWORD\n        - inspect.Parameter.VAR_POSITIONAL\n        - inspect.Parameter.KEYWORD_ONLY\n        - inspect.Parameter.VAR_KEYWORD\n    \"\"\"\n    if self.__cached_param_names_of_callable is None:\n        self.__cached_param_names_of_callable = get_param_names_all_kinds(self._callable)\n    return self.__cached_param_names_of_callable\n</code></pre>"},{"location":"reference/bridgic-core/bridgic/core/model/","title":"model","text":"<p>The Model module provides core abstraction entities for LLMs (Large Language Models).</p> <p>This module defines core abstraction entities for interacting with models, providing  foundational type abstractions for different model implementations.</p>"},{"location":"reference/bridgic-core/bridgic/core/model/#bridgic.core.model.BaseLlm","title":"BaseLlm","text":"<p>               Bases: <code>ABC</code>, <code>Serializable</code></p> <p>Base class for Large Language Model implementations.</p> Source code in <code>bridgic/core/model/_base_llm.py</code> <pre><code>class BaseLlm(ABC, Serializable):\n    \"\"\"\n    Base class for Large Language Model implementations.\n    \"\"\"\n\n    @abstractmethod\n    def chat(self, messages: List[Message], **kwargs) -&gt; Response:\n        ...\n\n    @abstractmethod\n    def stream(self, messages: List[Message], **kwargs) -&gt; StreamResponse:\n        ...\n\n    @abstractmethod\n    async def achat(self, messages: List[Message], **kwargs) -&gt; Response:\n        ...\n\n    @abstractmethod\n    async def astream(self, messages: List[Message], **kwargs) -&gt; AsyncStreamResponse:\n        ...\n</code></pre>"},{"location":"reference/bridgic-core/bridgic/core/model/protocols/","title":"protocols","text":"<p>The Model Protocols module defines high-level interface protocols for model interaction.</p> <p>This module contains several important interface protocol definitions to provide  capabilities needed in real-world application development, such as tool selection and  structured output. These interfaces have clear input and output definitions and are  \"model-neutral\", aiming to reduce the details developers need to consider when  implementing features, thereby improving development efficiency.</p>"},{"location":"reference/bridgic-core/bridgic/core/model/protocols/#bridgic.core.model.protocols.ToolSelection","title":"ToolSelection","text":"<p>               Bases: <code>Protocol</code></p> <p>Protocol for LLM providers that support tool selection and parameter determination.</p> <p>ToolSelection defines the interface for language models that can intelligently  select appropriate tools from a given tools and determine the specific parameters  needed for tool execution.</p> <p>Methods:</p> Name Description <code>select_tool</code> <p>Synchronous method for tool selection based on conversation context.</p> <code>aselect_tool</code> <p>Asynchronous method for tool selection based on conversation context.</p> Notes <ol> <li>Both synchronous and asynchronous methods must be implemented</li> <li>Tool selection should be based on conversation context and available tools</li> <li>Return value includes both selected tool calls and optional response text</li> </ol> Source code in <code>bridgic/core/model/protocols/_tool_selection.py</code> <pre><code>class ToolSelection(Protocol):\n    \"\"\"\n    Protocol for LLM providers that support tool selection and parameter determination.\n\n    ToolSelection defines the interface for language models that can intelligently \n    select appropriate tools from a given tools and determine the specific parameters \n    needed for tool execution.\n\n    Methods\n    -------\n    select_tool\n        Synchronous method for tool selection based on conversation context.\n    aselect_tool\n        Asynchronous method for tool selection based on conversation context.\n\n    Notes\n    ----\n    1. Both synchronous and asynchronous methods must be implemented\n    2. Tool selection should be based on conversation context and available tools\n    3. Return value includes both selected tool calls and optional response text\n    \"\"\"\n\n    def select_tool(\n        self,\n        messages: List[Message],\n        tools: List[Tool],\n        **kwargs,\n    ) -&gt; Tuple[List[ToolCall], Optional[str]]:\n        \"\"\"\n        Select appropriate tools and determine their parameters based on conversation context.\n\n        Parameters\n        ----------\n        messages : List[Message]\n            The conversation history and current context.\n        tools : List[Tool]\n            Available tools that can be selected for use.\n        **kwargs\n            Additional keyword arguments for tool selection configuration.\n\n        Returns\n        -------\n        Tuple[List[ToolCall], Optional[str]]\n            A tuple containing:\n            - List of selected tool calls with determined parameters\n            - Optional response text from the LLM\n        \"\"\"\n        ...\n\n    async def aselect_tool(\n        self,\n        messages: List[Message],\n        tools: List[Tool],\n        **kwargs,\n    ) -&gt; Tuple[List[ToolCall], Optional[str]]:\n        \"\"\"\n        Asynchronously select appropriate tools and determine their parameters.\n\n        Parameters\n        ----------\n        messages : List[Message]\n            The conversation history and current context.\n        tools : List[Tool]\n            Available tools that can be selected for use.\n        **kwargs\n            Additional keyword arguments for tool selection configuration.\n\n        Returns\n        -------\n        Tuple[List[ToolCall], Optional[str]]\n            A tuple containing:\n            - List of selected tool calls with determined parameters\n            - Optional response text from the LLM\n        \"\"\"\n        ...\n</code></pre>"},{"location":"reference/bridgic-core/bridgic/core/model/protocols/#bridgic.core.model.protocols.ToolSelection.select_tool","title":"select_tool","text":"<pre><code>select_tool(\n    messages: List[Message], tools: List[Tool], **kwargs\n) -&gt; Tuple[List[ToolCall], Optional[str]]\n</code></pre> <p>Select appropriate tools and determine their parameters based on conversation context.</p> <p>Parameters:</p> Name Type Description Default <code>messages</code> <code>List[Message]</code> <p>The conversation history and current context.</p> required <code>tools</code> <code>List[Tool]</code> <p>Available tools that can be selected for use.</p> required <code>**kwargs</code> <p>Additional keyword arguments for tool selection configuration.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Tuple[List[ToolCall], Optional[str]]</code> <p>A tuple containing: - List of selected tool calls with determined parameters - Optional response text from the LLM</p> Source code in <code>bridgic/core/model/protocols/_tool_selection.py</code> <pre><code>def select_tool(\n    self,\n    messages: List[Message],\n    tools: List[Tool],\n    **kwargs,\n) -&gt; Tuple[List[ToolCall], Optional[str]]:\n    \"\"\"\n    Select appropriate tools and determine their parameters based on conversation context.\n\n    Parameters\n    ----------\n    messages : List[Message]\n        The conversation history and current context.\n    tools : List[Tool]\n        Available tools that can be selected for use.\n    **kwargs\n        Additional keyword arguments for tool selection configuration.\n\n    Returns\n    -------\n    Tuple[List[ToolCall], Optional[str]]\n        A tuple containing:\n        - List of selected tool calls with determined parameters\n        - Optional response text from the LLM\n    \"\"\"\n    ...\n</code></pre>"},{"location":"reference/bridgic-core/bridgic/core/model/protocols/#bridgic.core.model.protocols.ToolSelection.aselect_tool","title":"aselect_tool","text":"<code>async</code> <pre><code>aselect_tool(\n    messages: List[Message], tools: List[Tool], **kwargs\n) -&gt; Tuple[List[ToolCall], Optional[str]]\n</code></pre> <p>Asynchronously select appropriate tools and determine their parameters.</p> <p>Parameters:</p> Name Type Description Default <code>messages</code> <code>List[Message]</code> <p>The conversation history and current context.</p> required <code>tools</code> <code>List[Tool]</code> <p>Available tools that can be selected for use.</p> required <code>**kwargs</code> <p>Additional keyword arguments for tool selection configuration.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Tuple[List[ToolCall], Optional[str]]</code> <p>A tuple containing: - List of selected tool calls with determined parameters - Optional response text from the LLM</p> Source code in <code>bridgic/core/model/protocols/_tool_selection.py</code> <pre><code>async def aselect_tool(\n    self,\n    messages: List[Message],\n    tools: List[Tool],\n    **kwargs,\n) -&gt; Tuple[List[ToolCall], Optional[str]]:\n    \"\"\"\n    Asynchronously select appropriate tools and determine their parameters.\n\n    Parameters\n    ----------\n    messages : List[Message]\n        The conversation history and current context.\n    tools : List[Tool]\n        Available tools that can be selected for use.\n    **kwargs\n        Additional keyword arguments for tool selection configuration.\n\n    Returns\n    -------\n    Tuple[List[ToolCall], Optional[str]]\n        A tuple containing:\n        - List of selected tool calls with determined parameters\n        - Optional response text from the LLM\n    \"\"\"\n    ...\n</code></pre>"},{"location":"reference/bridgic-core/bridgic/core/model/protocols/#bridgic.core.model.protocols.StructuredOutput","title":"StructuredOutput","text":"<p>               Bases: <code>Protocol</code></p> <p>Protocol for LLM providers that support structured output generation.</p> <p>StructuredOutput defines the interface for language models that can generate  responses in specific formats according to given constraints. This protocol  enables controlled output generation for various data structures and formats.</p> <p>Methods:</p> Name Description <code>structured_output</code> <p>Synchronous method for generating structured output based on constraints.</p> <code>astructured_output</code> <p>Asynchronous method for generating structured output based on constraints.</p> Notes <ol> <li>Both synchronous and asynchronous methods must be implemented</li> <li>Supported constraint types depend on the specific LLM provider implementation</li> <li>Output format is determined by the constraint type provided</li> <li>Common constraint types include PydanticModel, JsonSchema, Regex, Choice, etc.</li> </ol> Source code in <code>bridgic/core/model/protocols/_structured_output.py</code> <pre><code>class StructuredOutput(Protocol):\n    \"\"\"\n    Protocol for LLM providers that support structured output generation.\n\n    StructuredOutput defines the interface for language models that can generate \n    responses in specific formats according to given constraints. This protocol \n    enables controlled output generation for various data structures and formats.\n\n    Methods\n    -------\n    structured_output\n        Synchronous method for generating structured output based on constraints.\n    astructured_output\n        Asynchronous method for generating structured output based on constraints.\n\n    Notes\n    ----\n    1. Both synchronous and asynchronous methods must be implemented\n    2. Supported constraint types depend on the specific LLM provider implementation\n    3. Output format is determined by the constraint type provided\n    4. Common constraint types include PydanticModel, JsonSchema, Regex, Choice, etc.\n    \"\"\"\n\n    def structured_output(\n        self,\n        messages: List[Message],\n        constraint: Constraint,\n        **kwargs,\n    ) -&gt; Any:\n        \"\"\"\n        Generate structured output based on conversation context and constraints.\n\n        Parameters\n        ----------\n        messages : List[Message]\n            The conversation history and current context.\n        constraint : Constraint\n            The output format constraint. Supported types:\n            - PydanticModel: Output as Pydantic model instance\n            - JsonSchema: Output as JSON matching the schema\n            - Regex: Output matching the regex pattern\n            - Choice: Output from predefined choices\n            - EbnfGrammar: Output following EBNF grammar rules\n            - LarkGrammar: Output following Lark grammar rules\n        **kwargs\n            Additional keyword arguments for output generation configuration.\n\n        Returns\n        -------\n        Any\n            The structured output matching the specified constraint format.\n        \"\"\"\n        ...\n\n    async def astructured_output(\n        self,\n        messages: List[Message],\n        constraint: Constraint,\n        **kwargs,\n    ) -&gt; Any:\n        \"\"\"\n        Asynchronously generate structured output based on conversation context and constraints.\n\n        Parameters\n        ----------\n        messages : List[Message]\n            The conversation history and current context.\n        constraint : Constraint\n            The output format constraint. Supported types:\n            - PydanticModel: Output as Pydantic model instance\n            - JsonSchema: Output as JSON matching the schema\n            - Regex: Output matching the regex pattern\n            - Choice: Output from predefined choices\n            - EbnfGrammar: Output following EBNF grammar rules\n            - LarkGrammar: Output following Lark grammar rules\n        **kwargs\n            Additional keyword arguments for output generation configuration.\n\n        Returns\n        -------\n        Any\n            The structured output matching the specified constraint format.\n        \"\"\"\n        ...\n</code></pre>"},{"location":"reference/bridgic-core/bridgic/core/model/protocols/#bridgic.core.model.protocols.StructuredOutput.structured_output","title":"structured_output","text":"<pre><code>structured_output(\n    messages: List[Message],\n    constraint: Constraint,\n    **kwargs\n) -&gt; Any\n</code></pre> <p>Generate structured output based on conversation context and constraints.</p> <p>Parameters:</p> Name Type Description Default <code>messages</code> <code>List[Message]</code> <p>The conversation history and current context.</p> required <code>constraint</code> <code>Constraint</code> <p>The output format constraint. Supported types: - PydanticModel: Output as Pydantic model instance - JsonSchema: Output as JSON matching the schema - Regex: Output matching the regex pattern - Choice: Output from predefined choices - EbnfGrammar: Output following EBNF grammar rules - LarkGrammar: Output following Lark grammar rules</p> required <code>**kwargs</code> <p>Additional keyword arguments for output generation configuration.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Any</code> <p>The structured output matching the specified constraint format.</p> Source code in <code>bridgic/core/model/protocols/_structured_output.py</code> <pre><code>def structured_output(\n    self,\n    messages: List[Message],\n    constraint: Constraint,\n    **kwargs,\n) -&gt; Any:\n    \"\"\"\n    Generate structured output based on conversation context and constraints.\n\n    Parameters\n    ----------\n    messages : List[Message]\n        The conversation history and current context.\n    constraint : Constraint\n        The output format constraint. Supported types:\n        - PydanticModel: Output as Pydantic model instance\n        - JsonSchema: Output as JSON matching the schema\n        - Regex: Output matching the regex pattern\n        - Choice: Output from predefined choices\n        - EbnfGrammar: Output following EBNF grammar rules\n        - LarkGrammar: Output following Lark grammar rules\n    **kwargs\n        Additional keyword arguments for output generation configuration.\n\n    Returns\n    -------\n    Any\n        The structured output matching the specified constraint format.\n    \"\"\"\n    ...\n</code></pre>"},{"location":"reference/bridgic-core/bridgic/core/model/protocols/#bridgic.core.model.protocols.StructuredOutput.astructured_output","title":"astructured_output","text":"<code>async</code> <pre><code>astructured_output(\n    messages: List[Message],\n    constraint: Constraint,\n    **kwargs\n) -&gt; Any\n</code></pre> <p>Asynchronously generate structured output based on conversation context and constraints.</p> <p>Parameters:</p> Name Type Description Default <code>messages</code> <code>List[Message]</code> <p>The conversation history and current context.</p> required <code>constraint</code> <code>Constraint</code> <p>The output format constraint. Supported types: - PydanticModel: Output as Pydantic model instance - JsonSchema: Output as JSON matching the schema - Regex: Output matching the regex pattern - Choice: Output from predefined choices - EbnfGrammar: Output following EBNF grammar rules - LarkGrammar: Output following Lark grammar rules</p> required <code>**kwargs</code> <p>Additional keyword arguments for output generation configuration.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Any</code> <p>The structured output matching the specified constraint format.</p> Source code in <code>bridgic/core/model/protocols/_structured_output.py</code> <pre><code>async def astructured_output(\n    self,\n    messages: List[Message],\n    constraint: Constraint,\n    **kwargs,\n) -&gt; Any:\n    \"\"\"\n    Asynchronously generate structured output based on conversation context and constraints.\n\n    Parameters\n    ----------\n    messages : List[Message]\n        The conversation history and current context.\n    constraint : Constraint\n        The output format constraint. Supported types:\n        - PydanticModel: Output as Pydantic model instance\n        - JsonSchema: Output as JSON matching the schema\n        - Regex: Output matching the regex pattern\n        - Choice: Output from predefined choices\n        - EbnfGrammar: Output following EBNF grammar rules\n        - LarkGrammar: Output following Lark grammar rules\n    **kwargs\n        Additional keyword arguments for output generation configuration.\n\n    Returns\n    -------\n    Any\n        The structured output matching the specified constraint format.\n    \"\"\"\n    ...\n</code></pre>"},{"location":"reference/bridgic-core/bridgic/core/model/types/","title":"types","text":"<p>The Model Types module defines core data types for interacting with models.</p> <p>This module contains type definitions for messages, content blocks, tool calls,  responses, and more, providing a unified data structure representation for model  input and output.</p>"},{"location":"reference/bridgic-core/bridgic/core/model/types/#bridgic.core.model.types.TextBlock","title":"TextBlock","text":"<p>               Bases: <code>BaseModel</code></p> <p>Encapsulates plain text data that is passed to or received from language models.</p> <p>Attributes:</p> Name Type Description <code>block_type</code> <code>Literal['text']</code> <p>The type identifier for this content block.</p> <code>text</code> <code>str</code> <p>The actual text content.</p> Source code in <code>bridgic/core/model/types/_content_block.py</code> <pre><code>class TextBlock(BaseModel):\n    \"\"\"\n    Encapsulates plain text data that is passed to or received from language models.\n\n    Attributes\n    ----------\n    block_type : Literal[\"text\"]\n        The type identifier for this content block.\n    text : str\n        The actual text content.\n    \"\"\"\n    block_type: Literal[\"text\"] = Field(default=\"text\")\n    text: str\n</code></pre>"},{"location":"reference/bridgic-core/bridgic/core/model/types/#bridgic.core.model.types.ToolCallBlock","title":"ToolCallBlock","text":"<p>               Bases: <code>BaseModel</code></p> <p>Encapsulates tool invocation data that is received from language models.</p> <p>Attributes:</p> Name Type Description <code>block_type</code> <code>Literal['tool_call']</code> <p>The type identifier for this content block.</p> <code>id</code> <code>str</code> <p>Unique identifier for the tool call instance.</p> <code>name</code> <code>str</code> <p>Name of the tool to be called.</p> <code>arguments</code> <code>Dict[str, Any]</code> <p>Parameters to be passed to the tool function.</p> Source code in <code>bridgic/core/model/types/_content_block.py</code> <pre><code>class ToolCallBlock(BaseModel):\n    \"\"\"\n    Encapsulates tool invocation data that is received from language models.\n\n    Attributes\n    ----------\n    block_type : Literal[\"tool_call\"]\n        The type identifier for this content block.\n    id : str\n        Unique identifier for the tool call instance.\n    name : str\n        Name of the tool to be called.\n    arguments : Dict[str, Any]\n        Parameters to be passed to the tool function.\n    \"\"\"\n    block_type: Literal[\"tool_call\"] = Field(default=\"tool_call\")\n    id: str = Field(..., description=\"The ID of the tool call.\")\n    name: str = Field(..., description=\"The name of the tool call.\")\n    arguments: Dict[str, Any] = Field(..., description=\"The arguments of the tool call.\")\n</code></pre>"},{"location":"reference/bridgic-core/bridgic/core/model/types/#bridgic.core.model.types.ToolResultBlock","title":"ToolResultBlock","text":"<p>               Bases: <code>BaseModel</code></p> <p>Encapsulates the results returned from tool executions.</p> <p>Attributes:</p> Name Type Description <code>block_type</code> <code>Literal['tool_result']</code> <p>The type identifier for this content block.</p> <code>id</code> <code>str</code> <p>Unique identifier matching the corresponding tool call.</p> <code>content</code> <code>str</code> <p>The result content returned from the tool execution.</p> Source code in <code>bridgic/core/model/types/_content_block.py</code> <pre><code>class ToolResultBlock(BaseModel):\n    \"\"\"\n    Encapsulates the results returned from tool executions.\n\n    Attributes\n    ----------\n    block_type : Literal[\"tool_result\"]\n        The type identifier for this content block.\n    id : str\n        Unique identifier matching the corresponding tool call.\n    content : str\n        The result content returned from the tool execution.\n    \"\"\"\n    block_type: Literal[\"tool_result\"] = Field(default=\"tool_result\")\n    id: str = Field(..., description=\"The ID of the tool call.\")\n    content: str = Field(..., description=\"The result content of the tool call.\")\n</code></pre>"},{"location":"reference/bridgic-core/bridgic/core/model/types/#bridgic.core.model.types.Role","title":"Role","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p> <p>Message role enumeration for LLM conversations.</p> <p>Defines the different roles that can be assigned to messages in a conversation with language models, following standard chat completion formats.</p> <p>Attributes:</p> Name Type Description <code>SYSTEM</code> <code>str</code> <p>System role for providing instructions or context to the model.</p> <code>USER</code> <code>str</code> <p>User role for human input and queries.</p> <code>AI</code> <code>str</code> <p>Assistant role for model responses and outputs.</p> <code>TOOL</code> <code>str</code> <p>Tool role for tool execution results and responses.</p> Source code in <code>bridgic/core/model/types/_message.py</code> <pre><code>class Role(str, Enum):\n    \"\"\"\n    Message role enumeration for LLM conversations.\n\n    Defines the different roles that can be assigned to messages in a conversation\n    with language models, following standard chat completion formats.\n\n    Attributes\n    ----------\n    SYSTEM : str\n        System role for providing instructions or context to the model.\n    USER : str\n        User role for human input and queries.\n    AI : str\n        Assistant role for model responses and outputs.\n    TOOL : str\n        Tool role for tool execution results and responses.\n    \"\"\"\n    SYSTEM = \"system\"\n    USER = \"user\"\n    AI = \"assistant\"\n    TOOL = \"tool\"\n\n    @classmethod\n    def get_all_roles(cls) -&gt; List[str]:\n        return [role.value for role in Role]\n</code></pre>"},{"location":"reference/bridgic-core/bridgic/core/model/types/#bridgic.core.model.types.Message","title":"Message","text":"<p>               Bases: <code>BaseModel</code></p> <p>LLM message container for conversation exchanges.</p> <p>Represents a single message in a conversation with language models, containing role information, content blocks, and optional metadata. Supports various content types including text, tool calls, and tool results.</p> <p>Attributes:</p> Name Type Description <code>role</code> <code>Role</code> <p>The role of the message sender (system, user, assistant, or tool).</p> <code>blocks</code> <code>List[ContentBlock]</code> <p>List of content blocks containing the actual message data.</p> <code>extras</code> <code>Dict[str, Any]</code> <p>Additional metadata and custom fields for the message.</p> Source code in <code>bridgic/core/model/types/_message.py</code> <pre><code>class Message(BaseModel):\n    \"\"\"\n    LLM message container for conversation exchanges.\n\n    Represents a single message in a conversation with language models, containing\n    role information, content blocks, and optional metadata. Supports various\n    content types including text, tool calls, and tool results.\n\n    Attributes\n    ----------\n    role : Role\n        The role of the message sender (system, user, assistant, or tool).\n    blocks : List[ContentBlock]\n        List of content blocks containing the actual message data.\n    extras : Dict[str, Any]\n        Additional metadata and custom fields for the message.\n    \"\"\"\n    role: Role = Field(default=Role.USER)\n    blocks: List[ContentBlock] = Field(default=[])\n    extras: Dict[str, Any] = Field(default={})\n\n    @classmethod\n    def from_text(\n        cls,\n        text: str,\n        role: Union[Role, str] = Role.USER,\n        extras: Optional[Dict[str, Any]] = {},\n    ) -&gt; \"Message\":\n        if isinstance(role, str):\n            role = Role(role)\n        return cls(role=role, blocks=[TextBlock(text=text)], extras=extras)\n\n    @classmethod\n    def from_tool_call(\n        cls,\n        tool_calls: Union[\n            \"ToolCallDict\", \n            List[\"ToolCallDict\"], \n            \"ToolCall\",\n            List[\"ToolCall\"]\n        ],\n        text: Optional[str] = None,\n        extras: Optional[Dict[str, Any]] = {},\n    ) -&gt; \"Message\":\n        \"\"\"\n        Create a message with tool call blocks and optional text content.\n\n        Parameters\n        ----------\n        tool_calls : Union[ToolCallDict, List[ToolCallDict], ToolCall, List[ToolCall]]\n            Tool call data in various formats:\n            - Single tool call dict: {\"id\": \"call_123\", \"name\": \"get_weather\", \"arguments\": {...}}\n            - List of tool call dicts: [{\"id\": \"call_123\", ...}, {\"id\": \"call_124\", ...}]\n            - Single ToolCall instance\n            - List of ToolCall instances\n        text : Optional[str], optional\n            Optional text content to include in the message\n\n        extras : Optional[Dict[str, Any]], optional\n            Additional metadata for the message\n\n        Returns\n        -------\n        Message\n            A message containing the tool call blocks and optional text\n\n        Examples\n        --------\n        &gt;&gt;&gt; # Build from single tool call dict.\n        ... message = Message.from_tool_call(\n        ...     tool_calls={\n        ...         \"id\": \"call_id_123\",\n        ...         \"name\": \"get_weather\",\n        ...         \"arguments\": {\"city\": \"Tokyo\", \"unit\": \"celsius\"}\n        ...     },\n        ...     text=\"I will check the weather for you.\"\n        ... )\n\n        &gt;&gt;&gt; # Build from multiple tool call dicts.\n        ... message = Message.from_tool_call(\n        ...     tool_calls=[\n        ...         {\"id\": \"call_id_123\", \"name\": \"get_weather\", \"arguments\": {\"city\": \"Tokyo\"}},\n        ...         {\"id\": \"call_id_456\", \"name\": \"get_news\", \"arguments\": {\"topic\": \"weather\"}},\n        ...     ],\n        ...     text=\"I will get weather and news for you.\"\n        ... )\n\n        &gt;&gt;&gt; # Build from single ToolCall object.\n        ... tool_call = ToolCall(id=\"call_123\", name=\"get_weather\", arguments={\"city\": \"Tokyo\"})\n        ... message = Message.from_tool_call(tool_calls=tool_call, text=\"I will check the weather.\")\n\n        &gt;&gt;&gt; # Build from multiple ToolCall objects.\n        ... tool_calls = [\n        ...     ToolCall(id=\"call_id_123\", name=\"get_weather\", arguments={\"city\": \"Tokyo\"}),\n        ...     ToolCall(id=\"call_id_456\", name=\"get_news\", arguments={\"topic\": \"weather\"}),\n        ... ]\n        ... message = Message.from_tool_call(tool_calls=tool_calls, text=\"I will get weather and news.\")\n        \"\"\"\n        role = Role(Role.AI)\n        blocks = []\n\n        # Add text content if provided\n        if text:\n            blocks.append(TextBlock(text=text))\n\n        # Handle different tool_calls formats\n        if isinstance(tool_calls, dict):\n            # Single tool call dict\n            tool_calls = [tool_calls]\n        if isinstance(tool_calls, list):\n            # List of tool calls (dicts or ToolCall)\n            for tool_call in tool_calls:\n                if isinstance(tool_call, dict):\n                    # Tool call dict\n                    blocks.append(ToolCallBlock(\n                        id=tool_call[\"id\"],\n                        name=tool_call[\"name\"],\n                        arguments=tool_call[\"arguments\"]\n                    ))\n                elif hasattr(tool_call, 'id') and hasattr(tool_call, 'name') and hasattr(tool_call, 'arguments'):\n                    blocks.append(ToolCallBlock(\n                        id=tool_call.id,\n                        name=tool_call.name,\n                        arguments=tool_call.arguments\n                    ))\n                else:\n                    raise ValueError(f\"Invalid tool call format: {tool_call}\")\n        elif hasattr(tool_calls, 'id') and hasattr(tool_calls, 'name') and hasattr(tool_calls, 'arguments'):\n            blocks.append(ToolCallBlock(\n                id=tool_calls.id,\n                name=tool_calls.name,\n                arguments=tool_calls.arguments\n            ))\n        else:\n            raise ValueError(f\"Invalid tool_calls format: {type(tool_calls)}\")\n\n        return cls(role=role, blocks=blocks, extras=extras)\n\n    @classmethod\n    def from_tool_result(\n        cls,\n        tool_id: str,\n        content: str,\n        extras: Optional[Dict[str, Any]] = {},\n    ) -&gt; \"Message\":\n        \"\"\"\n        Create a message with a tool result block.\n\n        Parameters\n        ----------\n        tool_id : str\n            The ID of the tool call that this result corresponds to\n        content : str\n            The result content from the tool execution\n        extras : Optional[Dict[str, Any]], optional\n            Additional metadata for the message\n\n        Returns\n        -------\n        Message\n            A message containing the tool result block\n\n        Examples\n        --------\n        &gt;&gt;&gt; message = Message.from_tool_result(\n        ...     tool_id=\"call_id_123\",\n        ...     content=\"The weather in Tokyo is 22\u00b0C and sunny.\"\n        ... )\n        \"\"\"\n        role = Role(Role.TOOL)\n        return cls(\n            role=role, \n            blocks=[ToolResultBlock(id=tool_id, content=content)], \n            extras=extras\n        )\n\n    @property\n    def content(self) -&gt; str:\n        return \"\\n\\n\".join([block.text for block in self.blocks if isinstance(block, TextBlock)])\n\n    @content.setter\n    def content(self, text: str):\n        if not self.blocks:\n            self.blocks = [TextBlock(text=text)]\n        elif len(self.blocks) == 1 and isinstance(self.blocks[0], TextBlock):\n            self.blocks = [TextBlock(text=text)]\n        else:\n            raise ValueError(\n                \"Message contains multiple blocks or contains a non-text block, thus it could not be \"\n                \"easily set by the property \\\"Message.content\\\". Use \\\"Message.blocks\\\" instead.\"\n            )\n</code></pre>"},{"location":"reference/bridgic-core/bridgic/core/model/types/#bridgic.core.model.types.Message.from_tool_call","title":"from_tool_call","text":"<code>classmethod</code> <pre><code>from_tool_call(\n    tool_calls: Union[\n        ToolCallDict,\n        List[ToolCallDict],\n        ToolCall,\n        List[ToolCall],\n    ],\n    text: Optional[str] = None,\n    extras: Optional[Dict[str, Any]] = {},\n) -&gt; Message\n</code></pre> <p>Create a message with tool call blocks and optional text content.</p> <p>Parameters:</p> Name Type Description Default <code>tool_calls</code> <code>Union[ToolCallDict, List[ToolCallDict], ToolCall, List[ToolCall]]</code> <p>Tool call data in various formats: - Single tool call dict: {\"id\": \"call_123\", \"name\": \"get_weather\", \"arguments\": {...}} - List of tool call dicts: [{\"id\": \"call_123\", ...}, {\"id\": \"call_124\", ...}] - Single ToolCall instance - List of ToolCall instances</p> required <code>text</code> <code>Optional[str]</code> <p>Optional text content to include in the message</p> <code>None</code> <code>extras</code> <code>Optional[Dict[str, Any]]</code> <p>Additional metadata for the message</p> <code>{}</code> <p>Returns:</p> Type Description <code>Message</code> <p>A message containing the tool call blocks and optional text</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; # Build from single tool call dict.\n... message = Message.from_tool_call(\n...     tool_calls={\n...         \"id\": \"call_id_123\",\n...         \"name\": \"get_weather\",\n...         \"arguments\": {\"city\": \"Tokyo\", \"unit\": \"celsius\"}\n...     },\n...     text=\"I will check the weather for you.\"\n... )\n</code></pre> <pre><code>&gt;&gt;&gt; # Build from multiple tool call dicts.\n... message = Message.from_tool_call(\n...     tool_calls=[\n...         {\"id\": \"call_id_123\", \"name\": \"get_weather\", \"arguments\": {\"city\": \"Tokyo\"}},\n...         {\"id\": \"call_id_456\", \"name\": \"get_news\", \"arguments\": {\"topic\": \"weather\"}},\n...     ],\n...     text=\"I will get weather and news for you.\"\n... )\n</code></pre> <pre><code>&gt;&gt;&gt; # Build from single ToolCall object.\n... tool_call = ToolCall(id=\"call_123\", name=\"get_weather\", arguments={\"city\": \"Tokyo\"})\n... message = Message.from_tool_call(tool_calls=tool_call, text=\"I will check the weather.\")\n</code></pre> <pre><code>&gt;&gt;&gt; # Build from multiple ToolCall objects.\n... tool_calls = [\n...     ToolCall(id=\"call_id_123\", name=\"get_weather\", arguments={\"city\": \"Tokyo\"}),\n...     ToolCall(id=\"call_id_456\", name=\"get_news\", arguments={\"topic\": \"weather\"}),\n... ]\n... message = Message.from_tool_call(tool_calls=tool_calls, text=\"I will get weather and news.\")\n</code></pre> Source code in <code>bridgic/core/model/types/_message.py</code> <pre><code>@classmethod\ndef from_tool_call(\n    cls,\n    tool_calls: Union[\n        \"ToolCallDict\", \n        List[\"ToolCallDict\"], \n        \"ToolCall\",\n        List[\"ToolCall\"]\n    ],\n    text: Optional[str] = None,\n    extras: Optional[Dict[str, Any]] = {},\n) -&gt; \"Message\":\n    \"\"\"\n    Create a message with tool call blocks and optional text content.\n\n    Parameters\n    ----------\n    tool_calls : Union[ToolCallDict, List[ToolCallDict], ToolCall, List[ToolCall]]\n        Tool call data in various formats:\n        - Single tool call dict: {\"id\": \"call_123\", \"name\": \"get_weather\", \"arguments\": {...}}\n        - List of tool call dicts: [{\"id\": \"call_123\", ...}, {\"id\": \"call_124\", ...}]\n        - Single ToolCall instance\n        - List of ToolCall instances\n    text : Optional[str], optional\n        Optional text content to include in the message\n\n    extras : Optional[Dict[str, Any]], optional\n        Additional metadata for the message\n\n    Returns\n    -------\n    Message\n        A message containing the tool call blocks and optional text\n\n    Examples\n    --------\n    &gt;&gt;&gt; # Build from single tool call dict.\n    ... message = Message.from_tool_call(\n    ...     tool_calls={\n    ...         \"id\": \"call_id_123\",\n    ...         \"name\": \"get_weather\",\n    ...         \"arguments\": {\"city\": \"Tokyo\", \"unit\": \"celsius\"}\n    ...     },\n    ...     text=\"I will check the weather for you.\"\n    ... )\n\n    &gt;&gt;&gt; # Build from multiple tool call dicts.\n    ... message = Message.from_tool_call(\n    ...     tool_calls=[\n    ...         {\"id\": \"call_id_123\", \"name\": \"get_weather\", \"arguments\": {\"city\": \"Tokyo\"}},\n    ...         {\"id\": \"call_id_456\", \"name\": \"get_news\", \"arguments\": {\"topic\": \"weather\"}},\n    ...     ],\n    ...     text=\"I will get weather and news for you.\"\n    ... )\n\n    &gt;&gt;&gt; # Build from single ToolCall object.\n    ... tool_call = ToolCall(id=\"call_123\", name=\"get_weather\", arguments={\"city\": \"Tokyo\"})\n    ... message = Message.from_tool_call(tool_calls=tool_call, text=\"I will check the weather.\")\n\n    &gt;&gt;&gt; # Build from multiple ToolCall objects.\n    ... tool_calls = [\n    ...     ToolCall(id=\"call_id_123\", name=\"get_weather\", arguments={\"city\": \"Tokyo\"}),\n    ...     ToolCall(id=\"call_id_456\", name=\"get_news\", arguments={\"topic\": \"weather\"}),\n    ... ]\n    ... message = Message.from_tool_call(tool_calls=tool_calls, text=\"I will get weather and news.\")\n    \"\"\"\n    role = Role(Role.AI)\n    blocks = []\n\n    # Add text content if provided\n    if text:\n        blocks.append(TextBlock(text=text))\n\n    # Handle different tool_calls formats\n    if isinstance(tool_calls, dict):\n        # Single tool call dict\n        tool_calls = [tool_calls]\n    if isinstance(tool_calls, list):\n        # List of tool calls (dicts or ToolCall)\n        for tool_call in tool_calls:\n            if isinstance(tool_call, dict):\n                # Tool call dict\n                blocks.append(ToolCallBlock(\n                    id=tool_call[\"id\"],\n                    name=tool_call[\"name\"],\n                    arguments=tool_call[\"arguments\"]\n                ))\n            elif hasattr(tool_call, 'id') and hasattr(tool_call, 'name') and hasattr(tool_call, 'arguments'):\n                blocks.append(ToolCallBlock(\n                    id=tool_call.id,\n                    name=tool_call.name,\n                    arguments=tool_call.arguments\n                ))\n            else:\n                raise ValueError(f\"Invalid tool call format: {tool_call}\")\n    elif hasattr(tool_calls, 'id') and hasattr(tool_calls, 'name') and hasattr(tool_calls, 'arguments'):\n        blocks.append(ToolCallBlock(\n            id=tool_calls.id,\n            name=tool_calls.name,\n            arguments=tool_calls.arguments\n        ))\n    else:\n        raise ValueError(f\"Invalid tool_calls format: {type(tool_calls)}\")\n\n    return cls(role=role, blocks=blocks, extras=extras)\n</code></pre>"},{"location":"reference/bridgic-core/bridgic/core/model/types/#bridgic.core.model.types.Message.from_tool_result","title":"from_tool_result","text":"<code>classmethod</code> <pre><code>from_tool_result(\n    tool_id: str,\n    content: str,\n    extras: Optional[Dict[str, Any]] = {},\n) -&gt; Message\n</code></pre> <p>Create a message with a tool result block.</p> <p>Parameters:</p> Name Type Description Default <code>tool_id</code> <code>str</code> <p>The ID of the tool call that this result corresponds to</p> required <code>content</code> <code>str</code> <p>The result content from the tool execution</p> required <code>extras</code> <code>Optional[Dict[str, Any]]</code> <p>Additional metadata for the message</p> <code>{}</code> <p>Returns:</p> Type Description <code>Message</code> <p>A message containing the tool result block</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; message = Message.from_tool_result(\n...     tool_id=\"call_id_123\",\n...     content=\"The weather in Tokyo is 22\u00b0C and sunny.\"\n... )\n</code></pre> Source code in <code>bridgic/core/model/types/_message.py</code> <pre><code>@classmethod\ndef from_tool_result(\n    cls,\n    tool_id: str,\n    content: str,\n    extras: Optional[Dict[str, Any]] = {},\n) -&gt; \"Message\":\n    \"\"\"\n    Create a message with a tool result block.\n\n    Parameters\n    ----------\n    tool_id : str\n        The ID of the tool call that this result corresponds to\n    content : str\n        The result content from the tool execution\n    extras : Optional[Dict[str, Any]], optional\n        Additional metadata for the message\n\n    Returns\n    -------\n    Message\n        A message containing the tool result block\n\n    Examples\n    --------\n    &gt;&gt;&gt; message = Message.from_tool_result(\n    ...     tool_id=\"call_id_123\",\n    ...     content=\"The weather in Tokyo is 22\u00b0C and sunny.\"\n    ... )\n    \"\"\"\n    role = Role(Role.TOOL)\n    return cls(\n        role=role, \n        blocks=[ToolResultBlock(id=tool_id, content=content)], \n        extras=extras\n    )\n</code></pre>"},{"location":"reference/bridgic-core/bridgic/core/model/types/#bridgic.core.model.types.MessageChunk","title":"MessageChunk","text":"<p>               Bases: <code>BaseModel</code></p> <p>Streaming message chunk for real-time LLM responses.</p> <p>Represents a partial message chunk received during streaming responses from language models, allowing for real-time processing of incremental content.</p> <p>Attributes:</p> Name Type Description <code>delta</code> <code>Optional[str]</code> <p>The incremental text content of this chunk.</p> <code>raw</code> <code>Optional[Any]</code> <p>Raw response data from the LLM provider.</p> Source code in <code>bridgic/core/model/types/_message.py</code> <pre><code>class MessageChunk(BaseModel):\n    \"\"\"\n    Streaming message chunk for real-time LLM responses.\n\n    Represents a partial message chunk received during streaming responses from\n    language models, allowing for real-time processing of incremental content.\n\n    Attributes\n    ----------\n    delta : Optional[str]\n        The incremental text content of this chunk.\n    raw : Optional[Any]\n        Raw response data from the LLM provider.\n    \"\"\"\n    delta: Optional[str] = None\n    raw: Optional[Any] = None\n</code></pre>"},{"location":"reference/bridgic-core/bridgic/core/model/types/#bridgic.core.model.types.Response","title":"Response","text":"<p>               Bases: <code>BaseModel</code></p> <p>LLM response container for model outputs.</p> <p>Represents the complete response from a language model, containing both the message content and the raw response data from the underlying model  provider.</p> <p>Attributes:</p> Name Type Description <code>message</code> <code>Optional[Message]</code> <p>The structured message containing the model's response content.</p> <code>raw</code> <code>Optional[Any]</code> <p>Raw response data from the LLM provider for debugging or custom processing.</p> Source code in <code>bridgic/core/model/types/_response.py</code> <pre><code>class Response(BaseModel):\n    \"\"\"\n    LLM response container for model outputs.\n\n    Represents the complete response from a language model, containing both\n    the message content and the raw response data from the underlying model \n    provider.\n\n    Attributes\n    ----------\n    message : Optional[Message]\n        The structured message containing the model's response content.\n    raw : Optional[Any]\n        Raw response data from the LLM provider for debugging or custom processing.\n    \"\"\"\n    message: Optional[Message] = None\n    raw: Optional[Any] = None\n</code></pre>"},{"location":"reference/bridgic-core/bridgic/core/prompt/","title":"prompt","text":"<p>The Prompt module provides core functionality for managing and rendering prompt templates.</p> <p>This module contains multiple prompt template implementations for more convenient  construction of dynamic LLM prompt content.</p>"},{"location":"reference/bridgic-core/bridgic/core/prompt/#bridgic.core.prompt.BasePromptTemplate","title":"BasePromptTemplate","text":"<p>               Bases: <code>BaseModel</code></p> <p>Abstract base class for prompt templates.</p> <p>This class provides a common interface for messages from template strings with variable substitutions.    </p> <p>Attributes:</p> Name Type Description <code>template_str</code> <code>str</code> <p>The template string containing placeholders for variable substitution. The specific placeholder syntax depends on the concrete implementation (e.g., f-string, Jinja2, etc.).</p> <p>Methods:</p> Name Description <code>format_message</code> <p>Format a single message from the template.</p> <code>format_messages</code> <p>Format multiple messages from the template.</p> Notes <p>This is an abstract base class that must be subclassed to provide concrete implementations. Subclasses should implement the <code>format_message</code> and <code>format_messages</code> methods according to their specific template formatting requirements.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; class MyTemplate(BasePromptTemplate):\n...     def format_message(self, role=Role.USER, **kwargs):\n...         # Implementation here\n...         pass\n...     \n...     def format_messages(self, **kwargs):\n...         # Implementation here\n...         pass\n&gt;&gt;&gt; \n&gt;&gt;&gt; template = MyTemplate(template_str=\"Hello {name}!\")\n&gt;&gt;&gt; message = template.format_message(name=\"World\")\n</code></pre> Source code in <code>bridgic/core/prompt/_base_template.py</code> <pre><code>class BasePromptTemplate(BaseModel):\n    \"\"\"\n    Abstract base class for prompt templates.\n\n    This class provides a common interface for messages from template strings with variable substitutions.    \n\n    Attributes\n    ----------\n    template_str : str\n        The template string containing placeholders for variable substitution.\n        The specific placeholder syntax depends on the concrete implementation\n        (e.g., f-string, Jinja2, etc.).\n\n    Methods\n    -------\n    format_message(role, **kwargs)\n        Format a single message from the template.\n    format_messages(**kwargs)\n        Format multiple messages from the template.\n\n    Notes\n    -----\n    This is an abstract base class that must be subclassed to provide\n    concrete implementations. Subclasses should implement the `format_message`\n    and `format_messages` methods according to their specific template\n    formatting requirements.\n\n    Examples\n    --------\n    &gt;&gt;&gt; class MyTemplate(BasePromptTemplate):\n    ...     def format_message(self, role=Role.USER, **kwargs):\n    ...         # Implementation here\n    ...         pass\n    ...     \n    ...     def format_messages(self, **kwargs):\n    ...         # Implementation here\n    ...         pass\n    &gt;&gt;&gt; \n    &gt;&gt;&gt; template = MyTemplate(template_str=\"Hello {name}!\")\n    &gt;&gt;&gt; message = template.format_message(name=\"World\")\n    \"\"\"\n\n    template_str: str\n\n    def format_message(self, role: Union[Role, str] = Role.USER, **kwargs) -&gt; Message:\n        \"\"\"\n        Format a single message from the template.\n\n        Parameters\n        ----------\n        role : Union[Role, str], default=Role.USER\n            The role of the message (e.g., 'user', 'assistant', 'system').\n        **kwargs\n            Additional keyword arguments to be substituted into the template.\n\n        Returns\n        -------\n        Message\n            A formatted message object.\n\n        Raises\n        ------\n        NotImplementedError\n            This method must be implemented by subclasses.\n        \"\"\"\n        raise NotImplementedError(f\"format_message is not implemented in class {self.__class__.__name__}\")\n\n    def format_messages(self, **kwargs) -&gt; List[Message]:\n        \"\"\"\n        Format multiple messages from the template.\n\n        Parameters\n        ----------\n        **kwargs\n            Additional keyword arguments to be substituted into the template.\n\n        Returns\n        -------\n        List[Message]\n            A list of formatted message objects.\n\n        Raises\n        ------\n        NotImplementedError\n            This method must be implemented by subclasses.\n        \"\"\"\n        raise NotImplementedError(f\"format_messages is not implemented in class {self.__class__.__name__}\")\n</code></pre>"},{"location":"reference/bridgic-core/bridgic/core/prompt/#bridgic.core.prompt.BasePromptTemplate.format_message","title":"format_message","text":"<pre><code>format_message(\n    role: Union[Role, str] = USER, **kwargs\n) -&gt; Message\n</code></pre> <p>Format a single message from the template.</p> <p>Parameters:</p> Name Type Description Default <code>role</code> <code>Union[Role, str]</code> <p>The role of the message (e.g., 'user', 'assistant', 'system').</p> <code>Role.USER</code> <code>**kwargs</code> <p>Additional keyword arguments to be substituted into the template.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Message</code> <p>A formatted message object.</p> <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>This method must be implemented by subclasses.</p> Source code in <code>bridgic/core/prompt/_base_template.py</code> <pre><code>def format_message(self, role: Union[Role, str] = Role.USER, **kwargs) -&gt; Message:\n    \"\"\"\n    Format a single message from the template.\n\n    Parameters\n    ----------\n    role : Union[Role, str], default=Role.USER\n        The role of the message (e.g., 'user', 'assistant', 'system').\n    **kwargs\n        Additional keyword arguments to be substituted into the template.\n\n    Returns\n    -------\n    Message\n        A formatted message object.\n\n    Raises\n    ------\n    NotImplementedError\n        This method must be implemented by subclasses.\n    \"\"\"\n    raise NotImplementedError(f\"format_message is not implemented in class {self.__class__.__name__}\")\n</code></pre>"},{"location":"reference/bridgic-core/bridgic/core/prompt/#bridgic.core.prompt.BasePromptTemplate.format_messages","title":"format_messages","text":"<pre><code>format_messages(**kwargs) -&gt; List[Message]\n</code></pre> <p>Format multiple messages from the template.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <p>Additional keyword arguments to be substituted into the template.</p> <code>{}</code> <p>Returns:</p> Type Description <code>List[Message]</code> <p>A list of formatted message objects.</p> <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>This method must be implemented by subclasses.</p> Source code in <code>bridgic/core/prompt/_base_template.py</code> <pre><code>def format_messages(self, **kwargs) -&gt; List[Message]:\n    \"\"\"\n    Format multiple messages from the template.\n\n    Parameters\n    ----------\n    **kwargs\n        Additional keyword arguments to be substituted into the template.\n\n    Returns\n    -------\n    List[Message]\n        A list of formatted message objects.\n\n    Raises\n    ------\n    NotImplementedError\n        This method must be implemented by subclasses.\n    \"\"\"\n    raise NotImplementedError(f\"format_messages is not implemented in class {self.__class__.__name__}\")\n</code></pre>"},{"location":"reference/bridgic-core/bridgic/core/prompt/#bridgic.core.prompt.FstringPromptTemplate","title":"FstringPromptTemplate","text":"<p>               Bases: <code>BasePromptTemplate</code></p> <p>This template implementation uses Python's f-string syntax (braces <code>{}</code>).</p> <p>Methods:</p> Name Description <code>format_message</code> <p>Format a single message from the template.</p> Notes <p>This template supports single message rendering via <code>format_message()</code>. The template uses Python's built-in <code>str.format()</code> method for variable substitution, which provides basic formatting capabilities.</p> <p>Examples:</p> <p>Basic usage:</p> <pre><code>&gt;&gt;&gt; template = FstringPromptTemplate(\"Hello {name}, you are {age} years old.\")\n&gt;&gt;&gt; message = template.format_message(role=\"user\", name=\"Alice\", age=25)\n</code></pre> <p>With context:</p> <pre><code>&gt;&gt;&gt; template = FstringPromptTemplate('''\n... Context: {context}\n... Question: {question}\n... Please provide a helpful answer.\n... ''')\n&gt;&gt;&gt; message = template.format_message(\n...     role=\"system\", \n...     context=\"Python programming\", \n...     question=\"What is a decorator?\"\n... )\n</code></pre> <p>Multiple variables:</p> <pre><code>&gt;&gt;&gt; template = FstringPromptTemplate(\"{greeting} {name}! Today is {date}.\")\n&gt;&gt;&gt; message = template.format_message(\n...     role=\"assistant\",\n...     greeting=\"Good morning\",\n...     name=\"Bob\", \n...     date=\"Monday\"\n... )\n</code></pre> Source code in <code>bridgic/core/prompt/_fstring_template.py</code> <pre><code>class FstringPromptTemplate(BasePromptTemplate):\n    \"\"\"    \n    This template implementation uses Python's f-string syntax (braces `{}`).\n\n    Methods\n    -------\n    format_message(role, **kwargs)\n        Format a single message from the template.\n\n    Notes\n    -----\n    This template supports single message rendering via `format_message()`.\n    The template uses Python's built-in `str.format()` method for variable\n    substitution, which provides basic formatting capabilities.\n\n    Examples\n    --------\n    Basic usage:\n    &gt;&gt;&gt; template = FstringPromptTemplate(\"Hello {name}, you are {age} years old.\")\n    &gt;&gt;&gt; message = template.format_message(role=\"user\", name=\"Alice\", age=25)\n\n    With context:\n    &gt;&gt;&gt; template = FstringPromptTemplate('''\n    ... Context: {context}\n    ... Question: {question}\n    ... Please provide a helpful answer.\n    ... ''')\n    &gt;&gt;&gt; message = template.format_message(\n    ...     role=\"system\", \n    ...     context=\"Python programming\", \n    ...     question=\"What is a decorator?\"\n    ... )\n\n    Multiple variables:\n    &gt;&gt;&gt; template = FstringPromptTemplate(\"{greeting} {name}! Today is {date}.\")\n    &gt;&gt;&gt; message = template.format_message(\n    ...     role=\"assistant\",\n    ...     greeting=\"Good morning\",\n    ...     name=\"Bob\", \n    ...     date=\"Monday\"\n    ... )\n    \"\"\"\n\n    def format_message(self, role: Union[Role, str], **kwargs) -&gt; Message:\n        \"\"\"\n        Format a single message from the template.\n\n        Parameters\n        ----------\n        role : Union[Role, str]\n            The role of the message (e.g., 'user', 'assistant', 'system').\n            Required parameter for this template implementation.\n        **kwargs\n            Keyword arguments containing values for all variables referenced\n            in the template string. All variables must be provided.\n\n        Returns\n        -------\n        Message\n            A formatted message object with the specified role and rendered content.\n\n        Raises\n        ------\n        PromptRenderError\n            If any variables referenced in the template are missing from\n            the provided keyword arguments.\n        \"\"\"\n        if isinstance(role, str):\n            role = Role(role)\n\n        all_vars = self._find_variables()\n        missing_vars = set(all_vars) - set(kwargs.keys())\n        if missing_vars:\n            raise PromptRenderError(f\"Missing variables that are required to render the prompt template: {', '.join(missing_vars)}\")\n\n        rendered = self.template_str.format(**kwargs)\n        return Message.from_text(text=rendered, role=role)\n\n    def _find_variables(self) -&gt; List[str]:\n        \"\"\"\n        Extract variable names from the template string.\n\n        Returns\n        -------\n        List[str]\n            A list of unique variable names found in the template string,\n            in the order they first appear. Variable names are extracted\n            from curly brace syntax `{variable_name}`.\n        \"\"\"\n        var_list = re.findall(r'{([^}]+)}', self.template_str)\n        var_list = [var.strip() for var in var_list]\n        return unique_list_in_order(var_list)\n</code></pre>"},{"location":"reference/bridgic-core/bridgic/core/prompt/#bridgic.core.prompt.FstringPromptTemplate.format_message","title":"format_message","text":"<pre><code>format_message(role: Union[Role, str], **kwargs) -&gt; Message\n</code></pre> <p>Format a single message from the template.</p> <p>Parameters:</p> Name Type Description Default <code>role</code> <code>Union[Role, str]</code> <p>The role of the message (e.g., 'user', 'assistant', 'system'). Required parameter for this template implementation.</p> required <code>**kwargs</code> <p>Keyword arguments containing values for all variables referenced in the template string. All variables must be provided.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Message</code> <p>A formatted message object with the specified role and rendered content.</p> <p>Raises:</p> Type Description <code>PromptRenderError</code> <p>If any variables referenced in the template are missing from the provided keyword arguments.</p> Source code in <code>bridgic/core/prompt/_fstring_template.py</code> <pre><code>def format_message(self, role: Union[Role, str], **kwargs) -&gt; Message:\n    \"\"\"\n    Format a single message from the template.\n\n    Parameters\n    ----------\n    role : Union[Role, str]\n        The role of the message (e.g., 'user', 'assistant', 'system').\n        Required parameter for this template implementation.\n    **kwargs\n        Keyword arguments containing values for all variables referenced\n        in the template string. All variables must be provided.\n\n    Returns\n    -------\n    Message\n        A formatted message object with the specified role and rendered content.\n\n    Raises\n    ------\n    PromptRenderError\n        If any variables referenced in the template are missing from\n        the provided keyword arguments.\n    \"\"\"\n    if isinstance(role, str):\n        role = Role(role)\n\n    all_vars = self._find_variables()\n    missing_vars = set(all_vars) - set(kwargs.keys())\n    if missing_vars:\n        raise PromptRenderError(f\"Missing variables that are required to render the prompt template: {', '.join(missing_vars)}\")\n\n    rendered = self.template_str.format(**kwargs)\n    return Message.from_text(text=rendered, role=role)\n</code></pre>"},{"location":"reference/bridgic-core/bridgic/core/prompt/#bridgic.core.prompt.EjinjaPromptTemplate","title":"EjinjaPromptTemplate","text":"<p>               Bases: <code>BasePromptTemplate</code></p> <p>Extended Jinja2-based prompt template with custom message blocks.</p> <p>This template implementation extends the standard Jinja2 syntax with custom <code>{% msg %}</code> blocks to create structured Message objects. It supports both single message and multiple message rendering with variable substitution and content block parsing.</p> <p>Attributes:</p> Name Type Description <code>_env_template</code> <code>Template</code> <p>The compiled Jinja2 template object.</p> <code>_render_cache</code> <code>MemoryCache</code> <p>Cache for rendered template results to improve performance.</p> <p>Methods:</p> Name Description <code>format_message</code> <p>Format a single message from the template.</p> <code>format_messages</code> <p>Format multiple messages from the template.</p> Notes <p>This template supports two rendering modes:</p> <ol> <li>Single Message Mode: Use <code>format_message()</code> to render one message.    </li> <li>Multiple Messages Mode: Use <code>format_messages()</code> to render multiple messages.</li> </ol> <p>Examples:</p> <p>Single message with role in template:</p> <pre><code>&gt;&gt;&gt; template = EjinjaPromptTemplate('''\n... {% msg role=\"system\" %}\n... You are a helpful assistant. User name: {{ name }}\n... {% endmsg %}\n... ''')\n&gt;&gt;&gt; message = template.format_message(name=\"Alice\")\n</code></pre> <p>Single message with role as parameter:</p> <pre><code>&gt;&gt;&gt; template = EjinjaPromptTemplate(\"Hello {{ name }}, how are you?\")\n&gt;&gt;&gt; message = template.format_message(role=\"user\", name=\"Bob\")\n</code></pre> <p>Multiple messages:</p> <pre><code>&gt;&gt;&gt; template = EjinjaPromptTemplate('''\n... {% msg role=\"system\" %}You are helpful{% endmsg %}\n... {% msg role=\"user\" %}Hello {{ name }}{% endmsg %}\n... ''')\n&gt;&gt;&gt; messages = template.format_messages(name=\"Charlie\")\n</code></pre> Source code in <code>bridgic/core/prompt/_ejinja_template.py</code> <pre><code>class EjinjaPromptTemplate(BasePromptTemplate):\n    \"\"\"\n    Extended Jinja2-based prompt template with custom message blocks.\n\n    This template implementation extends the standard Jinja2 syntax with custom\n    `{% msg %}` blocks to create structured Message objects. It supports both\n    single message and multiple message rendering with variable substitution\n    and content block parsing.\n\n    Attributes\n    ----------\n    _env_template : Template\n        The compiled Jinja2 template object.\n    _render_cache : MemoryCache\n        Cache for rendered template results to improve performance.\n\n    Methods\n    -------\n    format_message(role, **kwargs)\n        Format a single message from the template.\n    format_messages(**kwargs)\n        Format multiple messages from the template.\n\n    Notes\n    -----\n    This template supports two rendering modes:\n\n    1. **Single Message Mode**: Use `format_message()` to render one message.    \n    2. **Multiple Messages Mode**: Use `format_messages()` to render multiple messages.\n\n    Examples\n    --------\n    Single message with role in template:\n    &gt;&gt;&gt; template = EjinjaPromptTemplate('''\n    ... {% msg role=\"system\" %}\n    ... You are a helpful assistant. User name: {{ name }}\n    ... {% endmsg %}\n    ... ''')\n    &gt;&gt;&gt; message = template.format_message(name=\"Alice\")\n\n    Single message with role as parameter:\n    &gt;&gt;&gt; template = EjinjaPromptTemplate(\"Hello {{ name }}, how are you?\")\n    &gt;&gt;&gt; message = template.format_message(role=\"user\", name=\"Bob\")\n\n    Multiple messages:\n    &gt;&gt;&gt; template = EjinjaPromptTemplate('''\n    ... {% msg role=\"system\" %}You are helpful{% endmsg %}\n    ... {% msg role=\"user\" %}Hello {{ name }}{% endmsg %}\n    ... ''')\n    &gt;&gt;&gt; messages = template.format_messages(name=\"Charlie\")\n    \"\"\"\n\n    _env_template: Template\n    _render_cache: MemoryCache\n\n    def __init__(self, template_str: str):\n        \"\"\"\n        Initialize the EjinjaPromptTemplate.\n\n        Parameters\n        ----------\n        template_str : str\n            The Jinja2 template string with optional `{% msg %}` blocks.\n        \"\"\"\n        super().__init__(template_str=template_str)\n        self._env_template = env.from_string(template_str)\n        self._render_cache = MemoryCache()\n\n    def format_message(self, role: Union[Role, str] = None, **kwargs) -&gt; Message:\n        \"\"\"\n        Format a single message from the template.\n\n        Parameters\n        ----------\n        role : Union[Role, str], optional\n            The role of the message. If the template contains a `{% msg %}` block,\n            this parameter should be None as the role will be extracted from\n            the template. If no `{% msg %}` block exists, this parameter is required.\n        **kwargs\n            Additional keyword arguments to be substituted into the template.\n\n        Returns\n        -------\n        Message\n            A formatted message object with the specified role and content.\n\n        Raises\n        ------\n        PromptSyntaxError\n            If the template contains more than one `{% msg %}` block.\n        PromptRenderError\n            If role parameter conflicts with template-defined role, or if\n            no role is specified when template has no `{% msg %}` block.\n        \"\"\"\n        if isinstance(role, str):\n            role = Role(role)\n\n        rendered = self._env_template.render(**kwargs)\n        match_list = re.findall(r\"{%\\s*msg\\s*role=\\\"(.*?)\\\"\\s*%}(.*?){%\\s*endmsg\\s*%}\", rendered)\n        if len(match_list) &gt; 1:\n            raise PromptSyntaxError(\n                f\"It is required to just have one {{% msg %}} block in the template, \"\n                f\"but got {len(match_list)}\"\n            )\n        elif len(match_list) == 1:\n            if role is not None:\n                raise PromptRenderError(\n                    f\"If you want to render a single message, the role has to be only specified in the template \"\n                    f\"and not be passed as an argument to the \\\"format_message\\\" method in {type(self).__name__}\"\n                )\n            role, content = match_list[0][0], match_list[0][1]\n        else:\n            if role is None:\n                raise PromptRenderError(\n                    f\"If you want to render a template without {{% msg %}} blocks, the role has to be specified \"\n                    f\"as an argument to the \\\"format_message\\\" method in {type(self).__name__}\"\n                )\n            role, content = role, rendered\n        return Message.from_text(text=content, role=role)\n\n    def format_messages(self, **kwargs) -&gt; List[Message]:\n        \"\"\"\n        Format multiple messages from the template.\n\n        Parameters\n        ----------\n        **kwargs\n            Additional keyword arguments to be substituted into the template.\n\n        Returns\n        -------\n        List[Message]\n            A list of formatted message objects. Each line of the rendered\n            template should be a valid JSON representation of a Message object.\n            If no valid messages are found but content exists, a default user\n            message is created.\n\n        Raises\n        ------\n        PromptRenderError\n            If any line in the rendered template is not a valid JSON\n            representation of a Message object.\n\n        Notes\n        -----\n        This method uses caching to improve performance for repeated calls\n        with the same parameters. The rendered template is cached based on\n        the provided keyword arguments.\n        \"\"\"\n        rendered = self._render_cache.get(kwargs)\n        if not rendered:\n            rendered = self._env_template.render(kwargs)\n            self._render_cache.set(kwargs, rendered)\n\n        messages: List[Message] = []\n        for line in rendered.strip().split(\"\\n\"):\n            try:\n                messages.append(Message.model_validate_json(line))\n            except Exception:\n                raise PromptRenderError(\n                    f\"It is required to wrap each content in a {{% msg %}} block when calling the \"\n                    f\"\\\"format_messages\\\" method of {type(self).__name__}, but got: {line}\"\n                )\n\n        if not messages and rendered.strip():\n            messages.append(_chat_message_from_text(role=\"user\", content=rendered))\n        return messages\n</code></pre>"},{"location":"reference/bridgic-core/bridgic/core/prompt/#bridgic.core.prompt.EjinjaPromptTemplate.format_message","title":"format_message","text":"<pre><code>format_message(\n    role: Union[Role, str] = None, **kwargs\n) -&gt; Message\n</code></pre> <p>Format a single message from the template.</p> <p>Parameters:</p> Name Type Description Default <code>role</code> <code>Union[Role, str]</code> <p>The role of the message. If the template contains a <code>{% msg %}</code> block, this parameter should be None as the role will be extracted from the template. If no <code>{% msg %}</code> block exists, this parameter is required.</p> <code>None</code> <code>**kwargs</code> <p>Additional keyword arguments to be substituted into the template.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Message</code> <p>A formatted message object with the specified role and content.</p> <p>Raises:</p> Type Description <code>PromptSyntaxError</code> <p>If the template contains more than one <code>{% msg %}</code> block.</p> <code>PromptRenderError</code> <p>If role parameter conflicts with template-defined role, or if no role is specified when template has no <code>{% msg %}</code> block.</p> Source code in <code>bridgic/core/prompt/_ejinja_template.py</code> <pre><code>def format_message(self, role: Union[Role, str] = None, **kwargs) -&gt; Message:\n    \"\"\"\n    Format a single message from the template.\n\n    Parameters\n    ----------\n    role : Union[Role, str], optional\n        The role of the message. If the template contains a `{% msg %}` block,\n        this parameter should be None as the role will be extracted from\n        the template. If no `{% msg %}` block exists, this parameter is required.\n    **kwargs\n        Additional keyword arguments to be substituted into the template.\n\n    Returns\n    -------\n    Message\n        A formatted message object with the specified role and content.\n\n    Raises\n    ------\n    PromptSyntaxError\n        If the template contains more than one `{% msg %}` block.\n    PromptRenderError\n        If role parameter conflicts with template-defined role, or if\n        no role is specified when template has no `{% msg %}` block.\n    \"\"\"\n    if isinstance(role, str):\n        role = Role(role)\n\n    rendered = self._env_template.render(**kwargs)\n    match_list = re.findall(r\"{%\\s*msg\\s*role=\\\"(.*?)\\\"\\s*%}(.*?){%\\s*endmsg\\s*%}\", rendered)\n    if len(match_list) &gt; 1:\n        raise PromptSyntaxError(\n            f\"It is required to just have one {{% msg %}} block in the template, \"\n            f\"but got {len(match_list)}\"\n        )\n    elif len(match_list) == 1:\n        if role is not None:\n            raise PromptRenderError(\n                f\"If you want to render a single message, the role has to be only specified in the template \"\n                f\"and not be passed as an argument to the \\\"format_message\\\" method in {type(self).__name__}\"\n            )\n        role, content = match_list[0][0], match_list[0][1]\n    else:\n        if role is None:\n            raise PromptRenderError(\n                f\"If you want to render a template without {{% msg %}} blocks, the role has to be specified \"\n                f\"as an argument to the \\\"format_message\\\" method in {type(self).__name__}\"\n            )\n        role, content = role, rendered\n    return Message.from_text(text=content, role=role)\n</code></pre>"},{"location":"reference/bridgic-core/bridgic/core/prompt/#bridgic.core.prompt.EjinjaPromptTemplate.format_messages","title":"format_messages","text":"<pre><code>format_messages(**kwargs) -&gt; List[Message]\n</code></pre> <p>Format multiple messages from the template.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <p>Additional keyword arguments to be substituted into the template.</p> <code>{}</code> <p>Returns:</p> Type Description <code>List[Message]</code> <p>A list of formatted message objects. Each line of the rendered template should be a valid JSON representation of a Message object. If no valid messages are found but content exists, a default user message is created.</p> <p>Raises:</p> Type Description <code>PromptRenderError</code> <p>If any line in the rendered template is not a valid JSON representation of a Message object.</p> Notes <p>This method uses caching to improve performance for repeated calls with the same parameters. The rendered template is cached based on the provided keyword arguments.</p> Source code in <code>bridgic/core/prompt/_ejinja_template.py</code> <pre><code>def format_messages(self, **kwargs) -&gt; List[Message]:\n    \"\"\"\n    Format multiple messages from the template.\n\n    Parameters\n    ----------\n    **kwargs\n        Additional keyword arguments to be substituted into the template.\n\n    Returns\n    -------\n    List[Message]\n        A list of formatted message objects. Each line of the rendered\n        template should be a valid JSON representation of a Message object.\n        If no valid messages are found but content exists, a default user\n        message is created.\n\n    Raises\n    ------\n    PromptRenderError\n        If any line in the rendered template is not a valid JSON\n        representation of a Message object.\n\n    Notes\n    -----\n    This method uses caching to improve performance for repeated calls\n    with the same parameters. The rendered template is cached based on\n    the provided keyword arguments.\n    \"\"\"\n    rendered = self._render_cache.get(kwargs)\n    if not rendered:\n        rendered = self._env_template.render(kwargs)\n        self._render_cache.set(kwargs, rendered)\n\n    messages: List[Message] = []\n    for line in rendered.strip().split(\"\\n\"):\n        try:\n            messages.append(Message.model_validate_json(line))\n        except Exception:\n            raise PromptRenderError(\n                f\"It is required to wrap each content in a {{% msg %}} block when calling the \"\n                f\"\\\"format_messages\\\" method of {type(self).__name__}, but got: {line}\"\n            )\n\n    if not messages and rendered.strip():\n        messages.append(_chat_message_from_text(role=\"user\", content=rendered))\n    return messages\n</code></pre>"},{"location":"reference/bridgic-core/bridgic/core/types/","title":"types","text":"<p>The Types module defines several basic data types for the framework.</p>"},{"location":"reference/bridgic-core/bridgic/core/types/#bridgic.core.types.Serializable","title":"Serializable","text":"<p>               Bases: <code>Protocol</code></p> <p>Serializable is a protocol that defines the interfaces that customizes serialization.</p> Source code in <code>bridgic/core/types/_serialization.py</code> <pre><code>@runtime_checkable\nclass Serializable(Protocol):\n    \"\"\"\n    Serializable is a protocol that defines the interfaces that customizes serialization.\n    \"\"\"\n    @abstractmethod\n    def dump_to_dict(self) -&gt; Dict[str, Any]:\n        \"\"\"\n        Dump the object to a dictionary, which will finally be serialized to bytes.\n        \"\"\"\n        ...\n\n    @abstractmethod\n    def load_from_dict(self, state_dict: Dict[str, Any]) -&gt; None:\n        \"\"\"\n        Load the object state from a dictionary previously obtained by deserializing from bytes.\n        \"\"\"\n        ...\n</code></pre>"},{"location":"reference/bridgic-core/bridgic/core/types/#bridgic.core.types.Serializable.dump_to_dict","title":"dump_to_dict","text":"<code>abstractmethod</code> <pre><code>dump_to_dict() -&gt; Dict[str, Any]\n</code></pre> <p>Dump the object to a dictionary, which will finally be serialized to bytes.</p> Source code in <code>bridgic/core/types/_serialization.py</code> <pre><code>@abstractmethod\ndef dump_to_dict(self) -&gt; Dict[str, Any]:\n    \"\"\"\n    Dump the object to a dictionary, which will finally be serialized to bytes.\n    \"\"\"\n    ...\n</code></pre>"},{"location":"reference/bridgic-core/bridgic/core/types/#bridgic.core.types.Serializable.load_from_dict","title":"load_from_dict","text":"<code>abstractmethod</code> <pre><code>load_from_dict(state_dict: Dict[str, Any]) -&gt; None\n</code></pre> <p>Load the object state from a dictionary previously obtained by deserializing from bytes.</p> Source code in <code>bridgic/core/types/_serialization.py</code> <pre><code>@abstractmethod\ndef load_from_dict(self, state_dict: Dict[str, Any]) -&gt; None:\n    \"\"\"\n    Load the object state from a dictionary previously obtained by deserializing from bytes.\n    \"\"\"\n    ...\n</code></pre>"},{"location":"reference/bridgic-core/bridgic/core/types/#bridgic.core.types.Picklable","title":"Picklable","text":"<p>               Bases: <code>Protocol</code></p> <p>Picklable is a protocol that defines the interfaces that customizes serialization using pickle.</p> Notes <p>If a class implements both Serializable and Picklable, the object of the class will be  serialized using the implementation provided by Serializable instead of using pickle.</p> Source code in <code>bridgic/core/types/_serialization.py</code> <pre><code>@runtime_checkable\nclass Picklable(Protocol):\n    \"\"\"\n    Picklable is a protocol that defines the interfaces that customizes serialization using pickle.\n\n    Notes\n    -----\n    If a class implements both Serializable and Picklable, the object of the class will be \n    serialized using the implementation provided by Serializable instead of using pickle.\n    \"\"\"\n\n    def __picklable_marker__(self) -&gt; None:\n        \"\"\"\n        This is just a marker method to distinguish Picklable objects from other objects.\n        Since it is not necessary to implement this method in the subclass, thus no \n        @abstractmethod is used here.\n        \"\"\"\n        ...\n</code></pre>"},{"location":"reference/bridgic-llms-openai/bridgic/llms/openai/","title":"openai","text":"<p>The OpenAI integration module provides support for the OpenAI API.</p> <p>This module implements integration interfaces with OpenAI language models, supporting  calls to large language models provided by OpenAI such as the GPT series, and provides  several wrappers for advanced functionality.</p> <p>You can install the OpenAI integration package for Bridgic by running:</p> <pre><code>pip install bridgic-llms-openai\n</code></pre>"},{"location":"reference/bridgic-llms-openai/bridgic/llms/openai/#bridgic.llms.openai.OpenAIConfiguration","title":"OpenAIConfiguration","text":"<p>               Bases: <code>BaseModel</code></p> <p>Default configuration for OpenAI chat completions.</p> <p>model : str     Model ID used to generate the response, like <code>gpt-4o</code> or <code>gpt-4</code>. temperature : Optional[float]     What sampling temperature to use, between 0 and 2. Higher values like 0.8 will     make the output more random, while lower values like 0.2 will make it more     focused and deterministic. top_p : Optional[float]     An alternative to sampling with temperature, called nucleus sampling, where the     model considers the results of the tokens with top_p probability mass. presence_penalty : Optional[float]     Number between -2.0 and 2.0. Positive values penalize new tokens based on     whether they appear in the text so far, increasing the model's likelihood to     talk about new topics. frequency_penalty : Optional[float]     Number between -2.0 and 2.0. Positive values penalize new tokens based on their     existing frequency in the text so far, decreasing the model's likelihood to     repeat the same line verbatim. max_tokens : Optional[int]     The maximum number of tokens that can be generated in the chat completion.     This value is now deprecated in favor of <code>max_completion_tokens</code>. stop : Optional[List[str]]     Up to 4 sequences where the API will stop generating further tokens.     Not supported with latest reasoning models <code>o3</code> and <code>o3-mini</code>.</p> Source code in <code>bridgic/llms/openai/openai_llm.py</code> <pre><code>class OpenAIConfiguration(BaseModel):\n    \"\"\"Default configuration for OpenAI chat completions.\n\n    model : str\n        Model ID used to generate the response, like `gpt-4o` or `gpt-4`.\n    temperature : Optional[float]\n        What sampling temperature to use, between 0 and 2. Higher values like 0.8 will\n        make the output more random, while lower values like 0.2 will make it more\n        focused and deterministic.\n    top_p : Optional[float]\n        An alternative to sampling with temperature, called nucleus sampling, where the\n        model considers the results of the tokens with top_p probability mass.\n    presence_penalty : Optional[float]\n        Number between -2.0 and 2.0. Positive values penalize new tokens based on\n        whether they appear in the text so far, increasing the model's likelihood to\n        talk about new topics.\n    frequency_penalty : Optional[float]\n        Number between -2.0 and 2.0. Positive values penalize new tokens based on their\n        existing frequency in the text so far, decreasing the model's likelihood to\n        repeat the same line verbatim.\n    max_tokens : Optional[int]\n        The maximum number of tokens that can be generated in the chat completion.\n        This value is now deprecated in favor of `max_completion_tokens`.\n    stop : Optional[List[str]]\n        Up to 4 sequences where the API will stop generating further tokens.\n        Not supported with latest reasoning models `o3` and `o3-mini`.\n    \"\"\"\n    model: Optional[str] = None\n    temperature: Optional[float] = None\n    top_p: Optional[float] = None\n    presence_penalty: Optional[float] = None\n    frequency_penalty: Optional[float] = None\n    max_tokens: Optional[int] = None\n    stop: Optional[List[str]] = None\n</code></pre>"},{"location":"reference/bridgic-llms-openai/bridgic/llms/openai/#bridgic.llms.openai.OpenAILlm","title":"OpenAILlm","text":"<p>               Bases: <code>BaseLlm</code>, <code>StructuredOutput</code>, <code>ToolSelection</code></p> <p>Wrapper class for OpenAI, providing common chat and stream calling interfaces for OpenAI model and implementing the common protocols in the Bridgic framework.</p> <p>Parameters:</p> Name Type Description Default <code>api_key</code> <code>str</code> <p>The API key for OpenAI services. Required for authentication.</p> required <code>api_base</code> <code>Optional[str]</code> <p>The base URL for the OpenAI API. If None, uses the default OpenAI endpoint.</p> <code>None</code> <code>timeout</code> <code>Optional[float]</code> <p>Request timeout in seconds. If None, no timeout is applied.</p> <code>None</code> <code>http_client</code> <code>Optional[Client]</code> <p>Custom synchronous HTTP client for requests. If None, creates a default client.</p> <code>None</code> <code>http_async_client</code> <code>Optional[AsyncClient]</code> <p>Custom asynchronous HTTP client for requests. If None, creates a default client.</p> <code>None</code> <p>Attributes:</p> Name Type Description <code>client</code> <code>OpenAI</code> <p>The synchronous OpenAI client instance.</p> <code>async_client</code> <code>AsyncOpenAI</code> <p>The asynchronous OpenAI client instance.</p> <p>Examples:</p> <p>Basic usage for chat completion:</p> <pre><code>llm = OpenAILlm(api_key=\"your-api-key\")\nmessages = [Message.from_text(\"Hello!\", role=Role.USER)]\nresponse = llm.chat(messages=messages, model=\"gpt-4\")\n</code></pre> <p>Structured output with Pydantic model:</p> <pre><code>class Answer(BaseModel):\n    reasoning: str\n    result: int\n\nconstraint = PydanticModel(model=Answer)\nstructured_response = llm.structured_output(\n    messages=messages,\n    constraint=constraint,\n    model=\"gpt-4\"\n)\n</code></pre> <p>Tool calling:</p> <pre><code>tools = [Tool(name=\"calculator\", description=\"Calculate math\", parameters={})]\ntool_calls, tool_call_response = llm.select_tool(messages=messages, tools=tools, model=\"gpt-4\")\n</code></pre> Source code in <code>bridgic/llms/openai/openai_llm.py</code> <pre><code>class OpenAILlm(BaseLlm, StructuredOutput, ToolSelection):\n    \"\"\"\n    Wrapper class for OpenAI, providing common chat and stream calling interfaces for OpenAI model\n    and implementing the common protocols in the Bridgic framework.\n\n    Parameters\n    ----------\n    api_key : str\n        The API key for OpenAI services. Required for authentication.\n    api_base : Optional[str]\n        The base URL for the OpenAI API. If None, uses the default OpenAI endpoint.\n    timeout : Optional[float]\n        Request timeout in seconds. If None, no timeout is applied.\n    http_client : Optional[httpx.Client]\n        Custom synchronous HTTP client for requests. If None, creates a default client.\n    http_async_client : Optional[httpx.AsyncClient]\n        Custom asynchronous HTTP client for requests. If None, creates a default client.\n\n    Attributes\n    ----------\n    client : openai.OpenAI\n        The synchronous OpenAI client instance.\n    async_client : openai.AsyncOpenAI\n        The asynchronous OpenAI client instance.\n\n    Examples\n    --------\n    Basic usage for chat completion:\n\n    ```python\n    llm = OpenAILlm(api_key=\"your-api-key\")\n    messages = [Message.from_text(\"Hello!\", role=Role.USER)]\n    response = llm.chat(messages=messages, model=\"gpt-4\")\n    ```\n\n    Structured output with Pydantic model:\n\n    ```python\n    class Answer(BaseModel):\n        reasoning: str\n        result: int\n\n    constraint = PydanticModel(model=Answer)\n    structured_response = llm.structured_output(\n        messages=messages,\n        constraint=constraint,\n        model=\"gpt-4\"\n    )\n    ```\n\n    Tool calling:\n\n    ```python\n    tools = [Tool(name=\"calculator\", description=\"Calculate math\", parameters={})]\n    tool_calls, tool_call_response = llm.select_tool(messages=messages, tools=tools, model=\"gpt-4\")\n    ```\n    \"\"\"\n\n    api_base: str\n    api_key: str\n    configuration: OpenAIConfiguration\n    timeout: float\n    http_client: httpx.Client\n    http_async_client: httpx.AsyncClient\n\n    client: OpenAI\n    async_client: AsyncOpenAI\n\n    def __init__(\n        self,\n        api_key: str,\n        api_base: Optional[str] = None,\n        configuration: Optional[OpenAIConfiguration] = OpenAIConfiguration(),\n        timeout: Optional[float] = None,\n        http_client: Optional[httpx.Client] = None,\n        http_async_client: Optional[httpx.AsyncClient] = None,\n    ):\n        \"\"\"\n        Initialize the OpenAI LLM client with configuration parameters.\n\n        Parameters\n        ----------\n        api_key : str\n            The API key for OpenAI services. Required for authentication.\n        api_base : Optional[str]\n            The base URL for the OpenAI API. If None, uses the default OpenAI endpoint.\n        configuration : Optional[OpenAIConfiguration]\n            The configuration for the OpenAI API. If None, uses the default configuration.\n        timeout : Optional[float]\n            Request timeout in seconds. If None, no timeout is applied.\n        http_client : Optional[httpx.Client]\n            Custom synchronous HTTP client for requests. If None, creates a default client.\n        http_async_client : Optional[httpx.AsyncClient]\n            Custom asynchronous HTTP client for requests. If None, creates a default client.\n        \"\"\"\n        # Record for serialization / deserialization.\n        self.api_base = api_base\n        self.api_key = api_key\n        self.configuration = configuration\n        self.timeout = timeout\n        self.http_client = http_client\n        self.http_async_client = http_async_client\n\n        # Initialize clients.\n        self.client = OpenAI(base_url=api_base, api_key=api_key, timeout=timeout, http_client=http_client)\n        self.async_client = AsyncOpenAI(base_url=api_base, api_key=api_key, timeout=timeout, http_client=http_async_client)\n\n    def chat(\n        self,\n        messages: List[Message],\n        model: Optional[str] = None,\n        temperature: Optional[float] = None,\n        top_p: Optional[float] = None,\n        presence_penalty: Optional[float] = None,\n        frequency_penalty: Optional[float] = None,\n        max_tokens: Optional[int] = None,\n        stop: Optional[List[str]] = None,\n        tools: Optional[List[Tool]] = None,\n        extra_body: Optional[Dict[str, Any]] = None,\n        **kwargs,\n    ) -&gt; Response:\n        \"\"\"\n        Send a synchronous chat completion request to OpenAI.\n\n        Parameters\n        ----------\n        messages : List[Message]\n            A list of messages comprising the conversation so far.\n        model : str\n            Model ID used to generate the response, like `gpt-4o` or `gpt-4`.\n        temperature : Optional[float]\n            What sampling temperature to use, between 0 and 2. Higher values like 0.8 will\n            make the output more random, while lower values like 0.2 will make it more\n            focused and deterministic.\n        top_p : Optional[float]\n            An alternative to sampling with temperature, called nucleus sampling, where the\n            model considers the results of the tokens with top_p probability mass.\n        presence_penalty : Optional[float]\n            Number between -2.0 and 2.0. Positive values penalize new tokens based on\n            whether they appear in the text so far, increasing the model's likelihood to\n            talk about new topics.\n        frequency_penalty : Optional[float]\n            Number between -2.0 and 2.0. Positive values penalize new tokens based on their\n            existing frequency in the text so far, decreasing the model's likelihood to\n            repeat the same line verbatim.\n        max_tokens : Optional[int]\n            The maximum number of tokens that can be generated in the chat completion.\n            This value is now deprecated in favor of `max_completion_tokens`.\n        stop : Optional[List[str]]\n            Up to 4 sequences where the API will stop generating further tokens.\n            Not supported with latest reasoning models `o3` and `o3-mini`.\n        tools : Optional[List[Tool]]\n            A list of tools to use in the chat completion.\n        extra_body : Optional[Dict[str, Any]]\n            Add additional JSON properties to the request.\n        **kwargs\n            Additional keyword arguments passed to the OpenAI API.\n\n        Returns\n        -------\n        Response\n            A response object containing the generated message and raw API response.\n        \"\"\"\n        params = self._build_parameters(\n            messages=messages,\n            model=model,\n            temperature=temperature,\n            top_p=top_p,\n            presence_penalty=presence_penalty,\n            frequency_penalty=frequency_penalty,\n            max_tokens=max_tokens,\n            stop=stop,\n            extra_body=extra_body,\n            **kwargs,\n        )\n        # Validate required parameters for non-streaming chat completion\n        validate_required_params(params, [\"messages\", \"model\"])\n\n        response: ChatCompletion = self.client.chat.completions.create(**params)\n        return self._handle_chat_response(response)\n\n    def stream(\n        self,\n        messages: List[Message],\n        model: Optional[str] = None,\n        temperature: Optional[float] = None,\n        top_p: Optional[float] = None,\n        presence_penalty: Optional[float] = None,\n        frequency_penalty: Optional[float] = None,\n        max_tokens: Optional[int] = None,\n        stop: Optional[List[str]] = None,\n        extra_body: Optional[Dict[str, Any]] = None,\n        **kwargs,\n    ) -&gt; StreamResponse:\n        \"\"\"\n        Send a streaming chat completion request to OpenAI.\n\n        Parameters\n        ----------\n        messages : List[Message]\n            A list of messages comprising the conversation so far.\n        model : str\n            Model ID used to generate the response, like `gpt-4o` or `gpt-4`.\n        temperature : Optional[float]\n            What sampling temperature to use, between 0 and 2. Higher values like 0.8 will\n            make the output more random, while lower values like 0.2 will make it more\n            focused and deterministic.\n        top_p : Optional[float]\n            An alternative to sampling with temperature, called nucleus sampling, where the\n            model considers the results of the tokens with top_p probability mass.\n        presence_penalty : Optional[float]\n            Number between -2.0 and 2.0. Positive values penalize new tokens based on\n            whether they appear in the text so far, increasing the model's likelihood to\n            talk about new topics.\n        frequency_penalty : Optional[float]\n            Number between -2.0 and 2.0. Positive values penalize new tokens based on their\n            existing frequency in the text so far, decreasing the model's likelihood to\n            repeat the same line verbatim.\n        max_tokens : Optional[int]\n            The maximum number of tokens that can be generated in the chat completion.\n            This value is now deprecated in favor of `max_completion_tokens`.\n        stop : Optional[List[str]]\n            Up to 4 sequences where the API will stop generating further tokens.\n            Not supported with latest reasoning models `o3` and `o3-mini`.\n        extra_body : Optional[Dict[str, Any]]\n            Add additional JSON properties to the request.\n        **kwargs\n            Additional keyword arguments passed to the OpenAI API.\n\n        Yields\n        ------\n        MessageChunk\n            Individual chunks of the response as they are received from the API.\n            Each chunk contains a delta (partial content) and the raw response.\n\n        Notes\n        -----\n        This method enables real-time streaming of the model's response,\n        useful for providing incremental updates to users as the response is generated.\n        \"\"\"\n        params = self._build_parameters(\n            messages=messages,\n            model=model,\n            temperature=temperature,\n            top_p=top_p,\n            presence_penalty=presence_penalty,\n            frequency_penalty=frequency_penalty,\n            max_tokens=max_tokens,\n            stop=stop,\n            extra_body=extra_body,\n            stream=True,\n            **kwargs,\n        )\n        # Validate required parameters for streaming chat completion\n        validate_required_params(params, [\"messages\", \"model\", \"stream\"])\n\n        response: Stream[ChatCompletionChunk] = self.client.chat.completions.create(**params)\n        for chunk in response:\n            if chunk.choices and chunk.choices[0].delta.content:\n                delta_content = chunk.choices[0].delta.content\n                delta_content = delta_content if delta_content else \"\"\n                yield MessageChunk(delta=delta_content, raw=chunk)\n\n    async def achat(\n        self,\n        messages: List[Message],\n        model: Optional[str] = None,\n        temperature: Optional[float] = None,\n        top_p: Optional[float] = None,\n        presence_penalty: Optional[float] = None,\n        frequency_penalty: Optional[float] = None,\n        max_tokens: Optional[int] = None,\n        stop: Optional[List[str]] = None,\n        tools: Optional[List[Tool]] = None,\n        extra_body: Optional[Dict[str, Any]] = None,\n        **kwargs,\n    ) -&gt; Response:\n        \"\"\"\n        Send an asynchronous chat completion request to OpenAI.\n\n        Parameters\n        ----------\n        messages : List[Message]\n            A list of messages comprising the conversation so far.\n        model : str\n            Model ID used to generate the response, like `gpt-4o` or `gpt-4`.\n        temperature : Optional[float]\n            What sampling temperature to use, between 0 and 2. Higher values like 0.8 will\n            make the output more random, while lower values like 0.2 will make it more\n            focused and deterministic.\n        top_p : Optional[float]\n            An alternative to sampling with temperature, called nucleus sampling, where the\n            model considers the results of the tokens with top_p probability mass.\n        presence_penalty : Optional[float]\n            Number between -2.0 and 2.0. Positive values penalize new tokens based on\n            whether they appear in the text so far, increasing the model's likelihood to\n            talk about new topics.\n        frequency_penalty : Optional[float]\n            Number between -2.0 and 2.0. Positive values penalize new tokens based on their\n            existing frequency in the text so far, decreasing the model's likelihood to\n            repeat the same line verbatim.\n        max_tokens : Optional[int]\n            The maximum number of tokens that can be generated in the chat completion.\n            This value is now deprecated in favor of `max_completion_tokens`.\n        stop : Optional[List[str]]\n            Up to 4 sequences where the API will stop generating further tokens.\n            Not supported with latest reasoning models `o3` and `o3-mini`.\n        tools : Optional[List[Tool]]\n            A list of tools to use in the chat completion.\n        extra_body : Optional[Dict[str, Any]]\n            Add additional JSON properties to the request.\n        **kwargs\n            Additional keyword arguments passed to the OpenAI API.\n\n        Returns\n        -------\n        Response\n            A response object containing the generated message and raw API response.\n\n        Notes\n        -----\n        This is the asynchronous version of the chat method, suitable for\n        concurrent processing and non-blocking I/O operations.\n        \"\"\"\n        params = self._build_parameters(\n            messages=messages,\n            model=model,\n            temperature=temperature,\n            top_p=top_p,\n            presence_penalty=presence_penalty,\n            frequency_penalty=frequency_penalty,\n            max_tokens=max_tokens,\n            stop=stop,\n            extra_body=extra_body,\n            **kwargs,\n        )\n        # Validate required parameters for non-streaming chat completion\n        validate_required_params(params, [\"messages\", \"model\"])\n\n        response = await self.async_client.chat.completions.create(**params)\n        return self._handle_chat_response(response)\n\n    async def astream(\n        self,\n        messages: List[Message],\n        model: Optional[str] = None,\n        temperature: Optional[float] = None,\n        top_p: Optional[float] = None,\n        presence_penalty: Optional[float] = None,\n        frequency_penalty: Optional[float] = None,\n        max_tokens: Optional[int] = None,\n        stop: Optional[List[str]] = None,\n        extra_body: Optional[Dict[str, Any]] = None,\n        **kwargs,\n    ) -&gt; AsyncStreamResponse:\n        \"\"\"\n        Send an asynchronous streaming chat completion request to OpenAI.\n\n        Parameters\n        ----------\n        messages : List[Message]\n            A list of messages comprising the conversation so far.\n        model : str\n            Model ID used to generate the response, like `gpt-4o` or `gpt-4`.\n        temperature : Optional[float]\n            What sampling temperature to use, between 0 and 2. Higher values like 0.8 will\n            make the output more random, while lower values like 0.2 will make it more\n            focused and deterministic.\n        top_p : Optional[float]\n            An alternative to sampling with temperature, called nucleus sampling, where the\n            model considers the results of the tokens with top_p probability mass.\n        presence_penalty : Optional[float]\n            Number between -2.0 and 2.0. Positive values penalize new tokens based on\n            whether they appear in the text so far, increasing the model's likelihood to\n            talk about new topics.\n        frequency_penalty : Optional[float]\n            Number between -2.0 and 2.0. Positive values penalize new tokens based on their\n            existing frequency in the text so far, decreasing the model's likelihood to\n            repeat the same line verbatim.\n        max_tokens : Optional[int]\n            The maximum number of tokens that can be generated in the chat completion.\n            This value is now deprecated in favor of `max_completion_tokens`.\n        stop : Optional[List[str]]\n            Up to 4 sequences where the API will stop generating further tokens.\n            Not supported with latest reasoning models `o3` and `o3-mini`.\n        extra_body : Optional[Dict[str, Any]]\n            Add additional JSON properties to the request.\n        **kwargs\n            Additional keyword arguments passed to the OpenAI API.\n\n        Yields\n        ------\n        MessageChunk\n            Individual chunks of the response as they are received from the API.\n            Each chunk contains a delta (partial content) and the raw response.\n\n        Notes\n        -----\n        This is the asynchronous version of the stream method, suitable for\n        concurrent processing and non-blocking streaming operations.\n        \"\"\"\n        params = self._build_parameters(\n            messages=messages,\n            model=model,\n            temperature=temperature,\n            top_p=top_p,\n            presence_penalty=presence_penalty,\n            frequency_penalty=frequency_penalty,\n            max_tokens=max_tokens,\n            stop=stop,\n            extra_body=extra_body,\n            stream=True,\n            **kwargs,\n        )\n        # Validate required parameters for streaming chat completion\n        validate_required_params(params, [\"messages\", \"model\", \"stream\"])\n\n        response = await self.async_client.chat.completions.create(**params)\n        async for chunk in response:\n            if chunk.choices and chunk.choices[0].delta.content:\n                delta_content = chunk.choices[0].delta.content\n                delta_content = delta_content if delta_content else \"\"\n                yield MessageChunk(delta=delta_content, raw=chunk)\n\n    def _build_parameters(\n        self,\n        messages: List[Message],\n        model: Optional[str] = None,\n        temperature: Optional[float] = None,\n        top_p: Optional[float] = None,\n        presence_penalty: Optional[float] = None,\n        frequency_penalty: Optional[float] = None,\n        max_tokens: Optional[int] = None,\n        stop: Optional[List[str]] = None,\n        tools: Optional[List[Tool]] = None,\n        extra_body: Optional[Dict[str, Any]] = None,\n        stream: Optional[bool] = None,\n        response_format: Optional[Dict[str, Any]] = None,\n        tool_choice: Optional[ChatCompletionToolChoiceOptionParam] = None,\n        parallel_tool_calls: Optional[bool] = None,\n        **kwargs,\n    ) -&gt; Dict[str, Any]:\n        msgs: List[ChatCompletionMessageParam] = [self._convert_chat_completions_message(msg) for msg in messages]\n\n        # Handle tools parameter - convert to list if provided, otherwise use empty list\n        json_desc_tools = [self._convert_tool_to_json(tool) for tool in tools] if tools is not None else None\n\n        # Build parameters dictionary and filter out None values\n        # The priority order is as follows: configuration passed through the interface &gt; configuration of the instance itself.\n        merge_params = merge_dict(self.configuration.model_dump(), {\n            \"messages\": msgs,\n            \"model\": model,\n            \"temperature\": temperature,\n            \"top_p\": top_p,\n            \"presence_penalty\": presence_penalty,\n            \"frequency_penalty\": frequency_penalty,\n            \"max_tokens\": max_tokens,\n            \"stop\": stop,\n            \"tools\": json_desc_tools,\n            \"extra_body\": extra_body,\n            \"stream\": stream,\n            \"response_format\": response_format,\n            \"tool_choice\": tool_choice,\n            \"parallel_tool_calls\": parallel_tool_calls,\n            **kwargs,\n        })\n\n        params = filter_dict(merge_params, exclude_none=True)\n        return params\n\n    def _handle_chat_response(self, response: ChatCompletion) -&gt; Response:\n        openai_message = response.choices[0].message\n        text = openai_message.content if openai_message.content else \"\"\n\n        if openai_message.refusal:\n            warnings.warn(openai_message.refusal, RuntimeWarning)\n\n        # Handle tool calls in the response\n        # if openai_message.tool_calls:\n        #     # Create a message with both text content and tool calls\n        #     blocks = []\n        #     if text:\n        #         blocks.append(TextBlock(text=text))\n        #     else:\n        #         # Ensure there's always some text content, even if empty\n        #         blocks.append(TextBlock(text=\"\"))\n\n        #     for tool_call in openai_message.tool_calls:\n        #         tool_call_block = ToolCallBlock(\n        #             id=tool_call.id,\n        #             name=tool_call.function.name,\n        #             arguments=json.loads(tool_call.function.arguments)\n        #         )\n        #         blocks.append(tool_call_block)\n\n        #     message = Message(role=Role.AI, blocks=blocks)\n        # else:\n        #     # Regular text response\n        #     message = Message.from_text(text, role=Role.AI)\n\n        return Response(\n            message=Message.from_text(text, role=Role.AI),\n            raw=response,\n        )\n\n    def _convert_chat_completions_message(self, message: Message) -&gt; ChatCompletionMessageParam:\n        \"\"\"\n        Convert a Bridgic Message to OpenAI ChatCompletionMessageParam.\n\n        This method handles different message types including:\n        - Text messages\n        - Messages with tool calls (ToolCallBlock)\n        - Messages with tool results (ToolResultBlock)\n\n        Parameters\n        ----\n        message : Message\n            The Bridgic message to convert\n\n        Returns\n        ----\n        ChatCompletionMessageParam\n            The converted OpenAI message parameter\n        \"\"\"\n        # Extract text content from TextBlocks and ToolResultBlocks\n        content_list = []\n        for block in message.blocks:\n            if isinstance(block, TextBlock):\n                content_list.append(block.text)\n            elif isinstance(block, ToolResultBlock):\n                content_list.append(block.content)\n        content_txt = \"\\n\\n\".join(content_list) if content_list else \"\"\n\n        # Extract tool calls from ToolCallBlocks\n        tool_calls = []\n        for block in message.blocks:\n            if isinstance(block, ToolCallBlock):\n                tool_call = ChatCompletionMessageToolCallParam(\n                    id=block.id,\n                    type=\"function\",\n                    function=Function(\n                        name=block.name,\n                        arguments=json.dumps(block.arguments)\n                    )\n                )\n                tool_calls.append(tool_call)\n\n        # Handle different message roles\n        if message.role == Role.SYSTEM:\n            return ChatCompletionSystemMessageParam(content=content_txt, role=\"system\", **message.extras)\n        elif message.role == Role.USER:\n            return ChatCompletionUserMessageParam(content=content_txt, role=\"user\", **message.extras)\n        elif message.role == Role.AI:\n            # For AI messages, include tool calls if present\n            if tool_calls:\n                return ChatCompletionAssistantMessageParam(\n                    content=content_txt, \n                    role=\"assistant\", \n                    tool_calls=tool_calls,\n                    **message.extras\n                )\n            else:\n                return ChatCompletionAssistantMessageParam(content=content_txt, role=\"assistant\", **message.extras)\n        elif message.role == Role.TOOL:\n            # For tool messages, extract tool_call_id from ToolResultBlock\n            tool_call_id = None\n            for block in message.blocks:\n                if isinstance(block, ToolResultBlock):\n                    tool_call_id = block.id\n                    break\n\n            if tool_call_id is None:\n                raise ValueError(\"Tool message must contain a ToolResultBlock with an ID\")\n\n            return ChatCompletionToolMessageParam(\n                content=content_txt, \n                role=\"tool\", \n                tool_call_id=tool_call_id,\n                **message.extras\n            )\n        else:\n            raise ValueError(f\"Invalid role: {message.role}\")\n\n    @overload\n    def structured_output(\n        self,\n        messages: List[Message],\n        constraint: PydanticModel,\n        model: Optional[str] = None,\n        temperature: Optional[float] = ...,\n        top_p: Optional[float] = ...,\n        presence_penalty: Optional[float] = ...,\n        frequency_penalty: Optional[float] = ...,\n        extra_body: Optional[Dict[str, Any]] = ...,\n        **kwargs,\n    ) -&gt; BaseModel: ...\n\n    @overload\n    def structured_output(\n        self,\n        messages: List[Message],\n        constraint: JsonSchema,\n        model: Optional[str] = None,\n        temperature: Optional[float] = ...,\n        top_p: Optional[float] = ...,\n        presence_penalty: Optional[float] = ...,\n        frequency_penalty: Optional[float] = ...,\n        extra_body: Optional[Dict[str, Any]] = ...,\n        **kwargs,\n    ) -&gt; Dict[str, Any]: ...\n\n\n    def structured_output(\n        self,\n        messages: List[Message],\n        constraint: Union[PydanticModel, JsonSchema],\n        model: Optional[str] = None,\n        temperature: Optional[float] = None,\n        top_p: Optional[float] = None,\n        presence_penalty: Optional[float] = None,\n        frequency_penalty: Optional[float] = None,\n        extra_body: Optional[Dict[str, Any]] = None,\n        **kwargs,\n    ) -&gt; Union[BaseModel, Dict[str, Any]]:\n        \"\"\"\n        Generate structured output in a specified format using OpenAI's structured output API.\n\n        This method leverages OpenAI's structured output capabilities to ensure the model\n        response conforms to a specified schema. Recommended for use with GPT-4o and later models.\n\n        Parameters\n        ----------\n        messages : List[Message]\n            A list of messages comprising the conversation so far.\n        constraint : Constraint\n            The constraint defining the desired output format (PydanticModel or JsonSchema).\n        model : str\n            Model ID used to generate the response. Structured outputs work best with GPT-4o and later.\n        temperature : Optional[float]\n            What sampling temperature to use, between 0 and 2. Higher values like 0.8 will\n            make the output more random, while lower values like 0.2 will make it more\n            focused and deterministic.\n        top_p : Optional[float]\n            An alternative to sampling with temperature, called nucleus sampling, where the\n            model considers the results of the tokens with top_p probability mass.\n        presence_penalty : Optional[float]\n            Number between -2.0 and 2.0. Positive values penalize new tokens based on\n            whether they appear in the text so far, increasing the model's likelihood to\n            talk about new topics.\n        frequency_penalty : Optional[float]\n            Number between -2.0 and 2.0. Positive values penalize new tokens based on their\n            existing frequency in the text so far, decreasing the model's likelihood to\n            repeat the same line verbatim.\n        extra_body : Optional[Dict[str, Any]]\n            Add additional JSON properties to the request.\n        **kwargs\n            Additional keyword arguments passed to the OpenAI API.\n\n        Returns\n        -------\n        Union[BaseModel, Dict[str, Any]]\n            The structured response in the format specified by the constraint:\n            - BaseModel instance if constraint is PydanticModel\n            - Dict[str, Any] if constraint is JsonSchema\n\n        Examples\n        --------\n        Using a Pydantic model constraint:\n\n        ```python\n        class Answer(BaseModel):\n            reasoning: str\n            result: int\n\n        constraint = PydanticModel(model=Answer)\n        response = llm.structured_output(\n            messages=[Message.from_text(\"What is 2+2?\", role=Role.USER)],\n            constraint=constraint,\n            model=\"gpt-4o\"\n        )\n        print(response.reasoning, response.result)\n        ```\n\n        Using a JSON schema constraint:\n\n        ```python\n        schema = {\"type\": \"object\", \"properties\": {\"answer\": {\"type\": \"string\"}}}\n        constraint = JsonSchema(schema=schema)\n        response = llm.structured_output(\n            messages=[Message.from_text(\"Hello\", role=Role.USER)],\n            constraint=constraint,\n            model=\"gpt-4o\"\n        )\n        print(response[\"answer\"])\n        ```\n\n        Notes\n        -----\n        - Utilizes OpenAI's native structured output API with strict schema validation\n        - All schemas automatically have additionalProperties set to False\n        - Best performance achieved with GPT-4o and later models (gpt-4o-mini, gpt-4o-2024-08-06, and later)\n        \"\"\"\n        params = self._build_parameters(\n            messages=messages,\n            model=model,\n            temperature=temperature,\n            top_p=top_p,\n            presence_penalty=presence_penalty,\n            frequency_penalty=frequency_penalty,\n            extra_body=extra_body,\n            response_format=self._get_response_format(constraint),\n            **kwargs,\n        )\n        # Validate required parameters for structured output\n        validate_required_params(params, [\"messages\", \"model\"])\n\n        response = self.client.chat.completions.parse(**params)\n        return self._convert_response(constraint, response.choices[0].message.content)\n\n    async def astructured_output(\n        self,\n        messages: List[Message],\n        constraint: Union[PydanticModel, JsonSchema],\n        model: Optional[str] = None,\n        temperature: Optional[float] = None,\n        top_p: Optional[float] = None,\n        presence_penalty: Optional[float] = None,\n        frequency_penalty: Optional[float] = None,\n        extra_body: Optional[Dict[str, Any]] = None,\n        **kwargs,\n    ) -&gt; Union[BaseModel, Dict[str, Any]]:\n        \"\"\"\n        Asynchronously generate structured output in a specified format using OpenAI's API.\n\n        This is the asynchronous version of structured_output, suitable for concurrent\n        processing and non-blocking operations. It leverages OpenAI's structured output\n        capabilities to ensure the model response conforms to a specified schema.\n\n        Parameters\n        ----------\n        messages : List[Message]\n            A list of messages comprising the conversation so far.\n        constraint : Constraint\n            The constraint defining the desired output format (PydanticModel or JsonSchema).\n        model : str\n            Model ID used to generate the response. Structured outputs work best with GPT-4o and later.\n        temperature : Optional[float]\n            What sampling temperature to use, between 0 and 2. Higher values like 0.8 will\n            make the output more random, while lower values like 0.2 will make it more\n            focused and deterministic.\n        top_p : Optional[float]\n            An alternative to sampling with temperature, called nucleus sampling, where the\n            model considers the results of the tokens with top_p probability mass.\n        presence_penalty : Optional[float]\n            Number between -2.0 and 2.0. Positive values penalize new tokens based on\n            whether they appear in the text so far, increasing the model's likelihood to\n            talk about new topics.\n        frequency_penalty : Optional[float]\n            Number between -2.0 and 2.0. Positive values penalize new tokens based on their\n            existing frequency in the text so far, decreasing the model's likelihood to\n            repeat the same line verbatim.\n        extra_body : Optional[Dict[str, Any]]\n            Add additional JSON properties to the request.\n        **kwargs\n            Additional keyword arguments passed to the OpenAI API.\n\n        Returns\n        -------\n        Union[BaseModel, Dict[str, Any]]\n            The structured response in the format specified by the constraint:\n            - BaseModel instance if constraint is PydanticModel\n            - Dict[str, Any] if constraint is JsonSchema\n\n        Examples\n        --------\n        Using asynchronous structured output:\n\n        ```python\n        async def get_structured_response():\n            llm = OpenAILlm(api_key=\"your-key\")\n            constraint = PydanticModel(model=Answer)\n            response = await llm.astructured_output(\n                messages=[Message.from_text(\"Calculate 5+3\", role=Role.USER)],\n                constraint=constraint,\n                model=\"gpt-4o\"\n            )\n            return response\n        ```\n\n        Notes\n        -----\n        - This is the asynchronous version of structured_output\n        - Utilizes OpenAI's native structured output API with strict schema validation\n        - Suitable for concurrent processing and high-throughput applications\n        - Best performance achieved with GPT-4o and later models (gpt-4o-mini, gpt-4o-2024-08-06, and later)\n        \"\"\"\n        params = self._build_parameters(\n            messages=messages,\n            model=model,\n            temperature=temperature,\n            top_p=top_p,\n            presence_penalty=presence_penalty,\n            frequency_penalty=frequency_penalty,\n            extra_body=extra_body,\n            response_format=self._get_response_format(constraint),\n            **kwargs,\n        )\n        # Validate required parameters for structured output\n        validate_required_params(params, [\"messages\", \"model\"])\n\n        response = await self.async_client.chat.completions.parse(**params)\n        return self._convert_response(constraint, response.choices[0].message.content)\n\n    def _add_schema_properties(self, schema: Dict[str, Any]) -&gt; Dict[str, Any]:\n        \"\"\"\n        OpenAI requires additionalProperties to be set to False for all objects\n        in structured output schemas. See:\n        [AdditionalProperties False Must Always Be Set in Objects](https://platform.openai.com/docs/guides/structured-outputs?example=moderation#additionalproperties-false-must-always-be-set-in-objects)\n        \"\"\"\n        schema[\"additionalProperties\"] = False\n        return schema\n\n    def _get_response_format(self, constraint: Union[PydanticModel, JsonSchema]) -&gt; Dict[str, Any]:\n        if isinstance(constraint, PydanticModel):\n            result = {\n                \"type\": \"json_schema\",\n                \"json_schema\": {\n                    \"schema\": self._add_schema_properties(constraint.model.model_json_schema()),\n                    \"name\": constraint.model.__name__,\n                    \"strict\": True,\n                },\n            }\n            return result\n        elif isinstance(constraint, JsonSchema):\n            return {\n                \"type\": \"json_schema\",\n                \"json_schema\": {\n                    \"schema\": self._add_schema_properties(constraint.schema_dict),\n                    \"name\": constraint.name,\n                    \"strict\": True,\n                },\n            }\n        else:\n            raise ValueError(f\"Unsupported constraint type '{constraint.constraint_type}'. More info about OpenAI structured output: https://platform.openai.com/docs/guides/structured-outputs\")\n\n    def _convert_response(\n        self,\n        constraint: Union[PydanticModel, JsonSchema],\n        content: str,\n    ) -&gt; Union[BaseModel, Dict[str, Any]]:\n        if isinstance(constraint, PydanticModel):\n            return constraint.model.model_validate_json(content)\n        elif isinstance(constraint, JsonSchema):\n            return json.loads(content)\n        else:\n            raise ValueError(f\"Unsupported constraint type '{constraint.constraint_type}'. More info about OpenAI structured output: https://platform.openai.com/docs/guides/structured-outputs\")\n\n    def select_tool(\n        self,\n        messages: List[Message],\n        tools: List[Tool],\n        model: Optional[str] = None,\n        temperature: Optional[float] = None,\n        top_p: Optional[float] = None,\n        presence_penalty: Optional[float] = None,\n        frequency_penalty: Optional[float] = None,\n        extra_body: Optional[Dict[str, Any]] = None,\n        parallel_tool_calls: Optional[bool] = None,\n        tool_choice: Union[Literal[\"auto\", \"required\", \"none\"], ChatCompletionNamedToolChoiceParam] = None,\n        **kwargs,\n    ) -&gt; Tuple[List[ToolCall], Optional[str]]:\n        \"\"\"\n        Select and invoke tools from a list based on conversation context.\n\n        This method enables the model to intelligently select and call appropriate tools\n        from a provided list based on the conversation context. It supports OpenAI's\n        function calling capabilities with parallel execution and various control options.\n\n        More OpenAI information: [function-calling](https://platform.openai.com/docs/guides/function-calling)\n\n        Parameters\n        ----------\n        messages : List[Message]\n            A list of messages comprising the conversation so far providing context for tool selection.\n        tools : List[Tool]\n            A list of tools the model may call.\n        model : str\n            Model ID used to generate the response. Function calling requires compatible models.\n        temperature : Optional[float]\n            What sampling temperature to use, between 0 and 2. Higher values like 0.8 will\n            make the output more random, while lower values like 0.2 will make it more\n            focused and deterministic.\n        top_p : Optional[float]\n            An alternative to sampling with temperature, called nucleus sampling, where the\n            model considers the results of the tokens with top_p probability mass.\n        presence_penalty : Optional[float]\n            Number between -2.0 and 2.0. Positive values penalize new tokens based on\n            whether they appear in the text so far, increasing the model's likelihood to\n            talk about new topics.\n        frequency_penalty : Optional[float]\n            Number between -2.0 and 2.0. Positive values penalize new tokens based on their\n            existing frequency in the text so far, decreasing the model's likelihood to\n            repeat the same line verbatim.\n        extra_body : Optional[Dict[str, Any]]\n            Add additional JSON properties to the request.\n        parallel_tool_calls : Optional[bool]\n            Whether to enable parallel function calling during tool use.\n        tool_choice : Union[Literal[\"auto\", \"required\", \"none\"], ChatCompletionNamedToolChoiceParam]\n            Controls which tool, if any, the model may call.\n            - `none`: The model will not call any tool and will instead generate a message. This is the default when no tools are provided.\n            - `auto`: The model may choose to generate a message or call one or more tools. This is the default when tools are provided.\n            - `required`: The model must call one or more tools.\n            - To force a specific tool, pass `{\"type\": \"function\", \"function\": {\"name\": \"my_function\"}}`.\n        **kwargs\n            Additional keyword arguments passed to the OpenAI API.\n\n        Returns\n        -------\n        List[ToolCall]\n            List of selected tool calls with their IDs, names, and parsed arguments.\n        Union[str, None]\n            The content of the message from the model.\n        \"\"\"\n        params = self._build_parameters(\n            messages=messages,\n            model=model,\n            temperature=temperature,\n            top_p=top_p,\n            presence_penalty=presence_penalty,\n            frequency_penalty=frequency_penalty,\n            tools=tools,\n            tool_choice=tool_choice,\n            parallel_tool_calls=parallel_tool_calls,\n            extra_body=extra_body,\n            **kwargs,\n        )\n        # Validate required parameters for tool selection\n        validate_required_params(params, [\"messages\", \"model\"])\n\n        response: ChatCompletion = self.client.chat.completions.create(**params)\n        tool_calls = response.choices[0].message.tool_calls\n        content = response.choices[0].message.content\n        return (self._convert_tool_calls(tool_calls), content)\n\n    async def aselect_tool(\n        self,\n        messages: List[Message],\n        tools: List[Tool],\n        model: Optional[str] = None,\n        temperature: Optional[float] = None,\n        top_p: Optional[float] = None,\n        presence_penalty: Optional[float] = None,\n        frequency_penalty: Optional[float] = None,\n        extra_body: Optional[Dict[str, Any]] = None,\n        parallel_tool_calls: Optional[bool] = None,\n        tool_choice: Union[Literal[\"auto\", \"required\", \"none\"], ChatCompletionNamedToolChoiceParam] = None,\n        **kwargs,\n    )-&gt; Tuple[List[ToolCall], Optional[str]]:\n        \"\"\"\n        Select and invoke tools from a list based on conversation context.\n\n        This method enables the model to intelligently select and call appropriate tools\n        from a provided list based on the conversation context. It supports OpenAI's\n        function calling capabilities with parallel execution and various control options.\n\n        More OpenAI information: [function-calling](https://platform.openai.com/docs/guides/function-calling)\n\n        Parameters\n        ----------\n        messages : List[Message]\n            A list of messages comprising the conversation so far providing context for tool selection.\n        tools : List[Tool]\n            A list of tools the model may call.\n        model : str\n            Model ID used to generate the response. Function calling requires compatible models.\n        temperature : Optional[float]\n            What sampling temperature to use, between 0 and 2. Higher values like 0.8 will\n            make the output more random, while lower values like 0.2 will make it more\n            focused and deterministic.\n        top_p : Optional[float]\n            An alternative to sampling with temperature, called nucleus sampling, where the\n            model considers the results of the tokens with top_p probability mass.\n        presence_penalty : Optional[float]\n            Number between -2.0 and 2.0. Positive values penalize new tokens based on\n            whether they appear in the text so far, increasing the model's likelihood to\n            talk about new topics.\n        frequency_penalty : Optional[float]\n            Number between -2.0 and 2.0. Positive values penalize new tokens based on their\n            existing frequency in the text so far, decreasing the model's likelihood to\n            repeat the same line verbatim.\n        extra_body : Optional[Dict[str, Any]]\n            Add additional JSON properties to the request.\n        parallel_tool_calls : Optional[bool]\n            Whether to enable parallel function calling during tool use.\n        tool_choice : Union[Literal[\"auto\", \"required\", \"none\"], ChatCompletionNamedToolChoiceParam]\n            Controls which tool, if any, the model may call.\n            - `none`: The model will not call any tool and will instead generate a message. This is the default when no tools are provided.\n            - `auto`: The model may choose to generate a message or call one or more tools. This is the default when tools are provided.\n            - `required`: The model must call one or more tools.\n            - To force a specific tool, pass `{\"type\": \"function\", \"function\": {\"name\": \"my_function\"}}`.\n\n        **kwargs\n            Additional keyword arguments passed to the OpenAI API.\n\n        Returns\n        -------\n        List[ToolCall]\n            List of selected tool calls with their IDs, names, and parsed arguments.\n        Union[str, None]\n            The content of the message from the model.\n        \"\"\"\n        params = self._build_parameters(\n            messages=messages,\n            model=model,\n            temperature=temperature,\n            top_p=top_p,\n            presence_penalty=presence_penalty,\n            frequency_penalty=frequency_penalty,\n            tools=tools,\n            tool_choice=tool_choice,\n            parallel_tool_calls=parallel_tool_calls,\n            extra_body=extra_body,\n            **kwargs,\n        )\n        # Validate required parameters for tool selection\n        validate_required_params(params, [\"messages\", \"model\"])\n\n        response: ChatCompletion = await self.async_client.chat.completions.create(**params)\n        tool_calls = response.choices[0].message.tool_calls\n        content = response.choices[0].message.content\n        return (self._convert_tool_calls(tool_calls), content)\n\n    def _convert_parameters(self, parameters: Dict[str, Any]) -&gt; Dict[str, Any]:\n        return {\n            \"type\": \"object\",\n            \"properties\": parameters.get(\"properties\", {}),\n            \"required\": parameters.get(\"required\", []),\n            \"additionalProperties\": False\n        }\n\n    def _convert_tool_to_json(self, tool: Tool) -&gt; Dict[str, Any]:\n        return {\n            \"type\": \"function\",\n            \"function\": {\n                \"name\": tool.name,\n                \"description\": tool.description,\n                \"parameters\": self._convert_parameters(tool.parameters),\n            }\n        }\n\n    def _convert_tool_calls(self, tool_calls: List[ChatCompletionMessageFunctionToolCall]) -&gt; List[ToolCall]:\n        return [] if tool_calls is None else [\n            ToolCall(\n                id=tool_call.id,\n                name=tool_call.function.name,\n                arguments=json.loads(tool_call.function.arguments),\n            ) for tool_call in tool_calls\n        ]\n\n    @override\n    def dump_to_dict(self) -&gt; Dict[str, Any]:\n        state_dict = {\n            \"api_base\": self.api_base,\n            \"api_key\": self.api_key,\n            \"timeout\": self.timeout,\n            \"configuration\": self.configuration.model_dump(),\n        }\n        if self.http_client:\n            warnings.warn(\n                \"httpx.Client is not serializable, so it will be set to None in the deserialization.\",\n                RuntimeWarning,\n            )\n        if self.http_async_client:\n            warnings.warn(\n                \"httpx.AsyncClient is not serializable, so it will be set to None in the deserialization.\",\n                RuntimeWarning,\n            )\n        return state_dict\n\n    @override\n    def load_from_dict(self, state_dict: Dict[str, Any]) -&gt; None:\n        self.api_base = state_dict[\"api_base\"]\n        self.api_key = state_dict[\"api_key\"]\n        self.timeout = state_dict[\"timeout\"]\n        self.configuration = OpenAIConfiguration(**state_dict.get(\"configuration\", {}))\n        self.http_client = None\n        self.http_async_client = None\n\n        self.client = OpenAI(\n            base_url=self.api_base,\n            api_key=self.api_key,\n            timeout=self.timeout,\n            http_client=self.http_client,\n        )\n        self.async_client = AsyncOpenAI(\n            base_url=self.api_base,\n            api_key=self.api_key,\n            timeout=self.timeout,\n            http_client=self.http_async_client,\n        )\n</code></pre>"},{"location":"reference/bridgic-llms-openai/bridgic/llms/openai/#bridgic.llms.openai.OpenAILlm.chat","title":"chat","text":"<pre><code>chat(\n    messages: List[Message],\n    model: Optional[str] = None,\n    temperature: Optional[float] = None,\n    top_p: Optional[float] = None,\n    presence_penalty: Optional[float] = None,\n    frequency_penalty: Optional[float] = None,\n    max_tokens: Optional[int] = None,\n    stop: Optional[List[str]] = None,\n    tools: Optional[List[Tool]] = None,\n    extra_body: Optional[Dict[str, Any]] = None,\n    **kwargs\n) -&gt; Response\n</code></pre> <p>Send a synchronous chat completion request to OpenAI.</p> <p>Parameters:</p> Name Type Description Default <code>messages</code> <code>List[Message]</code> <p>A list of messages comprising the conversation so far.</p> required <code>model</code> <code>str</code> <p>Model ID used to generate the response, like <code>gpt-4o</code> or <code>gpt-4</code>.</p> <code>None</code> <code>temperature</code> <code>Optional[float]</code> <p>What sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic.</p> <code>None</code> <code>top_p</code> <code>Optional[float]</code> <p>An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass.</p> <code>None</code> <code>presence_penalty</code> <code>Optional[float]</code> <p>Number between -2.0 and 2.0. Positive values penalize new tokens based on whether they appear in the text so far, increasing the model's likelihood to talk about new topics.</p> <code>None</code> <code>frequency_penalty</code> <code>Optional[float]</code> <p>Number between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model's likelihood to repeat the same line verbatim.</p> <code>None</code> <code>max_tokens</code> <code>Optional[int]</code> <p>The maximum number of tokens that can be generated in the chat completion. This value is now deprecated in favor of <code>max_completion_tokens</code>.</p> <code>None</code> <code>stop</code> <code>Optional[List[str]]</code> <p>Up to 4 sequences where the API will stop generating further tokens. Not supported with latest reasoning models <code>o3</code> and <code>o3-mini</code>.</p> <code>None</code> <code>tools</code> <code>Optional[List[Tool]]</code> <p>A list of tools to use in the chat completion.</p> <code>None</code> <code>extra_body</code> <code>Optional[Dict[str, Any]]</code> <p>Add additional JSON properties to the request.</p> <code>None</code> <code>**kwargs</code> <p>Additional keyword arguments passed to the OpenAI API.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Response</code> <p>A response object containing the generated message and raw API response.</p> Source code in <code>bridgic/llms/openai/openai_llm.py</code> <pre><code>def chat(\n    self,\n    messages: List[Message],\n    model: Optional[str] = None,\n    temperature: Optional[float] = None,\n    top_p: Optional[float] = None,\n    presence_penalty: Optional[float] = None,\n    frequency_penalty: Optional[float] = None,\n    max_tokens: Optional[int] = None,\n    stop: Optional[List[str]] = None,\n    tools: Optional[List[Tool]] = None,\n    extra_body: Optional[Dict[str, Any]] = None,\n    **kwargs,\n) -&gt; Response:\n    \"\"\"\n    Send a synchronous chat completion request to OpenAI.\n\n    Parameters\n    ----------\n    messages : List[Message]\n        A list of messages comprising the conversation so far.\n    model : str\n        Model ID used to generate the response, like `gpt-4o` or `gpt-4`.\n    temperature : Optional[float]\n        What sampling temperature to use, between 0 and 2. Higher values like 0.8 will\n        make the output more random, while lower values like 0.2 will make it more\n        focused and deterministic.\n    top_p : Optional[float]\n        An alternative to sampling with temperature, called nucleus sampling, where the\n        model considers the results of the tokens with top_p probability mass.\n    presence_penalty : Optional[float]\n        Number between -2.0 and 2.0. Positive values penalize new tokens based on\n        whether they appear in the text so far, increasing the model's likelihood to\n        talk about new topics.\n    frequency_penalty : Optional[float]\n        Number between -2.0 and 2.0. Positive values penalize new tokens based on their\n        existing frequency in the text so far, decreasing the model's likelihood to\n        repeat the same line verbatim.\n    max_tokens : Optional[int]\n        The maximum number of tokens that can be generated in the chat completion.\n        This value is now deprecated in favor of `max_completion_tokens`.\n    stop : Optional[List[str]]\n        Up to 4 sequences where the API will stop generating further tokens.\n        Not supported with latest reasoning models `o3` and `o3-mini`.\n    tools : Optional[List[Tool]]\n        A list of tools to use in the chat completion.\n    extra_body : Optional[Dict[str, Any]]\n        Add additional JSON properties to the request.\n    **kwargs\n        Additional keyword arguments passed to the OpenAI API.\n\n    Returns\n    -------\n    Response\n        A response object containing the generated message and raw API response.\n    \"\"\"\n    params = self._build_parameters(\n        messages=messages,\n        model=model,\n        temperature=temperature,\n        top_p=top_p,\n        presence_penalty=presence_penalty,\n        frequency_penalty=frequency_penalty,\n        max_tokens=max_tokens,\n        stop=stop,\n        extra_body=extra_body,\n        **kwargs,\n    )\n    # Validate required parameters for non-streaming chat completion\n    validate_required_params(params, [\"messages\", \"model\"])\n\n    response: ChatCompletion = self.client.chat.completions.create(**params)\n    return self._handle_chat_response(response)\n</code></pre>"},{"location":"reference/bridgic-llms-openai/bridgic/llms/openai/#bridgic.llms.openai.OpenAILlm.stream","title":"stream","text":"<pre><code>stream(\n    messages: List[Message],\n    model: Optional[str] = None,\n    temperature: Optional[float] = None,\n    top_p: Optional[float] = None,\n    presence_penalty: Optional[float] = None,\n    frequency_penalty: Optional[float] = None,\n    max_tokens: Optional[int] = None,\n    stop: Optional[List[str]] = None,\n    extra_body: Optional[Dict[str, Any]] = None,\n    **kwargs\n) -&gt; StreamResponse\n</code></pre> <p>Send a streaming chat completion request to OpenAI.</p> <p>Parameters:</p> Name Type Description Default <code>messages</code> <code>List[Message]</code> <p>A list of messages comprising the conversation so far.</p> required <code>model</code> <code>str</code> <p>Model ID used to generate the response, like <code>gpt-4o</code> or <code>gpt-4</code>.</p> <code>None</code> <code>temperature</code> <code>Optional[float]</code> <p>What sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic.</p> <code>None</code> <code>top_p</code> <code>Optional[float]</code> <p>An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass.</p> <code>None</code> <code>presence_penalty</code> <code>Optional[float]</code> <p>Number between -2.0 and 2.0. Positive values penalize new tokens based on whether they appear in the text so far, increasing the model's likelihood to talk about new topics.</p> <code>None</code> <code>frequency_penalty</code> <code>Optional[float]</code> <p>Number between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model's likelihood to repeat the same line verbatim.</p> <code>None</code> <code>max_tokens</code> <code>Optional[int]</code> <p>The maximum number of tokens that can be generated in the chat completion. This value is now deprecated in favor of <code>max_completion_tokens</code>.</p> <code>None</code> <code>stop</code> <code>Optional[List[str]]</code> <p>Up to 4 sequences where the API will stop generating further tokens. Not supported with latest reasoning models <code>o3</code> and <code>o3-mini</code>.</p> <code>None</code> <code>extra_body</code> <code>Optional[Dict[str, Any]]</code> <p>Add additional JSON properties to the request.</p> <code>None</code> <code>**kwargs</code> <p>Additional keyword arguments passed to the OpenAI API.</p> <code>{}</code> <p>Yields:</p> Type Description <code>MessageChunk</code> <p>Individual chunks of the response as they are received from the API. Each chunk contains a delta (partial content) and the raw response.</p> Notes <p>This method enables real-time streaming of the model's response, useful for providing incremental updates to users as the response is generated.</p> Source code in <code>bridgic/llms/openai/openai_llm.py</code> <pre><code>def stream(\n    self,\n    messages: List[Message],\n    model: Optional[str] = None,\n    temperature: Optional[float] = None,\n    top_p: Optional[float] = None,\n    presence_penalty: Optional[float] = None,\n    frequency_penalty: Optional[float] = None,\n    max_tokens: Optional[int] = None,\n    stop: Optional[List[str]] = None,\n    extra_body: Optional[Dict[str, Any]] = None,\n    **kwargs,\n) -&gt; StreamResponse:\n    \"\"\"\n    Send a streaming chat completion request to OpenAI.\n\n    Parameters\n    ----------\n    messages : List[Message]\n        A list of messages comprising the conversation so far.\n    model : str\n        Model ID used to generate the response, like `gpt-4o` or `gpt-4`.\n    temperature : Optional[float]\n        What sampling temperature to use, between 0 and 2. Higher values like 0.8 will\n        make the output more random, while lower values like 0.2 will make it more\n        focused and deterministic.\n    top_p : Optional[float]\n        An alternative to sampling with temperature, called nucleus sampling, where the\n        model considers the results of the tokens with top_p probability mass.\n    presence_penalty : Optional[float]\n        Number between -2.0 and 2.0. Positive values penalize new tokens based on\n        whether they appear in the text so far, increasing the model's likelihood to\n        talk about new topics.\n    frequency_penalty : Optional[float]\n        Number between -2.0 and 2.0. Positive values penalize new tokens based on their\n        existing frequency in the text so far, decreasing the model's likelihood to\n        repeat the same line verbatim.\n    max_tokens : Optional[int]\n        The maximum number of tokens that can be generated in the chat completion.\n        This value is now deprecated in favor of `max_completion_tokens`.\n    stop : Optional[List[str]]\n        Up to 4 sequences where the API will stop generating further tokens.\n        Not supported with latest reasoning models `o3` and `o3-mini`.\n    extra_body : Optional[Dict[str, Any]]\n        Add additional JSON properties to the request.\n    **kwargs\n        Additional keyword arguments passed to the OpenAI API.\n\n    Yields\n    ------\n    MessageChunk\n        Individual chunks of the response as they are received from the API.\n        Each chunk contains a delta (partial content) and the raw response.\n\n    Notes\n    -----\n    This method enables real-time streaming of the model's response,\n    useful for providing incremental updates to users as the response is generated.\n    \"\"\"\n    params = self._build_parameters(\n        messages=messages,\n        model=model,\n        temperature=temperature,\n        top_p=top_p,\n        presence_penalty=presence_penalty,\n        frequency_penalty=frequency_penalty,\n        max_tokens=max_tokens,\n        stop=stop,\n        extra_body=extra_body,\n        stream=True,\n        **kwargs,\n    )\n    # Validate required parameters for streaming chat completion\n    validate_required_params(params, [\"messages\", \"model\", \"stream\"])\n\n    response: Stream[ChatCompletionChunk] = self.client.chat.completions.create(**params)\n    for chunk in response:\n        if chunk.choices and chunk.choices[0].delta.content:\n            delta_content = chunk.choices[0].delta.content\n            delta_content = delta_content if delta_content else \"\"\n            yield MessageChunk(delta=delta_content, raw=chunk)\n</code></pre>"},{"location":"reference/bridgic-llms-openai/bridgic/llms/openai/#bridgic.llms.openai.OpenAILlm.achat","title":"achat","text":"<code>async</code> <pre><code>achat(\n    messages: List[Message],\n    model: Optional[str] = None,\n    temperature: Optional[float] = None,\n    top_p: Optional[float] = None,\n    presence_penalty: Optional[float] = None,\n    frequency_penalty: Optional[float] = None,\n    max_tokens: Optional[int] = None,\n    stop: Optional[List[str]] = None,\n    tools: Optional[List[Tool]] = None,\n    extra_body: Optional[Dict[str, Any]] = None,\n    **kwargs\n) -&gt; Response\n</code></pre> <p>Send an asynchronous chat completion request to OpenAI.</p> <p>Parameters:</p> Name Type Description Default <code>messages</code> <code>List[Message]</code> <p>A list of messages comprising the conversation so far.</p> required <code>model</code> <code>str</code> <p>Model ID used to generate the response, like <code>gpt-4o</code> or <code>gpt-4</code>.</p> <code>None</code> <code>temperature</code> <code>Optional[float]</code> <p>What sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic.</p> <code>None</code> <code>top_p</code> <code>Optional[float]</code> <p>An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass.</p> <code>None</code> <code>presence_penalty</code> <code>Optional[float]</code> <p>Number between -2.0 and 2.0. Positive values penalize new tokens based on whether they appear in the text so far, increasing the model's likelihood to talk about new topics.</p> <code>None</code> <code>frequency_penalty</code> <code>Optional[float]</code> <p>Number between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model's likelihood to repeat the same line verbatim.</p> <code>None</code> <code>max_tokens</code> <code>Optional[int]</code> <p>The maximum number of tokens that can be generated in the chat completion. This value is now deprecated in favor of <code>max_completion_tokens</code>.</p> <code>None</code> <code>stop</code> <code>Optional[List[str]]</code> <p>Up to 4 sequences where the API will stop generating further tokens. Not supported with latest reasoning models <code>o3</code> and <code>o3-mini</code>.</p> <code>None</code> <code>tools</code> <code>Optional[List[Tool]]</code> <p>A list of tools to use in the chat completion.</p> <code>None</code> <code>extra_body</code> <code>Optional[Dict[str, Any]]</code> <p>Add additional JSON properties to the request.</p> <code>None</code> <code>**kwargs</code> <p>Additional keyword arguments passed to the OpenAI API.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Response</code> <p>A response object containing the generated message and raw API response.</p> Notes <p>This is the asynchronous version of the chat method, suitable for concurrent processing and non-blocking I/O operations.</p> Source code in <code>bridgic/llms/openai/openai_llm.py</code> <pre><code>async def achat(\n    self,\n    messages: List[Message],\n    model: Optional[str] = None,\n    temperature: Optional[float] = None,\n    top_p: Optional[float] = None,\n    presence_penalty: Optional[float] = None,\n    frequency_penalty: Optional[float] = None,\n    max_tokens: Optional[int] = None,\n    stop: Optional[List[str]] = None,\n    tools: Optional[List[Tool]] = None,\n    extra_body: Optional[Dict[str, Any]] = None,\n    **kwargs,\n) -&gt; Response:\n    \"\"\"\n    Send an asynchronous chat completion request to OpenAI.\n\n    Parameters\n    ----------\n    messages : List[Message]\n        A list of messages comprising the conversation so far.\n    model : str\n        Model ID used to generate the response, like `gpt-4o` or `gpt-4`.\n    temperature : Optional[float]\n        What sampling temperature to use, between 0 and 2. Higher values like 0.8 will\n        make the output more random, while lower values like 0.2 will make it more\n        focused and deterministic.\n    top_p : Optional[float]\n        An alternative to sampling with temperature, called nucleus sampling, where the\n        model considers the results of the tokens with top_p probability mass.\n    presence_penalty : Optional[float]\n        Number between -2.0 and 2.0. Positive values penalize new tokens based on\n        whether they appear in the text so far, increasing the model's likelihood to\n        talk about new topics.\n    frequency_penalty : Optional[float]\n        Number between -2.0 and 2.0. Positive values penalize new tokens based on their\n        existing frequency in the text so far, decreasing the model's likelihood to\n        repeat the same line verbatim.\n    max_tokens : Optional[int]\n        The maximum number of tokens that can be generated in the chat completion.\n        This value is now deprecated in favor of `max_completion_tokens`.\n    stop : Optional[List[str]]\n        Up to 4 sequences where the API will stop generating further tokens.\n        Not supported with latest reasoning models `o3` and `o3-mini`.\n    tools : Optional[List[Tool]]\n        A list of tools to use in the chat completion.\n    extra_body : Optional[Dict[str, Any]]\n        Add additional JSON properties to the request.\n    **kwargs\n        Additional keyword arguments passed to the OpenAI API.\n\n    Returns\n    -------\n    Response\n        A response object containing the generated message and raw API response.\n\n    Notes\n    -----\n    This is the asynchronous version of the chat method, suitable for\n    concurrent processing and non-blocking I/O operations.\n    \"\"\"\n    params = self._build_parameters(\n        messages=messages,\n        model=model,\n        temperature=temperature,\n        top_p=top_p,\n        presence_penalty=presence_penalty,\n        frequency_penalty=frequency_penalty,\n        max_tokens=max_tokens,\n        stop=stop,\n        extra_body=extra_body,\n        **kwargs,\n    )\n    # Validate required parameters for non-streaming chat completion\n    validate_required_params(params, [\"messages\", \"model\"])\n\n    response = await self.async_client.chat.completions.create(**params)\n    return self._handle_chat_response(response)\n</code></pre>"},{"location":"reference/bridgic-llms-openai/bridgic/llms/openai/#bridgic.llms.openai.OpenAILlm.astream","title":"astream","text":"<code>async</code> <pre><code>astream(\n    messages: List[Message],\n    model: Optional[str] = None,\n    temperature: Optional[float] = None,\n    top_p: Optional[float] = None,\n    presence_penalty: Optional[float] = None,\n    frequency_penalty: Optional[float] = None,\n    max_tokens: Optional[int] = None,\n    stop: Optional[List[str]] = None,\n    extra_body: Optional[Dict[str, Any]] = None,\n    **kwargs\n) -&gt; AsyncStreamResponse\n</code></pre> <p>Send an asynchronous streaming chat completion request to OpenAI.</p> <p>Parameters:</p> Name Type Description Default <code>messages</code> <code>List[Message]</code> <p>A list of messages comprising the conversation so far.</p> required <code>model</code> <code>str</code> <p>Model ID used to generate the response, like <code>gpt-4o</code> or <code>gpt-4</code>.</p> <code>None</code> <code>temperature</code> <code>Optional[float]</code> <p>What sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic.</p> <code>None</code> <code>top_p</code> <code>Optional[float]</code> <p>An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass.</p> <code>None</code> <code>presence_penalty</code> <code>Optional[float]</code> <p>Number between -2.0 and 2.0. Positive values penalize new tokens based on whether they appear in the text so far, increasing the model's likelihood to talk about new topics.</p> <code>None</code> <code>frequency_penalty</code> <code>Optional[float]</code> <p>Number between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model's likelihood to repeat the same line verbatim.</p> <code>None</code> <code>max_tokens</code> <code>Optional[int]</code> <p>The maximum number of tokens that can be generated in the chat completion. This value is now deprecated in favor of <code>max_completion_tokens</code>.</p> <code>None</code> <code>stop</code> <code>Optional[List[str]]</code> <p>Up to 4 sequences where the API will stop generating further tokens. Not supported with latest reasoning models <code>o3</code> and <code>o3-mini</code>.</p> <code>None</code> <code>extra_body</code> <code>Optional[Dict[str, Any]]</code> <p>Add additional JSON properties to the request.</p> <code>None</code> <code>**kwargs</code> <p>Additional keyword arguments passed to the OpenAI API.</p> <code>{}</code> <p>Yields:</p> Type Description <code>MessageChunk</code> <p>Individual chunks of the response as they are received from the API. Each chunk contains a delta (partial content) and the raw response.</p> Notes <p>This is the asynchronous version of the stream method, suitable for concurrent processing and non-blocking streaming operations.</p> Source code in <code>bridgic/llms/openai/openai_llm.py</code> <pre><code>async def astream(\n    self,\n    messages: List[Message],\n    model: Optional[str] = None,\n    temperature: Optional[float] = None,\n    top_p: Optional[float] = None,\n    presence_penalty: Optional[float] = None,\n    frequency_penalty: Optional[float] = None,\n    max_tokens: Optional[int] = None,\n    stop: Optional[List[str]] = None,\n    extra_body: Optional[Dict[str, Any]] = None,\n    **kwargs,\n) -&gt; AsyncStreamResponse:\n    \"\"\"\n    Send an asynchronous streaming chat completion request to OpenAI.\n\n    Parameters\n    ----------\n    messages : List[Message]\n        A list of messages comprising the conversation so far.\n    model : str\n        Model ID used to generate the response, like `gpt-4o` or `gpt-4`.\n    temperature : Optional[float]\n        What sampling temperature to use, between 0 and 2. Higher values like 0.8 will\n        make the output more random, while lower values like 0.2 will make it more\n        focused and deterministic.\n    top_p : Optional[float]\n        An alternative to sampling with temperature, called nucleus sampling, where the\n        model considers the results of the tokens with top_p probability mass.\n    presence_penalty : Optional[float]\n        Number between -2.0 and 2.0. Positive values penalize new tokens based on\n        whether they appear in the text so far, increasing the model's likelihood to\n        talk about new topics.\n    frequency_penalty : Optional[float]\n        Number between -2.0 and 2.0. Positive values penalize new tokens based on their\n        existing frequency in the text so far, decreasing the model's likelihood to\n        repeat the same line verbatim.\n    max_tokens : Optional[int]\n        The maximum number of tokens that can be generated in the chat completion.\n        This value is now deprecated in favor of `max_completion_tokens`.\n    stop : Optional[List[str]]\n        Up to 4 sequences where the API will stop generating further tokens.\n        Not supported with latest reasoning models `o3` and `o3-mini`.\n    extra_body : Optional[Dict[str, Any]]\n        Add additional JSON properties to the request.\n    **kwargs\n        Additional keyword arguments passed to the OpenAI API.\n\n    Yields\n    ------\n    MessageChunk\n        Individual chunks of the response as they are received from the API.\n        Each chunk contains a delta (partial content) and the raw response.\n\n    Notes\n    -----\n    This is the asynchronous version of the stream method, suitable for\n    concurrent processing and non-blocking streaming operations.\n    \"\"\"\n    params = self._build_parameters(\n        messages=messages,\n        model=model,\n        temperature=temperature,\n        top_p=top_p,\n        presence_penalty=presence_penalty,\n        frequency_penalty=frequency_penalty,\n        max_tokens=max_tokens,\n        stop=stop,\n        extra_body=extra_body,\n        stream=True,\n        **kwargs,\n    )\n    # Validate required parameters for streaming chat completion\n    validate_required_params(params, [\"messages\", \"model\", \"stream\"])\n\n    response = await self.async_client.chat.completions.create(**params)\n    async for chunk in response:\n        if chunk.choices and chunk.choices[0].delta.content:\n            delta_content = chunk.choices[0].delta.content\n            delta_content = delta_content if delta_content else \"\"\n            yield MessageChunk(delta=delta_content, raw=chunk)\n</code></pre>"},{"location":"reference/bridgic-llms-openai/bridgic/llms/openai/#bridgic.llms.openai.OpenAILlm.structured_output","title":"structured_output","text":"<pre><code>structured_output(\n    messages: List[Message],\n    constraint: Union[PydanticModel, JsonSchema],\n    model: Optional[str] = None,\n    temperature: Optional[float] = None,\n    top_p: Optional[float] = None,\n    presence_penalty: Optional[float] = None,\n    frequency_penalty: Optional[float] = None,\n    extra_body: Optional[Dict[str, Any]] = None,\n    **kwargs\n) -&gt; Union[BaseModel, Dict[str, Any]]\n</code></pre> <p>Generate structured output in a specified format using OpenAI's structured output API.</p> <p>This method leverages OpenAI's structured output capabilities to ensure the model response conforms to a specified schema. Recommended for use with GPT-4o and later models.</p> <p>Parameters:</p> Name Type Description Default <code>messages</code> <code>List[Message]</code> <p>A list of messages comprising the conversation so far.</p> required <code>constraint</code> <code>Constraint</code> <p>The constraint defining the desired output format (PydanticModel or JsonSchema).</p> required <code>model</code> <code>str</code> <p>Model ID used to generate the response. Structured outputs work best with GPT-4o and later.</p> <code>None</code> <code>temperature</code> <code>Optional[float]</code> <p>What sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic.</p> <code>None</code> <code>top_p</code> <code>Optional[float]</code> <p>An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass.</p> <code>None</code> <code>presence_penalty</code> <code>Optional[float]</code> <p>Number between -2.0 and 2.0. Positive values penalize new tokens based on whether they appear in the text so far, increasing the model's likelihood to talk about new topics.</p> <code>None</code> <code>frequency_penalty</code> <code>Optional[float]</code> <p>Number between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model's likelihood to repeat the same line verbatim.</p> <code>None</code> <code>extra_body</code> <code>Optional[Dict[str, Any]]</code> <p>Add additional JSON properties to the request.</p> <code>None</code> <code>**kwargs</code> <p>Additional keyword arguments passed to the OpenAI API.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Union[BaseModel, Dict[str, Any]]</code> <p>The structured response in the format specified by the constraint: - BaseModel instance if constraint is PydanticModel - Dict[str, Any] if constraint is JsonSchema</p> <p>Examples:</p> <p>Using a Pydantic model constraint:</p> <pre><code>class Answer(BaseModel):\n    reasoning: str\n    result: int\n\nconstraint = PydanticModel(model=Answer)\nresponse = llm.structured_output(\n    messages=[Message.from_text(\"What is 2+2?\", role=Role.USER)],\n    constraint=constraint,\n    model=\"gpt-4o\"\n)\nprint(response.reasoning, response.result)\n</code></pre> <p>Using a JSON schema constraint:</p> <pre><code>schema = {\"type\": \"object\", \"properties\": {\"answer\": {\"type\": \"string\"}}}\nconstraint = JsonSchema(schema=schema)\nresponse = llm.structured_output(\n    messages=[Message.from_text(\"Hello\", role=Role.USER)],\n    constraint=constraint,\n    model=\"gpt-4o\"\n)\nprint(response[\"answer\"])\n</code></pre> Notes <ul> <li>Utilizes OpenAI's native structured output API with strict schema validation</li> <li>All schemas automatically have additionalProperties set to False</li> <li>Best performance achieved with GPT-4o and later models (gpt-4o-mini, gpt-4o-2024-08-06, and later)</li> </ul> Source code in <code>bridgic/llms/openai/openai_llm.py</code> <pre><code>def structured_output(\n    self,\n    messages: List[Message],\n    constraint: Union[PydanticModel, JsonSchema],\n    model: Optional[str] = None,\n    temperature: Optional[float] = None,\n    top_p: Optional[float] = None,\n    presence_penalty: Optional[float] = None,\n    frequency_penalty: Optional[float] = None,\n    extra_body: Optional[Dict[str, Any]] = None,\n    **kwargs,\n) -&gt; Union[BaseModel, Dict[str, Any]]:\n    \"\"\"\n    Generate structured output in a specified format using OpenAI's structured output API.\n\n    This method leverages OpenAI's structured output capabilities to ensure the model\n    response conforms to a specified schema. Recommended for use with GPT-4o and later models.\n\n    Parameters\n    ----------\n    messages : List[Message]\n        A list of messages comprising the conversation so far.\n    constraint : Constraint\n        The constraint defining the desired output format (PydanticModel or JsonSchema).\n    model : str\n        Model ID used to generate the response. Structured outputs work best with GPT-4o and later.\n    temperature : Optional[float]\n        What sampling temperature to use, between 0 and 2. Higher values like 0.8 will\n        make the output more random, while lower values like 0.2 will make it more\n        focused and deterministic.\n    top_p : Optional[float]\n        An alternative to sampling with temperature, called nucleus sampling, where the\n        model considers the results of the tokens with top_p probability mass.\n    presence_penalty : Optional[float]\n        Number between -2.0 and 2.0. Positive values penalize new tokens based on\n        whether they appear in the text so far, increasing the model's likelihood to\n        talk about new topics.\n    frequency_penalty : Optional[float]\n        Number between -2.0 and 2.0. Positive values penalize new tokens based on their\n        existing frequency in the text so far, decreasing the model's likelihood to\n        repeat the same line verbatim.\n    extra_body : Optional[Dict[str, Any]]\n        Add additional JSON properties to the request.\n    **kwargs\n        Additional keyword arguments passed to the OpenAI API.\n\n    Returns\n    -------\n    Union[BaseModel, Dict[str, Any]]\n        The structured response in the format specified by the constraint:\n        - BaseModel instance if constraint is PydanticModel\n        - Dict[str, Any] if constraint is JsonSchema\n\n    Examples\n    --------\n    Using a Pydantic model constraint:\n\n    ```python\n    class Answer(BaseModel):\n        reasoning: str\n        result: int\n\n    constraint = PydanticModel(model=Answer)\n    response = llm.structured_output(\n        messages=[Message.from_text(\"What is 2+2?\", role=Role.USER)],\n        constraint=constraint,\n        model=\"gpt-4o\"\n    )\n    print(response.reasoning, response.result)\n    ```\n\n    Using a JSON schema constraint:\n\n    ```python\n    schema = {\"type\": \"object\", \"properties\": {\"answer\": {\"type\": \"string\"}}}\n    constraint = JsonSchema(schema=schema)\n    response = llm.structured_output(\n        messages=[Message.from_text(\"Hello\", role=Role.USER)],\n        constraint=constraint,\n        model=\"gpt-4o\"\n    )\n    print(response[\"answer\"])\n    ```\n\n    Notes\n    -----\n    - Utilizes OpenAI's native structured output API with strict schema validation\n    - All schemas automatically have additionalProperties set to False\n    - Best performance achieved with GPT-4o and later models (gpt-4o-mini, gpt-4o-2024-08-06, and later)\n    \"\"\"\n    params = self._build_parameters(\n        messages=messages,\n        model=model,\n        temperature=temperature,\n        top_p=top_p,\n        presence_penalty=presence_penalty,\n        frequency_penalty=frequency_penalty,\n        extra_body=extra_body,\n        response_format=self._get_response_format(constraint),\n        **kwargs,\n    )\n    # Validate required parameters for structured output\n    validate_required_params(params, [\"messages\", \"model\"])\n\n    response = self.client.chat.completions.parse(**params)\n    return self._convert_response(constraint, response.choices[0].message.content)\n</code></pre>"},{"location":"reference/bridgic-llms-openai/bridgic/llms/openai/#bridgic.llms.openai.OpenAILlm.astructured_output","title":"astructured_output","text":"<code>async</code> <pre><code>astructured_output(\n    messages: List[Message],\n    constraint: Union[PydanticModel, JsonSchema],\n    model: Optional[str] = None,\n    temperature: Optional[float] = None,\n    top_p: Optional[float] = None,\n    presence_penalty: Optional[float] = None,\n    frequency_penalty: Optional[float] = None,\n    extra_body: Optional[Dict[str, Any]] = None,\n    **kwargs\n) -&gt; Union[BaseModel, Dict[str, Any]]\n</code></pre> <p>Asynchronously generate structured output in a specified format using OpenAI's API.</p> <p>This is the asynchronous version of structured_output, suitable for concurrent processing and non-blocking operations. It leverages OpenAI's structured output capabilities to ensure the model response conforms to a specified schema.</p> <p>Parameters:</p> Name Type Description Default <code>messages</code> <code>List[Message]</code> <p>A list of messages comprising the conversation so far.</p> required <code>constraint</code> <code>Constraint</code> <p>The constraint defining the desired output format (PydanticModel or JsonSchema).</p> required <code>model</code> <code>str</code> <p>Model ID used to generate the response. Structured outputs work best with GPT-4o and later.</p> <code>None</code> <code>temperature</code> <code>Optional[float]</code> <p>What sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic.</p> <code>None</code> <code>top_p</code> <code>Optional[float]</code> <p>An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass.</p> <code>None</code> <code>presence_penalty</code> <code>Optional[float]</code> <p>Number between -2.0 and 2.0. Positive values penalize new tokens based on whether they appear in the text so far, increasing the model's likelihood to talk about new topics.</p> <code>None</code> <code>frequency_penalty</code> <code>Optional[float]</code> <p>Number between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model's likelihood to repeat the same line verbatim.</p> <code>None</code> <code>extra_body</code> <code>Optional[Dict[str, Any]]</code> <p>Add additional JSON properties to the request.</p> <code>None</code> <code>**kwargs</code> <p>Additional keyword arguments passed to the OpenAI API.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Union[BaseModel, Dict[str, Any]]</code> <p>The structured response in the format specified by the constraint: - BaseModel instance if constraint is PydanticModel - Dict[str, Any] if constraint is JsonSchema</p> <p>Examples:</p> <p>Using asynchronous structured output:</p> <pre><code>async def get_structured_response():\n    llm = OpenAILlm(api_key=\"your-key\")\n    constraint = PydanticModel(model=Answer)\n    response = await llm.astructured_output(\n        messages=[Message.from_text(\"Calculate 5+3\", role=Role.USER)],\n        constraint=constraint,\n        model=\"gpt-4o\"\n    )\n    return response\n</code></pre> Notes <ul> <li>This is the asynchronous version of structured_output</li> <li>Utilizes OpenAI's native structured output API with strict schema validation</li> <li>Suitable for concurrent processing and high-throughput applications</li> <li>Best performance achieved with GPT-4o and later models (gpt-4o-mini, gpt-4o-2024-08-06, and later)</li> </ul> Source code in <code>bridgic/llms/openai/openai_llm.py</code> <pre><code>async def astructured_output(\n    self,\n    messages: List[Message],\n    constraint: Union[PydanticModel, JsonSchema],\n    model: Optional[str] = None,\n    temperature: Optional[float] = None,\n    top_p: Optional[float] = None,\n    presence_penalty: Optional[float] = None,\n    frequency_penalty: Optional[float] = None,\n    extra_body: Optional[Dict[str, Any]] = None,\n    **kwargs,\n) -&gt; Union[BaseModel, Dict[str, Any]]:\n    \"\"\"\n    Asynchronously generate structured output in a specified format using OpenAI's API.\n\n    This is the asynchronous version of structured_output, suitable for concurrent\n    processing and non-blocking operations. It leverages OpenAI's structured output\n    capabilities to ensure the model response conforms to a specified schema.\n\n    Parameters\n    ----------\n    messages : List[Message]\n        A list of messages comprising the conversation so far.\n    constraint : Constraint\n        The constraint defining the desired output format (PydanticModel or JsonSchema).\n    model : str\n        Model ID used to generate the response. Structured outputs work best with GPT-4o and later.\n    temperature : Optional[float]\n        What sampling temperature to use, between 0 and 2. Higher values like 0.8 will\n        make the output more random, while lower values like 0.2 will make it more\n        focused and deterministic.\n    top_p : Optional[float]\n        An alternative to sampling with temperature, called nucleus sampling, where the\n        model considers the results of the tokens with top_p probability mass.\n    presence_penalty : Optional[float]\n        Number between -2.0 and 2.0. Positive values penalize new tokens based on\n        whether they appear in the text so far, increasing the model's likelihood to\n        talk about new topics.\n    frequency_penalty : Optional[float]\n        Number between -2.0 and 2.0. Positive values penalize new tokens based on their\n        existing frequency in the text so far, decreasing the model's likelihood to\n        repeat the same line verbatim.\n    extra_body : Optional[Dict[str, Any]]\n        Add additional JSON properties to the request.\n    **kwargs\n        Additional keyword arguments passed to the OpenAI API.\n\n    Returns\n    -------\n    Union[BaseModel, Dict[str, Any]]\n        The structured response in the format specified by the constraint:\n        - BaseModel instance if constraint is PydanticModel\n        - Dict[str, Any] if constraint is JsonSchema\n\n    Examples\n    --------\n    Using asynchronous structured output:\n\n    ```python\n    async def get_structured_response():\n        llm = OpenAILlm(api_key=\"your-key\")\n        constraint = PydanticModel(model=Answer)\n        response = await llm.astructured_output(\n            messages=[Message.from_text(\"Calculate 5+3\", role=Role.USER)],\n            constraint=constraint,\n            model=\"gpt-4o\"\n        )\n        return response\n    ```\n\n    Notes\n    -----\n    - This is the asynchronous version of structured_output\n    - Utilizes OpenAI's native structured output API with strict schema validation\n    - Suitable for concurrent processing and high-throughput applications\n    - Best performance achieved with GPT-4o and later models (gpt-4o-mini, gpt-4o-2024-08-06, and later)\n    \"\"\"\n    params = self._build_parameters(\n        messages=messages,\n        model=model,\n        temperature=temperature,\n        top_p=top_p,\n        presence_penalty=presence_penalty,\n        frequency_penalty=frequency_penalty,\n        extra_body=extra_body,\n        response_format=self._get_response_format(constraint),\n        **kwargs,\n    )\n    # Validate required parameters for structured output\n    validate_required_params(params, [\"messages\", \"model\"])\n\n    response = await self.async_client.chat.completions.parse(**params)\n    return self._convert_response(constraint, response.choices[0].message.content)\n</code></pre>"},{"location":"reference/bridgic-llms-openai/bridgic/llms/openai/#bridgic.llms.openai.OpenAILlm.select_tool","title":"select_tool","text":"<pre><code>select_tool(\n    messages: List[Message],\n    tools: List[Tool],\n    model: Optional[str] = None,\n    temperature: Optional[float] = None,\n    top_p: Optional[float] = None,\n    presence_penalty: Optional[float] = None,\n    frequency_penalty: Optional[float] = None,\n    extra_body: Optional[Dict[str, Any]] = None,\n    parallel_tool_calls: Optional[bool] = None,\n    tool_choice: Union[\n        Literal[\"auto\", \"required\", \"none\"],\n        ChatCompletionNamedToolChoiceParam,\n    ] = None,\n    **kwargs\n) -&gt; Tuple[List[ToolCall], Optional[str]]\n</code></pre> <p>Select and invoke tools from a list based on conversation context.</p> <p>This method enables the model to intelligently select and call appropriate tools from a provided list based on the conversation context. It supports OpenAI's function calling capabilities with parallel execution and various control options.</p> <p>More OpenAI information: function-calling</p> <p>Parameters:</p> Name Type Description Default <code>messages</code> <code>List[Message]</code> <p>A list of messages comprising the conversation so far providing context for tool selection.</p> required <code>tools</code> <code>List[Tool]</code> <p>A list of tools the model may call.</p> required <code>model</code> <code>str</code> <p>Model ID used to generate the response. Function calling requires compatible models.</p> <code>None</code> <code>temperature</code> <code>Optional[float]</code> <p>What sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic.</p> <code>None</code> <code>top_p</code> <code>Optional[float]</code> <p>An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass.</p> <code>None</code> <code>presence_penalty</code> <code>Optional[float]</code> <p>Number between -2.0 and 2.0. Positive values penalize new tokens based on whether they appear in the text so far, increasing the model's likelihood to talk about new topics.</p> <code>None</code> <code>frequency_penalty</code> <code>Optional[float]</code> <p>Number between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model's likelihood to repeat the same line verbatim.</p> <code>None</code> <code>extra_body</code> <code>Optional[Dict[str, Any]]</code> <p>Add additional JSON properties to the request.</p> <code>None</code> <code>parallel_tool_calls</code> <code>Optional[bool]</code> <p>Whether to enable parallel function calling during tool use.</p> <code>None</code> <code>tool_choice</code> <code>Union[Literal['auto', 'required', 'none'], ChatCompletionNamedToolChoiceParam]</code> <p>Controls which tool, if any, the model may call. - <code>none</code>: The model will not call any tool and will instead generate a message. This is the default when no tools are provided. - <code>auto</code>: The model may choose to generate a message or call one or more tools. This is the default when tools are provided. - <code>required</code>: The model must call one or more tools. - To force a specific tool, pass <code>{\"type\": \"function\", \"function\": {\"name\": \"my_function\"}}</code>.</p> <code>None</code> <code>**kwargs</code> <p>Additional keyword arguments passed to the OpenAI API.</p> <code>{}</code> <p>Returns:</p> Type Description <code>List[ToolCall]</code> <p>List of selected tool calls with their IDs, names, and parsed arguments.</p> <code>Union[str, None]</code> <p>The content of the message from the model.</p> Source code in <code>bridgic/llms/openai/openai_llm.py</code> <pre><code>def select_tool(\n    self,\n    messages: List[Message],\n    tools: List[Tool],\n    model: Optional[str] = None,\n    temperature: Optional[float] = None,\n    top_p: Optional[float] = None,\n    presence_penalty: Optional[float] = None,\n    frequency_penalty: Optional[float] = None,\n    extra_body: Optional[Dict[str, Any]] = None,\n    parallel_tool_calls: Optional[bool] = None,\n    tool_choice: Union[Literal[\"auto\", \"required\", \"none\"], ChatCompletionNamedToolChoiceParam] = None,\n    **kwargs,\n) -&gt; Tuple[List[ToolCall], Optional[str]]:\n    \"\"\"\n    Select and invoke tools from a list based on conversation context.\n\n    This method enables the model to intelligently select and call appropriate tools\n    from a provided list based on the conversation context. It supports OpenAI's\n    function calling capabilities with parallel execution and various control options.\n\n    More OpenAI information: [function-calling](https://platform.openai.com/docs/guides/function-calling)\n\n    Parameters\n    ----------\n    messages : List[Message]\n        A list of messages comprising the conversation so far providing context for tool selection.\n    tools : List[Tool]\n        A list of tools the model may call.\n    model : str\n        Model ID used to generate the response. Function calling requires compatible models.\n    temperature : Optional[float]\n        What sampling temperature to use, between 0 and 2. Higher values like 0.8 will\n        make the output more random, while lower values like 0.2 will make it more\n        focused and deterministic.\n    top_p : Optional[float]\n        An alternative to sampling with temperature, called nucleus sampling, where the\n        model considers the results of the tokens with top_p probability mass.\n    presence_penalty : Optional[float]\n        Number between -2.0 and 2.0. Positive values penalize new tokens based on\n        whether they appear in the text so far, increasing the model's likelihood to\n        talk about new topics.\n    frequency_penalty : Optional[float]\n        Number between -2.0 and 2.0. Positive values penalize new tokens based on their\n        existing frequency in the text so far, decreasing the model's likelihood to\n        repeat the same line verbatim.\n    extra_body : Optional[Dict[str, Any]]\n        Add additional JSON properties to the request.\n    parallel_tool_calls : Optional[bool]\n        Whether to enable parallel function calling during tool use.\n    tool_choice : Union[Literal[\"auto\", \"required\", \"none\"], ChatCompletionNamedToolChoiceParam]\n        Controls which tool, if any, the model may call.\n        - `none`: The model will not call any tool and will instead generate a message. This is the default when no tools are provided.\n        - `auto`: The model may choose to generate a message or call one or more tools. This is the default when tools are provided.\n        - `required`: The model must call one or more tools.\n        - To force a specific tool, pass `{\"type\": \"function\", \"function\": {\"name\": \"my_function\"}}`.\n    **kwargs\n        Additional keyword arguments passed to the OpenAI API.\n\n    Returns\n    -------\n    List[ToolCall]\n        List of selected tool calls with their IDs, names, and parsed arguments.\n    Union[str, None]\n        The content of the message from the model.\n    \"\"\"\n    params = self._build_parameters(\n        messages=messages,\n        model=model,\n        temperature=temperature,\n        top_p=top_p,\n        presence_penalty=presence_penalty,\n        frequency_penalty=frequency_penalty,\n        tools=tools,\n        tool_choice=tool_choice,\n        parallel_tool_calls=parallel_tool_calls,\n        extra_body=extra_body,\n        **kwargs,\n    )\n    # Validate required parameters for tool selection\n    validate_required_params(params, [\"messages\", \"model\"])\n\n    response: ChatCompletion = self.client.chat.completions.create(**params)\n    tool_calls = response.choices[0].message.tool_calls\n    content = response.choices[0].message.content\n    return (self._convert_tool_calls(tool_calls), content)\n</code></pre>"},{"location":"reference/bridgic-llms-openai/bridgic/llms/openai/#bridgic.llms.openai.OpenAILlm.aselect_tool","title":"aselect_tool","text":"<code>async</code> <pre><code>aselect_tool(\n    messages: List[Message],\n    tools: List[Tool],\n    model: Optional[str] = None,\n    temperature: Optional[float] = None,\n    top_p: Optional[float] = None,\n    presence_penalty: Optional[float] = None,\n    frequency_penalty: Optional[float] = None,\n    extra_body: Optional[Dict[str, Any]] = None,\n    parallel_tool_calls: Optional[bool] = None,\n    tool_choice: Union[\n        Literal[\"auto\", \"required\", \"none\"],\n        ChatCompletionNamedToolChoiceParam,\n    ] = None,\n    **kwargs\n) -&gt; Tuple[List[ToolCall], Optional[str]]\n</code></pre> <p>Select and invoke tools from a list based on conversation context.</p> <p>This method enables the model to intelligently select and call appropriate tools from a provided list based on the conversation context. It supports OpenAI's function calling capabilities with parallel execution and various control options.</p> <p>More OpenAI information: function-calling</p> <p>Parameters:</p> Name Type Description Default <code>messages</code> <code>List[Message]</code> <p>A list of messages comprising the conversation so far providing context for tool selection.</p> required <code>tools</code> <code>List[Tool]</code> <p>A list of tools the model may call.</p> required <code>model</code> <code>str</code> <p>Model ID used to generate the response. Function calling requires compatible models.</p> <code>None</code> <code>temperature</code> <code>Optional[float]</code> <p>What sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic.</p> <code>None</code> <code>top_p</code> <code>Optional[float]</code> <p>An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass.</p> <code>None</code> <code>presence_penalty</code> <code>Optional[float]</code> <p>Number between -2.0 and 2.0. Positive values penalize new tokens based on whether they appear in the text so far, increasing the model's likelihood to talk about new topics.</p> <code>None</code> <code>frequency_penalty</code> <code>Optional[float]</code> <p>Number between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model's likelihood to repeat the same line verbatim.</p> <code>None</code> <code>extra_body</code> <code>Optional[Dict[str, Any]]</code> <p>Add additional JSON properties to the request.</p> <code>None</code> <code>parallel_tool_calls</code> <code>Optional[bool]</code> <p>Whether to enable parallel function calling during tool use.</p> <code>None</code> <code>tool_choice</code> <code>Union[Literal['auto', 'required', 'none'], ChatCompletionNamedToolChoiceParam]</code> <p>Controls which tool, if any, the model may call. - <code>none</code>: The model will not call any tool and will instead generate a message. This is the default when no tools are provided. - <code>auto</code>: The model may choose to generate a message or call one or more tools. This is the default when tools are provided. - <code>required</code>: The model must call one or more tools. - To force a specific tool, pass <code>{\"type\": \"function\", \"function\": {\"name\": \"my_function\"}}</code>.</p> <code>None</code> <code>**kwargs</code> <p>Additional keyword arguments passed to the OpenAI API.</p> <code>{}</code> <p>Returns:</p> Type Description <code>List[ToolCall]</code> <p>List of selected tool calls with their IDs, names, and parsed arguments.</p> <code>Union[str, None]</code> <p>The content of the message from the model.</p> Source code in <code>bridgic/llms/openai/openai_llm.py</code> <pre><code>async def aselect_tool(\n    self,\n    messages: List[Message],\n    tools: List[Tool],\n    model: Optional[str] = None,\n    temperature: Optional[float] = None,\n    top_p: Optional[float] = None,\n    presence_penalty: Optional[float] = None,\n    frequency_penalty: Optional[float] = None,\n    extra_body: Optional[Dict[str, Any]] = None,\n    parallel_tool_calls: Optional[bool] = None,\n    tool_choice: Union[Literal[\"auto\", \"required\", \"none\"], ChatCompletionNamedToolChoiceParam] = None,\n    **kwargs,\n)-&gt; Tuple[List[ToolCall], Optional[str]]:\n    \"\"\"\n    Select and invoke tools from a list based on conversation context.\n\n    This method enables the model to intelligently select and call appropriate tools\n    from a provided list based on the conversation context. It supports OpenAI's\n    function calling capabilities with parallel execution and various control options.\n\n    More OpenAI information: [function-calling](https://platform.openai.com/docs/guides/function-calling)\n\n    Parameters\n    ----------\n    messages : List[Message]\n        A list of messages comprising the conversation so far providing context for tool selection.\n    tools : List[Tool]\n        A list of tools the model may call.\n    model : str\n        Model ID used to generate the response. Function calling requires compatible models.\n    temperature : Optional[float]\n        What sampling temperature to use, between 0 and 2. Higher values like 0.8 will\n        make the output more random, while lower values like 0.2 will make it more\n        focused and deterministic.\n    top_p : Optional[float]\n        An alternative to sampling with temperature, called nucleus sampling, where the\n        model considers the results of the tokens with top_p probability mass.\n    presence_penalty : Optional[float]\n        Number between -2.0 and 2.0. Positive values penalize new tokens based on\n        whether they appear in the text so far, increasing the model's likelihood to\n        talk about new topics.\n    frequency_penalty : Optional[float]\n        Number between -2.0 and 2.0. Positive values penalize new tokens based on their\n        existing frequency in the text so far, decreasing the model's likelihood to\n        repeat the same line verbatim.\n    extra_body : Optional[Dict[str, Any]]\n        Add additional JSON properties to the request.\n    parallel_tool_calls : Optional[bool]\n        Whether to enable parallel function calling during tool use.\n    tool_choice : Union[Literal[\"auto\", \"required\", \"none\"], ChatCompletionNamedToolChoiceParam]\n        Controls which tool, if any, the model may call.\n        - `none`: The model will not call any tool and will instead generate a message. This is the default when no tools are provided.\n        - `auto`: The model may choose to generate a message or call one or more tools. This is the default when tools are provided.\n        - `required`: The model must call one or more tools.\n        - To force a specific tool, pass `{\"type\": \"function\", \"function\": {\"name\": \"my_function\"}}`.\n\n    **kwargs\n        Additional keyword arguments passed to the OpenAI API.\n\n    Returns\n    -------\n    List[ToolCall]\n        List of selected tool calls with their IDs, names, and parsed arguments.\n    Union[str, None]\n        The content of the message from the model.\n    \"\"\"\n    params = self._build_parameters(\n        messages=messages,\n        model=model,\n        temperature=temperature,\n        top_p=top_p,\n        presence_penalty=presence_penalty,\n        frequency_penalty=frequency_penalty,\n        tools=tools,\n        tool_choice=tool_choice,\n        parallel_tool_calls=parallel_tool_calls,\n        extra_body=extra_body,\n        **kwargs,\n    )\n    # Validate required parameters for tool selection\n    validate_required_params(params, [\"messages\", \"model\"])\n\n    response: ChatCompletion = await self.async_client.chat.completions.create(**params)\n    tool_calls = response.choices[0].message.tool_calls\n    content = response.choices[0].message.content\n    return (self._convert_tool_calls(tool_calls), content)\n</code></pre>"},{"location":"reference/bridgic-llms-openai-like/bridgic/llms/openai_like/","title":"openai_like","text":"<p>The OpenAI-Like integration module provides support for third-party services  compatible with the OpenAI API.</p> <p>This package is a thin wrapper for the OpenAI API, designed to meet the needs  for calling third-party model services compatible with the OpenAI API.</p> <p>Note that this integration does not adapt to specific model providers, but  provides general-purpose interfaces. Therefore, it is not fully comprehensive  in functionality and only supports basic chat/stream operations and their  corresponding async interfaces.</p> <p>You can install the OpenAI-Like integration package for Bridgic by running:</p> <pre><code>pip install bridgic-llms-openai-like\n</code></pre>"},{"location":"reference/bridgic-llms-openai-like/bridgic/llms/openai_like/#bridgic.llms.openai_like.OpenAILikeConfiguration","title":"OpenAILikeConfiguration","text":"<p>               Bases: <code>BaseModel</code></p> <p>Default configuration for OpenAI-compatible chat completions.</p> <p>Attributes:</p> Name Type Description <code>model</code> <code>(str, optional)</code> <p>Default model to use when a call-time <code>model</code> is not provided.</p> <code>temperature</code> <code>(float, optional)</code> <p>Sampling temperature in [0, 2]. Higher is more random, lower is more deterministic.</p> <code>top_p</code> <code>(float, optional)</code> <p>Nucleus sampling probability mass in (0, 1]. Alternative to temperature.</p> <code>presence_penalty</code> <code>(float, optional)</code> <p>Penalize new tokens based on whether they appear so far. [-2.0, 2.0].</p> <code>frequency_penalty</code> <code>(float, optional)</code> <p>Penalize new tokens based on their frequency so far. [-2.0, 2.0].</p> <code>max_tokens</code> <code>(int, optional)</code> <p>Maximum number of tokens to generate for the completion.</p> <code>stop</code> <code>(list[str], optional)</code> <p>Up to 4 sequences where generation will stop.</p> Source code in <code>bridgic/llms/openai_like/openai_like_llm.py</code> <pre><code>class OpenAILikeConfiguration(BaseModel):\n    \"\"\"\n    Default configuration for OpenAI-compatible chat completions.\n\n    Attributes\n    ----------\n    model : str, optional\n        Default model to use when a call-time `model` is not provided.\n    temperature : float, optional\n        Sampling temperature in [0, 2]. Higher is more random, lower is more deterministic.\n    top_p : float, optional\n        Nucleus sampling probability mass in (0, 1]. Alternative to temperature.\n    presence_penalty : float, optional\n        Penalize new tokens based on whether they appear so far. [-2.0, 2.0].\n    frequency_penalty : float, optional\n        Penalize new tokens based on their frequency so far. [-2.0, 2.0].\n    max_tokens : int, optional\n        Maximum number of tokens to generate for the completion.\n    stop : list[str], optional\n        Up to 4 sequences where generation will stop.\n    \"\"\"\n    model: Optional[str] = None\n    temperature: Optional[float] = None\n    top_p: Optional[float] = None\n    presence_penalty: Optional[float] = None\n    frequency_penalty: Optional[float] = None\n    max_tokens: Optional[int] = None\n    stop: Optional[List[str]] = None\n</code></pre>"},{"location":"reference/bridgic-llms-openai-like/bridgic/llms/openai_like/#bridgic.llms.openai_like.OpenAILikeLlm","title":"OpenAILikeLlm","text":"<p>               Bases: <code>BaseLlm</code></p> <p>OpenAILikeLlm is a thin wrapper around the LLM providers that makes it compatible with the  services that provide OpenAI compatible API. To support the widest range of model providers,  this wrapper only supports text-modal usage.</p> <p>Parameters:</p> Name Type Description Default <code>api_base</code> <code>str</code> <p>The base URL of the LLM provider.</p> required <code>api_key</code> <code>str</code> <p>The API key of the LLM provider.</p> required <code>timeout</code> <code>Optional[float]</code> <p>The timeout in seconds.</p> <code>None</code> Source code in <code>bridgic/llms/openai_like/openai_like_llm.py</code> <pre><code>class OpenAILikeLlm(BaseLlm):\n    \"\"\"\n    OpenAILikeLlm is a thin wrapper around the LLM providers that makes it compatible with the \n    services that provide OpenAI compatible API. To support the widest range of model providers, \n    this wrapper only supports text-modal usage.\n\n    Parameters\n    ----------\n    api_base: str\n        The base URL of the LLM provider.\n    api_key: str\n        The API key of the LLM provider.\n    timeout: Optional[float]\n        The timeout in seconds.\n    \"\"\"\n\n    api_base: str\n    api_key: str\n    configuration: OpenAILikeConfiguration\n    timeout: float\n    http_client: httpx.Client\n    http_async_client: httpx.AsyncClient\n\n    client: OpenAI\n    async_client: AsyncOpenAI\n\n    def __init__(\n        self,\n        api_base: str,\n        api_key: str,\n        configuration: Optional[OpenAILikeConfiguration] = OpenAILikeConfiguration(),\n        timeout: Optional[float] = None,\n        http_client: Optional[httpx.Client] = None,\n        http_async_client: Optional[httpx.AsyncClient] = None,\n    ):\n        # Record for serialization / deserialization.\n        self.api_base = api_base\n        self.api_key = api_key\n        self.configuration = configuration\n        self.timeout = timeout\n        self.http_client = http_client\n        self.http_async_client = http_async_client\n\n        # Initialize clients.\n        self.client = OpenAI(base_url=api_base, api_key=api_key, timeout=timeout, http_client=http_client)\n        self.async_client = AsyncOpenAI(base_url=api_base, api_key=api_key, timeout=timeout, http_client=http_async_client)\n\n    def chat(\n        self,\n        messages: List[Message],\n        model: Optional[str] = None,\n        temperature: Optional[float] = None,\n        top_p: Optional[float] = None,\n        presence_penalty: Optional[float] = None,\n        frequency_penalty: Optional[float] = None,\n        max_tokens: Optional[int] = None,\n        stop: Optional[List[str]] = None,\n        extra_body: Optional[Dict[str, Any]] = None,\n        **kwargs,\n    ) -&gt; Response:\n        \"\"\"\n        Send a synchronous chat completion request to an OpenAI-compatible provider.\n\n        Parameters\n        ----------\n        messages : list[Message]\n            Conversation messages.\n        model : str, optional\n            Model ID to use. Required unless provided in `configuration.model`.\n        temperature : float, optional\n            Sampling temperature in [0, 2]. Higher is more random, lower is more deterministic.\n        top_p : float, optional\n            Nucleus sampling probability mass in (0, 1]. Alternative to temperature.\n        presence_penalty : float, optional\n            Penalize new tokens based on whether they appear so far. [-2.0, 2.0].\n        frequency_penalty : float, optional\n            Penalize new tokens based on their frequency so far. [-2.0, 2.0].\n        max_tokens : int, optional\n            Maximum tokens to generate for completion.\n        stop : list[str], optional\n            Up to 4 sequences where generation will stop.\n        extra_body : dict, optional\n            Extra JSON payload sent to the provider.\n        **kwargs\n            Additional provider-specific arguments.\n\n        Returns\n        -------\n        Response\n            Bridgic response containing the generated message and raw API response.\n\n        Notes\n        -----\n        - Required parameter validation ensures `messages` and final `model` are present\n          (from either the call or `configuration`).\n        \"\"\"\n        params = self._build_parameters(\n            messages=messages,\n            model=model,\n            temperature=temperature,\n            top_p=top_p,\n            presence_penalty=presence_penalty,\n            frequency_penalty=frequency_penalty,\n            max_tokens=max_tokens,\n            stop=stop,\n            extra_body=extra_body,\n            **kwargs,\n        )\n        validate_required_params(params, [\"messages\", \"model\"])\n        response = self.client.chat.completions.create(**params)\n        openai_message: ChatCompletionMessage = response.choices[0].message\n        text: str = openai_message.content if openai_message.content else \"\"\n\n        if openai_message.refusal:\n            warnings.warn(openai_message.refusal, RuntimeWarning)\n\n        return Response(\n            message=Message.from_text(text, role=Role.AI),\n            raw=response,\n        )\n\n    def stream(\n        self,\n        messages: List[Message],\n        model: Optional[str] = None,\n        temperature: Optional[float] = None,\n        top_p: Optional[float] = None,\n        presence_penalty: Optional[float] = None,\n        frequency_penalty: Optional[float] = None,\n        max_tokens: Optional[int] = None,\n        stop: Optional[List[str]] = None,\n        extra_body: Optional[Dict[str, Any]] = None,\n        **kwargs,\n    ) -&gt; StreamResponse:\n        \"\"\"\n        Stream a chat completion response incrementally.\n\n        Parameters\n        ----------\n        messages : list[Message]\n            Conversation messages.\n        model : str, optional\n            Model ID to use. Required unless provided in `configuration.model`.\n        temperature, top_p, presence_penalty, frequency_penalty, max_tokens, stop, extra_body\n            See `chat` for details.\n        **kwargs\n            Additional provider-specific arguments.\n\n        Yields\n        ------\n        MessageChunk\n            Delta chunks as they arrive from the provider.\n\n        Notes\n        -----\n        - Validates `messages`, final `model`, and `stream=True`.\n        \"\"\"\n        params = self._build_parameters(\n            messages=messages,\n            model=model,\n            temperature=temperature,\n            top_p=top_p,\n            presence_penalty=presence_penalty,\n            frequency_penalty=frequency_penalty,\n            max_tokens=max_tokens,\n            stop=stop,\n            extra_body=extra_body,\n            stream=True,\n            **kwargs,\n        )\n        validate_required_params(params, [\"messages\", \"model\", \"stream\"])\n        response = self.client.chat.completions.create(**params)\n        for chunk in response:\n            delta_content = chunk.choices[0].delta.content\n            delta_content = delta_content if delta_content else \"\"\n            yield MessageChunk(delta=delta_content, raw=chunk)\n\n    async def achat(\n        self,\n        messages: List[Message],\n        model: Optional[str] = None,\n        temperature: Optional[float] = None,\n        top_p: Optional[float] = None,\n        presence_penalty: Optional[float] = None,\n        frequency_penalty: Optional[float] = None,\n        max_tokens: Optional[int] = None,\n        stop: Optional[List[str]] = None,\n        extra_body: Optional[Dict[str, Any]] = None,\n        **kwargs,\n    ) -&gt; Response:\n        \"\"\"\n        Asynchronously send a chat completion request to an OpenAI-compatible provider.\n\n        Parameters\n        ----------\n        messages : list[Message]\n            Conversation messages.\n        model : str, optional\n            Model ID to use. Required unless provided in `configuration.model`.\n        temperature, top_p, presence_penalty, frequency_penalty, max_tokens, stop, extra_body\n            See `chat` for details.\n        **kwargs\n            Additional provider-specific arguments.\n\n        Returns\n        -------\n        Response\n            Bridgic response containing the generated message and raw API response.\n        \"\"\"\n        params = self._build_parameters(\n            messages=messages,\n            model=model,\n            temperature=temperature,\n            top_p=top_p,\n            presence_penalty=presence_penalty,\n            frequency_penalty=frequency_penalty,\n            max_tokens=max_tokens,\n            stop=stop,\n            extra_body=extra_body,\n            **kwargs,\n        )\n        validate_required_params(params, [\"messages\", \"model\"])\n        response = await self.async_client.chat.completions.create(**params)\n        openai_message: ChatCompletionMessage = response.choices[0].message\n        text: str = openai_message.content if openai_message.content else \"\"\n\n        if openai_message.refusal:\n            warnings.warn(openai_message.refusal, RuntimeWarning)\n\n        return Response(\n            message=Message.from_text(text, role=Role.AI),\n            raw=response,\n        )\n\n    async def astream(\n        self,\n        messages: List[Message],\n        model: Optional[str] = None,\n        temperature: Optional[float] = None,\n        top_p: Optional[float] = None,\n        presence_penalty: Optional[float] = None,\n        frequency_penalty: Optional[float] = None,\n        max_tokens: Optional[int] = None,\n        stop: Optional[List[str]] = None,\n        extra_body: Optional[Dict[str, Any]] = None,\n        **kwargs,\n    ) -&gt; AsyncStreamResponse:\n        \"\"\"\n        Asynchronously stream a chat completion response incrementally.\n\n        Parameters\n        ----------\n        messages : list[Message]\n            Conversation messages.\n        model : str, optional\n            Model ID to use. Required unless provided in `configuration.model`.\n        temperature, top_p, presence_penalty, frequency_penalty, max_tokens, stop, extra_body\n            See `chat` for details.\n        **kwargs\n            Additional provider-specific arguments.\n\n        Yields\n        ------\n        MessageChunk\n            Delta chunks as they arrive from the provider.\n\n        Notes\n        -----\n        - Validates `messages`, final `model`, and `stream=True`.\n        \"\"\"\n        params = self._build_parameters(\n            messages=messages,\n            model=model,\n            temperature=temperature,\n            top_p=top_p,\n            presence_penalty=presence_penalty,\n            frequency_penalty=frequency_penalty,\n            max_tokens=max_tokens,\n            stop=stop,\n            extra_body=extra_body,\n            stream=True,\n            **kwargs,\n        )\n        validate_required_params(params, [\"messages\", \"model\", \"stream\"])\n        response = await self.async_client.chat.completions.create(**params)\n        async for chunk in response:\n            delta_content = chunk.choices[0].delta.content\n            delta_content = delta_content if delta_content else \"\"\n            yield MessageChunk(delta=delta_content, raw=chunk)\n\n    def _build_parameters(\n        self,\n        messages: List[Message],\n        model: Optional[str] = None,\n        temperature: Optional[float] = None,\n        top_p: Optional[float] = None,\n        presence_penalty: Optional[float] = None,\n        frequency_penalty: Optional[float] = None,\n        max_tokens: Optional[int] = None,\n        stop: Optional[List[str]] = None,\n        extra_body: Optional[Dict[str, Any]] = None,\n        stream: Optional[bool] = None,\n        **kwargs,\n    ) -&gt; Dict[str, Any]:\n        \"\"\"\n        Merge configuration defaults with per-call parameters and remove None values.\n\n        Parameters\n        ----------\n        messages : list[Message]\n            Conversation messages to send.\n        model : str, optional\n            Model identifier. May be omitted if `configuration.model` is set.\n        temperature, top_p, presence_penalty, frequency_penalty, max_tokens, stop, extra_body, stream\n            Standard OpenAI chat parameters.\n        **kwargs\n            Additional provider-specific parameters.\n\n        Returns\n        -------\n        dict\n            Final parameter dictionary for the OpenAI-compatible API.\n        \"\"\"\n        msgs: List[ChatCompletionMessageParam] = [self._convert_message(msg) for msg in messages]\n        merge_params = merge_dict(self.configuration.model_dump(), {\n            \"messages\": msgs,\n            \"model\": model,\n            \"temperature\": temperature,\n            \"top_p\": top_p,\n            \"presence_penalty\": presence_penalty,\n            \"frequency_penalty\": frequency_penalty,\n            \"max_tokens\": max_tokens,\n            \"stop\": stop,\n            \"extra_body\": extra_body,\n            \"stream\": stream,\n            **kwargs,\n        })\n        return filter_dict(merge_params, exclude_none=True)\n\n    def _convert_message(self, message: Message, strict: bool = False) -&gt; ChatCompletionMessageParam:\n        if strict:\n            return self._convert_message_strict(message)\n        else:\n            return self._convert_message_normal(message)\n\n    def _convert_message_normal(self, message: Message) -&gt; ChatCompletionMessageParam:\n        content_list = []\n        for block in message.blocks:\n            if isinstance(block, TextBlock):\n                content_list.append(block.text)\n            if isinstance(block, ToolCallBlock):\n                content_list.append(\n                    f\"Tool call:\\n\"\n                    f\"- id: {block.id}\\n\"\n                    f\"- name: {block.name}\\n\"\n                    f\"- arguments: {block.arguments}\"\n                )\n            if isinstance(block, ToolResultBlock):\n                content_list.append(f\"Tool result: {block.content}\")\n        content_txt = \"\\n\\n\".join(content_list)\n\n        if message.role == Role.SYSTEM:\n            return ChatCompletionSystemMessageParam(content=content_txt, role=\"system\")\n        elif message.role == Role.USER:\n            return ChatCompletionUserMessageParam(content=content_txt, role=\"user\")\n        elif message.role == Role.AI:\n            return ChatCompletionAssistantMessageParam(content=content_txt, role=\"assistant\")\n        elif message.role == Role.TOOL:\n            return ChatCompletionToolMessageParam(content=content_txt, role=\"tool\")\n        else:\n            raise ValueError(f\"Invalid role: {message.role}\")\n\n    def _convert_message_strict(self, message: Message) -&gt; ChatCompletionMessageParam:\n        content_list = []\n        tool_call_list = []\n        tool_result = \"\"\n        tool_result_call_id = None\n\n        for block in message.blocks:\n            if isinstance(block, TextBlock):\n                content_list.append(block.text)\n            if isinstance(block, ToolCallBlock):\n                tool_call: ChatCompletionMessageFunctionToolCallParam = {\n                    \"type\": \"function\",\n                    \"id\": block.id,\n                    \"function\": {\n                        \"name\": block.name,\n                        \"arguments\": json.dumps(block.arguments),\n                    },\n                }\n                tool_call_list.append(tool_call)\n            if isinstance(block, ToolResultBlock):\n                tool_result = block.content\n                tool_result_call_id = block.id\n\n        content_txt = \"\\n\\n\".join(content_list)\n\n        if message.role == Role.SYSTEM:\n            return ChatCompletionSystemMessageParam(content=content_txt, role=\"system\")\n        elif message.role == Role.USER:\n            return ChatCompletionUserMessageParam(content=content_txt, role=\"user\")\n        elif message.role == Role.AI:\n            return ChatCompletionAssistantMessageParam(content=content_txt, tool_calls=tool_call_list, role=\"assistant\")\n        elif message.role == Role.TOOL:\n            content_txt = \"\\n\\n\".join([content_txt, tool_result])\n            return ChatCompletionToolMessageParam(content=content_txt, tool_call_id=tool_result_call_id, role=\"tool\")\n        else:\n            raise ValueError(f\"Invalid role: {message.role}\")\n\n    @override\n    def dump_to_dict(self) -&gt; Dict[str, Any]:\n        state_dict = {\n            \"api_base\": self.api_base,\n            \"api_key\": self.api_key,\n            \"timeout\": self.timeout,\n            \"configuration\": self.configuration.model_dump(),\n        }\n        if self.http_client:\n            warnings.warn(\n                \"httpx.Client is not serializable, so it will be set to None in the deserialization.\",\n                RuntimeWarning,\n            )\n        if self.http_async_client:\n            warnings.warn(\n                \"httpx.AsyncClient is not serializable, so it will be set to None in the deserialization.\",\n                RuntimeWarning,\n            )\n        return state_dict\n\n    @override\n    def load_from_dict(self, state_dict: Dict[str, Any]) -&gt; None:\n        self.api_base = state_dict[\"api_base\"]\n        self.api_key = state_dict[\"api_key\"]\n        self.timeout = state_dict[\"timeout\"]\n        self.configuration = OpenAILikeConfiguration(**state_dict.get(\"configuration\", {}))\n\n        self.http_client = None\n        self.http_async_client = None\n\n        self.client = OpenAI(\n            base_url=self.api_base,\n            api_key=self.api_key,\n            timeout=self.timeout,\n            http_client=self.http_client,\n        )\n        self.async_client = AsyncOpenAI(\n            base_url=self.api_base,\n            api_key=self.api_key,\n            timeout=self.timeout,\n            http_client=self.http_async_client,\n        )\n</code></pre>"},{"location":"reference/bridgic-llms-openai-like/bridgic/llms/openai_like/#bridgic.llms.openai_like.OpenAILikeLlm.chat","title":"chat","text":"<pre><code>chat(\n    messages: List[Message],\n    model: Optional[str] = None,\n    temperature: Optional[float] = None,\n    top_p: Optional[float] = None,\n    presence_penalty: Optional[float] = None,\n    frequency_penalty: Optional[float] = None,\n    max_tokens: Optional[int] = None,\n    stop: Optional[List[str]] = None,\n    extra_body: Optional[Dict[str, Any]] = None,\n    **kwargs\n) -&gt; Response\n</code></pre> <p>Send a synchronous chat completion request to an OpenAI-compatible provider.</p> <p>Parameters:</p> Name Type Description Default <code>messages</code> <code>list[Message]</code> <p>Conversation messages.</p> required <code>model</code> <code>str</code> <p>Model ID to use. Required unless provided in <code>configuration.model</code>.</p> <code>None</code> <code>temperature</code> <code>float</code> <p>Sampling temperature in [0, 2]. Higher is more random, lower is more deterministic.</p> <code>None</code> <code>top_p</code> <code>float</code> <p>Nucleus sampling probability mass in (0, 1]. Alternative to temperature.</p> <code>None</code> <code>presence_penalty</code> <code>float</code> <p>Penalize new tokens based on whether they appear so far. [-2.0, 2.0].</p> <code>None</code> <code>frequency_penalty</code> <code>float</code> <p>Penalize new tokens based on their frequency so far. [-2.0, 2.0].</p> <code>None</code> <code>max_tokens</code> <code>int</code> <p>Maximum tokens to generate for completion.</p> <code>None</code> <code>stop</code> <code>list[str]</code> <p>Up to 4 sequences where generation will stop.</p> <code>None</code> <code>extra_body</code> <code>dict</code> <p>Extra JSON payload sent to the provider.</p> <code>None</code> <code>**kwargs</code> <p>Additional provider-specific arguments.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Response</code> <p>Bridgic response containing the generated message and raw API response.</p> Notes <ul> <li>Required parameter validation ensures <code>messages</code> and final <code>model</code> are present   (from either the call or <code>configuration</code>).</li> </ul> Source code in <code>bridgic/llms/openai_like/openai_like_llm.py</code> <pre><code>def chat(\n    self,\n    messages: List[Message],\n    model: Optional[str] = None,\n    temperature: Optional[float] = None,\n    top_p: Optional[float] = None,\n    presence_penalty: Optional[float] = None,\n    frequency_penalty: Optional[float] = None,\n    max_tokens: Optional[int] = None,\n    stop: Optional[List[str]] = None,\n    extra_body: Optional[Dict[str, Any]] = None,\n    **kwargs,\n) -&gt; Response:\n    \"\"\"\n    Send a synchronous chat completion request to an OpenAI-compatible provider.\n\n    Parameters\n    ----------\n    messages : list[Message]\n        Conversation messages.\n    model : str, optional\n        Model ID to use. Required unless provided in `configuration.model`.\n    temperature : float, optional\n        Sampling temperature in [0, 2]. Higher is more random, lower is more deterministic.\n    top_p : float, optional\n        Nucleus sampling probability mass in (0, 1]. Alternative to temperature.\n    presence_penalty : float, optional\n        Penalize new tokens based on whether they appear so far. [-2.0, 2.0].\n    frequency_penalty : float, optional\n        Penalize new tokens based on their frequency so far. [-2.0, 2.0].\n    max_tokens : int, optional\n        Maximum tokens to generate for completion.\n    stop : list[str], optional\n        Up to 4 sequences where generation will stop.\n    extra_body : dict, optional\n        Extra JSON payload sent to the provider.\n    **kwargs\n        Additional provider-specific arguments.\n\n    Returns\n    -------\n    Response\n        Bridgic response containing the generated message and raw API response.\n\n    Notes\n    -----\n    - Required parameter validation ensures `messages` and final `model` are present\n      (from either the call or `configuration`).\n    \"\"\"\n    params = self._build_parameters(\n        messages=messages,\n        model=model,\n        temperature=temperature,\n        top_p=top_p,\n        presence_penalty=presence_penalty,\n        frequency_penalty=frequency_penalty,\n        max_tokens=max_tokens,\n        stop=stop,\n        extra_body=extra_body,\n        **kwargs,\n    )\n    validate_required_params(params, [\"messages\", \"model\"])\n    response = self.client.chat.completions.create(**params)\n    openai_message: ChatCompletionMessage = response.choices[0].message\n    text: str = openai_message.content if openai_message.content else \"\"\n\n    if openai_message.refusal:\n        warnings.warn(openai_message.refusal, RuntimeWarning)\n\n    return Response(\n        message=Message.from_text(text, role=Role.AI),\n        raw=response,\n    )\n</code></pre>"},{"location":"reference/bridgic-llms-openai-like/bridgic/llms/openai_like/#bridgic.llms.openai_like.OpenAILikeLlm.stream","title":"stream","text":"<pre><code>stream(\n    messages: List[Message],\n    model: Optional[str] = None,\n    temperature: Optional[float] = None,\n    top_p: Optional[float] = None,\n    presence_penalty: Optional[float] = None,\n    frequency_penalty: Optional[float] = None,\n    max_tokens: Optional[int] = None,\n    stop: Optional[List[str]] = None,\n    extra_body: Optional[Dict[str, Any]] = None,\n    **kwargs\n) -&gt; StreamResponse\n</code></pre> <p>Stream a chat completion response incrementally.</p> <p>Parameters:</p> Name Type Description Default <code>messages</code> <code>list[Message]</code> <p>Conversation messages.</p> required <code>model</code> <code>str</code> <p>Model ID to use. Required unless provided in <code>configuration.model</code>.</p> <code>None</code> <code>temperature</code> <code>Optional[float]</code> <p>See <code>chat</code> for details.</p> <code>None</code> <code>top_p</code> <code>Optional[float]</code> <p>See <code>chat</code> for details.</p> <code>None</code> <code>presence_penalty</code> <code>Optional[float]</code> <p>See <code>chat</code> for details.</p> <code>None</code> <code>frequency_penalty</code> <code>Optional[float]</code> <p>See <code>chat</code> for details.</p> <code>None</code> <code>max_tokens</code> <code>Optional[float]</code> <p>See <code>chat</code> for details.</p> <code>None</code> <code>stop</code> <code>Optional[float]</code> <p>See <code>chat</code> for details.</p> <code>None</code> <code>extra_body</code> <code>Optional[float]</code> <p>See <code>chat</code> for details.</p> <code>None</code> <code>**kwargs</code> <p>Additional provider-specific arguments.</p> <code>{}</code> <p>Yields:</p> Type Description <code>MessageChunk</code> <p>Delta chunks as they arrive from the provider.</p> Notes <ul> <li>Validates <code>messages</code>, final <code>model</code>, and <code>stream=True</code>.</li> </ul> Source code in <code>bridgic/llms/openai_like/openai_like_llm.py</code> <pre><code>def stream(\n    self,\n    messages: List[Message],\n    model: Optional[str] = None,\n    temperature: Optional[float] = None,\n    top_p: Optional[float] = None,\n    presence_penalty: Optional[float] = None,\n    frequency_penalty: Optional[float] = None,\n    max_tokens: Optional[int] = None,\n    stop: Optional[List[str]] = None,\n    extra_body: Optional[Dict[str, Any]] = None,\n    **kwargs,\n) -&gt; StreamResponse:\n    \"\"\"\n    Stream a chat completion response incrementally.\n\n    Parameters\n    ----------\n    messages : list[Message]\n        Conversation messages.\n    model : str, optional\n        Model ID to use. Required unless provided in `configuration.model`.\n    temperature, top_p, presence_penalty, frequency_penalty, max_tokens, stop, extra_body\n        See `chat` for details.\n    **kwargs\n        Additional provider-specific arguments.\n\n    Yields\n    ------\n    MessageChunk\n        Delta chunks as they arrive from the provider.\n\n    Notes\n    -----\n    - Validates `messages`, final `model`, and `stream=True`.\n    \"\"\"\n    params = self._build_parameters(\n        messages=messages,\n        model=model,\n        temperature=temperature,\n        top_p=top_p,\n        presence_penalty=presence_penalty,\n        frequency_penalty=frequency_penalty,\n        max_tokens=max_tokens,\n        stop=stop,\n        extra_body=extra_body,\n        stream=True,\n        **kwargs,\n    )\n    validate_required_params(params, [\"messages\", \"model\", \"stream\"])\n    response = self.client.chat.completions.create(**params)\n    for chunk in response:\n        delta_content = chunk.choices[0].delta.content\n        delta_content = delta_content if delta_content else \"\"\n        yield MessageChunk(delta=delta_content, raw=chunk)\n</code></pre>"},{"location":"reference/bridgic-llms-openai-like/bridgic/llms/openai_like/#bridgic.llms.openai_like.OpenAILikeLlm.achat","title":"achat","text":"<code>async</code> <pre><code>achat(\n    messages: List[Message],\n    model: Optional[str] = None,\n    temperature: Optional[float] = None,\n    top_p: Optional[float] = None,\n    presence_penalty: Optional[float] = None,\n    frequency_penalty: Optional[float] = None,\n    max_tokens: Optional[int] = None,\n    stop: Optional[List[str]] = None,\n    extra_body: Optional[Dict[str, Any]] = None,\n    **kwargs\n) -&gt; Response\n</code></pre> <p>Asynchronously send a chat completion request to an OpenAI-compatible provider.</p> <p>Parameters:</p> Name Type Description Default <code>messages</code> <code>list[Message]</code> <p>Conversation messages.</p> required <code>model</code> <code>str</code> <p>Model ID to use. Required unless provided in <code>configuration.model</code>.</p> <code>None</code> <code>temperature</code> <code>Optional[float]</code> <p>See <code>chat</code> for details.</p> <code>None</code> <code>top_p</code> <code>Optional[float]</code> <p>See <code>chat</code> for details.</p> <code>None</code> <code>presence_penalty</code> <code>Optional[float]</code> <p>See <code>chat</code> for details.</p> <code>None</code> <code>frequency_penalty</code> <code>Optional[float]</code> <p>See <code>chat</code> for details.</p> <code>None</code> <code>max_tokens</code> <code>Optional[float]</code> <p>See <code>chat</code> for details.</p> <code>None</code> <code>stop</code> <code>Optional[float]</code> <p>See <code>chat</code> for details.</p> <code>None</code> <code>extra_body</code> <code>Optional[float]</code> <p>See <code>chat</code> for details.</p> <code>None</code> <code>**kwargs</code> <p>Additional provider-specific arguments.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Response</code> <p>Bridgic response containing the generated message and raw API response.</p> Source code in <code>bridgic/llms/openai_like/openai_like_llm.py</code> <pre><code>async def achat(\n    self,\n    messages: List[Message],\n    model: Optional[str] = None,\n    temperature: Optional[float] = None,\n    top_p: Optional[float] = None,\n    presence_penalty: Optional[float] = None,\n    frequency_penalty: Optional[float] = None,\n    max_tokens: Optional[int] = None,\n    stop: Optional[List[str]] = None,\n    extra_body: Optional[Dict[str, Any]] = None,\n    **kwargs,\n) -&gt; Response:\n    \"\"\"\n    Asynchronously send a chat completion request to an OpenAI-compatible provider.\n\n    Parameters\n    ----------\n    messages : list[Message]\n        Conversation messages.\n    model : str, optional\n        Model ID to use. Required unless provided in `configuration.model`.\n    temperature, top_p, presence_penalty, frequency_penalty, max_tokens, stop, extra_body\n        See `chat` for details.\n    **kwargs\n        Additional provider-specific arguments.\n\n    Returns\n    -------\n    Response\n        Bridgic response containing the generated message and raw API response.\n    \"\"\"\n    params = self._build_parameters(\n        messages=messages,\n        model=model,\n        temperature=temperature,\n        top_p=top_p,\n        presence_penalty=presence_penalty,\n        frequency_penalty=frequency_penalty,\n        max_tokens=max_tokens,\n        stop=stop,\n        extra_body=extra_body,\n        **kwargs,\n    )\n    validate_required_params(params, [\"messages\", \"model\"])\n    response = await self.async_client.chat.completions.create(**params)\n    openai_message: ChatCompletionMessage = response.choices[0].message\n    text: str = openai_message.content if openai_message.content else \"\"\n\n    if openai_message.refusal:\n        warnings.warn(openai_message.refusal, RuntimeWarning)\n\n    return Response(\n        message=Message.from_text(text, role=Role.AI),\n        raw=response,\n    )\n</code></pre>"},{"location":"reference/bridgic-llms-openai-like/bridgic/llms/openai_like/#bridgic.llms.openai_like.OpenAILikeLlm.astream","title":"astream","text":"<code>async</code> <pre><code>astream(\n    messages: List[Message],\n    model: Optional[str] = None,\n    temperature: Optional[float] = None,\n    top_p: Optional[float] = None,\n    presence_penalty: Optional[float] = None,\n    frequency_penalty: Optional[float] = None,\n    max_tokens: Optional[int] = None,\n    stop: Optional[List[str]] = None,\n    extra_body: Optional[Dict[str, Any]] = None,\n    **kwargs\n) -&gt; AsyncStreamResponse\n</code></pre> <p>Asynchronously stream a chat completion response incrementally.</p> <p>Parameters:</p> Name Type Description Default <code>messages</code> <code>list[Message]</code> <p>Conversation messages.</p> required <code>model</code> <code>str</code> <p>Model ID to use. Required unless provided in <code>configuration.model</code>.</p> <code>None</code> <code>temperature</code> <code>Optional[float]</code> <p>See <code>chat</code> for details.</p> <code>None</code> <code>top_p</code> <code>Optional[float]</code> <p>See <code>chat</code> for details.</p> <code>None</code> <code>presence_penalty</code> <code>Optional[float]</code> <p>See <code>chat</code> for details.</p> <code>None</code> <code>frequency_penalty</code> <code>Optional[float]</code> <p>See <code>chat</code> for details.</p> <code>None</code> <code>max_tokens</code> <code>Optional[float]</code> <p>See <code>chat</code> for details.</p> <code>None</code> <code>stop</code> <code>Optional[float]</code> <p>See <code>chat</code> for details.</p> <code>None</code> <code>extra_body</code> <code>Optional[float]</code> <p>See <code>chat</code> for details.</p> <code>None</code> <code>**kwargs</code> <p>Additional provider-specific arguments.</p> <code>{}</code> <p>Yields:</p> Type Description <code>MessageChunk</code> <p>Delta chunks as they arrive from the provider.</p> Notes <ul> <li>Validates <code>messages</code>, final <code>model</code>, and <code>stream=True</code>.</li> </ul> Source code in <code>bridgic/llms/openai_like/openai_like_llm.py</code> <pre><code>async def astream(\n    self,\n    messages: List[Message],\n    model: Optional[str] = None,\n    temperature: Optional[float] = None,\n    top_p: Optional[float] = None,\n    presence_penalty: Optional[float] = None,\n    frequency_penalty: Optional[float] = None,\n    max_tokens: Optional[int] = None,\n    stop: Optional[List[str]] = None,\n    extra_body: Optional[Dict[str, Any]] = None,\n    **kwargs,\n) -&gt; AsyncStreamResponse:\n    \"\"\"\n    Asynchronously stream a chat completion response incrementally.\n\n    Parameters\n    ----------\n    messages : list[Message]\n        Conversation messages.\n    model : str, optional\n        Model ID to use. Required unless provided in `configuration.model`.\n    temperature, top_p, presence_penalty, frequency_penalty, max_tokens, stop, extra_body\n        See `chat` for details.\n    **kwargs\n        Additional provider-specific arguments.\n\n    Yields\n    ------\n    MessageChunk\n        Delta chunks as they arrive from the provider.\n\n    Notes\n    -----\n    - Validates `messages`, final `model`, and `stream=True`.\n    \"\"\"\n    params = self._build_parameters(\n        messages=messages,\n        model=model,\n        temperature=temperature,\n        top_p=top_p,\n        presence_penalty=presence_penalty,\n        frequency_penalty=frequency_penalty,\n        max_tokens=max_tokens,\n        stop=stop,\n        extra_body=extra_body,\n        stream=True,\n        **kwargs,\n    )\n    validate_required_params(params, [\"messages\", \"model\", \"stream\"])\n    response = await self.async_client.chat.completions.create(**params)\n    async for chunk in response:\n        delta_content = chunk.choices[0].delta.content\n        delta_content = delta_content if delta_content else \"\"\n        yield MessageChunk(delta=delta_content, raw=chunk)\n</code></pre>"},{"location":"reference/bridgic-llms-vllm/bridgic/llms/vllm/","title":"vllm","text":"<p>The vLLM integration module provides support for the vLLM inference engine.</p> <p>This module implements communication interfaces with vLLM inference services, supporting  highly reliable calls to large language models deployed via vLLM, and provides several  encapsulations for common seen high-level functionality.</p> <p>You can install the vLLM integration package for Bridgic by running:</p> <pre><code>pip install bridgic-llms-vllm\n</code></pre>"},{"location":"reference/bridgic-llms-vllm/bridgic/llms/vllm/#bridgic.llms.vllm.VllmServerLlm","title":"VllmServerLlm","text":"<p>               Bases: <code>OpenAILikeLlm</code>, <code>StructuredOutput</code>, <code>ToolSelection</code></p> <p>VllmServerLlm is a wrapper around the vLLM server, providing common calling interfaces for  self-hosted LLM service, such as chat, stream, as well as with encapsulation of common  seen high-level functionality.</p> <p>Parameters:</p> Name Type Description Default <code>api_base</code> <code>str</code> <p>The base URL of the LLM provider.</p> required <code>api_key</code> <code>str</code> <p>The API key of the LLM provider.</p> required <code>timeout</code> <code>Optional[float]</code> <p>The timeout in seconds.</p> <code>None</code> Source code in <code>bridgic/llms/vllm/vllm_server_llm.py</code> <pre><code>class VllmServerLlm(OpenAILikeLlm, StructuredOutput, ToolSelection):\n    \"\"\"\n    VllmServerLlm is a wrapper around the vLLM server, providing common calling interfaces for \n    self-hosted LLM service, such as chat, stream, as well as with encapsulation of common \n    seen high-level functionality.\n\n    Parameters\n    ----------\n    api_base: str\n        The base URL of the LLM provider.\n    api_key: str\n        The API key of the LLM provider.\n    timeout: Optional[float]\n        The timeout in seconds.\n    \"\"\"\n\n    def __init__(\n        self,\n        api_base: str,\n        api_key: str,\n        configuration: Optional[VllmServerConfiguration] = VllmServerConfiguration(),\n        timeout: Optional[float] = None,\n        http_client: Optional[httpx.Client] = None,\n        http_async_client: Optional[httpx.AsyncClient] = None,\n    ):\n        super().__init__(\n            api_base=api_base,\n            api_key=api_key,\n            configuration=configuration,\n            timeout=timeout,\n            http_client=http_client,\n            http_async_client=http_async_client,\n        )\n\n    @override\n    def dump_to_dict(self) -&gt; Dict[str, Any]:\n        return super().dump_to_dict()\n\n    @override\n    def load_from_dict(self, state_dict: Dict[str, Any]) -&gt; None:\n        super().load_from_dict(state_dict)\n\n    @overload\n    def structured_output(\n        self,\n        messages: List[Message],\n        constraint: PydanticModel,\n        model: Optional[str] = None,\n        temperature: Optional[float] = ...,\n        top_p: Optional[float] = ...,\n        presence_penalty: Optional[float] = ...,\n        frequency_penalty: Optional[float] = ...,\n        extra_body: Optional[Dict[str, Any]] = ...,\n        **kwargs,\n    ) -&gt; BaseModel: ...\n\n    @overload\n    def structured_output(\n        self,\n        messages: List[Message],\n        constraint: JsonSchema,\n        model: Optional[str] = None,\n        temperature: Optional[float] = ...,\n        top_p: Optional[float] = ...,\n        presence_penalty: Optional[float] = ...,\n        frequency_penalty: Optional[float] = ...,\n        extra_body: Optional[Dict[str, Any]] = ...,\n        **kwargs,\n    ) -&gt; Dict[str, Any]: ...\n\n    @overload\n    def structured_output(\n        self,\n        messages: List[Message],\n        constraint: Choice,\n        model: Optional[str] = None,\n        temperature: Optional[float] = ...,\n        top_p: Optional[float] = ...,\n        presence_penalty: Optional[float] = ...,\n        frequency_penalty: Optional[float] = ...,\n        extra_body: Optional[Dict[str, Any]] = ...,\n        **kwargs,\n    ) -&gt; str: ...\n\n    def structured_output(\n        self,\n        messages: List[Message],\n        constraint: Constraint,\n        model: Optional[str] = None,\n        temperature: Optional[float] = None,\n        top_p: Optional[float] = None,\n        presence_penalty: Optional[float] = None,\n        frequency_penalty: Optional[float] = None,\n        extra_body: Optional[Dict[str, Any]] = None,\n        **kwargs,\n    ) -&gt; Union[BaseModel, Dict[str, Any], str]:\n        '''\n        Structured output in a specified format. This part of the functionality is provided based on the \n        capabilities of [vLLM Structured Output](https://docs.vllm.ai/en/latest/features/structured_outputs.html).\n\n        Parameters\n        ----------\n        messages: List[Message]\n            The messages to send to the LLM.\n        constraint: Constraint\n            The constraint to use for the structured output.\n        model: Optional[str]\n            The model to use for the structured output.\n        temperature: Optional[float]\n            The temperature to use for the structured output.\n        top_p: Optional[float]\n            The top_p to use for the structured output.\n        presence_penalty: Optional[float]\n            The presence_penalty to use for the structured output.\n        frequency_penalty: Optional[float]\n            The frequency_penalty to use for the structured output.\n        extra_body: Optional[Dict[str, Any]]\n            The extra_body to use for the structured output.\n        **kwargs: Any\n            The kwargs to use for the structured output.\n\n        Returns\n        -------\n        Union[BaseModel, Dict[str, Any], str]\n            The return type is based on the constraint type:\n            * If the constraint is PydanticModel, return an instance of the corresponding Pydantic model.\n            * If the constraint is JsonSchema, return a Dict[str, Any] that is the parsed JSON.\n            * Otherwise, return a str.\n        '''\n        response = self.chat(\n            messages=messages,\n            model=model,\n            temperature=temperature,\n            top_p=top_p,\n            presence_penalty=presence_penalty,\n            frequency_penalty=frequency_penalty,\n            extra_body=self._convert_constraint(constraint, extra_body),\n            **kwargs,\n        )\n        return self._convert_response(constraint, response)\n\n    async def astructured_output(\n        self,\n        messages: List[Message],\n        constraint: Constraint,\n        model: Optional[str] = None,\n        temperature: Optional[float] = None,\n        top_p: Optional[float] = None,\n        presence_penalty: Optional[float] = None,\n        frequency_penalty: Optional[float] = None,\n        extra_body: Optional[Dict[str, Any]] = None,\n        **kwargs,\n    ) -&gt; Union[BaseModel, Dict[str, Any], str]:\n        '''\n        Structured output in a specified format. This part of the functionality is provided based on the \n        capabilities of [vLLM Structured Output](https://docs.vllm.ai/en/latest/features/structured_outputs.html).\n\n        Parameters\n        ----------\n        messages: List[Message]\n            The messages to send to the LLM.\n        constraint: Constraint\n            The constraint to use for the structured output.\n        model: Optional[str]\n            The model to use for the structured output.\n        temperature: Optional[float]\n            The temperature to use for the structured output.\n        top_p: Optional[float]\n            The top_p to use for the structured output.\n        presence_penalty: Optional[float]\n            The presence_penalty to use for the structured output.\n        frequency_penalty: Optional[float]\n            The frequency_penalty to use for the structured output.\n        extra_body: Optional[Dict[str, Any]]\n            The extra_body to use for the structured output.\n        **kwargs: Any\n            The kwargs to use for the structured output.\n\n        Returns\n        -------\n        Union[BaseModel, Dict[str, Any], str]\n            The return type is based on the constraint type:\n            * If the constraint is PydanticModel, return an instance of the corresponding Pydantic model.\n            * If the constraint is JsonSchema, return a Dict[str, Any] that is the parsed JSON.\n            * Otherwise, return a str.\n        '''\n        response = await self.achat(\n            messages=messages,\n            model=model,\n            temperature=temperature,\n            top_p=top_p,\n            presence_penalty=presence_penalty,\n            frequency_penalty=frequency_penalty,\n            extra_body=self._convert_constraint(constraint, extra_body),\n            **kwargs,\n        )\n        return self._convert_response(constraint, response)\n\n    def _convert_constraint(\n        self,\n        constraint: Constraint,\n        extra_body: Optional[Dict[str, Any]] = None,\n    ) -&gt; Dict[str, Any]:\n        extra_body = {} if extra_body is None else extra_body\n\n        if isinstance(constraint, PydanticModel):\n            extra_body[\"guided_json\"] = constraint.model.model_json_schema()\n        elif isinstance(constraint, JsonSchema):\n            extra_body[\"guided_json\"] = constraint.schema_dict\n        elif isinstance(constraint, Regex):\n            extra_body[\"guided_regex\"] = constraint.pattern\n        elif isinstance(constraint, Choice):\n            extra_body[\"guided_choice\"] = constraint.choices\n        elif isinstance(constraint, EbnfGrammar):\n            extra_body[\"guided_grammar\"] = constraint.syntax\n        else:\n            raise ValueError(f\"Invalid constraint: {constraint}\")\n\n        return extra_body\n\n    def _convert_response(\n        self,\n        constraint: Constraint,\n        response: Response,\n    ) -&gt; Union[BaseModel, Dict[str, Any], str]:\n        content = response.message.content\n\n        if isinstance(constraint, PydanticModel):\n            return constraint.model.model_validate_json(content)\n        elif isinstance(constraint, JsonSchema):\n            return json.loads(content)\n        return content\n\n    def select_tool(\n        self,\n        messages: List[Message],\n        tools: List[Tool],\n        model: Optional[str] = None,\n        temperature: Optional[float] = None,\n        top_p: Optional[float] = None,\n        presence_penalty: Optional[float] = None,\n        frequency_penalty: Optional[float] = None,\n        extra_body: Optional[Dict[str, Any]] = None,\n        tool_choice: Union[Literal[\"auto\", \"required\", \"none\"], ChatCompletionNamedToolChoiceParam] = \"auto\",\n        **kwargs,\n    ) -&gt; Tuple[List[ToolCall], Optional[Dict]]:\n        \"\"\"\n        Select tools from a specified list of tools.\n\n        Parameters\n        ----------\n        messages: List[Message]\n            The messages to send to the LLM.\n        tools: List[Tool]\n            The tools to use for the tool select.\n        model: Optional[str]\n            The model to use for the tool select.\n        temperature: Optional[float]\n            The temperature to use for the tool select.\n        top_p: Optional[float]\n            The top_p to use for the tool select.\n        presence_penalty: Optional[float]\n            The presence_penalty to use for the tool select.\n        frequency_penalty: Optional[float]\n            The frequency_penalty to use for the tool select.\n        extra_body: Optional[Dict[str, Any]]\n            The extra_body to use for the tool select.\n        tool_choice : Union[Literal[\"auto\", \"required\", \"none\"], ChatCompletionNamedToolChoiceParam]\n            Tool choice mode for tool calling. There are 4 choices that are supported:\n            - `auto` means the model can pick between generating a message or calling one or more tools.\n            To enable this feature, you should set the tags `--enable-auto-tool-choice` and `--tool-call-parser` \n            when starting the vLLM server.\n            - `required` means the model must generate one or more tool calls based on the specified tool list \n            in the `tools` parameter. The number of tool calls depends on the user's query.\n            - `none` means the model will not call any tool and instead generates a message. When tools are \n            specified in the request, vLLM includes tool definitions in the prompt by default, regardless \n            of the tool_choice setting. To exclude tool definitions when tool_choice='none', use the \n            `--exclude-tools-when-tool-choice-none` option when starting the vLLM server.\n            - You can also specify a particular function using named function calling by setting `tool_choice` \n            parameter to a json object, like `tool_choice={\"type\": \"function\", \"function\": {\"name\": \"get_weather\"}}`.\n\n        **kwargs: Any\n            The kwargs to use for the tool select.\n\n        Returns\n        -------\n        Tuple[List[ToolCall], Optional[str]]\n            A list that contains the selected tools and their arguments.\n\n        Notes\n        -----\n        See more on [Tool Calling](https://docs.vllm.ai/en/stable/features/tool_calling.html).\n        \"\"\"\n        # Build parameters dictionary for validation\n        params = filter_dict(merge_dict(self.configuration.model_dump(), {\n            \"model\": model,\n            \"temperature\": temperature,\n            \"top_p\": top_p,\n            \"presence_penalty\": presence_penalty,\n            \"frequency_penalty\": frequency_penalty,\n            \"extra_body\": extra_body,\n            **kwargs,\n        }))\n\n        # Validate required parameters for tool selection\n        validate_required_params(params, [\"model\"])\n\n        input_messages = [self._convert_message(message=msg, strict=True) for msg in messages]\n        input_tools = [\n            {\n                \"type\": \"function\",\n                \"function\": {\n                    \"name\": tool.name,\n                    \"description\": tool.description,\n                    \"parameters\": tool.parameters,\n                },\n            } for tool in tools\n        ]\n\n        response = self.client.chat.completions.create(\n            model=model,\n            messages=input_messages,\n            tools=input_tools,\n            tool_choice=tool_choice,\n            **kwargs,\n        )\n        tool_calls = response.choices[0].message.tool_calls\n\n        output_content = \"\"\n        if response.choices[0].message.content:\n            output_content = response.choices[0].message.content\n\n        output_tool_calls = []\n        if tool_calls:\n            output_tool_calls = self._convert_tool_calls(tool_calls)\n\n        return (output_tool_calls, output_content)\n\n    async def aselect_tool(\n        self,\n        messages: List[Message],\n        tools: List[Tool],\n        model: Optional[str] = None,\n        temperature: Optional[float] = None,\n        top_p: Optional[float] = None,\n        presence_penalty: Optional[float] = None,\n        frequency_penalty: Optional[float] = None,\n        extra_body: Optional[Dict[str, Any]] = None,\n        tool_choice: Union[Literal[\"auto\", \"required\", \"none\"], ChatCompletionNamedToolChoiceParam] = \"auto\",\n        **kwargs,\n    ) -&gt; Tuple[List[ToolCall], Optional[str]]:\n        \"\"\"\n        Select tools from a specified list of tools.\n\n        Parameters\n        ----------\n        messages: List[Message]\n            The messages to send to the LLM.\n        tools: List[Tool]\n            The tools to use for the tool select.\n        model: Optional[str]\n            The model to use for the tool select.\n        temperature: Optional[float]\n            The temperature to use for the tool select.\n        top_p: Optional[float]\n            The top_p to use for the tool select.\n        presence_penalty: Optional[float]\n            The presence_penalty to use for the tool select.\n        frequency_penalty: Optional[float]\n            The frequency_penalty to use for the tool select.\n        extra_body: Optional[Dict[str, Any]]\n            The extra_body to use for the tool select.\n        tool_choice : Union[Literal[\"auto\", \"required\", \"none\"], ChatCompletionNamedToolChoiceParam]\n            Tool choice mode for tool calling. There are 4 choices that are supported:\n            - `auto` means the model can pick between generating a message or calling one or more tools.\n            To enable this feature, you should set the tags `--enable-auto-tool-choice` and `--tool-call-parser` \n            when starting the vLLM server.\n            - `required` means the model must generate one or more tool calls based on the specified tool list \n            in the `tools` parameter. The number of tool calls depends on the user's query.\n            - `none` means the model will not call any tool and instead generates a message. When tools are \n            specified in the request, vLLM includes tool definitions in the prompt by default, regardless \n            of the tool_choice setting. To exclude tool definitions when tool_choice='none', use the \n            `--exclude-tools-when-tool-choice-none` option when starting the vLLM server.\n            - You can also specify a particular function using named function calling by setting `tool_choice` \n            parameter to a json object, like `tool_choice={\"type\": \"function\", \"function\": {\"name\": \"get_weather\"}}`.\n\n        **kwargs: Any\n            The kwargs to use for the tool select.\n\n        Returns\n        -------\n        Tuple[List[ToolCall], Optional[str]]\n            A list that contains the selected tools and their arguments.\n\n        Notes\n        -----\n        See more on [Tool Calling](https://docs.vllm.ai/en/stable/features/tool_calling.html).\n        \"\"\"\n        # Build parameters dictionary for validation\n        params = filter_dict(merge_dict(self.configuration.model_dump(), {\n            \"model\": model,\n            \"temperature\": temperature,\n            \"top_p\": top_p,\n            \"presence_penalty\": presence_penalty,\n            \"frequency_penalty\": frequency_penalty,\n            \"extra_body\": extra_body,\n            **kwargs,\n        }))\n\n        # Validate required parameters for tool selection\n        validate_required_params(params, [\"model\"])\n\n        input_messages = [self._convert_message(message=msg, strict=True) for msg in messages]\n        input_tools = [\n            {\n                \"type\": \"function\",\n                \"function\": {\n                    \"name\": tool.name,\n                    \"description\": tool.description,\n                    \"parameters\": tool.parameters,\n                },\n            } for tool in tools\n        ]\n\n        response = self.client.chat.completions.create(\n            model=model,\n            messages=input_messages,\n            tools=input_tools,\n            tool_choice=tool_choice,\n            **kwargs,\n        )\n        tool_calls = response.choices[0].message.tool_calls\n\n        output_content = \"\"\n        if response.choices[0].message.content:\n            output_content = response.choices[0].message.content\n\n        output_tool_calls = []\n        if tool_calls:\n            output_tool_calls = self._convert_tool_calls(tool_calls)\n\n        return (output_tool_calls, output_content)\n\n    def _convert_tool_calls(self, tool_calls: List[ChatCompletionMessageFunctionToolCall]) -&gt; List[ToolCall]:\n        return [\n            ToolCall(\n                id=tool_call.id,\n                name=tool_call.function.name,\n                arguments=json.loads(tool_call.function.arguments),\n            ) for tool_call in tool_calls\n        ]\n</code></pre>"},{"location":"reference/bridgic-llms-vllm/bridgic/llms/vllm/#bridgic.llms.vllm.VllmServerLlm.structured_output","title":"structured_output","text":"<pre><code>structured_output(\n    messages: List[Message],\n    constraint: Constraint,\n    model: Optional[str] = None,\n    temperature: Optional[float] = None,\n    top_p: Optional[float] = None,\n    presence_penalty: Optional[float] = None,\n    frequency_penalty: Optional[float] = None,\n    extra_body: Optional[Dict[str, Any]] = None,\n    **kwargs\n) -&gt; Union[BaseModel, Dict[str, Any], str]\n</code></pre> <p>Structured output in a specified format. This part of the functionality is provided based on the  capabilities of vLLM Structured Output.</p> <p>Parameters:</p> Name Type Description Default <code>messages</code> <code>List[Message]</code> <p>The messages to send to the LLM.</p> required <code>constraint</code> <code>Constraint</code> <p>The constraint to use for the structured output.</p> required <code>model</code> <code>Optional[str]</code> <p>The model to use for the structured output.</p> <code>None</code> <code>temperature</code> <code>Optional[float]</code> <p>The temperature to use for the structured output.</p> <code>None</code> <code>top_p</code> <code>Optional[float]</code> <p>The top_p to use for the structured output.</p> <code>None</code> <code>presence_penalty</code> <code>Optional[float]</code> <p>The presence_penalty to use for the structured output.</p> <code>None</code> <code>frequency_penalty</code> <code>Optional[float]</code> <p>The frequency_penalty to use for the structured output.</p> <code>None</code> <code>extra_body</code> <code>Optional[Dict[str, Any]]</code> <p>The extra_body to use for the structured output.</p> <code>None</code> <code>**kwargs</code> <p>The kwargs to use for the structured output.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Union[BaseModel, Dict[str, Any], str]</code> <p>The return type is based on the constraint type: * If the constraint is PydanticModel, return an instance of the corresponding Pydantic model. * If the constraint is JsonSchema, return a Dict[str, Any] that is the parsed JSON. * Otherwise, return a str.</p> Source code in <code>bridgic/llms/vllm/vllm_server_llm.py</code> <pre><code>def structured_output(\n    self,\n    messages: List[Message],\n    constraint: Constraint,\n    model: Optional[str] = None,\n    temperature: Optional[float] = None,\n    top_p: Optional[float] = None,\n    presence_penalty: Optional[float] = None,\n    frequency_penalty: Optional[float] = None,\n    extra_body: Optional[Dict[str, Any]] = None,\n    **kwargs,\n) -&gt; Union[BaseModel, Dict[str, Any], str]:\n    '''\n    Structured output in a specified format. This part of the functionality is provided based on the \n    capabilities of [vLLM Structured Output](https://docs.vllm.ai/en/latest/features/structured_outputs.html).\n\n    Parameters\n    ----------\n    messages: List[Message]\n        The messages to send to the LLM.\n    constraint: Constraint\n        The constraint to use for the structured output.\n    model: Optional[str]\n        The model to use for the structured output.\n    temperature: Optional[float]\n        The temperature to use for the structured output.\n    top_p: Optional[float]\n        The top_p to use for the structured output.\n    presence_penalty: Optional[float]\n        The presence_penalty to use for the structured output.\n    frequency_penalty: Optional[float]\n        The frequency_penalty to use for the structured output.\n    extra_body: Optional[Dict[str, Any]]\n        The extra_body to use for the structured output.\n    **kwargs: Any\n        The kwargs to use for the structured output.\n\n    Returns\n    -------\n    Union[BaseModel, Dict[str, Any], str]\n        The return type is based on the constraint type:\n        * If the constraint is PydanticModel, return an instance of the corresponding Pydantic model.\n        * If the constraint is JsonSchema, return a Dict[str, Any] that is the parsed JSON.\n        * Otherwise, return a str.\n    '''\n    response = self.chat(\n        messages=messages,\n        model=model,\n        temperature=temperature,\n        top_p=top_p,\n        presence_penalty=presence_penalty,\n        frequency_penalty=frequency_penalty,\n        extra_body=self._convert_constraint(constraint, extra_body),\n        **kwargs,\n    )\n    return self._convert_response(constraint, response)\n</code></pre>"},{"location":"reference/bridgic-llms-vllm/bridgic/llms/vllm/#bridgic.llms.vllm.VllmServerLlm.astructured_output","title":"astructured_output","text":"<code>async</code> <pre><code>astructured_output(\n    messages: List[Message],\n    constraint: Constraint,\n    model: Optional[str] = None,\n    temperature: Optional[float] = None,\n    top_p: Optional[float] = None,\n    presence_penalty: Optional[float] = None,\n    frequency_penalty: Optional[float] = None,\n    extra_body: Optional[Dict[str, Any]] = None,\n    **kwargs\n) -&gt; Union[BaseModel, Dict[str, Any], str]\n</code></pre> <p>Structured output in a specified format. This part of the functionality is provided based on the  capabilities of vLLM Structured Output.</p> <p>Parameters:</p> Name Type Description Default <code>messages</code> <code>List[Message]</code> <p>The messages to send to the LLM.</p> required <code>constraint</code> <code>Constraint</code> <p>The constraint to use for the structured output.</p> required <code>model</code> <code>Optional[str]</code> <p>The model to use for the structured output.</p> <code>None</code> <code>temperature</code> <code>Optional[float]</code> <p>The temperature to use for the structured output.</p> <code>None</code> <code>top_p</code> <code>Optional[float]</code> <p>The top_p to use for the structured output.</p> <code>None</code> <code>presence_penalty</code> <code>Optional[float]</code> <p>The presence_penalty to use for the structured output.</p> <code>None</code> <code>frequency_penalty</code> <code>Optional[float]</code> <p>The frequency_penalty to use for the structured output.</p> <code>None</code> <code>extra_body</code> <code>Optional[Dict[str, Any]]</code> <p>The extra_body to use for the structured output.</p> <code>None</code> <code>**kwargs</code> <p>The kwargs to use for the structured output.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Union[BaseModel, Dict[str, Any], str]</code> <p>The return type is based on the constraint type: * If the constraint is PydanticModel, return an instance of the corresponding Pydantic model. * If the constraint is JsonSchema, return a Dict[str, Any] that is the parsed JSON. * Otherwise, return a str.</p> Source code in <code>bridgic/llms/vllm/vllm_server_llm.py</code> <pre><code>async def astructured_output(\n    self,\n    messages: List[Message],\n    constraint: Constraint,\n    model: Optional[str] = None,\n    temperature: Optional[float] = None,\n    top_p: Optional[float] = None,\n    presence_penalty: Optional[float] = None,\n    frequency_penalty: Optional[float] = None,\n    extra_body: Optional[Dict[str, Any]] = None,\n    **kwargs,\n) -&gt; Union[BaseModel, Dict[str, Any], str]:\n    '''\n    Structured output in a specified format. This part of the functionality is provided based on the \n    capabilities of [vLLM Structured Output](https://docs.vllm.ai/en/latest/features/structured_outputs.html).\n\n    Parameters\n    ----------\n    messages: List[Message]\n        The messages to send to the LLM.\n    constraint: Constraint\n        The constraint to use for the structured output.\n    model: Optional[str]\n        The model to use for the structured output.\n    temperature: Optional[float]\n        The temperature to use for the structured output.\n    top_p: Optional[float]\n        The top_p to use for the structured output.\n    presence_penalty: Optional[float]\n        The presence_penalty to use for the structured output.\n    frequency_penalty: Optional[float]\n        The frequency_penalty to use for the structured output.\n    extra_body: Optional[Dict[str, Any]]\n        The extra_body to use for the structured output.\n    **kwargs: Any\n        The kwargs to use for the structured output.\n\n    Returns\n    -------\n    Union[BaseModel, Dict[str, Any], str]\n        The return type is based on the constraint type:\n        * If the constraint is PydanticModel, return an instance of the corresponding Pydantic model.\n        * If the constraint is JsonSchema, return a Dict[str, Any] that is the parsed JSON.\n        * Otherwise, return a str.\n    '''\n    response = await self.achat(\n        messages=messages,\n        model=model,\n        temperature=temperature,\n        top_p=top_p,\n        presence_penalty=presence_penalty,\n        frequency_penalty=frequency_penalty,\n        extra_body=self._convert_constraint(constraint, extra_body),\n        **kwargs,\n    )\n    return self._convert_response(constraint, response)\n</code></pre>"},{"location":"reference/bridgic-llms-vllm/bridgic/llms/vllm/#bridgic.llms.vllm.VllmServerLlm.select_tool","title":"select_tool","text":"<pre><code>select_tool(\n    messages: List[Message],\n    tools: List[Tool],\n    model: Optional[str] = None,\n    temperature: Optional[float] = None,\n    top_p: Optional[float] = None,\n    presence_penalty: Optional[float] = None,\n    frequency_penalty: Optional[float] = None,\n    extra_body: Optional[Dict[str, Any]] = None,\n    tool_choice: Union[\n        Literal[\"auto\", \"required\", \"none\"],\n        ChatCompletionNamedToolChoiceParam,\n    ] = \"auto\",\n    **kwargs\n) -&gt; Tuple[List[ToolCall], Optional[Dict]]\n</code></pre> <p>Select tools from a specified list of tools.</p> <p>Parameters:</p> Name Type Description Default <code>messages</code> <code>List[Message]</code> <p>The messages to send to the LLM.</p> required <code>tools</code> <code>List[Tool]</code> <p>The tools to use for the tool select.</p> required <code>model</code> <code>Optional[str]</code> <p>The model to use for the tool select.</p> <code>None</code> <code>temperature</code> <code>Optional[float]</code> <p>The temperature to use for the tool select.</p> <code>None</code> <code>top_p</code> <code>Optional[float]</code> <p>The top_p to use for the tool select.</p> <code>None</code> <code>presence_penalty</code> <code>Optional[float]</code> <p>The presence_penalty to use for the tool select.</p> <code>None</code> <code>frequency_penalty</code> <code>Optional[float]</code> <p>The frequency_penalty to use for the tool select.</p> <code>None</code> <code>extra_body</code> <code>Optional[Dict[str, Any]]</code> <p>The extra_body to use for the tool select.</p> <code>None</code> <code>tool_choice</code> <code>Union[Literal['auto', 'required', 'none'], ChatCompletionNamedToolChoiceParam]</code> <p>Tool choice mode for tool calling. There are 4 choices that are supported: - <code>auto</code> means the model can pick between generating a message or calling one or more tools. To enable this feature, you should set the tags <code>--enable-auto-tool-choice</code> and <code>--tool-call-parser</code>  when starting the vLLM server. - <code>required</code> means the model must generate one or more tool calls based on the specified tool list  in the <code>tools</code> parameter. The number of tool calls depends on the user's query. - <code>none</code> means the model will not call any tool and instead generates a message. When tools are  specified in the request, vLLM includes tool definitions in the prompt by default, regardless  of the tool_choice setting. To exclude tool definitions when tool_choice='none', use the  <code>--exclude-tools-when-tool-choice-none</code> option when starting the vLLM server. - You can also specify a particular function using named function calling by setting <code>tool_choice</code>  parameter to a json object, like <code>tool_choice={\"type\": \"function\", \"function\": {\"name\": \"get_weather\"}}</code>.</p> <code>'auto'</code> <code>**kwargs</code> <p>The kwargs to use for the tool select.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Tuple[List[ToolCall], Optional[str]]</code> <p>A list that contains the selected tools and their arguments.</p> Notes <p>See more on Tool Calling.</p> Source code in <code>bridgic/llms/vllm/vllm_server_llm.py</code> <pre><code>def select_tool(\n    self,\n    messages: List[Message],\n    tools: List[Tool],\n    model: Optional[str] = None,\n    temperature: Optional[float] = None,\n    top_p: Optional[float] = None,\n    presence_penalty: Optional[float] = None,\n    frequency_penalty: Optional[float] = None,\n    extra_body: Optional[Dict[str, Any]] = None,\n    tool_choice: Union[Literal[\"auto\", \"required\", \"none\"], ChatCompletionNamedToolChoiceParam] = \"auto\",\n    **kwargs,\n) -&gt; Tuple[List[ToolCall], Optional[Dict]]:\n    \"\"\"\n    Select tools from a specified list of tools.\n\n    Parameters\n    ----------\n    messages: List[Message]\n        The messages to send to the LLM.\n    tools: List[Tool]\n        The tools to use for the tool select.\n    model: Optional[str]\n        The model to use for the tool select.\n    temperature: Optional[float]\n        The temperature to use for the tool select.\n    top_p: Optional[float]\n        The top_p to use for the tool select.\n    presence_penalty: Optional[float]\n        The presence_penalty to use for the tool select.\n    frequency_penalty: Optional[float]\n        The frequency_penalty to use for the tool select.\n    extra_body: Optional[Dict[str, Any]]\n        The extra_body to use for the tool select.\n    tool_choice : Union[Literal[\"auto\", \"required\", \"none\"], ChatCompletionNamedToolChoiceParam]\n        Tool choice mode for tool calling. There are 4 choices that are supported:\n        - `auto` means the model can pick between generating a message or calling one or more tools.\n        To enable this feature, you should set the tags `--enable-auto-tool-choice` and `--tool-call-parser` \n        when starting the vLLM server.\n        - `required` means the model must generate one or more tool calls based on the specified tool list \n        in the `tools` parameter. The number of tool calls depends on the user's query.\n        - `none` means the model will not call any tool and instead generates a message. When tools are \n        specified in the request, vLLM includes tool definitions in the prompt by default, regardless \n        of the tool_choice setting. To exclude tool definitions when tool_choice='none', use the \n        `--exclude-tools-when-tool-choice-none` option when starting the vLLM server.\n        - You can also specify a particular function using named function calling by setting `tool_choice` \n        parameter to a json object, like `tool_choice={\"type\": \"function\", \"function\": {\"name\": \"get_weather\"}}`.\n\n    **kwargs: Any\n        The kwargs to use for the tool select.\n\n    Returns\n    -------\n    Tuple[List[ToolCall], Optional[str]]\n        A list that contains the selected tools and their arguments.\n\n    Notes\n    -----\n    See more on [Tool Calling](https://docs.vllm.ai/en/stable/features/tool_calling.html).\n    \"\"\"\n    # Build parameters dictionary for validation\n    params = filter_dict(merge_dict(self.configuration.model_dump(), {\n        \"model\": model,\n        \"temperature\": temperature,\n        \"top_p\": top_p,\n        \"presence_penalty\": presence_penalty,\n        \"frequency_penalty\": frequency_penalty,\n        \"extra_body\": extra_body,\n        **kwargs,\n    }))\n\n    # Validate required parameters for tool selection\n    validate_required_params(params, [\"model\"])\n\n    input_messages = [self._convert_message(message=msg, strict=True) for msg in messages]\n    input_tools = [\n        {\n            \"type\": \"function\",\n            \"function\": {\n                \"name\": tool.name,\n                \"description\": tool.description,\n                \"parameters\": tool.parameters,\n            },\n        } for tool in tools\n    ]\n\n    response = self.client.chat.completions.create(\n        model=model,\n        messages=input_messages,\n        tools=input_tools,\n        tool_choice=tool_choice,\n        **kwargs,\n    )\n    tool_calls = response.choices[0].message.tool_calls\n\n    output_content = \"\"\n    if response.choices[0].message.content:\n        output_content = response.choices[0].message.content\n\n    output_tool_calls = []\n    if tool_calls:\n        output_tool_calls = self._convert_tool_calls(tool_calls)\n\n    return (output_tool_calls, output_content)\n</code></pre>"},{"location":"reference/bridgic-llms-vllm/bridgic/llms/vllm/#bridgic.llms.vllm.VllmServerLlm.aselect_tool","title":"aselect_tool","text":"<code>async</code> <pre><code>aselect_tool(\n    messages: List[Message],\n    tools: List[Tool],\n    model: Optional[str] = None,\n    temperature: Optional[float] = None,\n    top_p: Optional[float] = None,\n    presence_penalty: Optional[float] = None,\n    frequency_penalty: Optional[float] = None,\n    extra_body: Optional[Dict[str, Any]] = None,\n    tool_choice: Union[\n        Literal[\"auto\", \"required\", \"none\"],\n        ChatCompletionNamedToolChoiceParam,\n    ] = \"auto\",\n    **kwargs\n) -&gt; Tuple[List[ToolCall], Optional[str]]\n</code></pre> <p>Select tools from a specified list of tools.</p> <p>Parameters:</p> Name Type Description Default <code>messages</code> <code>List[Message]</code> <p>The messages to send to the LLM.</p> required <code>tools</code> <code>List[Tool]</code> <p>The tools to use for the tool select.</p> required <code>model</code> <code>Optional[str]</code> <p>The model to use for the tool select.</p> <code>None</code> <code>temperature</code> <code>Optional[float]</code> <p>The temperature to use for the tool select.</p> <code>None</code> <code>top_p</code> <code>Optional[float]</code> <p>The top_p to use for the tool select.</p> <code>None</code> <code>presence_penalty</code> <code>Optional[float]</code> <p>The presence_penalty to use for the tool select.</p> <code>None</code> <code>frequency_penalty</code> <code>Optional[float]</code> <p>The frequency_penalty to use for the tool select.</p> <code>None</code> <code>extra_body</code> <code>Optional[Dict[str, Any]]</code> <p>The extra_body to use for the tool select.</p> <code>None</code> <code>tool_choice</code> <code>Union[Literal['auto', 'required', 'none'], ChatCompletionNamedToolChoiceParam]</code> <p>Tool choice mode for tool calling. There are 4 choices that are supported: - <code>auto</code> means the model can pick between generating a message or calling one or more tools. To enable this feature, you should set the tags <code>--enable-auto-tool-choice</code> and <code>--tool-call-parser</code>  when starting the vLLM server. - <code>required</code> means the model must generate one or more tool calls based on the specified tool list  in the <code>tools</code> parameter. The number of tool calls depends on the user's query. - <code>none</code> means the model will not call any tool and instead generates a message. When tools are  specified in the request, vLLM includes tool definitions in the prompt by default, regardless  of the tool_choice setting. To exclude tool definitions when tool_choice='none', use the  <code>--exclude-tools-when-tool-choice-none</code> option when starting the vLLM server. - You can also specify a particular function using named function calling by setting <code>tool_choice</code>  parameter to a json object, like <code>tool_choice={\"type\": \"function\", \"function\": {\"name\": \"get_weather\"}}</code>.</p> <code>'auto'</code> <code>**kwargs</code> <p>The kwargs to use for the tool select.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Tuple[List[ToolCall], Optional[str]]</code> <p>A list that contains the selected tools and their arguments.</p> Notes <p>See more on Tool Calling.</p> Source code in <code>bridgic/llms/vllm/vllm_server_llm.py</code> <pre><code>async def aselect_tool(\n    self,\n    messages: List[Message],\n    tools: List[Tool],\n    model: Optional[str] = None,\n    temperature: Optional[float] = None,\n    top_p: Optional[float] = None,\n    presence_penalty: Optional[float] = None,\n    frequency_penalty: Optional[float] = None,\n    extra_body: Optional[Dict[str, Any]] = None,\n    tool_choice: Union[Literal[\"auto\", \"required\", \"none\"], ChatCompletionNamedToolChoiceParam] = \"auto\",\n    **kwargs,\n) -&gt; Tuple[List[ToolCall], Optional[str]]:\n    \"\"\"\n    Select tools from a specified list of tools.\n\n    Parameters\n    ----------\n    messages: List[Message]\n        The messages to send to the LLM.\n    tools: List[Tool]\n        The tools to use for the tool select.\n    model: Optional[str]\n        The model to use for the tool select.\n    temperature: Optional[float]\n        The temperature to use for the tool select.\n    top_p: Optional[float]\n        The top_p to use for the tool select.\n    presence_penalty: Optional[float]\n        The presence_penalty to use for the tool select.\n    frequency_penalty: Optional[float]\n        The frequency_penalty to use for the tool select.\n    extra_body: Optional[Dict[str, Any]]\n        The extra_body to use for the tool select.\n    tool_choice : Union[Literal[\"auto\", \"required\", \"none\"], ChatCompletionNamedToolChoiceParam]\n        Tool choice mode for tool calling. There are 4 choices that are supported:\n        - `auto` means the model can pick between generating a message or calling one or more tools.\n        To enable this feature, you should set the tags `--enable-auto-tool-choice` and `--tool-call-parser` \n        when starting the vLLM server.\n        - `required` means the model must generate one or more tool calls based on the specified tool list \n        in the `tools` parameter. The number of tool calls depends on the user's query.\n        - `none` means the model will not call any tool and instead generates a message. When tools are \n        specified in the request, vLLM includes tool definitions in the prompt by default, regardless \n        of the tool_choice setting. To exclude tool definitions when tool_choice='none', use the \n        `--exclude-tools-when-tool-choice-none` option when starting the vLLM server.\n        - You can also specify a particular function using named function calling by setting `tool_choice` \n        parameter to a json object, like `tool_choice={\"type\": \"function\", \"function\": {\"name\": \"get_weather\"}}`.\n\n    **kwargs: Any\n        The kwargs to use for the tool select.\n\n    Returns\n    -------\n    Tuple[List[ToolCall], Optional[str]]\n        A list that contains the selected tools and their arguments.\n\n    Notes\n    -----\n    See more on [Tool Calling](https://docs.vllm.ai/en/stable/features/tool_calling.html).\n    \"\"\"\n    # Build parameters dictionary for validation\n    params = filter_dict(merge_dict(self.configuration.model_dump(), {\n        \"model\": model,\n        \"temperature\": temperature,\n        \"top_p\": top_p,\n        \"presence_penalty\": presence_penalty,\n        \"frequency_penalty\": frequency_penalty,\n        \"extra_body\": extra_body,\n        **kwargs,\n    }))\n\n    # Validate required parameters for tool selection\n    validate_required_params(params, [\"model\"])\n\n    input_messages = [self._convert_message(message=msg, strict=True) for msg in messages]\n    input_tools = [\n        {\n            \"type\": \"function\",\n            \"function\": {\n                \"name\": tool.name,\n                \"description\": tool.description,\n                \"parameters\": tool.parameters,\n            },\n        } for tool in tools\n    ]\n\n    response = self.client.chat.completions.create(\n        model=model,\n        messages=input_messages,\n        tools=input_tools,\n        tool_choice=tool_choice,\n        **kwargs,\n    )\n    tool_calls = response.choices[0].message.tool_calls\n\n    output_content = \"\"\n    if response.choices[0].message.content:\n        output_content = response.choices[0].message.content\n\n    output_tool_calls = []\n    if tool_calls:\n        output_tool_calls = self._convert_tool_calls(tool_calls)\n\n    return (output_tool_calls, output_content)\n</code></pre>"},{"location":"reference/bridgic-llms-vllm/bridgic/llms/vllm/#bridgic.llms.vllm.VllmServerConfiguration","title":"VllmServerConfiguration","text":"<p>               Bases: <code>OpenAILikeConfiguration</code></p> <p>Configuration for the vLLM server.</p> Source code in <code>bridgic/llms/vllm/vllm_server_llm.py</code> <pre><code>class VllmServerConfiguration(OpenAILikeConfiguration):\n    \"\"\"\n    Configuration for the vLLM server.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"tutorials/","title":"Tutorials","text":"<p>Welcome to the Bridgic tutorial!</p> <p>Here, we offer the basic knowledge for developing agentic system in Bridgic framework, helping you quickly master necessary concepts and best practices of organizing program deterministic logic and non-deterministic model calls to build an agent.</p> <p>For each feature, we provide a simplified example process inspired by real-world development scenarios to help you better understand and apply the concepts.</p> <p>This tutorial will walk you through these tasks:</p> <ol> <li>Quick Start: Write your first Bridgic style program.</li> <li>Core Mechanism: Learn about the core mechanism and how Bridgic organizes program into modular building blocks.</li> <li>Model Integration: Discover how to incorporate AI models into your workflows for creating more autonomous and intelligent systems.</li> </ol> <p>Start your Bridgic journey! \ud83c\udf89</p>"},{"location":"tutorials/installation/","title":"Installation","text":"<p>Bridgic is a next-generation Agent development framework, designed to empower developers to build, orchestrate, and manage intelligent agents with ease.</p> <p>Centered on the paradigm of Agentic programming, Bridgic seamlessly integrates deterministic logic and advanced AI capabilities, enabling rapid prototyping, robust long-running agent workflows, and flexible system composability. Whether you're developing interactive assistants, autonomous workflows, or adaptive process automation, Bridgic will equip you with convenient tools and intuitive abstractions to give you support.</p> <p>The installation equires Python 3.9 or higher version.</p> pipuv <pre><code>pip install bridgic\n</code></pre> <pre><code>uv add bridgic\n</code></pre> <p>After installation, you can verify that the installation was successful by running:</p> <pre><code>python -c \"from bridgic.core import __version__; print(f'Bridgic version: {__version__}')\"\n</code></pre>"},{"location":"tutorials/items/core_mechanism/","title":"Core Mechanism","text":"<p>Bridgic lets you build agentic systems by breaking down your workflows into modular building blocks called worker. Each worker represents a specific task or behavior, making it easy to organize complex processes.</p> <p>Bridgic introduces clear abstractions for structuring flows, passing data between execution units, handling concurrency, and enabling dynamic control logic (such as conditional branching and routing). This design allows users to build systerm that scale efficiently from simple workflows to sophisticated agentic systems.</p> <p>Key features include:</p> <ol> <li>Concurrency Mode: Organize your concurrent execution units systematically and conveniently.</li> <li>Parameter Binding: Explore three ways for passing data between execution units, including Arguments Mapping, Arguments Injection, and Inputs Propagation.</li> <li>Dynamic Routing: Decide which execution unit to be executed in the short future dynamically.</li> <li>Modularity: Reuse and compose Automata by embedding one inside another for scalable workflows.</li> <li>Model Integration: Incorporate model to building a program with more autonomous capabilities.</li> <li>Human-in-the-loop: Enable human interaction or external input during workflow execution.</li> </ol> <p>This architectural foundation makes Bridgic a powerful platform for building agentic systems that are robust, adaptive, and easy to reason about, enabling you to bridge logic with the creative potential of AI.</p>"},{"location":"tutorials/items/core_mechanism/concurrency_mode/","title":"Concurrency Mode","text":"In\u00a0[\u00a0]: Copied! <pre>url=\"http://books.toscrape.com/catalogue/a-light-in-the-attic_1000/index.html\"\n</pre> url=\"http://books.toscrape.com/catalogue/a-light-in-the-attic_1000/index.html\" <p>The page looks like this:</p> <p>Note: We use Books to Scrape, a demo website created specifically for practicing web scraping, to introduce in this tutorial. Please note that the purpose of writing a crawler here is not to build a real scraper, but to provide a simple and safe example to demonstrate how Bridgic handles both synchronous and asynchronous execution models.</p> <p>We use <code>requests</code> to obtain the web content of the given url. Use <code>pip install requests</code> to install <code>requests</code> package and crawl the page like this:</p> In\u00a0[\u00a0]: Copied! <pre>import requests\n\ndef get_web_content(url):  # will return the web content of the given url\n    response = requests.get(url)\n    return response.text\n</pre> import requests  def get_web_content(url):  # will return the web content of the given url     response = requests.get(url)     return response.text In\u00a0[\u00a0]: Copied! <pre>import os\nfrom dotenv import load_dotenv\nload_dotenv()\n\n# Set the API base and key.\n_api_base = os.environ.get(\"VLLM_SERVER_API_BASE\")\n_api_key = os.environ.get(\"VLLM_SERVER_API_KEY\")\n_model_name = os.environ.get(\"VLLM_SERVER_MODEL_NAME\")\n\n# Import the necessary modules.\nfrom bridgic.core.automa import GraphAutoma, worker\nfrom bridgic.core.model.types import Message, Role\nfrom bridgic.llms.vllm.vllm_server_llm import VllmServerLlm\n\nllm = VllmServerLlm(api_base=_api_base, api_key=_api_key, timeout=30)\n</pre> import os from dotenv import load_dotenv load_dotenv()  # Set the API base and key. _api_base = os.environ.get(\"VLLM_SERVER_API_BASE\") _api_key = os.environ.get(\"VLLM_SERVER_API_KEY\") _model_name = os.environ.get(\"VLLM_SERVER_MODEL_NAME\")  # Import the necessary modules. from bridgic.core.automa import GraphAutoma, worker from bridgic.core.model.types import Message, Role from bridgic.llms.vllm.vllm_server_llm import VllmServerLlm  llm = VllmServerLlm(api_base=_api_base, api_key=_api_key, timeout=30) <p>Let's write web content analysis assistant.</p> In\u00a0[\u00a0]: Copied! <pre>class WebContentAnalysisAgent(GraphAutoma):\n    @worker(is_start=True)\n    def crawl_web_content(self, url: str) -&gt; str:\n        response = requests.get(url)\n        return response.text\n\n    @worker(dependencies=[\"crawl_web_content\"], is_output=True)\n    async def analyze_web_content(self, content: str) -&gt; str:\n        response = await llm.achat(\n            model=_model_name,\n            messages=[\n                Message.from_text(text=\"You are a web content analysis assistant. Your task is to analyze the given web content and summarize the main content.\", role=Role.SYSTEM),\n                Message.from_text(text=content, role=Role.USER),\n            ]\n        )\n        return response.message.content\n</pre> class WebContentAnalysisAgent(GraphAutoma):     @worker(is_start=True)     def crawl_web_content(self, url: str) -&gt; str:         response = requests.get(url)         return response.text      @worker(dependencies=[\"crawl_web_content\"], is_output=True)     async def analyze_web_content(self, content: str) -&gt; str:         response = await llm.achat(             model=_model_name,             messages=[                 Message.from_text(text=\"You are a web content analysis assistant. Your task is to analyze the given web content and summarize the main content.\", role=Role.SYSTEM),                 Message.from_text(text=content, role=Role.USER),             ]         )         return response.message.content <p>Now, let's use it to help us analyze the content.</p> In\u00a0[22]: Copied! <pre>web_content_analysis_agent = WebContentAnalysisAgent()\n\n# Input the url of the web page to be analyzed.\nurl = \"http://books.toscrape.com/catalogue/a-light-in-the-attic_1000/index.html\"\n\n# Call the agent to analyze the web content.\nres = await web_content_analysis_agent.arun(url)\n\n# Print the result.\nprint(f'- - - - - result - - - - -')\nprint(res)\nprint(f'- - - - - end - - - - -')\n</pre> web_content_analysis_agent = WebContentAnalysisAgent()  # Input the url of the web page to be analyzed. url = \"http://books.toscrape.com/catalogue/a-light-in-the-attic_1000/index.html\"  # Call the agent to analyze the web content. res = await web_content_analysis_agent.arun(url)  # Print the result. print(f'- - - - - result - - - - -') print(res) print(f'- - - - - end - - - - -')  <pre>- - - - - result - - - - -\nThe provided HTML content is from a product page on **Books to Scrape**, a demo website designed for web scraping education. Here's a clear summary of the main content:\n\n---\n\n### **Main Content Summary: A Light in the Attic**\n\n- **Product Title**: *A Light in the Attic*  \n- **Author**: Shel Silverstein  \n- **Category**: Poetry  \n- **Product Type**: Book (Poetry with illustrations)  \n- **Price**: \u00a351.77 (excl. and incl. tax; tax is \u00a30.00)  \n- **Availability**: In stock (22 units available)  \n- **Rating**: 5 stars (all full stars)  \n- **Number of Reviews**: 0  \n\n---\n\n### **Product Description Highlights**\n- Celebrates its 20th anniversary with a special edition.\n- Known for humorous, creative, and rhythmic poetry that appeals to both children and adults.\n- Features classic verses such as *\"Rockabye Baby\"*:\n  &gt; *\"Rockabye baby, in the treetop / Don't you know a treetop / Is no safe place to rock?\"*\n- Described as a timeless classic that brings joy and laughter to readers of all ages.\n\n---\n\n### **Important Note**\n&gt; \u26a0\ufe0f **This is a demo website** for web scraping training.  \n&gt; - Prices and ratings are **randomly assigned** and **do not reflect real-world data**.  \n&gt; - The site is not a real marketplace and should not be used for actual purchases.\n\n---\n\n### **Navigation Path**\nHome \u2192 Books \u2192 Poetry \u2192 *A Light in the Attic*\n\n---\n\n### **Visual Elements**\n- A single image of the book displayed in a carousel.\n- Clean, responsive layout with a header, product gallery, pricing, and description sections.\n\n---\n\n\u2705 **Purpose of the Page**: To demonstrate how to extract product details (title, price, description, availability, etc.) from e-commerce-style web pages \u2014 useful for teaching web scraping techniques.  \n\n\u274c **Not for real shopping** \u2014 all data is fictional.  \n\n--- \n\nIn short: This page showcases a fictional version of a beloved children's poetry book, presented in a realistic e-commerce format, but with no real pricing or user reviews.\n- - - - - end - - - - -\n</pre>"},{"location":"tutorials/items/core_mechanism/concurrency_mode/#concurrency-mode","title":"Concurrency Mode\u00b6","text":"<p>Bridgic runs primarily on an asynchronous event loop, while seamlessly supporting I/O-bound tasks through threads. This design ensures high concurrency across diverse workloads.</p>"},{"location":"tutorials/items/core_mechanism/concurrency_mode/#web-content-analysis-assistant","title":"Web content analysis assistant\u00b6","text":"<p>To explore Bridgic\u2019s support for concurrency, let\u2019s build a web content analysis assistant to summarize and introduce the main content of a given web page. The steps are as follows:</p> <ol> <li>Crawl relevant content of the input url.</li> <li>Summarize and introduce main content</li> </ol>"},{"location":"tutorials/items/core_mechanism/concurrency_mode/#1-crawl-relevant-content","title":"1. Crawl relevant content\u00b6","text":"<p>Taking the Books to Scrape website as an example, we are given the url of a book page on the website. Like this:</p>"},{"location":"tutorials/items/core_mechanism/concurrency_mode/#2-summarize-and-introduce-main-content","title":"2. Summarize and introduce main content\u00b6","text":"<p>We create an agent, input a url and crawl the corresponding page, and then let the model summarize the main content of the web page.</p> <p>Initialize the runtime environment.</p>"},{"location":"tutorials/items/core_mechanism/concurrency_mode/#what-have-we-done","title":"What have we done?\u00b6","text":"<p>It can be seen from the example that Bridgic can seamlessly schedule both asynchronous and synchronous workers within the same automa. Although <code>crawl_web_content</code> performs a blocking network request, Bridgic automatically dispatches it to a thread so that the event loop remains unblocked. Meanwhile, <code>analyze_web_content</code> runs asynchronously within the event loop.</p> <p>In this way, Bridgic keeps an asynchronous-first design, but also provides built-in support for I/O-bound operations through its thread pool, ensuring smooth execution across different types of workloads.</p>"},{"location":"tutorials/items/core_mechanism/dynamic_routing/","title":"Dynamic Routing","text":"In\u00a0[\u00a0]: Copied! <pre># Get the environment variables.\nimport os\nfrom dotenv import load_dotenv\nload_dotenv()\n\n_api_base = os.environ.get(\"VLLM_SERVER_API_BASE\")\n_api_key = os.environ.get(\"VLLM_SERVER_API_KEY\")\n_model_name = os.environ.get(\"VLLM_SERVER_MODEL_NAME\")\n\n# Import the necessary packages.\nfrom bridgic.core.automa import GraphAutoma, worker\nfrom bridgic.core.model.protocols import Regex\nfrom bridgic.core.model.types import Message, Role\nfrom bridgic.llms.vllm.vllm_server_llm import VllmServerLlm\n\nllm = VllmServerLlm(  # the llm instance\n    api_base=_api_base,\n    api_key=_api_key,\n    timeout=20,\n)\n</pre> # Get the environment variables. import os from dotenv import load_dotenv load_dotenv()  _api_base = os.environ.get(\"VLLM_SERVER_API_BASE\") _api_key = os.environ.get(\"VLLM_SERVER_API_KEY\") _model_name = os.environ.get(\"VLLM_SERVER_MODEL_NAME\")  # Import the necessary packages. from bridgic.core.automa import GraphAutoma, worker from bridgic.core.model.protocols import Regex from bridgic.core.model.types import Message, Role from bridgic.llms.vllm.vllm_server_llm import VllmServerLlm  llm = VllmServerLlm(  # the llm instance     api_base=_api_base,     api_key=_api_key,     timeout=20, ) In\u00a0[4]: Copied! <pre>class DomainChatbot(GraphAutoma):\n    @worker(is_start=True)\n    async def pre_query(self, query: str):  # receive user input and preprocess it\n        return query\n\n    @worker(dependencies=[\"pre_query\"])\n    async def domain_recognition(self, query: str):  # domain recognition and dynamic routing\n        response: str = await llm.astructured_output(\n            model=_model_name,\n            constraint=Regex(pattern=r\"^medical$|^legal$\", description=\"model output should be only 'medical' or 'legal'\"),\n            messages=[\n                Message.from_text(text=\"Please identify the domain of the following query\", role=Role.SYSTEM),\n                Message.from_text(text=query, role=Role.USER),\n            ]\n        )\n        print(f'The query domain is: {response}')  # print the response\n        \n        # dynamic routing\n        if response == \"medical\":\n            return self.ferry_to('answer_medical_query', query)\n        elif response == \"legal\":\n            return self.ferry_to('answer_legal_query', query)\n    \n    @worker(is_output=True)\n    async def answer_medical_query(self, query: str):  # answer medical-related query\n        response = await llm.achat(\n            model=_model_name,\n            messages=[\n                Message.from_text(text=\"You are a medical expert. Answer the following medical-related query\", role=Role.SYSTEM),\n                Message.from_text(text=query, role=Role.USER),\n            ]\n        )\n        return response.message.content\n\n    @worker(is_output=True)\n    async def answer_legal_query(self, query: str):  # answer legal-related query\n        response = await llm.achat(\n            model=_model_name,\n            messages=[\n                Message.from_text(text=\"You are a legal expert. Answer the following legal-related query\", role=Role.SYSTEM),\n                Message.from_text(text=query, role=Role.USER),\n            ]\n        )\n        return response.message.content\n</pre> class DomainChatbot(GraphAutoma):     @worker(is_start=True)     async def pre_query(self, query: str):  # receive user input and preprocess it         return query      @worker(dependencies=[\"pre_query\"])     async def domain_recognition(self, query: str):  # domain recognition and dynamic routing         response: str = await llm.astructured_output(             model=_model_name,             constraint=Regex(pattern=r\"^medical$|^legal$\", description=\"model output should be only 'medical' or 'legal'\"),             messages=[                 Message.from_text(text=\"Please identify the domain of the following query\", role=Role.SYSTEM),                 Message.from_text(text=query, role=Role.USER),             ]         )         print(f'The query domain is: {response}')  # print the response                  # dynamic routing         if response == \"medical\":             return self.ferry_to('answer_medical_query', query)         elif response == \"legal\":             return self.ferry_to('answer_legal_query', query)          @worker(is_output=True)     async def answer_medical_query(self, query: str):  # answer medical-related query         response = await llm.achat(             model=_model_name,             messages=[                 Message.from_text(text=\"You are a medical expert. Answer the following medical-related query\", role=Role.SYSTEM),                 Message.from_text(text=query, role=Role.USER),             ]         )         return response.message.content      @worker(is_output=True)     async def answer_legal_query(self, query: str):  # answer legal-related query         response = await llm.achat(             model=_model_name,             messages=[                 Message.from_text(text=\"You are a legal expert. Answer the following legal-related query\", role=Role.SYSTEM),                 Message.from_text(text=query, role=Role.USER),             ]         )         return response.message.content <p>Let's run it!</p> In\u00a0[5]: Copied! <pre>query = \"What is the aspirin?\"\ndomain_chatbot = DomainChatbot()\nawait domain_chatbot.arun(query)\n</pre> query = \"What is the aspirin?\" domain_chatbot = DomainChatbot() await domain_chatbot.arun(query) <pre>The query domain is: medical\n</pre> Out[5]: <pre>'Aspirin, also known as acetylsalicylic acid, is a widely used medication with several important medical properties. It is one of the oldest and most well-known over-the-counter (OTC) drugs, originally derived from willow bark.\\n\\n### Key Uses of Aspirin:\\n\\n1. **Pain Relief (Analgesic)**  \\n   Aspirin helps relieve mild to moderate pain such as headaches, toothaches, muscle aches, and menstrual cramps.\\n\\n2. **Reducing Fever (Antipyretic)**  \\n   It lowers body temperature in cases of fever, such as in infections.\\n\\n3. **Reducing Inflammation (Anti-inflammatory)**  \\n   Aspirin helps reduce inflammation, which can be beneficial in conditions like rheumatoid arthritis or other inflammatory disorders.\\n\\n4. **Preventing Blood Clots (Antiplatelet Effect)**  \\n   This is one of its most important therapeutic uses. Aspirin inhibits the formation of blood clots by blocking the action of a substance called cyclooxygenase (COX), which reduces the production of thromboxane\u2014a molecule that promotes platelet aggregation.  \\n   - **Low-dose aspirin (typically 81 mg)** is commonly prescribed for long-term use to prevent heart attacks and strokes in individuals at risk due to cardiovascular disease (e.g., history of heart attack, angina, or stroke).\\n\\n5. **Cardiovascular Protection**  \\n   Regular low-dose aspirin is recommended for secondary prevention in patients with a history of cardiovascular events to reduce the risk of recurrence.\\n\\n---\\n\\n### How It Works:\\nAspirin irreversibly inhibits the enzyme COX-1 and COX-2, which are involved in producing prostaglandins. Prostaglandins play roles in inflammation, pain, fever, and platelet aggregation. By blocking COX-1, aspirin reduces platelet activation and clot formation.\\n\\n---\\n\\n### Safety and Precautions:\\n- **Gastrointestinal Side Effects**: Aspirin can irritate the stomach lining and increase the risk of ulcers or bleeding, especially with long-term or high-dose use.\\n- **Bleeding Risk**: Due to its antiplatelet effect, it increases the risk of bleeding (e.g., gastrointestinal bleeding, bruising).\\n- **Allergic Reactions**: Some people may have a severe allergic reaction, including asthma exacerbation (especially in those with a history of aspirin sensitivity).\\n- **Reproductive Health**: Aspirin is not recommended during pregnancy, especially in the third trimester, due to risks of fetal complications.\\n- **Children and Fever**: Aspirin should be avoided in children and adolescents with viral infections (like chickenpox or flu) due to the risk of Reye\u2019s syndrome, a rare but serious condition.\\n\\n---\\n\\n### Summary:\\n**Aspirin is a versatile medication used for pain, fever, inflammation, and cardiovascular protection. Its most significant medical role today is in preventing heart attacks and strokes through long-term low-dose use. However, it should be used cautiously and only as directed by a healthcare provider.**\\n\\nAlways consult a doctor before starting or stopping aspirin therapy, especially if you have underlying health conditions or are taking other medications.'</pre> <p>If you encounter a Readtimeout error during the execution of the above process, it might be because the timeout period set during the initialization of the llm is too short, causing the model to time out before it finishes responding.</p> <p>Great! We have successfully completed the domain chatbot.</p>"},{"location":"tutorials/items/core_mechanism/dynamic_routing/#dynamic-routing","title":"Dynamic Routing\u00b6","text":"<p>Bridgic supports dynamic routing to the corresponding worker based on runtime conditions. Now let's understand it with a sample example.</p>"},{"location":"tutorials/items/core_mechanism/dynamic_routing/#domain-chatbot","title":"Domain chatbot\u00b6","text":"<p>Dynamic routing is a common requirement in chatbots. For instance, it involves analyzing the input query to determine the main domain or field it pertains to, and then dynamically routing it to the corresponding processing logic.</p> <p>The user inputs the original query, we identify whether it is a medical-related or legal-related issue, and then we answer it with specialized logic. There are three steps:</p> <ol> <li>Receive user input</li> <li>Domain recognition and dynamic routing to correct worker</li> <li>Answer query</li> </ol>"},{"location":"tutorials/items/core_mechanism/dynamic_routing/#1-initialize","title":"1. Initialize\u00b6","text":"<p>Before we start, let's prepare the running environment.</p>"},{"location":"tutorials/items/core_mechanism/dynamic_routing/#2-complete","title":"2. Complete\u00b6","text":"<p>We assume that the user query we receive is a string. Let's implement the domain chatbot to answer the user query.</p>"},{"location":"tutorials/items/core_mechanism/dynamic_routing/#what-have-we-done","title":"What have we done?\u00b6","text":"<p>We have implemented the routing mechanism using Bridgic.</p>"},{"location":"tutorials/items/core_mechanism/dynamic_routing/#ferry-to","title":"Ferry To\u00b6","text":"<p>In Bridgic, we use the <code>ferry_to</code> mechanism to achieve dynamic routing. Its first parameter is the <code>worker_key</code> of the worker to which the route will be directed, and the second parameter is the input parameter passed to the target worker.</p>"},{"location":"tutorials/items/core_mechanism/human_in_the_loop/","title":"Human-in-the-loop","text":"In\u00a0[\u00a0]: Copied! <pre>import os\nfrom dotenv import load_dotenv\nload_dotenv()\n\n# Set the API base and key.\n_api_base = os.environ.get(\"VLLM_SERVER_API_BASE\")\n_api_key = os.environ.get(\"VLLM_SERVER_API_KEY\")\n_model_name = os.environ.get(\"VLLM_SERVER_MODEL_NAME\")\n\nfrom pydantic import BaseModel, Field\nfrom bridgic.core.automa import GraphAutoma, worker, Snapshot\nfrom bridgic.core.automa.args import From\nfrom bridgic.core.automa.interaction import Event, Feedback, FeedbackSender, InteractionFeedback, InteractionException\nfrom bridgic.core.model.types import Message, Role\nfrom bridgic.core.model.protocols import PydanticModel\nfrom bridgic.llms.vllm.vllm_server_llm import VllmServerLlm\n</pre> import os from dotenv import load_dotenv load_dotenv()  # Set the API base and key. _api_base = os.environ.get(\"VLLM_SERVER_API_BASE\") _api_key = os.environ.get(\"VLLM_SERVER_API_KEY\") _model_name = os.environ.get(\"VLLM_SERVER_MODEL_NAME\")  from pydantic import BaseModel, Field from bridgic.core.automa import GraphAutoma, worker, Snapshot from bridgic.core.automa.args import From from bridgic.core.automa.interaction import Event, Feedback, FeedbackSender, InteractionFeedback, InteractionException from bridgic.core.model.types import Message, Role from bridgic.core.model.protocols import PydanticModel from bridgic.llms.vllm.vllm_server_llm import VllmServerLlm In\u00a0[30]: Copied! <pre># Set the LLM\nllm = VllmServerLlm(api_base=_api_base, api_key=_api_key, timeout=10)\n\nclass CodeBlock(BaseModel):\n    code: str = Field(description=\"The code to be executed.\")\n\nclass CodeAssistant(GraphAutoma):\n    @worker(is_start=True)\n    async def generate_code(self, user_requirement: str):\n        response = await llm.astructured_output(\n            model=_model_name,\n            messages=[\n                Message.from_text(text=f\"You are a programming assistant. Please generate code according to the user's requirements.\", role=Role.SYSTEM),\n                Message.from_text(text=user_requirement, role=Role.USER),\n            ],\n            constraint=PydanticModel(model=CodeBlock)\n        )\n        return response.code\n\n    @worker(dependencies=[\"generate_code\"])\n    async def ask_to_run_code(self, code: str):\n        event = Event(event_type=\"can_run_code\", data=code)\n        feedback = await self.request_feedback_async(event)\n        return feedback.data\n        \n    @worker(dependencies=[\"ask_to_run_code\"])\n    async def output_result(self, feedback: str, code: str = From(\"generate_code\")):\n        code = code.strip(\"```python\").strip(\"```\")\n        if feedback == \"yes\":\n            print(f\"- - - - - - Result - - - - - -\")\n            exec(code)\n            print(f\"- - - - - - End - - - - - -\")\n        else:\n            print(f\"This code was rejected for execution. In response to the requirements, I have generated the following code:\\n {code}\")\n</pre> # Set the LLM llm = VllmServerLlm(api_base=_api_base, api_key=_api_key, timeout=10)  class CodeBlock(BaseModel):     code: str = Field(description=\"The code to be executed.\")  class CodeAssistant(GraphAutoma):     @worker(is_start=True)     async def generate_code(self, user_requirement: str):         response = await llm.astructured_output(             model=_model_name,             messages=[                 Message.from_text(text=f\"You are a programming assistant. Please generate code according to the user's requirements.\", role=Role.SYSTEM),                 Message.from_text(text=user_requirement, role=Role.USER),             ],             constraint=PydanticModel(model=CodeBlock)         )         return response.code      @worker(dependencies=[\"generate_code\"])     async def ask_to_run_code(self, code: str):         event = Event(event_type=\"can_run_code\", data=code)         feedback = await self.request_feedback_async(event)         return feedback.data              @worker(dependencies=[\"ask_to_run_code\"])     async def output_result(self, feedback: str, code: str = From(\"generate_code\")):         code = code.strip(\"```python\").strip(\"```\")         if feedback == \"yes\":             print(f\"- - - - - - Result - - - - - -\")             exec(code)             print(f\"- - - - - - End - - - - - -\")         else:             print(f\"This code was rejected for execution. In response to the requirements, I have generated the following code:\\n {code}\") <p>In the <code>ask_to_run_code()</code> method of <code>CodeAssistant</code>, we use <code>request_feedback_async()</code> to throw a specified Event and expect to receive feedback. To handle this Event, the corresponding logic needs to be registered in automa, like this:</p> In\u00a0[31]: Copied! <pre>import getpass\n\n# Handle can_run_code event\ndef can_run_code_handler(event: Event, feedback_sender: FeedbackSender):\n    print(f\"Can I run this code now to verify if it's correct?\")\n    print(event.data)\n    res = input(\"Please input your answer (yes/no): \")\n    print(f\"\\nPlease input your answer (yes/no): {res}\\n\")  # print the input\n    if res in [\"yes\", \"no\"]:\n        feedback_sender.send(Feedback(data=res))\n    else:\n        print(\"Invalid input. Please input yes or no.\")\n        feedback_sender.send(Feedback(data=\"no\"))\n\n# register can_run_code event handler to `CodeAssistant` automa\ncode_assistant = CodeAssistant()\ncode_assistant.register_event_handler(\"can_run_code\", can_run_code_handler)\n</pre> import getpass  # Handle can_run_code event def can_run_code_handler(event: Event, feedback_sender: FeedbackSender):     print(f\"Can I run this code now to verify if it's correct?\")     print(event.data)     res = input(\"Please input your answer (yes/no): \")     print(f\"\\nPlease input your answer (yes/no): {res}\\n\")  # print the input     if res in [\"yes\", \"no\"]:         feedback_sender.send(Feedback(data=res))     else:         print(\"Invalid input. Please input yes or no.\")         feedback_sender.send(Feedback(data=\"no\"))  # register can_run_code event handler to `CodeAssistant` automa code_assistant = CodeAssistant() code_assistant.register_event_handler(\"can_run_code\", can_run_code_handler) <p>Now let's use it!</p> In\u00a0[32]: Copied! <pre>await code_assistant.arun(user_requirement=\"Please write a function to print 'Hello, World!'\")\n</pre> await code_assistant.arun(user_requirement=\"Please write a function to print 'Hello, World!'\") <pre>Can I run this code now to verify if it's correct?\n```python\ndef print_hello_world():\n    print('Hello, World!')\n\n# Call the function to print 'Hello, World!'\nprint_hello_world()\n```\n\nPlease input your answer (yes/no): yes\n\n- - - - - - Result - - - - - -\nHello, World!\n- - - - - - End - - - - - -\n</pre> <p>In the above example, Bridgic uses <code>Event</code> to wrap the thrown event and <code>FeedBack</code> to wrap the external feedback information.</p> <ul> <li><code>Event</code> contains three fields:<ul> <li><code>event_type</code>: A string. The event type is used to identify the registered event handler.</li> <li><code>timestamp</code>: A Python datetime object. The timestamp of the event. The default is <code>datetime.now()</code>.</li> <li><code>data</code>: The data attached to the event.</li> </ul> </li> <li><code>FeedBack</code> contains one field:<ul> <li><code>data</code>: The data attached to the feedback.</li> </ul> </li> </ul> <p><code>request_feedback_async()</code> indicates throwing a specific event and simultaneously blocking the program to wait for feedback. The method registered for handling events, if it generates feedback, must be defined as <code>func(event: Event, feedback_sender: FeedbackSender)</code>, where the first parameter is the corresponding event and the second parameter is used to send feedback: <code>feedback_sender.send(Feedback(data=...))</code>, back to where the event was thrown.</p> In\u00a0[\u00a0]: Copied! <pre>class MessageNotifier(GraphAutoma):\n    @worker(is_start=True)\n    async def notify(self, user_input: int, notify_int: int):\n        print(f\"Loop from 1 to {user_input}\")\n        for i in range(1, user_input + 1):\n            if i == notify_int:\n                event = Event(event_type=\"message_notification\", data=f\"Loop {i} times\")\n                self.post_event(event)\n\ndef message_notification_handler(event: Event):\n    print(f'!! Now count to {event.data}. !!')\n\nmessage_notifier = MessageNotifier()\nmessage_notifier.register_event_handler(\"message_notification\", message_notification_handler)\nawait message_notifier.arun(user_input=10, notify_int=5)\n        \n</pre> class MessageNotifier(GraphAutoma):     @worker(is_start=True)     async def notify(self, user_input: int, notify_int: int):         print(f\"Loop from 1 to {user_input}\")         for i in range(1, user_input + 1):             if i == notify_int:                 event = Event(event_type=\"message_notification\", data=f\"Loop {i} times\")                 self.post_event(event)  def message_notification_handler(event: Event):     print(f'!! Now count to {event.data}. !!')  message_notifier = MessageNotifier() message_notifier.register_event_handler(\"message_notification\", message_notification_handler) await message_notifier.arun(user_input=10, notify_int=5)          <pre>Loop from 1 to 10\n!! Now count to Loop 5 times. !!\n</pre> In\u00a0[\u00a0]: Copied! <pre>llm = VllmServerLlm(api_base=_api_base, api_key=_api_key, timeout=10)\n\nclass MessageAssistant(GraphAutoma):\n    @worker(is_start=True)\n    async def receive_message(self, message: str):\n        print(f'- - - - - - Received message - - - - - -')\n        print(message)\n        print(f'- - - - - - End - - - - - -\\n')\n        return message\n    \n    @worker(dependencies=[\"receive_message\"])\n    async def reply_message_and_wait_reply(self, message: str):\n        print(f'- - - - - - Reply Message - - - - - -')\n        response = await llm.achat(\n            model=_model_name,\n            messages=[\n                Message.from_text(text=f\"You are a message assistant. Please reply to the following message.\", role=Role.SYSTEM),\n                Message.from_text(text=message, role=Role.USER),\n            ]\n        )\n        print(response.message.content)\n        print(f'- - - - - - End - - - - - -\\n')\n\n        if \"Bye!\" in message:  # if the message contains \"Bye!\", the reply is complete\n            return \n\n        # wait for reply            \n        event = Event(event_type=\"wait_for_reply\")\n        feedback: InteractionFeedback = self.interact_with_human(event)  # interrupt to wait for reply and resume when feedback is received\n        self.ferry_to(\"receive_message\", feedback.data)\n</pre> llm = VllmServerLlm(api_base=_api_base, api_key=_api_key, timeout=10)  class MessageAssistant(GraphAutoma):     @worker(is_start=True)     async def receive_message(self, message: str):         print(f'- - - - - - Received message - - - - - -')         print(message)         print(f'- - - - - - End - - - - - -\\n')         return message          @worker(dependencies=[\"receive_message\"])     async def reply_message_and_wait_reply(self, message: str):         print(f'- - - - - - Reply Message - - - - - -')         response = await llm.achat(             model=_model_name,             messages=[                 Message.from_text(text=f\"You are a message assistant. Please reply to the following message.\", role=Role.SYSTEM),                 Message.from_text(text=message, role=Role.USER),             ]         )         print(response.message.content)         print(f'- - - - - - End - - - - - -\\n')          if \"Bye!\" in message:  # if the message contains \"Bye!\", the reply is complete             return           # wait for reply                     event = Event(event_type=\"wait_for_reply\")         feedback: InteractionFeedback = self.interact_with_human(event)  # interrupt to wait for reply and resume when feedback is received         self.ferry_to(\"receive_message\", feedback.data) <p>Using <code>interact_with_human()</code> throws an event for which it is unknown how long it will take to process, at the same time, an <code>InteractionException</code> is thrown.</p> <p>The <code>InteractionException</code> contains two fields:</p> <ul> <li><code>Interactions</code>: A list of <code>Interaction</code>, each <code>Interaction</code> containing an <code>Interaction_id</code> and an <code>Event</code>.</li> <li><code>Snapshot</code>: a snapshot of the Automa's current state.</li> </ul> <p>Snapshots corresponding to the interaction can be persistently saved, and when needed in the future, the execution can be resumed.</p> In\u00a0[14]: Copied! <pre>import tempfile\nimport os\n\n# Use a temporary directory to achieve the purpose of persistent storage.\ntemp_dir = tempfile.TemporaryDirectory()\n\n# Use a dictionary to record the interaction id and the corresponding snapshot path.\ncache_dict = {}\n\n# deal with delayed interaction\nmessage_assistant = MessageAssistant()\ntry:\n    await message_assistant.arun(message=\"Hello, how are you?\")\nexcept InteractionException as e:\n    interaction_id = e.interactions[0].interaction_id\n    bytes_file = os.path.join(temp_dir.name, f\"message_assistant_{interaction_id}.bytes\")\n    version_file = os.path.join(temp_dir.name, f\"message_assistant_{interaction_id}.version\")\n    with open(bytes_file, \"wb\") as f:\n        f.write(e.snapshot.serialized_bytes)\n    with open(version_file, \"w\") as f:\n        f.write(e.snapshot.serialization_version)\n\n    cache_dict[\"A\"] = {\n        \"interaction_id\": interaction_id,\n        \"bytes_file\": bytes_file,\n        \"version_file\": version_file\n    }\n    print(f\"! ! ! State has been saved and can be resumed later. ! ! !\")\n</pre> import tempfile import os  # Use a temporary directory to achieve the purpose of persistent storage. temp_dir = tempfile.TemporaryDirectory()  # Use a dictionary to record the interaction id and the corresponding snapshot path. cache_dict = {}  # deal with delayed interaction message_assistant = MessageAssistant() try:     await message_assistant.arun(message=\"Hello, how are you?\") except InteractionException as e:     interaction_id = e.interactions[0].interaction_id     bytes_file = os.path.join(temp_dir.name, f\"message_assistant_{interaction_id}.bytes\")     version_file = os.path.join(temp_dir.name, f\"message_assistant_{interaction_id}.version\")     with open(bytes_file, \"wb\") as f:         f.write(e.snapshot.serialized_bytes)     with open(version_file, \"w\") as f:         f.write(e.snapshot.serialization_version)      cache_dict[\"A\"] = {         \"interaction_id\": interaction_id,         \"bytes_file\": bytes_file,         \"version_file\": version_file     }     print(f\"! ! ! State has been saved and can be resumed later. ! ! !\") <pre>- - - - - - Received message - - - - - -\nHello, how are you?\n- - - - - - End - - - - - -\n\n- - - - - - Reply Message - - - - - -\nHello! I'm functioning well, thank you for asking. I'm always excited to chat and help out! \ud83d\ude0a How can I assist you today?\n- - - - - - End - - - - - -\n\n! ! ! State has been saved and can be resumed later. ! ! !\n</pre> <p>Suppose after quite a long time, there was finally a reply.</p> In\u00a0[\u00a0]: Copied! <pre># build feedback\nuser = \"A\"\nreply_message = \"I really enjoy talking with you. Bye!\"\ninteraction_id = cache_dict[user][\"interaction_id\"]\nfeedback = InteractionFeedback(\n    interaction_id=interaction_id,\n    data=reply_message\n)\n\n# load snapshot\nbytes_file = cache_dict[user][\"bytes_file\"]\nversion_file = cache_dict[user][\"version_file\"]\nwith open(bytes_file, \"rb\") as f:\n    serialized_bytes = f.read()\nwith open(version_file, \"r\") as f:\n    serialization_version = f.read()\nsnapshot = Snapshot(\n    serialized_bytes=serialized_bytes, \n    serialization_version=serialization_version\n)\n\n# automa resumes from the snapshot\nmessage_assistant = MessageAssistant.load_from_snapshot(snapshot)\nawait message_assistant.arun(interaction_feedback=feedback)\n</pre> # build feedback user = \"A\" reply_message = \"I really enjoy talking with you. Bye!\" interaction_id = cache_dict[user][\"interaction_id\"] feedback = InteractionFeedback(     interaction_id=interaction_id,     data=reply_message )  # load snapshot bytes_file = cache_dict[user][\"bytes_file\"] version_file = cache_dict[user][\"version_file\"] with open(bytes_file, \"rb\") as f:     serialized_bytes = f.read() with open(version_file, \"r\") as f:     serialization_version = f.read() snapshot = Snapshot(     serialized_bytes=serialized_bytes,      serialization_version=serialization_version )  # automa resumes from the snapshot message_assistant = MessageAssistant.load_from_snapshot(snapshot) await message_assistant.arun(interaction_feedback=feedback) <pre>- - - - - - Reply Message - - - - - -\nHello! I'm functioning well, thank you for asking. I'm always excited to chat and help out! \ud83d\ude0a How can I assist you today?\n- - - - - - End - - - - - -\n\n- - - - - - Received message - - - - - -\nI really enjoy taking with you. Bye!\n- - - - - - End - - - - - -\n\n- - - - - - Reply Message - - - - - -\nThank you for enjoying our conversation! I'm glad I could help. Have a wonderful day, and take care! \ud83d\ude0a\u2728\n- - - - - - End - - - - - -\n\n</pre> <p>When facing a situation that requires feedback but the waiting time is uncertain, this mechanism saves the current state and re-enters when the right moment comes in the future. This not only enables the system to release resources that have been occupied for a long time, but also allows it to be awakened at an appropriate time.</p>"},{"location":"tutorials/items/core_mechanism/human_in_the_loop/#human-in-the-loop","title":"Human-in-the-loop\u00b6","text":"<p>Bridgic provides the capabilities of human-in-the-loop. Bridgic regards it occurring within an automa as the whole behavior that interacts with the outside world.</p>"},{"location":"tutorials/items/core_mechanism/human_in_the_loop/#interaction-scenarios","title":"Interaction Scenarios\u00b6","text":"<p>Let's understand this process through a few simple examples. Before we start, let's prepare the running environment.</p>"},{"location":"tutorials/items/core_mechanism/human_in_the_loop/#programming-assistant","title":"Programming assistant\u00b6","text":"<p>When developing a programming assistant, after it finishes writing a function, the assistant can run and verify it by itself. However, running a program is an action that will allocate system resources, and the user needs to determine whether to allow execution.</p> <p>Let's achieve it with Bridgic. The steps are as follows:</p> <ol> <li>Generate code based on user requirements.</li> <li>Ask the user if they allow you to perform the verification.</li> <li>Output result.</li> </ol>"},{"location":"tutorials/items/core_mechanism/human_in_the_loop/#counting-notifier","title":"Counting notifier\u00b6","text":"<p>But sometimes, it might only be necessary to throw an event type without expecting any feedback. For example, message notifications. At this point, we call the <code>post_event()</code> method and register the method <code>func(event: Event)</code> at the same time to achieve this process.</p> <p>For example, implement a counting notifier that counts from 1 to the number input by the user, and at the same time, the user sets which number to remind when it is counted to.</p>"},{"location":"tutorials/items/core_mechanism/human_in_the_loop/#message-assistant","title":"Message assistant\u00b6","text":"<p>When developing specific scenarios, sometimes after throwing an event, it is necessary to wait for its feedback. However, this feedback may take a very long time. If the system keeps waiting, there will be unnecessary waste of resources.</p> <p>Bridgic provides a powerful mechanism for interruption recovery in this situation. This enables the program to interrupt and save the current execution state when encountering such events, wait for a period of time, receive feedback, and then resume execution.</p> <p>Let's implement a message assistant that receives a message from user \"A\" and replies to it, but doesn't know how long it might have to wait for the reply.</p>"},{"location":"tutorials/items/core_mechanism/human_in_the_loop/#what-have-we-done","title":"What have we done\u00b6","text":"<p>No matter which form of human-in-the-loop it is, Bridgic provides flexible support.</p> <ul> <li><code>request_feedback_async</code>: Use when the event must return feedback before the program can proceed. The program blocks until feedback is received.</li> <li><code>post_event</code>: Use when you just want to notify or trigger an event without expecting any feedback. The main program continues immediately.</li> <li><code>interact_with_human</code>: Use when feedback is required but may arrive much later. The program is suspended and saved, and resumes only when feedback becomes available.</li> </ul>"},{"location":"tutorials/items/core_mechanism/modularity/","title":"Modularity","text":"In\u00a0[\u00a0]: Copied! <pre>import os\nfrom dotenv import load_dotenv\nload_dotenv()\n\n# Set the API base and key.\n_api_base = os.environ.get(\"VLLM_SERVER_API_BASE\")\n_api_key = os.environ.get(\"VLLM_SERVER_API_KEY\")\n_model_name = os.environ.get(\"VLLM_SERVER_MODEL_NAME\")\n\n# Import the necessary modules.\nfrom typing import List\nfrom pydantic import BaseModel, Field\nfrom bridgic.core.automa import GraphAutoma, worker\nfrom bridgic.core.model.types import Message, Role\nfrom bridgic.core.model.protocols import PydanticModel\nfrom bridgic.llms.vllm.vllm_server_llm import VllmServerLlm\n\nllm = VllmServerLlm(api_base=_api_base, api_key=_api_key, timeout=30)\n</pre> import os from dotenv import load_dotenv load_dotenv()  # Set the API base and key. _api_base = os.environ.get(\"VLLM_SERVER_API_BASE\") _api_key = os.environ.get(\"VLLM_SERVER_API_KEY\") _model_name = os.environ.get(\"VLLM_SERVER_MODEL_NAME\")  # Import the necessary modules. from typing import List from pydantic import BaseModel, Field from bridgic.core.automa import GraphAutoma, worker from bridgic.core.model.types import Message, Role from bridgic.core.model.protocols import PydanticModel from bridgic.llms.vllm.vllm_server_llm import VllmServerLlm  llm = VllmServerLlm(api_base=_api_base, api_key=_api_key, timeout=30) In\u00a0[\u00a0]: Copied! <pre>class Outline(BaseModel):\n    outline: List[str] = Field(description=\"The outline of the use input to write report\")\n\nclass OutlineWriter(GraphAutoma):\n    @worker(is_start=True)\n    async def pre_query(self, user_input: str):  # Receive the user's input and preprocess query\n        return user_input\n\n    @worker(dependencies=[\"pre_query\"], is_output=True)\n    async def topic_split(self, query: str):\n        response: Outline = await llm.astructured_output(\n            model=_model_name,\n            messages=[\n                Message.from_text(text=\"Write a sample report outline within 3 topics based on the user's input\", role=Role.SYSTEM),\n                Message.from_text(text=query, role=Role.USER,),\n            ],\n            constraint=PydanticModel(model=Outline)\n        )\n        return response\n</pre> class Outline(BaseModel):     outline: List[str] = Field(description=\"The outline of the use input to write report\")  class OutlineWriter(GraphAutoma):     @worker(is_start=True)     async def pre_query(self, user_input: str):  # Receive the user's input and preprocess query         return user_input      @worker(dependencies=[\"pre_query\"], is_output=True)     async def topic_split(self, query: str):         response: Outline = await llm.astructured_output(             model=_model_name,             messages=[                 Message.from_text(text=\"Write a sample report outline within 3 topics based on the user's input\", role=Role.SYSTEM),                 Message.from_text(text=query, role=Role.USER,),             ],             constraint=PydanticModel(model=Outline)         )         return response In\u00a0[\u00a0]: Copied! <pre>class ReportWriter(GraphAutoma):\n    @worker(dependencies=[\"outline_writer\"])\n    async def write_report(self, outline: Outline):\n        outline_str = \"\\n\".join(outline.outline)\n        print(f'- - - - - Outline - - - - -')\n        print(outline_str)\n        print(f'- - - - - End - - - - -\\n')\n        \n        response = await llm.achat(\n            model=_model_name,\n            messages=[\n                Message.from_text(text=\"write a sample report based on the user's input and strictly follow the outline\", role=Role.SYSTEM),\n                Message.from_text(text=f\"{outline_str}.\", role=Role.USER),\n            ],\n        )\n        print(f'- - - - - Report - - - - -')\n        print(response.message.content)\n        print(f'- - - - - End - - - - -\\n')\n        return response.message.content\n\nreport_writer = ReportWriter()\nreport_writer.add_worker(\n    key=\"outline_writer\",\n    worker=OutlineWriter(),\n    is_start=True\n)\n</pre> class ReportWriter(GraphAutoma):     @worker(dependencies=[\"outline_writer\"])     async def write_report(self, outline: Outline):         outline_str = \"\\n\".join(outline.outline)         print(f'- - - - - Outline - - - - -')         print(outline_str)         print(f'- - - - - End - - - - -\\n')                  response = await llm.achat(             model=_model_name,             messages=[                 Message.from_text(text=\"write a sample report based on the user's input and strictly follow the outline\", role=Role.SYSTEM),                 Message.from_text(text=f\"{outline_str}.\", role=Role.USER),             ],         )         print(f'- - - - - Report - - - - -')         print(response.message.content)         print(f'- - - - - End - - - - -\\n')         return response.message.content  report_writer = ReportWriter() report_writer.add_worker(     key=\"outline_writer\",     worker=OutlineWriter(),     is_start=True ) <p>Now, let's run it!</p> In\u00a0[19]: Copied! <pre>await report_writer.arun(user_input=\"What is an agent?\")\n</pre> await report_writer.arun(user_input=\"What is an agent?\") <pre>- - - - - Outline - - - - -\nDefinition of an Agent\nTypes of Agents in Different Contexts\nRole and Function of Agents in AI and Automation\n- - - - - End - - - - -\n\n- - - - - Report - - - - -\n**Report: Definition of an Agent, Types of Agents in Different Contexts, and Role and Function of Agents in AI and Automation**\n\n---\n\n**1. Definition of an Agent**\n\nAn *agent* is an autonomous entity that perceives its environment through sensors and acts upon that environment using actuators to achieve specific goals. In a broad sense, an agent is capable of making decisions based on its internal state and external inputs, adapting its behavior over time in response to changing conditions. The concept of an agent is foundational in artificial intelligence (AI), robotics, and automated systems. It operates independently or in coordination with other agents, following predefined rules, learning from experience, or using reasoning mechanisms to perform tasks efficiently.\n\nIn computational terms, an agent typically consists of:\n- **Perception**: Sensing and interpreting environmental inputs.\n- **Reasoning/Decision-making**: Processing information to determine the best course of action.\n- **Action**: Executing decisions through physical or digital means.\n\nAgents can be simple (e.g., a thermostat adjusting temperature) or complex (e.g., AI-driven trading bots in financial markets).\n\n---\n\n**2. Types of Agents in Different Contexts**\n\nAgents vary significantly depending on the domain or context in which they operate. Below are key types categorized by application areas:\n\n**a. In Artificial Intelligence (AI):**\n- **Simple Reflex Agents**: Make decisions based solely on current percept. Example: A thermostat that turns on the heater when the room temperature drops below a threshold.\n- **Model-Based Reflex Agents**: Use an internal model of the world to make decisions. Example: A self-driving car that predicts traffic patterns based on historical data.\n- **Goal-Based Agents**: Act to achieve specific goals. Example: A virtual assistant like Siri or Alexa that performs tasks such as setting reminders or sending messages.\n- **Utility-Based Agents**: Choose actions that maximize expected utility. Example: An AI recommending a product based on user preferences and potential profit.\n- **Learning Agents**: Improve performance over time through experience. Example: A recommendation engine that learns user behavior and adjusts suggestions.\n\n**b. In Robotics:**\n- **Autonomous Robots**: Operate independently in dynamic environments (e.g., drones, warehouse robots).\n- **Human-Robot Collaborative Agents**: Work alongside humans (e.g., robotic arms in manufacturing).\n- **Mobile Agents**: Move through physical environments to perform tasks (e.g., delivery robots).\n\n**c. In Software and Business Automation:**\n- **Business Process Agents**: Automate routine tasks like invoice processing or customer service responses.\n- **Workflow Agents**: Coordinate multiple tasks within a system (e.g., workflow management in CRM software).\n- **Customer Service Agents**: Chatbots that respond to user queries in real time.\n\n**d. In Distributed Systems:**\n- **Multi-Agent Systems (MAS)**: Groups of agents that interact to solve complex problems (e.g., traffic management, supply chain optimization).\n- **Swarm Agents**: Simulate collective behavior (e.g., ant colonies, drone swarms).\n\n---\n\n**3. Role and Function of Agents in AI and Automation**\n\nAgents play a pivotal role in advancing AI and automation by enabling intelligent, adaptive, and scalable systems. Their key functions include:\n\n- **Autonomy and Decision-Making**: Agents operate independently, making real-time decisions without constant human intervention, which is essential in dynamic environments such as autonomous vehicles or smart homes.\n\n- **Adaptability and Learning**: Through machine learning and reinforcement learning, agents evolve their strategies based on feedback, improving accuracy and efficiency over time.\n\n- **Efficiency and Scalability**: Agents can perform repetitive or time-consuming tasks quickly and consistently, reducing human workload and operational costs. For example, AI agents in customer service reduce response times and increase service availability.\n\n- **Interoperability and Integration**: Agents can communicate and collaborate across different platforms and systems, enabling seamless integration in enterprise environments (e.g., connecting ERP, CRM, and supply chain systems).\n\n- **Problem Solving and Optimization**: In complex domains like logistics or energy management, agents use algorithms to find optimal solutions, such as route planning or load balancing.\n\n- **Human-AI Collaboration**: Agents serve as intelligent interfaces between humans and machines, enhancing user experience by providing personalized, context-aware assistance.\n\nIn summary, agents are central to the development of intelligent systems that can perceive, reason, act, and learn\u2014making them indispensable in modern AI and automation ecosystems.\n\n---\n\n**Conclusion**\n\nThe concept of an agent bridges the gap between passive systems and intelligent, responsive automation. From simple rule-based tools to sophisticated AI-driven entities, agents are transforming how systems interact with their environments. As AI and automation continue to evolve, the design, deployment, and coordination of agents will remain a critical focus area across industries, driving innovation, efficiency, and human-machine collaboration.\n\n---  \n*End of Report*\n- - - - - End - - - - -\n\n</pre> <p>Great! We successfully reused <code>OutlineWriter</code> in <code>ReportWriter</code>.</p>"},{"location":"tutorials/items/core_mechanism/modularity/#modularity","title":"Modularity\u00b6","text":"<p>During the development process, we may have already had some individual automas. Now, we want to combine them to create a more powerful automa. An automa can also be added as a Worker to another Automa to achieve reuse.</p> <p>Let's understand it with a sample case.</p>"},{"location":"tutorials/items/core_mechanism/modularity/#report-writer","title":"Report Writer\u00b6","text":"<p>The typical process of report writing usually involves drafting an outline based on user input and then writing the report according to the outline. Now, let's first write an Automa for generating an outline, and then nest it within the Automa for writing the report.</p>"},{"location":"tutorials/items/core_mechanism/modularity/#1-initialize","title":"1. Initialize\u00b6","text":"<p>Initialize the runtime environment.</p>"},{"location":"tutorials/items/core_mechanism/modularity/#2-outline-automa","title":"2. Outline Automa\u00b6","text":"<p>Outline Automa drafts an outline based on user input. Output an <code>Outline</code> object for subsequent processing.</p>"},{"location":"tutorials/items/core_mechanism/modularity/#3-write-automa","title":"3. Write Automa\u00b6","text":"<p>We can reuse the <code>OutlineWriter</code> in <code>ReportWriter</code>.</p>"},{"location":"tutorials/items/core_mechanism/modularity/#what-have-we-done","title":"What have we done\u00b6","text":"<p>we added an automa as a <code>Worker</code> to another <code>Automa</code> to achieve nested reuse. There are several points that need special attention during the reuse process:</p> <ol> <li>Worker Dependency: A <code>Worker</code> in an <code>Automa</code> can only depend on other workers within the same <code>Automa</code>, and cannot depend across different <code>Automa</code>.</li> <li>Routing: A <code>Worker</code> in an <code>Automa</code> can only route to other workers within the same <code>Automa</code>, and cannot route across different <code>Automa</code>.</li> <li>Human-in-the-loop: When a worker inside throws an event, it hands over this event to the automa agent for handling. At this point, the event handling functions of nested automa need to be registered to the outermost Automa.</li> </ol>"},{"location":"tutorials/items/core_mechanism/parameter_binding/","title":"Parameter Binding","text":"In\u00a0[\u00a0]: Copied! <pre># Get the environment variables.\nimport os\nfrom dotenv import load_dotenv\nload_dotenv()\n\n_api_base = os.environ.get(\"VLLM_SERVER_API_BASE\")\n_api_key = os.environ.get(\"VLLM_SERVER_API_KEY\")\n_model_name = os.environ.get(\"VLLM_SERVER_MODEL_NAME\")\n\n# Import the necessary packages.\nfrom pydantic import BaseModel, Field\nfrom typing import List, Dict, Tuple\nfrom bridgic.core.automa import GraphAutoma, worker\nfrom bridgic.core.automa.args import ArgsMappingRule\nfrom bridgic.core.model.types import Message, Role\nfrom bridgic.core.model.protocols import PydanticModel\nfrom bridgic.llms.vllm.vllm_server_llm import VllmServerLlm\n\nllm = VllmServerLlm(  # the llm instance\n    api_base=_api_base,\n    api_key=_api_key,\n    timeout=5,\n)\n</pre> # Get the environment variables. import os from dotenv import load_dotenv load_dotenv()  _api_base = os.environ.get(\"VLLM_SERVER_API_BASE\") _api_key = os.environ.get(\"VLLM_SERVER_API_KEY\") _model_name = os.environ.get(\"VLLM_SERVER_MODEL_NAME\")  # Import the necessary packages. from pydantic import BaseModel, Field from typing import List, Dict, Tuple from bridgic.core.automa import GraphAutoma, worker from bridgic.core.automa.args import ArgsMappingRule from bridgic.core.model.types import Message, Role from bridgic.core.model.protocols import PydanticModel from bridgic.llms.vllm.vllm_server_llm import VllmServerLlm  llm = VllmServerLlm(  # the llm instance     api_base=_api_base,     api_key=_api_key,     timeout=5, ) <p>Now let's implement this query expansion. We assume that the user query we receive is in JSON format. It contains three keys:</p> <ol> <li><code>id</code>: A string that indicates who inputs the query.</li> <li><code>query</code>: A string in the form of <code>Q: user_query</code> representing the question input by the user.</li> <li><code>date</code>: The time when the user entered the query.</li> </ol> In\u00a0[3]: Copied! <pre>query_obj = {\n    \"id\": \"user_1\",\n    \"query\": \"Q: What new developments have there been in RAG in the past year?\",\n    \"date\": \"2025-09-30\"\n}\n</pre> query_obj = {     \"id\": \"user_1\",     \"query\": \"Q: What new developments have there been in RAG in the past year?\",     \"date\": \"2025-09-30\" } <p>Furthermore, we define that when the model completes entity extraction and query expansion, it returns the result in a Pydantic data structure.</p> In\u00a0[5]: Copied! <pre>class EntityList(BaseModel):  # The expected format of the model output in the extract_entity worker\n    entities: List[str] = Field(description=\"All entities in the input.\")\n\nclass QueryList(BaseModel):  # The expected format of the model output in the expand_query worker\n    queries: List[str] = Field(description=\"All queries in the input.\")\n</pre> class EntityList(BaseModel):  # The expected format of the model output in the extract_entity worker     entities: List[str] = Field(description=\"All entities in the input.\")  class QueryList(BaseModel):  # The expected format of the model output in the expand_query worker     queries: List[str] = Field(description=\"All queries in the input.\") <p>Next, let's complete the three steps of query expansion to achieve our goal:</p> In\u00a0[6]: Copied! <pre>class QueryExpansion(GraphAutoma):\n    @worker(is_start=True)\n    async def pre_query(self, query_obj: Dict):  # Receive the user's input and preprocess query\n        query = query_obj[\"query\"].split(\":\")[1].strip()  \n        return query\n\n    @worker(is_start=True)\n    async def pre_date(self, query_obj: Dict):  # Receive the user's input and preprocess date\n        date = query_obj[\"date\"]\n        return date\n\n    @worker(dependencies=[\"pre_query\", \"pre_date\"], args_mapping_rule=ArgsMappingRule.AS_IS)\n    async def extract_entity(self, query: str, date: str):  # Extract the entity information from the question, get entity information.\n        response: EntityList = await llm.astructured_output(  \n            model=_model_name,\n            constraint=PydanticModel(model=EntityList),\n            messages=[\n                Message.from_text(text=\"extract the entity information from the given query\", role=Role.SYSTEM),\n                Message.from_text(text=query, role=Role.USER,),\n            ]\n        )\n        return query, response.entities, date\n\n    @worker(dependencies=[\"extract_entity\"], is_output=True, args_mapping_rule=ArgsMappingRule.AS_IS)\n    async def expand_query(self, query_meta: Tuple[str, List[str]]):  # Expand and obtain multiple queries.\n        query, entities, date = query_meta\n        task_input = f\"Query: {query}\\nEntities: {entities}\"\n        response: str = await llm.astructured_output(\n            model=_model_name,\n            constraint=PydanticModel(model=QueryList),\n            messages=[\n                Message.from_text(text=f\"Centered around the given entities and given date {date}, expand the query to obtain multiple queries\", role=Role.SYSTEM),\n                Message.from_text(text=task_input, role=Role.USER,),\n            ]\n        )\n        return response.queries\n</pre> class QueryExpansion(GraphAutoma):     @worker(is_start=True)     async def pre_query(self, query_obj: Dict):  # Receive the user's input and preprocess query         query = query_obj[\"query\"].split(\":\")[1].strip()           return query      @worker(is_start=True)     async def pre_date(self, query_obj: Dict):  # Receive the user's input and preprocess date         date = query_obj[\"date\"]         return date      @worker(dependencies=[\"pre_query\", \"pre_date\"], args_mapping_rule=ArgsMappingRule.AS_IS)     async def extract_entity(self, query: str, date: str):  # Extract the entity information from the question, get entity information.         response: EntityList = await llm.astructured_output(               model=_model_name,             constraint=PydanticModel(model=EntityList),             messages=[                 Message.from_text(text=\"extract the entity information from the given query\", role=Role.SYSTEM),                 Message.from_text(text=query, role=Role.USER,),             ]         )         return query, response.entities, date      @worker(dependencies=[\"extract_entity\"], is_output=True, args_mapping_rule=ArgsMappingRule.AS_IS)     async def expand_query(self, query_meta: Tuple[str, List[str]]):  # Expand and obtain multiple queries.         query, entities, date = query_meta         task_input = f\"Query: {query}\\nEntities: {entities}\"         response: str = await llm.astructured_output(             model=_model_name,             constraint=PydanticModel(model=QueryList),             messages=[                 Message.from_text(text=f\"Centered around the given entities and given date {date}, expand the query to obtain multiple queries\", role=Role.SYSTEM),                 Message.from_text(text=task_input, role=Role.USER,),             ]         )         return response.queries <p>Let's run it!</p> In\u00a0[7]: Copied! <pre>query_expansion = QueryExpansion()\nawait query_expansion.arun(query_obj)\n</pre> query_expansion = QueryExpansion() await query_expansion.arun(query_obj) Out[7]: <pre>['What are the latest advancements in Retrieval-Augmented Generation (RAG) technology as of 2025?',\n 'What new developments have emerged in RAG systems over the past 12 months?',\n 'How have RAG implementations evolved in the last year?',\n 'What innovations in RAG have been introduced between 2024 and 2025?',\n 'What are the key breakthroughs in RAG technology in 2025?',\n 'What new features or improvements have been added to RAG models in the past year?',\n 'How has the performance of RAG systems improved in the last 12 months?',\n 'What are the most recent trends and developments in RAG research and deployment?',\n 'What new techniques have been introduced in RAG to improve accuracy and efficiency in 2025?',\n 'What are the major updates in RAG frameworks and tools from 2024 to 2025?']</pre> <p>Great! We have successfully completed the small module for query expansion.</p> In\u00a0[\u00a0]: Copied! <pre>@worker(dependencies=[\"extract_entity\"], is_output=True, args_mapping_rule=ArgsMappingRule.AS_IS)\nasync def expand_query(self, query_meta: Tuple[str, List[str]]):  # Expand and obtain multiple queries.\n    query, entities, date = query_meta\n    ...\n</pre> @worker(dependencies=[\"extract_entity\"], is_output=True, args_mapping_rule=ArgsMappingRule.AS_IS) async def expand_query(self, query_meta: Tuple[str, List[str]]):  # Expand and obtain multiple queries.     query, entities, date = query_meta     ... <p>This operation requires knowing what the parameters of <code>query_meta</code> as a whole contain, which might seem inconvenient. Could we complete the unpacking operation and fill in the corresponding parameters when returning? At this point, the <code>UNPACK</code> mode comes in handy.</p> <p>Let's modify the <code>expand_query</code> in the above example and add some print messages.</p> In\u00a0[8]: Copied! <pre>class QueryExpansion(GraphAutoma):\n    @worker(is_start=True)\n    async def pre_query(self, query_obj: Dict):  # Receive the user's input and preprocess query\n        query = query_obj[\"query\"].split(\":\")[1].strip()  \n        return query\n\n    @worker(is_start=True)\n    async def pre_date(self, query_obj: Dict):  # Receive the user's input and preprocess date\n        date = query_obj[\"date\"]\n        return date\n\n    @worker(dependencies=[\"pre_query\", \"pre_date\"], args_mapping_rule=ArgsMappingRule.AS_IS)\n    async def extract_entity(self, query: str, date: str):  # Extract the entity information from the question, get entity information.\n        response: EntityList = await llm.astructured_output(  \n            model=_model_name,\n            constraint=PydanticModel(model=EntityList),\n            messages=[\n                Message.from_text(text=\"extract the entity information from the given query\", role=Role.SYSTEM),\n                Message.from_text(text=query, role=Role.USER,),\n            ]\n        )\n        return query, response.entities, date\n\n    @worker(dependencies=[\"extract_entity\"], is_output=True, args_mapping_rule=ArgsMappingRule.UNPACK)\n    async def expand_query(self, query: str, entities: List[str], date: str):  # Expand and obtain multiple queries.\n        print(f\"query: {query}, entities: {entities}, date: {date}\")\n        task_input = f\"Query: {query}\\nEntities: {entities}\"\n        response: str = await llm.astructured_output(\n            model=_model_name,\n            constraint=PydanticModel(model=QueryList),\n            messages=[\n                Message.from_text(text=f\"Centered around the given entities and given date {date}, expand the query to obtain multiple queries\", role=Role.SYSTEM),\n                Message.from_text(text=task_input, role=Role.USER,),\n            ]\n        )\n        return response.queries\n</pre> class QueryExpansion(GraphAutoma):     @worker(is_start=True)     async def pre_query(self, query_obj: Dict):  # Receive the user's input and preprocess query         query = query_obj[\"query\"].split(\":\")[1].strip()           return query      @worker(is_start=True)     async def pre_date(self, query_obj: Dict):  # Receive the user's input and preprocess date         date = query_obj[\"date\"]         return date      @worker(dependencies=[\"pre_query\", \"pre_date\"], args_mapping_rule=ArgsMappingRule.AS_IS)     async def extract_entity(self, query: str, date: str):  # Extract the entity information from the question, get entity information.         response: EntityList = await llm.astructured_output(               model=_model_name,             constraint=PydanticModel(model=EntityList),             messages=[                 Message.from_text(text=\"extract the entity information from the given query\", role=Role.SYSTEM),                 Message.from_text(text=query, role=Role.USER,),             ]         )         return query, response.entities, date      @worker(dependencies=[\"extract_entity\"], is_output=True, args_mapping_rule=ArgsMappingRule.UNPACK)     async def expand_query(self, query: str, entities: List[str], date: str):  # Expand and obtain multiple queries.         print(f\"query: {query}, entities: {entities}, date: {date}\")         task_input = f\"Query: {query}\\nEntities: {entities}\"         response: str = await llm.astructured_output(             model=_model_name,             constraint=PydanticModel(model=QueryList),             messages=[                 Message.from_text(text=f\"Centered around the given entities and given date {date}, expand the query to obtain multiple queries\", role=Role.SYSTEM),                 Message.from_text(text=task_input, role=Role.USER,),             ]         )         return response.queries <p>Let's run it!</p> In\u00a0[9]: Copied! <pre>query_expansion = QueryExpansion()\nawait query_expansion.arun(query_obj)\n</pre> query_expansion = QueryExpansion() await query_expansion.arun(query_obj) <pre>query: What new developments have there been in RAG in the past year?, entities: ['RAG', 'new developments', 'past year'], date: 2025-09-30\n</pre> Out[9]: <pre>['What are the latest advancements in Retrieval-Augmented Generation (RAG) technologies as of 2025?',\n 'What new developments have emerged in RAG systems over the past 12 months?',\n 'How has RAG evolved in the last year with regard to accuracy, efficiency, and scalability?',\n 'What are the key innovations in RAG that have been introduced between 2024 and 2025?',\n 'What new tools and frameworks have been launched for RAG implementation in the past year?',\n 'What recent breakthroughs in RAG have improved context handling and retrieval precision?',\n 'How have large language models integrated with RAG in the past year to enhance performance?',\n 'What are the most significant updates in RAG-based applications from 2024 to 2025?',\n 'What new techniques in RAG have been proposed to reduce hallucinations and improve factual consistency?',\n 'How have RAG solutions adapted to real-time data retrieval and dynamic content updates in the past year?']</pre> <p>Great! All the parameters were unpacked and accepted. It can be seen that the <code>unpack</code> mode makes our task flow clearer!</p> <p>However, it should be noted that the UNPACK mechanism requires that the current worker can only directly depend on one worker; otherwise, the results of multiple workers will be confused when unpacking!</p> In\u00a0[10]: Copied! <pre>class QueryExpansion(GraphAutoma):\n    @worker(is_start=True)\n    async def pre_query(self, query_obj: Dict):  # Receive the user's input and preprocess query\n        query = query_obj[\"query\"].split(\":\")[1].strip()  \n        return query\n\n    @worker(is_start=True)\n    async def pre_date(self, query_obj: Dict):  # Receive the user's input and preprocess date\n        date = query_obj[\"date\"]\n        return date\n\n    @worker(dependencies=[\"pre_query\", \"pre_date\"], args_mapping_rule=ArgsMappingRule.MERGE)\n    async def extract_entity(self, query_meta: Tuple[str, str]):  # Extract the entity information from the question, get entity information.\n        print(f\"query_meta: {query_meta}\")\n        query, date = query_meta\n        response: EntityList = await llm.astructured_output(  \n            model=_model_name,\n            constraint=PydanticModel(model=EntityList),\n            messages=[\n                Message.from_text(text=\"extract the entity information from the given query\", role=Role.SYSTEM),\n                Message.from_text(text=query, role=Role.USER,),\n            ]\n        )\n        return query, response.entities, date\n\n    @worker(dependencies=[\"extract_entity\"], is_output=True, args_mapping_rule=ArgsMappingRule.UNPACK)\n    async def expand_query(self, query: str, entities: List[str], date: str):  # Expand and obtain multiple queries.\n        task_input = f\"Query: {query}\\nEntities: {entities}\"\n        response: str = await llm.astructured_output(\n            model=_model_name,\n            constraint=PydanticModel(model=QueryList),\n            messages=[\n                Message.from_text(text=f\"Centered around the given entities and given date {date}, expand the query to obtain multiple queries\", role=Role.SYSTEM),\n                Message.from_text(text=task_input, role=Role.USER,),\n            ]\n        )\n        return response.queries\n</pre> class QueryExpansion(GraphAutoma):     @worker(is_start=True)     async def pre_query(self, query_obj: Dict):  # Receive the user's input and preprocess query         query = query_obj[\"query\"].split(\":\")[1].strip()           return query      @worker(is_start=True)     async def pre_date(self, query_obj: Dict):  # Receive the user's input and preprocess date         date = query_obj[\"date\"]         return date      @worker(dependencies=[\"pre_query\", \"pre_date\"], args_mapping_rule=ArgsMappingRule.MERGE)     async def extract_entity(self, query_meta: Tuple[str, str]):  # Extract the entity information from the question, get entity information.         print(f\"query_meta: {query_meta}\")         query, date = query_meta         response: EntityList = await llm.astructured_output(               model=_model_name,             constraint=PydanticModel(model=EntityList),             messages=[                 Message.from_text(text=\"extract the entity information from the given query\", role=Role.SYSTEM),                 Message.from_text(text=query, role=Role.USER,),             ]         )         return query, response.entities, date      @worker(dependencies=[\"extract_entity\"], is_output=True, args_mapping_rule=ArgsMappingRule.UNPACK)     async def expand_query(self, query: str, entities: List[str], date: str):  # Expand and obtain multiple queries.         task_input = f\"Query: {query}\\nEntities: {entities}\"         response: str = await llm.astructured_output(             model=_model_name,             constraint=PydanticModel(model=QueryList),             messages=[                 Message.from_text(text=f\"Centered around the given entities and given date {date}, expand the query to obtain multiple queries\", role=Role.SYSTEM),                 Message.from_text(text=task_input, role=Role.USER,),             ]         )         return response.queries <p>Let's run it!</p> In\u00a0[11]: Copied! <pre>query_expansion = QueryExpansion()\nawait query_expansion.arun(query_obj)\n</pre> query_expansion = QueryExpansion() await query_expansion.arun(query_obj) <pre>query_meta: ['What new developments have there been in RAG in the past year?', '2025-09-30']\n</pre> Out[11]: <pre>['What are the latest advancements in Retrieval-Augmented Generation (RAG) technology as of 2025?',\n 'What new developments have emerged in RAG systems over the past 12 months?',\n 'How has RAG evolved in the last year with recent innovations in AI and NLP?',\n 'What are the key updates and breakthroughs in RAG models from 2024 to 2025?',\n 'What new features or improvements have been introduced in RAG implementations in the past year?',\n 'What are the most significant RAG developments reported in 2025?',\n 'How have retrieval and generation components in RAG been improved in the last year?',\n 'What are the recent trends and new developments in RAG applications from 2024 to 2025?',\n 'What innovations in RAG have been introduced by leading AI companies in the past year?',\n 'What new challenges and solutions have emerged in RAG research over the last 12 months?']</pre> <p>Great! The results that <code>extract_entity</code> depends on from the workers have all been collected into a list and passed to its parameters.</p> In\u00a0[\u00a0]: Copied! <pre># import the From marker\nfrom bridgic.core.automa import From\n\n@worker(dependencies=[\"extract_entity\"], is_output=True, args_mapping_rule=ArgsMappingRule.UNPACK)\nasync def expand_query(self, query_meta: Tuple[str, str], date: str = From(\"pre_date\", \"2025-01-01\")):  # Expand and obtain multiple queries.\n    ...\n</pre> # import the From marker from bridgic.core.automa import From  @worker(dependencies=[\"extract_entity\"], is_output=True, args_mapping_rule=ArgsMappingRule.UNPACK) async def expand_query(self, query_meta: Tuple[str, str], date: str = From(\"pre_date\", \"2025-01-01\")):  # Expand and obtain multiple queries.     ... <p><code>date: str = From(\"pre_date\", \"2025-01-01\")</code> indicates that the value of <code>date</code> will be assigned based on the result of the <code>pre_date</code> worker. If the result from this worker has not yet been produced, the default value will be used instead.</p> <p>If the pre_date worker does not exist, or if the pre_date worker has not yet produced a result, and there is no default value, an error will be reported: AutomaDataInjectionError.</p> <p>Let's modify the above example and add some print messages.</p> In\u00a0[13]: Copied! <pre>class QueryExpansion(GraphAutoma):\n    @worker(is_start=True)\n    async def pre_query(self, query_obj: Dict):  # Receive the user's input and preprocess query\n        query = query_obj[\"query\"].split(\":\")[1].strip()  \n        return query\n\n    @worker(is_start=True)\n    async def pre_date(self, query_obj: Dict):  # Receive the user's input and preprocess date\n        date = query_obj[\"date\"]\n        return date\n\n    @worker(dependencies=[\"pre_query\"], args_mapping_rule=ArgsMappingRule.AS_IS)\n    async def extract_entity(self, query: str):  # Extract the entity information from the question, get entity information.\n        response: EntityList = await llm.astructured_output(  \n            model=_model_name,\n            constraint=PydanticModel(model=EntityList),\n            messages=[\n                Message.from_text(text=\"extract the entity information from the given query\", role=Role.SYSTEM),\n                Message.from_text(text=query, role=Role.USER,),\n            ]\n        )\n        return query, response.entities\n\n    @worker(dependencies=[\"extract_entity\"], is_output=True, args_mapping_rule=ArgsMappingRule.UNPACK)\n    async def expand_query(self, query: str, entities: List[str], date: str = From(\"pre_date\", \"2025-01-01\")):  # Expand and obtain multiple queries.\n        print(f\"query: {query}, entities: {entities}, date: {date}\")\n        task_input = f\"Query: {query}\\nEntities: {entities}\"\n        response: str = await llm.astructured_output(\n            model=_model_name,\n            constraint=PydanticModel(model=QueryList),\n            messages=[\n                Message.from_text(text=f\"Centered around the given entities and given date {date}, expand the query to obtain multiple queries\", role=Role.SYSTEM),\n                Message.from_text(text=task_input, role=Role.USER,),\n            ]\n        )\n        return response.queries\n</pre> class QueryExpansion(GraphAutoma):     @worker(is_start=True)     async def pre_query(self, query_obj: Dict):  # Receive the user's input and preprocess query         query = query_obj[\"query\"].split(\":\")[1].strip()           return query      @worker(is_start=True)     async def pre_date(self, query_obj: Dict):  # Receive the user's input and preprocess date         date = query_obj[\"date\"]         return date      @worker(dependencies=[\"pre_query\"], args_mapping_rule=ArgsMappingRule.AS_IS)     async def extract_entity(self, query: str):  # Extract the entity information from the question, get entity information.         response: EntityList = await llm.astructured_output(               model=_model_name,             constraint=PydanticModel(model=EntityList),             messages=[                 Message.from_text(text=\"extract the entity information from the given query\", role=Role.SYSTEM),                 Message.from_text(text=query, role=Role.USER,),             ]         )         return query, response.entities      @worker(dependencies=[\"extract_entity\"], is_output=True, args_mapping_rule=ArgsMappingRule.UNPACK)     async def expand_query(self, query: str, entities: List[str], date: str = From(\"pre_date\", \"2025-01-01\")):  # Expand and obtain multiple queries.         print(f\"query: {query}, entities: {entities}, date: {date}\")         task_input = f\"Query: {query}\\nEntities: {entities}\"         response: str = await llm.astructured_output(             model=_model_name,             constraint=PydanticModel(model=QueryList),             messages=[                 Message.from_text(text=f\"Centered around the given entities and given date {date}, expand the query to obtain multiple queries\", role=Role.SYSTEM),                 Message.from_text(text=task_input, role=Role.USER,),             ]         )         return response.queries <p>Let's run it!</p> In\u00a0[14]: Copied! <pre>query_expansion = QueryExpansion()\nawait query_expansion.arun(query_obj)\n</pre> query_expansion = QueryExpansion() await query_expansion.arun(query_obj) <pre>query: What new developments have there been in RAG in the past year?, entities: ['RAG', 'new developments', 'past year'], date: 2025-09-30\n</pre> Out[14]: <pre>['What are the latest advancements in Retrieval-Augmented Generation (RAG) technology as of 2025?',\n 'What new developments have emerged in RAG systems over the past 12 months?',\n 'How have RAG implementations evolved in the last year in terms of performance and scalability?',\n 'What innovations in RAG have been introduced in 2025 that improve accuracy and context handling?',\n 'What are the key breakthroughs in RAG research and deployment from 2024 to 2025?',\n 'What new tools and frameworks have been released for RAG in the past year?',\n 'How have privacy and security features improved in RAG systems over the last year?',\n 'What are the most notable RAG developments in enterprise AI applications from 2024 to 2025?',\n \"What recent improvements have been made to RAG's ability to handle long-context inputs?\",\n 'How has the integration of RAG with large language models evolved in the past year?']</pre> <p>I have modified <code>extract_entity</code>, and now it only accepts <code>query</code>, making its functionality more pure. Also, in <code>expand_query</code>, I have correctly obtained the <code>date</code>.</p> <pre>@worker(dependencies=[\"extract_entity\"], is_output=True args_mapping_rule=ArgsMappingRule.UNPACK)\nasync def expand_query(\n    self, \n    query: str, \n    entities: List[str], \n+    query_obj: Dict,  # The input of the entire Automa\n    date: str = From(\"pre_date\", \"2025-01-01\"), \n):  # Expand and obtain multiple queries.\n    ...\n</pre> <p>Let's modify the above example and add some print messages.</p> In\u00a0[15]: Copied! <pre>class QueryExpansion(GraphAutoma):\n    @worker(is_start=True)\n    async def pre_query(self, query_obj: Dict):  # Receive the user's input and preprocess query\n        query = query_obj[\"query\"].split(\":\")[1].strip()  \n        return query\n\n    @worker(is_start=True)\n    async def pre_date(self, query_obj: Dict):  # Receive the user's input and preprocess date\n        date = query_obj[\"date\"]\n        return date\n\n    @worker(dependencies=[\"pre_query\"], args_mapping_rule=ArgsMappingRule.AS_IS)\n    async def extract_entity(self, query: str):  # Extract the entity information from the question, get entity information.\n        response: EntityList = await llm.astructured_output(  \n            model=_model_name,\n            constraint=PydanticModel(model=EntityList),\n            messages=[\n                Message.from_text(text=\"extract the entity information from the given query\", role=Role.SYSTEM),\n                Message.from_text(text=query, role=Role.USER,),\n            ]\n        )\n        return query, response.entities\n\n    @worker(dependencies=[\"extract_entity\"], is_output=True, args_mapping_rule=ArgsMappingRule.UNPACK)\n    async def expand_query(self, query: str, entities: List[str], query_obj: Dict, date: str = From(\"pre_date\", \"2025-01-01\")):  # Expand and obtain multiple queries.\n        print(f\"query: {query}, entities: {entities}, date: {date}, query_obj: {query_obj}\")\n        task_input = f\"Query: {query}\\nEntities: {entities}\"\n        response: str = await llm.astructured_output(\n            model=_model_name,\n            constraint=PydanticModel(model=QueryList),\n            messages=[\n                Message.from_text(text=f\"Centered around the given entities and given date {date}, expand the query to obtain multiple queries\", role=Role.SYSTEM),\n                Message.from_text(text=task_input, role=Role.USER,),\n            ]\n        )\n        return {\"id\": query_obj[\"id\"], \"queries\": response.queries}\n</pre> class QueryExpansion(GraphAutoma):     @worker(is_start=True)     async def pre_query(self, query_obj: Dict):  # Receive the user's input and preprocess query         query = query_obj[\"query\"].split(\":\")[1].strip()           return query      @worker(is_start=True)     async def pre_date(self, query_obj: Dict):  # Receive the user's input and preprocess date         date = query_obj[\"date\"]         return date      @worker(dependencies=[\"pre_query\"], args_mapping_rule=ArgsMappingRule.AS_IS)     async def extract_entity(self, query: str):  # Extract the entity information from the question, get entity information.         response: EntityList = await llm.astructured_output(               model=_model_name,             constraint=PydanticModel(model=EntityList),             messages=[                 Message.from_text(text=\"extract the entity information from the given query\", role=Role.SYSTEM),                 Message.from_text(text=query, role=Role.USER,),             ]         )         return query, response.entities      @worker(dependencies=[\"extract_entity\"], is_output=True, args_mapping_rule=ArgsMappingRule.UNPACK)     async def expand_query(self, query: str, entities: List[str], query_obj: Dict, date: str = From(\"pre_date\", \"2025-01-01\")):  # Expand and obtain multiple queries.         print(f\"query: {query}, entities: {entities}, date: {date}, query_obj: {query_obj}\")         task_input = f\"Query: {query}\\nEntities: {entities}\"         response: str = await llm.astructured_output(             model=_model_name,             constraint=PydanticModel(model=QueryList),             messages=[                 Message.from_text(text=f\"Centered around the given entities and given date {date}, expand the query to obtain multiple queries\", role=Role.SYSTEM),                 Message.from_text(text=task_input, role=Role.USER,),             ]         )         return {\"id\": query_obj[\"id\"], \"queries\": response.queries} <p>Let's run it! When using the Inputs Propagation, the startup parameters must be passed in the form of keywords at startup.</p> In\u00a0[\u00a0]: Copied! <pre>query_expansion = QueryExpansion()\nawait query_expansion.arun(query_obj=query_obj)  # using keyword parameter passing\n</pre> query_expansion = QueryExpansion() await query_expansion.arun(query_obj=query_obj)  # using keyword parameter passing <pre>query: What new developments have there been in RAG in the past year?, entities: ['RAG', 'new developments', 'past year'], date: 2025-09-30, query_obj: {'id': 'user_1', 'query': 'Q: What new developments have there been in RAG in the past year?', 'date': '2025-09-30'}\n</pre> Out[\u00a0]: <pre>{'id': 'user_1',\n 'queries': ['What are the latest advancements in Retrieval-Augmented Generation (RAG) technologies as of 2025?',\n  'What new developments have emerged in RAG systems over the past 12 months?',\n  'How have RAG implementations evolved in the last year in terms of performance and scalability?',\n  'What are the key innovations in RAG models reported between 2024 and 2025?',\n  'What new techniques have been introduced in RAG to improve accuracy and context retention in the past year?',\n  'What recent breakthroughs in RAG have been highlighted in 2025?',\n  'How have industry leaders advanced RAG technology in the last year?',\n  'What are the most significant updates in RAG frameworks and tools from 2024 to 2025?',\n  'What new challenges and solutions have been proposed in RAG research over the past year?',\n  'What developments in RAG have improved real-time retrieval and generation performance in 2025?']}</pre> <p>Among all the ways of parameter passing mentioned above, the priority order is: arguments mapping positional parameters &gt; arguments injection &gt; propagation &gt; arguments mapping keyword parameters.</p>"},{"location":"tutorials/items/core_mechanism/parameter_binding/#parameter-binding","title":"Parameter Binding\u00b6","text":"<p>There are three ways to pass data among workers, including Arguments Mapping, Arguments Injection, and Inputs Propagation. Now let's understand them with a sample example.</p>"},{"location":"tutorials/items/core_mechanism/parameter_binding/#query-expansion","title":"Query expansion\u00b6","text":"<p>Query expansion is a common step in RAG and can enhance the quality of RAG. To enhance the quality of query expansion, developers often first extract the entity information from the query and use it to assist the model in expanding the original query.</p> <p>Now let's implement this. The user inputs the original query, and then we expand the query to obtain more queries. There are three steps to complete the query expansion:</p> <ol> <li>Receive the user's input and perform preprocessing to get the original query.</li> <li>Extract the entity information from the query to get the entity information.</li> <li>Expand and obtain multiple queries.</li> </ol>"},{"location":"tutorials/items/core_mechanism/parameter_binding/#1-initialize","title":"1. Initialize\u00b6","text":"<p>Before we start, let's prepare the running environment.</p>"},{"location":"tutorials/items/core_mechanism/parameter_binding/#2-complete-query-expansion","title":"2. Complete Query Expansion\u00b6","text":""},{"location":"tutorials/items/core_mechanism/parameter_binding/#what-have-we-done","title":"What have we done?\u00b6","text":"<p>Reviewing the code, we find that each <code>@worker</code> decorator has an <code>args_mapping_rule</code> parameter. Let's understand what it does.</p>"},{"location":"tutorials/items/core_mechanism/parameter_binding/#arguments-mapping","title":"Arguments Mapping\u00b6","text":"<p>The <code>args_mapping_rule</code> defines the way data is passed between directly dependent workers, that is, how the result of the previous worker is mapped to the parameter of the next worker. Its value can only be specified through the properties of <code>ArgsMappingRule</code>.</p>"},{"location":"tutorials/items/core_mechanism/parameter_binding/#as_is-mode-default","title":"AS_IS mode (default)\u00b6","text":"<p>In the AS_IS mode, a worker will receive the output of all its directly dependent workers as input parameters in the order declared by the dependencies.</p> <p>In the above example, <code>extract_entity</code> declares dependencies: <code>dependencies=[\"pre_query\", \"pre_date\"]</code>, so the results of the two preceding workers will be mapped to the first and second parameters of <code>extract_entity</code> in the order specified by the dependencies declaration, the result of <code>pre_query</code> is mapped to <code>query</code> parameter and the result of <code>pre_date</code> is mapped to <code>date</code> parameter.</p> <p>Note: The declaration order in dependencies only affects the order of parameter mapping, but does not influence the execution order of the dependent workers.</p> <p>Additionally, if the previous worker returns a result with multiple values, such as <code>return x, y</code>, then all the results will be passed as a tuple result. So in the above example, the parameter <code>query_meta</code> of <code>expand_query</code> received all the result values from <code>extract_entity</code>.</p>"},{"location":"tutorials/items/core_mechanism/parameter_binding/#unpack-mode","title":"UNPACK mode\u00b6","text":"<p>Let's go back to the previous example. In the <code>expand_query</code>, we receive the parameters from the previous worker in the <code>AS_IS</code> mode and manually unpack them as a whole, like this:</p>"},{"location":"tutorials/items/core_mechanism/parameter_binding/#merge-mode","title":"MERGE mode\u00b6","text":"<p>At the same time, conversely, since there is an UNPACK mechanism, is there also a mechanism that can aggregate multiple results for receiving? This is particularly useful when a worker collects the results of multiple dependent workers. At this point, the <code>MERGE</code> mode comes in handy.</p> <p>Still referring to the example above, <code>extract_entity</code> actually received the results from two workers. Now let's try to make <code>extract_entity</code> receive all these results in a single parameter for use, instead of receiving two parameters.</p> <p>Let's modify the <code>extract_entity</code> in the above example and add some print messages.</p>"},{"location":"tutorials/items/core_mechanism/parameter_binding/#arguments-injection","title":"Arguments Injection\u00b6","text":"<p>Looking back at the example above, we actually find that the <code>date</code> information is passed through <code>pre_date</code>, <code>extract_entity</code>, and finally reaches <code>expand_query</code>. However, in reality, <code>extract_entity</code> doesn't use this information at all. Thus, passing <code>date</code> here seems redundant. And The use of <code>date</code> in <code>expand_query</code> essentially only means that the data depends on it, but whether it is executed or not, this control dependency does not directly rely on it.</p> <p>Bridgic emphasizes the separation of data dependency and control dependency. This is beneficial for the future construction of complex graphs, as it allows for decoupling and avoids the need to adjust the entire graph due to changes in data dependency.</p> <p>In Bridgic, we can use Arguments Injection to make it. We can indicate which worker's result to take by using the <code>From</code> marker when declaring parameters, and at the same time set the default value if no result is obtained. For example:</p>"},{"location":"tutorials/items/core_mechanism/parameter_binding/#inputs-propagation","title":"Inputs Propagation\u00b6","text":"<p>Looking back at the example above again, our program did not process the <code>id</code> field in the input at all. Eventually, we only returned a list of generalized problems, which might cause the external call to be unable to associate which \"id\" corresponds to the result. However, this ID neither requires preprocessing nor is it needed for entity extraction.</p> <p>We can use Inputs Propagation to resolve it. This can be achieved by adding the name of the startup parameter to the worker when declaring the parameters.</p>"},{"location":"tutorials/items/model_integration/","title":"Model Integration","text":""},{"location":"tutorials/items/model_integration/#our-philosophy","title":"Our Philosophy","text":"<ul> <li>Model Neutrality</li> <li>Protocol Driven Design</li> </ul> <p>Bridgic is designed as a model-neutral framework that treats all LLM providers - whether they are commercial vendors (OpenAI, etc.) or inference engines (vLLM, etc.) - as equals. This architectural decision will reduce the extra effort developers need to put in when switching models.</p> <p></p>"},{"location":"tutorials/items/model_integration/#model-neutrality","title":"Model Neutrality","text":"<p>Bridgic\u2019s API is fundamentally model-neutral. When designing our interfaces, we deliberately avoid assumptions about any particular model provider or technology. Instead, we focus on delivering:</p> <ul> <li>Functional Simplicity: The APIs are intuitive and easy to use, allowing you to interact with any supported model provider through clear and minimal calls, with no provider-specific complexity.</li> <li>Consistent Experience: Regardless of which underlying LLM or model engine you use, Bridgic ensures a uniform invocation model, standardized error handling, and predictable behavior.</li> </ul> <p>This model-agnostic approach allows you to swap, extend, or combine providers with minimal friction, promoting genuine flexibility and future-proof integration in your agent pipelines.</p>"},{"location":"tutorials/items/model_integration/#protocol-driven-design","title":"Protocol Driven Design","text":"<p>At the heart of Bridgic's model integration lies the Protocol Pattern, which defines clear behavioral contracts without imposing implementation details. This design enables:</p> <ol> <li>Extensibility: New model providers can be integrated by implementing well-defined protocols (<code>StructuredOutput</code>, <code>ToolSelection</code>, etc.)</li> <li>Capability Declaration: Each provider explicitly declares its capabilities through protocol implementation, making it clear what features are available</li> <li>Flexibility: Providers can implement only the protocols they support, avoiding forced compatibility with unsupported features</li> <li>Type Safety: Protocols provide compile-time type checking and IDE support, improving developer experience</li> </ol> <p> </p> <p>This architecture means that when you work with a model provider in Bridgic, you explicitly know what capabilities it offers, and the framework enforces these contracts at both development and runtime.</p>"},{"location":"tutorials/items/model_integration/llm_integration/","title":"LLM Integration","text":"In\u00a0[2]: Copied! <pre>import os\nfrom dotenv import load_dotenv\nfrom bridgic.llms.openai import OpenAILlm, OpenAIConfiguration\n\nload_dotenv()\n\n_api_key = os.environ.get(\"OPENAI_API_KEY\")\n\nllm = OpenAILlm(\n    api_key=_api_key,\n)\n\nconfig = OpenAIConfiguration(\n    model=\"gpt-4o\",\n    temperature=0.7,\n    max_tokens=2000,\n)\n\nllm = OpenAILlm(\n    api_key=_api_key,\n    configuration=config,\n    timeout=30.0,\n)\n</pre> import os from dotenv import load_dotenv from bridgic.llms.openai import OpenAILlm, OpenAIConfiguration  load_dotenv()  _api_key = os.environ.get(\"OPENAI_API_KEY\")  llm = OpenAILlm(     api_key=_api_key, )  config = OpenAIConfiguration(     model=\"gpt-4o\",     temperature=0.7,     max_tokens=2000, )  llm = OpenAILlm(     api_key=_api_key,     configuration=config,     timeout=30.0, ) In\u00a0[6]: Copied! <pre>from bridgic.core.model.types import Message, Role\n\n# Create messages\nmessages = [\n    Message.from_text(\"You are a helpful assistant.\", role=Role.SYSTEM),\n    Message.from_text(\"What is the capital of France?\", role=Role.USER),\n]\n\n# Get response\nresponse = llm.chat(\n    messages=messages,\n    model=\"gpt-4o\",\n    temperature=0.7,\n)\n\nprint(response.message.content)\n</pre> from bridgic.core.model.types import Message, Role  # Create messages messages = [     Message.from_text(\"You are a helpful assistant.\", role=Role.SYSTEM),     Message.from_text(\"What is the capital of France?\", role=Role.USER), ]  # Get response response = llm.chat(     messages=messages,     model=\"gpt-4o\",     temperature=0.7, )  print(response.message.content)  <pre>The capital of France is Paris.\n</pre> In\u00a0[8]: Copied! <pre># Stream response chunks\nfor chunk in llm.stream(messages=messages, model=\"gpt-4o\"):\n    print(chunk.delta, end=\"|\", flush=True)  # Print each chunk as it arrives\n</pre> # Stream response chunks for chunk in llm.stream(messages=messages, model=\"gpt-4o\"):     print(chunk.delta, end=\"|\", flush=True)  # Print each chunk as it arrives <pre>The| capital| of| France| is| Paris|.|</pre> In\u00a0[14]: Copied! <pre>from pydantic import BaseModel, Field\nfrom bridgic.core.model.protocols import PydanticModel, JsonSchema\n\n# Option 1: Using Pydantic Models\nclass MathProblemSolution(BaseModel):\n    \"\"\"Solution to a math problem with reasoning\"\"\"\n    reasoning: str = Field(description=\"Step-by-step reasoning\")\n    answer: int = Field(description=\"Final numerical answer\")\n\nmessages = [\n    Message.from_text(\"What is 15 * 23?\", role=Role.USER)\n]\n\n# Get structured output\nsolution = llm.structured_output(\n    messages=messages,\n    constraint=PydanticModel(model=MathProblemSolution),\n    model=\"gpt-4o\",\n)\n\nprint(f\"REASONING:\\n\\n{solution.reasoning}\\n\")\nprint(f\"ANSWER: {solution.answer}\\n\")\n</pre> from pydantic import BaseModel, Field from bridgic.core.model.protocols import PydanticModel, JsonSchema  # Option 1: Using Pydantic Models class MathProblemSolution(BaseModel):     \"\"\"Solution to a math problem with reasoning\"\"\"     reasoning: str = Field(description=\"Step-by-step reasoning\")     answer: int = Field(description=\"Final numerical answer\")  messages = [     Message.from_text(\"What is 15 * 23?\", role=Role.USER) ]  # Get structured output solution = llm.structured_output(     messages=messages,     constraint=PydanticModel(model=MathProblemSolution),     model=\"gpt-4o\", )  print(f\"REASONING:\\n\\n{solution.reasoning}\\n\") print(f\"ANSWER: {solution.answer}\\n\") <pre>REASONING:\n\n15 multiplied by 23 can be broken down into smaller, more manageable calculations using the distributive property of multiplication. Here's how:\n\n1. **Break down 23:**\n   - 23 can be expressed as 20 + 3.\n\n2. **Apply the distributive property:**\n   - 15 * 23 = 15 * (20 + 3)\n   - According to the distributive property, this can be expanded to:\n     - 15 * 20 + 15 * 3\n\n3. **Calculate the individual products:**\n   - **15 * 20**\n     - 15 * 2 = 30\n     - Append a zero (since you are multiplying by 20, which is 10 times 2):\n     - 15 * 20 = 300\n   \n   - **15 * 3**\n     - 15 * 3 = 45\n\n4. **Add the two results together:**\n   - 300 + 45 = 345\n\nThus, using the distributive property and breaking down the numbers into simpler parts, we find that 15 multiplied by 23 equals 345.\n\nANSWER: 345\n\n</pre> In\u00a0[15]: Copied! <pre>from bridgic.core.model.types import Tool\n\n# Define available tools\ntools = [\n    Tool(\n        name=\"get_weather\",\n        description=\"Get the current weather for a location\",\n        parameters={\n            \"type\": \"object\",\n            \"properties\": {\n                \"location\": {\n                    \"type\": \"string\",\n                    \"description\": \"City name, e.g., 'San Francisco, CA'\"\n                },\n                \"unit\": {\n                    \"type\": \"string\",\n                    \"enum\": [\"celsius\", \"fahrenheit\"],\n                    \"description\": \"Temperature unit\"\n                }\n            },\n            \"required\": [\"location\"]\n        }\n    ),\n    Tool(\n        name=\"calculate\",\n        description=\"Perform mathematical calculations\",\n        parameters={\n            \"type\": \"object\",\n            \"properties\": {\n                \"expression\": {\n                    \"type\": \"string\",\n                    \"description\": \"Mathematical expression to evaluate\"\n                }\n            },\n            \"required\": [\"expression\"]\n        }\n    )\n]\n\n# Model selects appropriate tool\nmessages = [\n    Message.from_text(\"What's the weather like in Paris?\", role=Role.USER)\n]\n\ntool_calls, content = llm.select_tool(\n    messages=messages,\n    tools=tools,\n    model=\"gpt-4o\",\n    tool_choice=\"auto\",\n)\n\n# Process tool calls\nfor tool_call in tool_calls:\n    print(f\"Tool: {tool_call.name}\")\n    print(f\"Arguments: {tool_call.arguments}\")\n    print(f\"Call ID: {tool_call.id}\")\n</pre> from bridgic.core.model.types import Tool  # Define available tools tools = [     Tool(         name=\"get_weather\",         description=\"Get the current weather for a location\",         parameters={             \"type\": \"object\",             \"properties\": {                 \"location\": {                     \"type\": \"string\",                     \"description\": \"City name, e.g., 'San Francisco, CA'\"                 },                 \"unit\": {                     \"type\": \"string\",                     \"enum\": [\"celsius\", \"fahrenheit\"],                     \"description\": \"Temperature unit\"                 }             },             \"required\": [\"location\"]         }     ),     Tool(         name=\"calculate\",         description=\"Perform mathematical calculations\",         parameters={             \"type\": \"object\",             \"properties\": {                 \"expression\": {                     \"type\": \"string\",                     \"description\": \"Mathematical expression to evaluate\"                 }             },             \"required\": [\"expression\"]         }     ) ]  # Model selects appropriate tool messages = [     Message.from_text(\"What's the weather like in Paris?\", role=Role.USER) ]  tool_calls, content = llm.select_tool(     messages=messages,     tools=tools,     model=\"gpt-4o\",     tool_choice=\"auto\", )  # Process tool calls for tool_call in tool_calls:     print(f\"Tool: {tool_call.name}\")     print(f\"Arguments: {tool_call.arguments}\")     print(f\"Call ID: {tool_call.id}\")  <pre>Tool: get_weather\nArguments: {'location': 'Paris'}\nCall ID: call_aLv7xon4zhsNVMcnLmxsGJ3v\n</pre>"},{"location":"tutorials/items/model_integration/llm_integration/#llm-integration","title":"LLM Integration\u00b6","text":""},{"location":"tutorials/items/model_integration/llm_integration/#installation","title":"Installation\u00b6","text":"<p>Bridgic uses a modular installation strategy\u2014install only the components you require.</p> <p>Each model integration is available as a separate package, so you can minimize dependencies and keep your environment streamlined.</p> <pre># For general OpenAI-compatible APIs, only support basic chat interfaces (Groq, Together AI, etc.)\npip install bridgic-llms-openai-like\n\n# For OpenAI models (GPT-4, GPT-3.5, etc.)\npip install bridgic-llms-openai\n\n# For vLLM server deployments\npip install bridgic-llms-vllm\n</pre> Package <code>BaseLlm</code> <code>StructuredOutput</code> <code>ToolSelection</code> <code>bridgic-llms-openai-like</code> \u2705 \u274c \u274c <code>bridgic-llms-openai</code> \u2705 \u2705 \u2705 <code>bridgic-llms-vllm</code> \u2705 \u2705 \u2705"},{"location":"tutorials/items/model_integration/llm_integration/#llm-usage","title":"LLM Usage\u00b6","text":"<p>This section demonstrates the complete lifecycle of working with models in Bridgic, from initialization to advanced features.</p>"},{"location":"tutorials/items/model_integration/llm_integration/#1-initialization","title":"1. Initialization\u00b6","text":"<p>Model initialization is straightforward and follows a consistent pattern across all providers.</p>"},{"location":"tutorials/items/model_integration/llm_integration/#2-basic-interfaces","title":"2. Basic Interfaces\u00b6","text":"<p>All LLM providers in Bridgic implement the <code>BaseLlm</code> abstract class, which defines the fundamental <code>chat</code>/<code>stream</code> interfaces and their asynchronous variant.</p>"},{"location":"tutorials/items/model_integration/llm_integration/#21-chat","title":"2.1 Chat\u00b6","text":"<p>The most basic interface for getting a complete response from the model:</p>"},{"location":"tutorials/items/model_integration/llm_integration/#22-streaming","title":"2.2 Streaming\u00b6","text":"<p>For real-time response generation:</p>"},{"location":"tutorials/items/model_integration/llm_integration/#3-advanced-protocols","title":"3. Advanced Protocols\u00b6","text":"<p>Advanced interfaces are provided through optional protocols that providers can implement based on their capabilities.</p>"},{"location":"tutorials/items/model_integration/llm_integration/#31-structured-output-structuredoutput-protocol","title":"3.1 Structured Output (<code>StructuredOutput</code> Protocol)\u00b6","text":"<p>Generate outputs that conform to specific schemas or formats:</p>"},{"location":"tutorials/items/model_integration/llm_integration/#32-tool-selection-toolselection-protocol","title":"3.2 Tool Selection (<code>ToolSelection</code> Protocol)\u00b6","text":"<p>Enable models to select and use tools (function calling):</p>"},{"location":"tutorials/items/quick_start/quick_start/","title":"Quick Start","text":"In\u00a0[18]: Copied! <pre># Get the environment variables.\nimport os\nimport dotenv\n\ndotenv.load_dotenv()\n\n# Import the necessary packages.\nfrom bridgic.core.automa import GraphAutoma, worker\nfrom bridgic.core.model.types import Message, Role\n\n# Here we use OpenAILikeLlm because the package `bridgic-llms-openai-like` is installed automatically \n# when you install Bridgic. This makes sure the OpenAI-like model integration works out of the box.\nfrom bridgic.llms.openai_like.openai_like_llm import OpenAILikeLlm, OpenAILikeConfiguration\n\n\n# In this tutorial, we use OpenAI as an example. \n# You can freely replace these model settings to use any LLM provider you like.\n_api_key = os.environ.get(\"OPENAI_API_KEY\")\n_api_base = os.environ.get(\"OPENAI_API_BASE\")\n_model_name = os.environ.get(\"OPENAI_MODEL_NAME\")\n\nllm = OpenAILikeLlm(\n    api_key=_api_key,\n    api_base=_api_base,\n    configuration=OpenAILikeConfiguration(model=_model_name),\n    timeout=20,\n)\n</pre> # Get the environment variables. import os import dotenv  dotenv.load_dotenv()  # Import the necessary packages. from bridgic.core.automa import GraphAutoma, worker from bridgic.core.model.types import Message, Role  # Here we use OpenAILikeLlm because the package `bridgic-llms-openai-like` is installed automatically  # when you install Bridgic. This makes sure the OpenAI-like model integration works out of the box. from bridgic.llms.openai_like.openai_like_llm import OpenAILikeLlm, OpenAILikeConfiguration   # In this tutorial, we use OpenAI as an example.  # You can freely replace these model settings to use any LLM provider you like. _api_key = os.environ.get(\"OPENAI_API_KEY\") _api_base = os.environ.get(\"OPENAI_API_BASE\") _model_name = os.environ.get(\"OPENAI_MODEL_NAME\")  llm = OpenAILikeLlm(     api_key=_api_key,     api_base=_api_base,     configuration=OpenAILikeConfiguration(model=_model_name),     timeout=20, ) In\u00a0[19]: Copied! <pre>class WordLearningAssistant(GraphAutoma):\n    @worker(is_start=True)\n    async def generate_derivatives(self, word: str):\n        print(f\"------Generating derivatives for {word}------\")\n        response = await llm.achat(\n            model=_model_name,\n            messages=[\n                Message.from_text(text=\"You are a word learning assistant. Generate derivatives of the input word in a list.\", role=Role.SYSTEM),\n                Message.from_text(text=word, role=Role.USER),\n            ]\n        )\n        print(response.message.content)\n        print(f\"------End of generating derivatives------\\n\")\n        return response.message.content\n\n    @worker(dependencies=[\"generate_derivatives\"], is_output=True)\n    async def make_sentences(self, derivatives):\n        print(f\"------Making sentences with------\")\n        response = await llm.achat(\n            model=_model_name,\n            messages=[\n                Message.from_text(text=\"You are a word learning assistant. Make sentences with the input derivatives in a list.\", role=Role.SYSTEM),\n                Message.from_text(text=derivatives, role=Role.USER),\n            ]\n        )\n        print(response.message.content)\n        print(f\"------End of making sentences------\\n\")\n        return response.message.content\n\nword_learning_assistant = WordLearningAssistant()\n</pre> class WordLearningAssistant(GraphAutoma):     @worker(is_start=True)     async def generate_derivatives(self, word: str):         print(f\"------Generating derivatives for {word}------\")         response = await llm.achat(             model=_model_name,             messages=[                 Message.from_text(text=\"You are a word learning assistant. Generate derivatives of the input word in a list.\", role=Role.SYSTEM),                 Message.from_text(text=word, role=Role.USER),             ]         )         print(response.message.content)         print(f\"------End of generating derivatives------\\n\")         return response.message.content      @worker(dependencies=[\"generate_derivatives\"], is_output=True)     async def make_sentences(self, derivatives):         print(f\"------Making sentences with------\")         response = await llm.achat(             model=_model_name,             messages=[                 Message.from_text(text=\"You are a word learning assistant. Make sentences with the input derivatives in a list.\", role=Role.SYSTEM),                 Message.from_text(text=derivatives, role=Role.USER),             ]         )         print(response.message.content)         print(f\"------End of making sentences------\\n\")         return response.message.content  word_learning_assistant = WordLearningAssistant() In\u00a0[20]: Copied! <pre>res = await word_learning_assistant.arun(word=\"happy\")\n</pre> res = await word_learning_assistant.arun(word=\"happy\") <pre>------Generating derivatives for happy------\nHere are some derivatives of the word \"happy\":\n\n1. Happiness\n2. Happily\n3. Happier\n4. Happiest\n5. Unhappy\n6. Unhappiness\n7. Happinesses (plural form)\n8. Happifying (gerund form)\n9. Happify (verb form)\n\nFeel free to ask for derivatives of another word!\n------End of generating derivatives------\n\n------Making sentences with------\nSure! Here are sentences using each of the derivatives of the word \"happy\":\n\n1. **Happiness**: The pursuit of happiness is a common goal for many people.\n2. **Happily**: She smiled happily as she opened her birthday gifts.\n3. **Happier**: After taking a vacation, I felt much happier than I had in months.\n4. **Happiest**: That day was the happiest moment of my life when my daughter graduated.\n5. **Unhappy**: He seemed unhappy at the party and left early.\n6. **Unhappiness**: Her unhappiness was evident in her quiet demeanor.\n7. **Happinesses**: Different people find happinesses in various aspects of life, like family, work, and hobbies.\n8. **Happifying**: The act of volunteering can be a happifying experience for both the giver and the receiver.\n9. **Happify**: Listening to uplifting music can help to happify your day.\n\nLet me know if you need sentences for another word!\n------End of making sentences------\n\n</pre> <p>Note: Ensure you have set up your .env file to store your OPENAI_API_KEY or set up your terminal environment variable. This key is necessary for authenticating requests to the OpenAI API.</p> <p>Great! We have successfully completed the word learning assistant. It correctly completed the task as per our requirements.</p> In\u00a0[\u00a0]: Copied! <pre>class MyAutoma(GraphAutoma):\n    @worker(is_start=True)\n    async def start(self, x: int):\n        return x    \n</pre> class MyAutoma(GraphAutoma):     @worker(is_start=True)     async def start(self, x: int):         return x     <p>Or, you can also use it like this:</p> In\u00a0[\u00a0]: Copied! <pre>class MyAutoma(GraphAutoma): ...\nmy_automa = MyAutoma()\n\n# Add the function as a worker with worker decorator in the instance of the automa\n@my_automa.worker(is_start=True)\nasync def start(x: int):\n    return x\n</pre> class MyAutoma(GraphAutoma): ... my_automa = MyAutoma()  # Add the function as a worker with worker decorator in the instance of the automa @my_automa.worker(is_start=True) async def start(x: int):     return x  <p>Another one, the interface <code>add_func_as_worker()</code> can also be used to add workers.</p> In\u00a0[\u00a0]: Copied! <pre>async def start(x: int):\n    return x\n\nclass MyAutoma(GraphAutoma): ...\nmy_automa = MyAutoma()\n\n# Add the function as a worker\nmy_automa.add_func_as_worker(\n    key=\"start\",\n    func=start,\n    is_start=True,\n)\n</pre> async def start(x: int):     return x  class MyAutoma(GraphAutoma): ... my_automa = MyAutoma()  # Add the function as a worker my_automa.add_func_as_worker(     key=\"start\",     func=start,     is_start=True, )  <p>In addition to functions being convertible to workers, classes that inherit from <code>Worker</code> and override either <code>run()</code> or <code>arun()</code> can also be used directly as workers by <code>add_worker()</code> interface.</p> In\u00a0[\u00a0]: Copied! <pre>from bridgic.core.automa.worker import Worker\n\nclass MyWorker(Worker):\n    async def arun(self, x: int):\n        return x\n\nmy_worker = MyWorker()\n\n# Add the worker to the automa\nclass MyAutoma(GraphAutoma): ...\nmy_automa = MyAutoma()\nmy_automa.add_worker(\n    key=\"my_worker\",\n    worker=my_worker,\n    is_start=True,\n)\n\n# Run the worker\nres = await my_automa.arun(x=1)\nprint(res)\n</pre> from bridgic.core.automa.worker import Worker  class MyWorker(Worker):     async def arun(self, x: int):         return x  my_worker = MyWorker()  # Add the worker to the automa class MyAutoma(GraphAutoma): ... my_automa = MyAutoma() my_automa.add_worker(     key=\"my_worker\",     worker=my_worker,     is_start=True, )  # Run the worker res = await my_automa.arun(x=1) print(res) <p>Note:</p> <ol> <li>A specific worker that inherits from <code>Worker</code> must override either the <code>run()</code> or <code>arun()</code> method.</li> <li>Bridgic is a framework primarily designed for asynchronous execution, if both <code>run()</code> and <code>arun()</code> of a worker are overridden, <code>arun()</code> will take precedence.</li> </ol> <p>Both of these ways can correctly add the workers to <code>MyAutoma</code>.</p> <p>Whether using decorator syntax or the corresponding interface, there are usually some parameters:</p> <ol> <li><code>key</code>: A string. As the unique identifier of a worker in the current automa, it must be ensured that there are no duplicate names within the same automa. Use function names or class names by default.</li> <li><code>func</code>(in <code>add_func_as_worker()</code>) or <code>worker</code>(in <code>add_worker()</code>): The actual callable object. The decorator syntax does not have this parameter.</li> <li><code>is_start</code>: <code>True</code> or <code>False</code>. Marking the worker as the start for automa. It can be set in multiple workers.</li> <li><code>dependencies</code>: A list of string. Mark the preceding workers that the worker depends on.</li> <li><code>is_output</code>: <code>True</code> or <code>False</code>. Marking the worker as the output for automa. There can only be one on an execution branch.</li> <li><code>args_mapping_rule</code>: Parameter passing rule. For detailed information on behavior classes, please refer to the tutorial: Parameter Passing</li> </ol> <p>Note: From the perspective of the Bridgic, a worker must be placed in an automa for scheduling before it can be executed. Of course, even after packaging it as a worker, you can directly call <code>worker.arun()</code> or <code>worker.run()</code> to run it, but this is not within the purview of Bridgic.</p> In\u00a0[3]: Copied! <pre># Write workers in MyAutoma\nclass MyAutoma(GraphAutoma):\n    @worker(is_start=True)\n    async def worker_0(self, a, b, x, y):\n        print(f\"worker_0: a={a}, b={b}, x={x}, y={y}\")\n\n    @worker(is_start=True)\n    async def worker_1(self, x, y):\n        print(f\"worker_1: x={x}, y={y}\")\n</pre> # Write workers in MyAutoma class MyAutoma(GraphAutoma):     @worker(is_start=True)     async def worker_0(self, a, b, x, y):         print(f\"worker_0: a={a}, b={b}, x={x}, y={y}\")      @worker(is_start=True)     async def worker_1(self, x, y):         print(f\"worker_1: x={x}, y={y}\") <p>After an automa has the required workers, it can call <code>await automa_obj.arun(*args, **kwargs)</code> to start the entire scheduling operation.</p> <p>Bridgic is an asynchronous framework, <code>Graphautoma</code> must be started using arun().</p> <p>At startup, the parameters of <code>automa_obj.arun(*args, **kwargs)</code> will be distributed to the worker with <code>is_start=True</code> according to positional parameters and keyword parameters.</p> <ul> <li>positional parameters: The positional parameters will be filled into the parameter list of the worker with <code>is_start=True</code> in the order of input. An error will be raised if the parameter list of some worker is shorter than the number of positional parameters of <code>arun()</code>.</li> <li>keyword parameters: The keyword parameter will be filled into the corresponding parameter of the corresponding worker with <code>is_start=True</code>.</li> <li>priority: Positional parameters have a higher priority than keyword parameters.</li> </ul> <p>For example: we pass positional arguments <code>1</code> and <code>2</code>, and keyword arguments <code>x=3</code>, <code>y=4</code>.</p> In\u00a0[4]: Copied! <pre>my_automa = MyAutoma()\nawait my_automa.arun(1, 2, x=3, y=4)\n</pre> my_automa = MyAutoma() await my_automa.arun(1, 2, x=3, y=4) <pre>worker_0: a=1, b=2, x=3, y=4\nworker_1: x=1, y=2\n</pre> <p><code>1</code> and <code>2</code> were received in sequence by the first and second parameters of <code>worker_0</code> and <code>worker_1</code> respectively. Because positional parameters have a higher priority than keyword parameters, even if the parameter names of <code>worker_1</code> are the same as the input keyword parameters, they will still preferentially receive positional parameters.</p> <p>An error will be raised if the parameter list of some worker is shorter than the number of positional parameters.</p> In\u00a0[\u00a0]: Copied! <pre>my_automa = MyAutoma()\nawait my_automa.arun(1, 2, 3, y=4)  # worker_1 raises an error\n</pre> my_automa = MyAutoma() await my_automa.arun(1, 2, 3, y=4)  # worker_1 raises an error  <p>If all parameters are input in the keyword parameters, each worker with <code>is_start=True</code> can receive the corresponding parameter value.</p> In\u00a0[10]: Copied! <pre>my_automa = MyAutoma()\nawait my_automa.arun(a=1, b=2, x=3, y=4)\n</pre> my_automa = MyAutoma() await my_automa.arun(a=1, b=2, x=3, y=4) <pre>worker_0: a=1, b=2, x=3, y=4\nworker_1: x=3, y=4\n</pre> <p>Now, we can start building our Bridgic project!</p>"},{"location":"tutorials/items/quick_start/quick_start/#quick-start","title":"Quick Start\u00b6","text":"<p>In this tutorial, we assume that Bridgic is already installed on your system. If that\u2019s not the case, see Installation.</p> <p>Let's create a simple word learning assistant as an example. You'll provide a word, and the assistant will generate its derivational forms and use them in sentences. Through this example, we'll also learn how to use Bridgic in practice.</p>"},{"location":"tutorials/items/quick_start/quick_start/#word-learning-assistant","title":"Word learning assistant\u00b6","text":""},{"location":"tutorials/items/quick_start/quick_start/#1-model-initialization","title":"1. Model Initialization\u00b6","text":"<p>Before getting started, let's set up our environment. In this quick start, we'll use the integration out of the box. For an in-depth explanation of model integration, see: LLM Integration.</p>"},{"location":"tutorials/items/quick_start/quick_start/#2-automa-orchestration","title":"2. Automa Orchestration\u00b6","text":"<p>There are two steps to complete the word learning assistant:</p> <ol> <li>Generate derivatives of the input word.</li> <li>Make sentences with derivatives.</li> </ol>"},{"location":"tutorials/items/quick_start/quick_start/#3-agent-running","title":"3. Agent Running\u00b6","text":"<p>Let's run this assistant, via <code>arun</code> method:</p>"},{"location":"tutorials/items/quick_start/quick_start/#what-have-we-done","title":"What have we done?\u00b6","text":"<p>The above example is the typical usage of writing an agent application with Bridgic. Now let's understand some of its components.</p>"},{"location":"tutorials/items/quick_start/quick_start/#worker","title":"Worker\u00b6","text":"<p>Any callable object (such as functions, methods, etc.) when used by the framework, will be converted into a worker object and serve as the basic execution unit for scheduling and orchestration.</p> <p>Just as in the example of the word learning assistant, we can use decorator syntax to wrap functions and methods into a worker object.</p>"},{"location":"tutorials/items/quick_start/quick_start/#graphautoma","title":"GraphAutoma\u00b6","text":"<p>Automa serves as the scheduling engine. In the example of the word learning assistant above, we used the <code>GraphAutoma</code>, which represents the scheduling according to the topological sorting among workers.</p> <p>You can use <code>GraphAutoma</code> by writing a class that inherits from it and writing or adding workers.</p>"}]}