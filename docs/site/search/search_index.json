{"config":{"lang":["en"],"separator":"[\\s\\-,:!=\\[\\]()\"`/]+|\\.(?!\\d)|&[lg]t;|(?!\\b)(?=[A-Z][a-z])","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to the world of Bridgic \ud83c\udf09!","text":""},{"location":"#what-does-bridgic-mean","title":"What does Bridgic mean?","text":"<p>Bridgic means bridging logic and magic together.</p> <ul> <li>Logic stands for deterministic execution flows;</li> <li>Magic stands for highly autonomous AI capabilities.</li> </ul> <p>Bridgic provides a programming paradigm that enables a more effective integration of the two.</p>"},{"location":"about/","title":"About Us","text":""},{"location":"api/","title":"API Reference","text":""},{"location":"api/bridgic-core/bridgic/core/automa/graph_automa/","title":"GraphAutoma","text":"<p>               Bases: <code>Automa</code></p> <p>Dynamic Directed Graph (abbreviated as DDG) implementation of Automa. <code>GraphAutoma</code> manages  the running control flow between workers automatically, via <code>dependencies</code> and <code>ferry_to</code>. Outputs of workers can be mapped and passed to their successor workers in the runtime,  following <code>args_mapping_rule</code>.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>Optional[str]</code> <p>The name of the automa.</p> <code>None</code> <code>output_worker_key</code> <code>Optional[str]</code> <p>The key of the output worker whose output will be returned by the automa.</p> <code>None</code> <code>thread_pool</code> <code>Optional[ThreadPoolExecutor]</code> <p>The thread pool for parallel running of I/O-bound tasks.</p> <ul> <li> <p>If not provided, a default thread pool will be used. The maximum number of threads in the default thread pool dependends on the number of CPU cores. Please refer to  the ThreadPoolExecutor for detail.</p> </li> <li> <p>If provided, all workers (including all nested Automa instances) will be run in it. In this case, the  application layer code is responsible to create it and shut it down.</p> </li> </ul> <code>None</code> <p>Examples:</p> <p>The following example shows how to use <code>GraphAutoma</code> to create a simple graph automa that prints \"Hello, Bridgic\".</p> <pre><code>import asyncio\nfrom bridgic.core.automa import GraphAutoma, worker, ArgsMappingRule\n\nclass MyGraphAutoma(GraphAutoma):\n    @worker(is_start=True)\n    async def greet(self) -&gt; list[str]:\n        return [\"Hello\", \"Bridgic\"]\n\n    @worker(dependencies=[\"greet\"], args_mapping_rule=ArgsMappingRule.AS_IS)\n    async def output(self, message: list[str]):\n        print(\"Echo: \" + \" \".join(message))\n\nasync def main():\n    automa_obj = MyGraphAutoma(name=\"my_graph_automa\", output_worker_key=\"output\")\n    await automa_obj.arun()\n\nasyncio.run(main())\n</code></pre> Source code in <code>bridgic/core/automa/graph_automa.py</code> <pre><code>class GraphAutoma(Automa, metaclass=GraphAutomaMeta):\n    \"\"\"\n    Dynamic Directed Graph (abbreviated as DDG) implementation of Automa. `GraphAutoma` manages \n    the running control flow between workers automatically, via `dependencies` and `ferry_to`.\n    Outputs of workers can be mapped and passed to their successor workers in the runtime, \n    following `args_mapping_rule`.\n\n    Parameters\n    ----------\n    name : Optional[str]\n        The name of the automa.\n\n    output_worker_key : Optional[str]\n        The key of the output worker whose output will be returned by the automa.\n\n    thread_pool : Optional[ThreadPoolExecutor]\n        The thread pool for parallel running of I/O-bound tasks.\n\n        - If not provided, a default thread pool will be used.\n        The maximum number of threads in the default thread pool dependends on the number of CPU cores. Please refer to \n        the [ThreadPoolExecutor](https://docs.python.org/3/library/concurrent.futures.html#threadpoolexecutor) for detail.\n\n        - If provided, all workers (including all nested Automa instances) will be run in it. In this case, the \n        application layer code is responsible to create it and shut it down.\n\n    Examples\n    --------\n\n    The following example shows how to use `GraphAutoma` to create a simple graph automa that prints \"Hello, Bridgic\".\n\n    ```python\n    import asyncio\n    from bridgic.core.automa import GraphAutoma, worker, ArgsMappingRule\n\n    class MyGraphAutoma(GraphAutoma):\n        @worker(is_start=True)\n        async def greet(self) -&gt; list[str]:\n            return [\"Hello\", \"Bridgic\"]\n\n        @worker(dependencies=[\"greet\"], args_mapping_rule=ArgsMappingRule.AS_IS)\n        async def output(self, message: list[str]):\n            print(\"Echo: \" + \" \".join(message))\n\n    async def main():\n        automa_obj = MyGraphAutoma(name=\"my_graph_automa\", output_worker_key=\"output\")\n        await automa_obj.arun()\n\n    asyncio.run(main())\n    ```\n    \"\"\"\n\n    # The initial topology defined by @worker functions.\n    _registered_worker_funcs: Dict[str, Callable] = {}\n\n    # [IMPORTANT] \n    # The whole states of the Automa are divided into two main parts:\n    #\n    # [Part One: Topology-Related Runtime States] The runtime states related to topology changes.\n    # -- Nodes: workers (A Worker can be another Automa): {_workers}\n    # -- Edges: Dependencies between workers: {dependencies in _workers, _worker_forwards}\n    # -- Worker Dynamics: the dynamic in-degrees of each workers. {_workers_dynamic_states}\n    # -- Configurations: Start workers, output worker, args_mapping_rule, etc. {_output_worker_key, is_start and args_mapping_rule in _workers}\n    # -- Deferred Tasks: {_topology_change_deferred_tasks, _set_output_worker_deferred_task}\n    #\n    # [Part Two: Task-Related Runtime States] The runtime states related to task execution.\n    # -- Current kickoff workers list. {_current_kickoff_workers}\n    # -- Running &amp; Ferry Tasks list. {_running_tasks, _ferry_deferred_tasks}\n    # -- Automa input buffer. {_input_buffer}\n    # -- Output buffer and local space for each worker. {in _workers}\n    # -- Ongoing human interactions... {_ongoing_interactions}\n    # -- Other runtime states...\n\n    # Note: Just declarations, NOT instances!!!\n    # So do not initialize them here!!!\n    _workers: Dict[str, _GraphAdaptedWorker]\n    _worker_output: Dict[str, Any]\n    _worker_forwards: Dict[str, List[str]]\n    _output_worker_key: Optional[str]\n\n    _current_kickoff_workers: List[_KickoffInfo]\n    _input_buffer: _AutomaInputBuffer\n    _workers_dynamic_states: Dict[str, _WorkerDynamicState]\n\n    # The whole running process of the DDG is divided into two main phases:\n    # 1. [Initialization Phase] The first phase (when _automa_running is False): the initial topology of DDG was constructed.\n    # 2. [Running Phase] The second phase (when _automa_running is True): the DDG is running, and the workers are executed in a dynamic step-by-step manner (DS loop).\n    _automa_running: bool\n\n    # Ongoing human interactions triggered by the `interact_with_human()` call from workers of the current Automa.\n    # worker_key -&gt; list of interactions.\n    _ongoing_interactions: Dict[str, List[_InteractionAndFeedback]]\n\n    #########################################################\n    #### The following fields need not to be serialized. ####\n    #########################################################\n    _running_tasks: List[_RunnningTask]\n\n    # TODO: The following deferred task structures need to be thread-safe.\n    # TODO: Need to be refactored when parallelization features are added.\n    _topology_change_deferred_tasks: List[Union[_AddWorkerDeferredTask, _RemoveWorkerDeferredTask]]\n    _set_output_worker_deferred_task: _SetOutputWorkerDeferredTask\n    _ferry_deferred_tasks: List[_FerryDeferredTask]\n\n    _event_handlers: Dict[str, EventHandlerType]\n    _default_event_handler: EventHandlerType\n\n    # worker_key -&gt; count\n    _worker_interaction_indices: Dict[str, int]\n\n    # The main event loop of the Automa, which is usually provided by the application layer.\n    _main_loop: asyncio.AbstractEventLoop\n\n    # The thread id of the main thread in which the main event loop is running.\n    _main_thread_id: int\n\n    # The thread pool for parallel running of I/O-bound tasks. It can be None.\n    _thread_pool: ThreadPoolExecutor\n\n    def __init__(\n        self,\n        name: Optional[str] = None,\n        output_worker_key: Optional[str] = None,\n        thread_pool: Optional[ThreadPoolExecutor] = None,\n    ):\n        \"\"\"\n        Parameters\n        ----------\n        name : Optional[str]\n            The name of the automa.\n\n        output_worker_key : Optional[str]\n            The key of the output worker whose output will be returned by the automa.\n\n        thread_pool : Optional[ThreadPoolExecutor]\n            The thread pool for parallel running of I/O-bound tasks.\n\n            - If not provided, a default thread pool will be used.\n            The maximum number of threads in the default thread pool dependends on the number of CPU cores. Please refer to \n            the [ThreadPoolExecutor](https://docs.python.org/3/library/concurrent.futures.html#threadpoolexecutor) for detail.\n\n            - If provided, all workers (including all nested Automa instances) will be run in it. In this case, the \n            application layer code is responsible to create it and shut it down.\n\n        state_dict : Optional[Dict[str, Any]]\n            A dictionary for initializing the automa's runtime states. This parameter is designed for framework use only.\n        \"\"\"\n        self._workers = {}\n        self._worker_outputs = {}\n        super().__init__(name=name)\n        self._automa_running = False\n\n        # Initialize the states that need to be serialized.\n        self._normal_init(output_worker_key)\n\n        ########\n        # The following states need not to be serialized.\n        ########\n        # The list of the tasks that are currently being executed.\n        self._running_tasks = []\n        # deferred tasks\n        self._topology_change_deferred_tasks = []\n        self._set_output_worker_deferred_task = None\n        self._ferry_deferred_tasks = []\n        # event handling and human interactions\n        self._event_handlers = {}\n        self._default_event_handler = None\n        self._worker_interaction_indices = {}\n        self._thread_pool = thread_pool\n\n    def _normal_init(\n        self,\n        output_worker_key: Optional[str] = None,\n    ):\n        ###############################################################################\n        # Initialization of [Part One: Topology-Related Runtime States] #### Strat ####\n        ###############################################################################\n\n        cls = type(self)\n\n        # _workers, _worker_forwards and _workers_dynamic_states will be initialized incrementally by add_worker()...\n        self._worker_forwards = {}\n        self._worker_output = {}\n        self._workers_dynamic_states = {}\n\n        if cls.worker_decorator_type() == AutomaType.Graph:\n            # The _registered_worker_funcs data are from @worker decorators.\n            for worker_key, worker_func in cls._registered_worker_funcs.items():\n                # The decorator based mechanism (i.e. @worker) is based on the add_worker() interface.\n                # Parameters check and other implementation details can be unified.\n                self._add_func_as_worker_internal(\n                    key=worker_key,\n                    func=worker_func,\n                    dependencies=worker_func.__dependencies__,\n                    is_start=worker_func.__is_start__,\n                    args_mapping_rule=worker_func.__args_mapping_rule__,\n                )\n\n        self._output_worker_key = output_worker_key\n\n        ###############################################################################\n        # Initialization of [Part One: Topology-Related Runtime States] ##### End #####\n        ###############################################################################\n\n        ###############################################################################\n        # Initialization of [Part Two: Task-Related Runtime States] ###### Strat ######\n        ###############################################################################\n\n        # -- Current kickoff workers list.\n        # The key list of the workers that are ready to be immediately executed in the next DS (Dynamic Step). It will be lazily initialized in _compile_graph_and_detect_risks().\n        self._current_kickoff_workers = []\n        # -- Automa input buffer.\n        self._input_buffer = _AutomaInputBuffer()\n        # -- Ongoing human interactions\n        self._ongoing_interactions = {}\n\n\n        ###############################################################################\n        # Initialization of [Part Two: Task-Related Runtime States] ####### End #######\n        ###############################################################################\n\n    ###############################################################\n    ########## [Bridgic Serialization Mechanism] starts ###########\n    ###############################################################\n\n    # The version of the serialization format.\n    SERIALIZATION_VERSION: str = \"1.0\"\n\n    @override\n    def dump_to_dict(self) -&gt; Dict[str, Any]:\n        state_dict = super().dump_to_dict()\n        # collect states to serialize \n        state_dict[\"name\"] = self.name\n        # TODO: serialize the workers, including the outbuf of each worker\n        state_dict[\"workers\"] = self._workers\n        state_dict[\"automa_running\"] = self._automa_running\n        state_dict[\"worker_forwards\"] = self._worker_forwards\n        state_dict[\"workers_dynamic_states\"] = self._workers_dynamic_states\n        state_dict[\"output_worker_key\"] = self._output_worker_key\n        # TODO: args &amp;&amp; kwargs must be serializable in order to serialize _current_kickoff_workers\n        state_dict[\"current_kickoff_workers\"] = self._current_kickoff_workers\n        state_dict[\"input_buffer\"] = self._input_buffer\n        # TODO: the data field of Event and Feedback must be serializable in order to serialize _ongoing_interactions\n        state_dict[\"ongoing_interactions\"] = self._ongoing_interactions\n        state_dict[\"worker_output\"] = self._worker_output\n        return state_dict\n\n    @override\n    def load_from_dict(self, state_dict: Dict[str, Any]) -&gt; None:\n        super().load_from_dict(state_dict)\n        # Deserialize from the state_dict\n        self._workers = state_dict[\"workers\"]\n        for worker in self._workers.values():\n            worker.parent = self\n        self._automa_running = state_dict[\"automa_running\"]\n        self._worker_forwards = state_dict[\"worker_forwards\"]\n        self._workers_dynamic_states = state_dict[\"workers_dynamic_states\"]\n        self._output_worker_key = state_dict[\"output_worker_key\"]\n        self._current_kickoff_workers = state_dict[\"current_kickoff_workers\"]\n        self._input_buffer = state_dict[\"input_buffer\"]\n        self._ongoing_interactions = state_dict[\"ongoing_interactions\"]\n        self._worker_output = state_dict[\"worker_output\"]\n\n    @classmethod\n    def load_from_snapshot(\n        cls, \n        snapshot: Snapshot,\n        thread_pool: Optional[ThreadPoolExecutor] = None,\n    ) -&gt; \"GraphAutoma\":\n        # Here you can compare snapshot.serialization_version with SERIALIZATION_VERSION, and handle any necessary version compatibility issues if needed.\n        automa = msgpackx.load_bytes(snapshot.serialized_bytes)\n        if thread_pool:\n            automa.thread_pool = thread_pool\n        return automa\n\n    ###############################################################\n    ########### [Bridgic Serialization Mechanism] ends ############\n    ###############################################################\n\n    @property\n    def thread_pool(self) -&gt; Optional[ThreadPoolExecutor]:\n        return self._thread_pool\n\n    @thread_pool.setter\n    def thread_pool(self, executor: ThreadPoolExecutor) -&gt; None:\n        \"\"\"\n        Set the thread pool for parallel running of I/O-bound tasks.\n\n        If an Automa is nested within another Automa, the thread pool of the top-level Automa will be used, rather than the thread pool of the nested Automa.\n        \"\"\"\n        self._thread_pool = executor\n\n    def _add_worker_incrementally(\n        self,\n        key: str,\n        worker: Worker,\n        *,\n        dependencies: List[str] = [],\n        is_start: bool = False,\n        args_mapping_rule: ArgsMappingRule = ArgsMappingRule.AS_IS,\n    ) -&gt; None:\n        \"\"\"\n        Incrementally add a worker into the automa. For internal use only.\n        This method is one of the very basic primitives of DDG for dynamic topology changes. \n        \"\"\"\n        if key in self._workers:\n            raise AutomaRuntimeError(\n                f\"duplicate workers with the same key '{key}' are not allowed to be added!\"\n            )\n\n        # Note: the dependencies argument must be a new copy of the list, created with list(dependencies).\n        # Refer to the Python documentation for more details:\n        # 1. https://docs.python.org/3/reference/compound_stmts.html#function-definitions\n        # \"Default parameter values are evaluated from left to right when the function definition is executed\"\n        # 2. https://docs.python.org/3/tutorial/controlflow.html#default-argument-values\n        # \"The default values are evaluated at the point of function definition in the defining scope\"\n        # \"Important warning: The default value is evaluated only once.\"\n        new_worker_obj = _GraphAdaptedWorker(\n            key=key,\n            worker=worker,\n            dependencies=list(dependencies),\n            is_start=is_start,\n            args_mapping_rule=args_mapping_rule,\n        )\n\n        # Register the worker_obj.\n        new_worker_obj.parent = self\n        self._workers[new_worker_obj.key] = new_worker_obj\n\n        # Incrementally update the dynamic states of added workers.\n        self._workers_dynamic_states[key] = _WorkerDynamicState(\n            dependency_triggers=set(dependencies)\n        )\n\n        # Incrementally update the forwards table.\n        for trigger in dependencies:\n            if trigger not in self._worker_forwards:\n                self._worker_forwards[trigger] = []\n            self._worker_forwards[trigger].append(key)\n\n        # TODO : Validation may be needed in appropriate time later, because we are not able to guarantee \n        # the existence of the trigger-workers in self._workers of the final automa graph after dynamically changing.\n\n    def _remove_worker_incrementally(\n        self,\n        key: str\n    ) -&gt; None:\n        \"\"\"\n        Incrementally remove a worker from the automa. For internal use only.\n        This method is one of the very basic primitives of DDG for dynamic topology changes.\n        \"\"\"\n        if key not in self._workers:\n            raise AutomaRuntimeError(\n                f\"fail to remove worker '{key}' that does not exist!\"\n            )\n\n        worker_to_remove = self._workers[key]\n\n        # Remove the worker.\n        del self._workers[key]\n        # Incrementally update the dynamic states of removed workers.\n        del self._workers_dynamic_states[key]\n\n        if key in self._worker_forwards:\n            # Update the dependencies of the successor workers, if needed.\n            for successor in self._worker_forwards[key]:\n                self._workers[successor].dependencies.remove(key)\n                # Note this detail here: use discard() instead of remove() to avoid KeyError.\n                # This case occurs when a worker call remove_worker() to remove its predecessor worker.\n                self._workers_dynamic_states[successor].dependency_triggers.discard(key)\n            # Incrementally update the forwards table.\n            del self._worker_forwards[key]\n\n        # Remove from the forwards list of all dependencies worker.\n        for trigger in worker_to_remove.dependencies:\n            self._worker_forwards[trigger].remove(key)\n        if key in self._worker_interaction_indices:\n            del self._worker_interaction_indices[key]\n        if key in self._ongoing_interactions:\n            del self._ongoing_interactions[key]\n\n        # clear the output worker key if needed.\n        if key == self._output_worker_key:\n            self._output_worker_key = None\n\n    def _add_dependency_incrementally(\n        self,\n        key: str,\n        dependency: str,\n    ) -&gt; None:\n        \"\"\"\n        Incrementally add a dependency from `key` to `depends`. For internal use only.\n        This method is one of the very basic primitives of DDG for dynamic topology changes.\n        \"\"\"\n        if key not in self._workers:\n            raise AutomaRuntimeError(\n                f\"fail to add dependency from a worker that does not exist: `{key}`!\"\n            )\n        if dependency not in self._workers:\n            raise AutomaRuntimeError(\n                f\"fail to add dependency to a worker that does not exist: `{dependency}`!\"\n            )\n        if dependency in self._workers[key].dependencies:\n            raise AutomaRuntimeError(\n                f\"dependency from '{key}' to '{dependency}' already exists!\"\n            )\n\n        self._workers[key].dependencies.append(dependency)\n        # Note this detail here for dynamic states change:\n        # The new dependency added here may be removed right away if the dependency is just the next kickoff worker. This is a valid behavior.\n        self._workers_dynamic_states[key].dependency_triggers.add(dependency)\n\n        if dependency not in self._worker_forwards:\n            self._worker_forwards[dependency] = []\n        self._worker_forwards[dependency].append(key)\n\n    def _add_worker_internal(\n        self,\n        key: str,\n        worker: Worker,\n        *,\n        dependencies: List[str] = [],\n        is_start: bool = False,\n        args_mapping_rule: ArgsMappingRule = ArgsMappingRule.AS_IS,\n    ) -&gt; None:\n        \"\"\"\n        The private version of the method `add_worker()`.\n        \"\"\"\n\n        def _basic_worker_params_check(key: str, worker_obj: Worker):\n            if not isinstance(worker_obj, Worker):\n                raise TypeError(\n                    f\"worker_obj to be registered must be a Worker, \"\n                    f\"but got {type(worker_obj)} for worker '{key}'\"\n                )\n\n            if not asyncio.iscoroutinefunction(worker_obj.arun):\n                raise WorkerSignatureError(\n                    f\"arun of Worker must be an async method, \"\n                    f\"but got {type(worker_obj.arun)} for worker '{key}'\"\n                )\n\n            if not isinstance(dependencies, list):\n                raise TypeError(\n                    f\"dependencies must be a list, \"\n                    f\"but got {type(dependencies)} for worker '{key}'\"\n                )\n            if not all([isinstance(d, str) for d in dependencies]):\n                raise ValueError(\n                    f\"dependencies must be a List of str, \"\n                    f\"but got {dependencies} for worker {key}\"\n                )\n\n            if args_mapping_rule not in ArgsMappingRule:\n                raise ValueError(\n                    f\"args_mapping_rule must be one of the following: {[e for e in ArgsMappingRule]}, \"\n                    f\"but got {args_mapping_rule} for worker {key}\"\n                )\n\n        # Ensure the parameters are valid.\n        _basic_worker_params_check(key, worker)\n\n        if not self._automa_running:\n            # Add worker during the [Initialization Phase].\n            self._add_worker_incrementally(\n                key=key,\n                worker=worker,\n                dependencies=dependencies,\n                is_start=is_start,\n                args_mapping_rule=args_mapping_rule,\n            )\n        else:\n            # Add worker during the [Running Phase].\n            deferred_task = _AddWorkerDeferredTask(\n                worker_key=key,\n                worker_obj=worker,\n                dependencies=dependencies,\n                is_start=is_start,\n                args_mapping_rule=args_mapping_rule,\n            )\n            # Note1: the execution order of topology change deferred tasks is important and is determined by the order of the calls of add_worker(), remove_worker() and add_dependency() in one DS.\n            # Note2: add_worker() and remove_worker() may be called in a new thread. But _topology_change_deferred_tasks is not necessary to be thread-safe due to Visibility Guarantees of the Bridgic Concurrency Model.\n            self._topology_change_deferred_tasks.append(deferred_task)\n\n    def _add_func_as_worker_internal(\n        self,\n        key: str,\n        func: Callable,\n        *,\n        dependencies: List[str] = [],\n        is_start: bool = False,\n        args_mapping_rule: ArgsMappingRule = ArgsMappingRule.AS_IS,\n    ) -&gt; None:\n        \"\"\"\n        The private version of the method `add_func_as_worker()`.\n        \"\"\"\n        # Register func as an instance of CallableWorker.\n        if not isinstance(func, MethodType):\n            func = MethodType(func, self)\n        else:\n            # Validate: bounded __self__ of `func` must be self when add_func_as_worker() is called.\n            if func.__self__ is not self:\n                raise AutomaRuntimeError(\n                    f\"the bounded instance of `func` must be the same as the instance of the GraphAutoma, \"\n                    f\"but got {func.__self__}\"\n                )\n        func_worker = CallableWorker(func)\n\n        self._add_worker_internal(\n            key=key,\n            worker=func_worker,\n            dependencies=dependencies,\n            is_start=is_start,\n            args_mapping_rule=args_mapping_rule,\n        )\n\n    def all_workers(self) -&gt; List[str]:\n        \"\"\"\n        Gets a list containing the keys of all workers registered in this Automa.\n\n        Returns\n        -------\n        List[str]\n            A list of worker keys.\n        \"\"\"\n        return list(self._workers.keys())\n\n    def add_worker(\n        self,\n        key: str,\n        worker: Worker,\n        *,\n        dependencies: List[str] = [],\n        is_start: bool = False,\n        args_mapping_rule: ArgsMappingRule = ArgsMappingRule.AS_IS,\n    ) -&gt; None:\n        \"\"\"\n        This method is used to add a worker dynamically into the automa.\n\n        If this method is called during the [Initialization Phase], the worker will be added immediately. If this method is called during the [Running Phase], the worker will be added as a deferred task which will be executed in the next DS.\n\n        The dependencies can be added together with a worker. However, you can add a worker without any dependencies.\n\n        Note: The args_mapping_rule can only be set together with adding a worker, even if the worker has no any dependencies.\n\n        Parameters\n        ----------\n        key : str\n            The key of the worker.\n        worker : Worker\n            The worker instance to be registered.\n        dependencies : List[str]\n            A list of worker keys that the worker depends on.\n        is_start : bool\n            Whether the worker is a start worker.\n        args_mapping_rule : ArgsMappingRule\n            The rule of arguments mapping.\n        \"\"\"\n        self._add_worker_internal(\n            key=key,\n            worker=worker,\n            dependencies=dependencies,\n            is_start=is_start,\n            args_mapping_rule=args_mapping_rule,\n        )\n\n    def add_func_as_worker(\n        self,\n        key: str,\n        func: Callable,\n        *,\n        dependencies: List[str] = [],\n        is_start: bool = False,\n        args_mapping_rule: ArgsMappingRule = ArgsMappingRule.AS_IS,\n    ) -&gt; None:\n        \"\"\"\n        This method is used to add a function as a worker into the automa.\n\n        The format of the parameters will follow that of the decorator @worker(...), so that the \n        behavior of the decorated function is consistent with that of normal CallableLandableWorker objects.\n\n        Parameters\n        ----------\n        key : str\n            The key of the function worker.\n        func : Callable\n            The function to be added as a worker to the automa.\n        dependencies : List[str]\n            A list of worker names that the decorated callable depends on.\n        is_start : bool\n            Whether the decorated callable is a start worker. True means it is, while False means it is not.\n        args_mapping_rule : ArgsMappingRule\n            The rule of arguments mapping.\n        \"\"\"\n        self._add_func_as_worker_internal(\n            key=key,\n            func=func,\n            dependencies=dependencies,\n            is_start=is_start,\n            args_mapping_rule=args_mapping_rule,\n        )\n\n    def worker(\n        self,\n        *,\n        key: Optional[str] = None,\n        dependencies: List[str] = [],\n        is_start: bool = False,\n        args_mapping_rule: ArgsMappingRule = ArgsMappingRule.AS_IS,\n    ) -&gt; Callable:\n        \"\"\"\n        This is a decorator used to mark a function as an GraphAutoma detectable Worker. Dislike the \n        global decorator @worker(...), it is usally used after an GraphAutoma instance is initialized.\n\n        The format of the parameters will follow that of the decorator @worker(...), so that the \n        behavior of the decorated function is consistent with that of normal CallableLandableWorker objects.\n\n        Parameters\n        ----------\n        key : str\n            The key of the worker. If not provided, the name of the decorated callable will be used.\n        dependencies : List[str]\n            A list of worker names that the decorated callable depends on.\n        is_start : bool\n            Whether the decorated callable is a start worker. True means it is, while False means it is not.\n        args_mapping_rule : str\n            The rule of arguments mapping. The options are: \"auto\", \"as_list\", \"as_dict\", \"suppressed\".\n        \"\"\"\n        def wrapper(func: Callable):\n            self._add_func_as_worker_internal(\n                key=(key or func.__name__),\n                func=func,\n                dependencies=dependencies,\n                is_start=is_start,\n                args_mapping_rule=args_mapping_rule,\n            )\n\n        return wrapper\n\n    def remove_worker(self, key: str) -&gt; None:\n        \"\"\"\n        Remove a worker from the Automa. This method can be called at any time to remove a worker from the Automa.\n\n        When a worker is removed, all dependencies related to this worker, including all the dependencies of the worker itself and the dependencies between the worker and its successor workers, will be also removed.\n\n        Parameters\n        ----------\n        key : str\n            The key of the worker to be removed.\n\n        Returns\n        -------\n        None\n\n        Raises\n        ------\n        AutomaDeclarationError\n            If the worker specified by key does not exist in the Automa, this exception will be raised.\n        \"\"\"\n        if not self._automa_running:\n            # remove immediately\n            self._remove_worker_incrementally(key)\n        else:\n            deferred_task = _RemoveWorkerDeferredTask(\n                worker_key=key,\n            )\n            # Note: the execution order of topology change deferred tasks is important and is determined by the order of the calls of add_worker(), remove_worker() and add_dependency() in one DS.\n            self._topology_change_deferred_tasks.append(deferred_task)\n\n    def add_dependency(\n        self,\n        key: str,\n        dependency: str,\n    ) -&gt; None:\n        \"\"\"\n        This method is used to dynamically add a dependency from `key` to `dependency`.\n\n        Note: args_mapping_rule is not allowed to be set by this method, instead it should be set together with add_worker() or add_func_as_worker().\n\n        Parameters\n        ----------\n        key : str\n            The key of the worker that will depend on the worker with key `dependency`.\n        dependency : str\n            The key of the worker on which the worker with key `key` will depend.\n        \"\"\"\n        ...\n        if not self._automa_running:\n            # add the dependency immediately\n            self._add_dependency_incrementally(key, dependency)\n        else:\n            deferred_task = _AddDependencyDeferredTask(\n                worker_key=key,\n                dependency=dependency,\n            )\n            # Note: the execution order of topology change deferred tasks is important and is determined by the order of the calls of add_worker(), remove_worker() and add_dependency() in one DS.\n            self._topology_change_deferred_tasks.append(deferred_task)\n\n    @property\n    def output_worker_key(self) -&gt; Optional[str]:\n        return self._output_worker_key\n\n    @output_worker_key.setter\n    def output_worker_key(self, worker_key: str):\n        \"\"\"\n        This method is used to set the output worker of the automa dynamically.\n        \"\"\"\n        if not self._automa_running:\n            self._output_worker_key = worker_key\n        else:\n            deferred_task = _SetOutputWorkerDeferredTask(\n                output_worker_key=worker_key,\n            )\n            # Note: Only the last _SetOutputWorkerDeferredTask is valid if self.output_worker_key is set multiple times in one DS.\n            self._set_output_worker_deferred_task = deferred_task\n\n    def _validate_canonical_graph(self):\n        \"\"\"\n        This method is used to validate that DDG graph is canonical.\n        \"\"\"\n        for worker_key, worker_obj in self._workers.items():\n            for dependency_key in worker_obj.dependencies:\n                if dependency_key not in self._workers:\n                    raise AutomaCompilationError(\n                        f\"the dependency `{dependency_key}` of worker `{worker_key}` does not exist\"\n                    )\n        assert set(self._workers.keys()) == set(self._workers_dynamic_states.keys())\n        for worker_key, worker_dynamic_state in self._workers_dynamic_states.items():\n            for dependency_key in worker_dynamic_state.dependency_triggers:\n                assert dependency_key in self._workers[worker_key].dependencies\n\n        for worker_key, worker_obj in self._workers.items():\n            for dependency_key in worker_obj.dependencies:\n                assert worker_key in self._worker_forwards[dependency_key]\n        for worker_key, successor_keys in self._worker_forwards.items():\n            for successor_key in successor_keys:\n                assert worker_key in self._workers[successor_key].dependencies\n\n    def _compile_graph_and_detect_risks(self):\n        \"\"\"\n        This method should be called at the very beginning of self.run() to ensure that:\n        1. The whole graph is built out of all of the following worker sources:\n            - Pre-defined workers, such as:\n                - Methods decorated with @worker(...)\n            - Post-added workers, such as:\n                - Functions decorated with @automa_obj.worker(...)\n                - Workers added via automa_obj.add_func_as_worker(...)\n                - Workers added via automa_obj.add_worker(...)\n        2. The dependencies of each worker are confirmed to satisfy the DAG constraints.\n        \"\"\"\n\n        # Validate the canonical graph.\n        self._validate_canonical_graph()\n        # Validate the DAG constraints.\n        GraphAutomaMeta.validate_dag_constraints(self._worker_forwards)\n        # TODO: More validations can be added here...\n\n        # Validate if the output worker exists.\n        if self._output_worker_key and self._output_worker_key not in self._workers:\n            raise AutomaCompilationError(\n                f\"the output worker is not found: \"\n                f\"output_worker_key={self._output_worker_key}\"\n            )\n\n        # Find all connected components of the whole automa graph.\n        self._find_connected_components()\n\n    def ferry_to(self, worker_key: str, /, *args, **kwargs):\n        \"\"\"\n        Defer the invocation to the specified worker, passing any provided arguments. This creates a \n        delayed call, ensuring the worker will be scheduled to run asynchronously in the next event loop, \n        independent of its dependencies.\n\n        This primitive is commonly used for:\n\n        1. Implementing dynamic branching based on runtime conditions.\n        2. Creating logic that forms cyclic graphs.\n\n        Parameters\n        ----------\n        worker_key : str\n            The key of the worker to run.\n        args : optional\n            Positional arguments to be passed.\n        kwargs : optional\n            Keyword arguments to be passed.\n\n        Examples\n        --------\n        ```python\n        class MyGraphAutoma(GraphAutoma):\n            @worker(is_start=True)\n            def start_worker(self):\n                number = random.randint(0, 1)\n                if number == 0:\n                    self.ferry_to(\"cond_1_worker\", number=number)\n                else:\n                    self.ferry_to(\"cond_2_worker\")\n\n            @worker()\n            def cond_1_worker(self, number: int):\n                print(f'Got {{number}}!')\n\n            @worker()\n            def cond_2_worker(self):\n                self.ferry_to(\"start_worker\")\n\n        automa = MyGraphAutoma()\n        await automa.arun()\n\n        # Output: Got 0!\n        ```\n        \"\"\"\n        # TODO: check worker_key is valid, maybe deferred check...\n        running_options = self._get_top_running_options()\n        # if debug is enabled, trace back the kickoff worker key from stacktrace.\n        kickoff_worker_key: str = self._trace_back_kickoff_worker_key_from_stack() if running_options.debug else None\n        deferred_task = _FerryDeferredTask(\n            ferry_to_worker_key=worker_key,\n            kickoff_worker_key=kickoff_worker_key,\n            args=args,\n            kwargs=kwargs,\n        )\n        # Note: ferry_to() may be called in a new thread.\n        # But _ferry_deferred_tasks is not necessary to be thread-safe due to Visibility Guarantees of the Bridgic Concurrency Model.\n        self._ferry_deferred_tasks.append(deferred_task)\n\n\n    def get_local_space(self, runtime_context: RuntimeContext) -&gt; Dict[str, Any]:\n        \"\"\"\n        Get the local space, if you want to clean the local space after automa.arun(), you can override the should_reset_local_space() method.\n\n        Parameters\n        ----------\n        runtime_context : RuntimeContext\n            The runtime context.\n\n        Returns\n        -------\n        Dict[str, Any]\n            The local space.\n        \"\"\"\n        worker_key = runtime_context.worker_key\n        worker_obj = self._workers[worker_key]\n        return worker_obj.local_space\n\n    def _clean_all_worker_local_space(self):\n        \"\"\"\n        Clean the local space of all workers.\n        \"\"\"\n        for worker_obj in self._workers.values():\n            worker_obj.local_space = {}\n\n    def should_reset_local_space(self) -&gt; bool:\n        \"\"\"\n        This method indicates whether to reset the local space at the end of the arun method of GraphAutoma. \n        By default, it returns True, standing for resetting. Otherwise, it means doing nothing.\n\n        Examples:\n        --------\n        ```python\n        class MyAutoma(GraphAutoma):\n            def should_reset_local_space(self) -&gt; bool:\n                return False\n        ```\n        \"\"\"\n        return True\n\n    async def arun(\n        self, \n        *args: Tuple[Any, ...],\n        interaction_feedback: Optional[InteractionFeedback] = None,\n        interaction_feedbacks: Optional[List[InteractionFeedback]] = None,\n        **kwargs: Dict[str, Any]\n    ) -&gt; Any:\n        \"\"\"\n        The entry point for running the constructed `GraphAutoma` instance.\n\n        Parameters\n        ----------\n        args : optional\n            Positional arguments to be passed.\n\n        interaction_feedback : Optional[InteractionFeedback]\n            Feedback that is received from a human interaction. Only one of interaction_feedback or \n            interaction_feedbacks should be provided at a time.\n\n        interaction_feedbacks : Optional[List[InteractionFeedback]]\n            Feedbacks that are received from multiple interactions occurred simultaneously before the Automa \n            was paused. Only one of interaction_feedback or interaction_feedbacks should be provided at a time.\n\n        kwargs : optional\n            Keyword arguments to be passed.\n\n        Returns\n        -------\n        Any\n            The execution result of the output-worker if `output_worker_key` is specified, otherwise None.\n\n        Raises\n        ------\n        InteractionException\n            If the Automa is the top-level Automa and the `interact_with_human()` method is called by \n            one or more workers, this exception will be raised to the application layer.\n\n        _InteractionEventException\n            If the Automa is not the top-level Automa and the `interact_with_human()` method is called by \n            one or more workers, this exception will be raised to the upper level Automa.\n        \"\"\"\n\n        def _reinit_current_kickoff_workers_if_needed():\n            # Note: After deserialization, the _current_kickoff_workers must not be empty!\n            # Therefore, _current_kickoff_workers will only be reinitialized when the Automa is run for the first time or rerun.\n            # It is guaranteed that _current_kickoff_workers will not be reinitialized when the Automa is resumed after deserialization.\n            if not self._current_kickoff_workers:\n                self._current_kickoff_workers = [\n                    _KickoffInfo(\n                        worker_key=worker_key,\n                        last_kickoff=\"__automa__\"\n                    ) for worker_key, worker_obj in self._workers.items()\n                    if getattr(worker_obj, \"is_start\", False)\n                ]\n                # Each time the Automa re-runs, buffer the input arguments here.\n                self._input_buffer.args = args\n                self._input_buffer.kwargs = kwargs\n\n        def _execute_topology_change_deferred_tasks(tc_tasks: List[Union[_AddWorkerDeferredTask, _RemoveWorkerDeferredTask, _AddDependencyDeferredTask]]):\n            for topology_task in tc_tasks:\n                if topology_task.task_type == \"add_worker\":\n                    self._add_worker_incrementally(\n                        key=topology_task.worker_key,\n                        worker=topology_task.worker_obj,\n                        dependencies=topology_task.dependencies,\n                        is_start=topology_task.is_start,\n                        args_mapping_rule=topology_task.args_mapping_rule,\n                    )\n                elif topology_task.task_type == \"remove_worker\":\n                    self._remove_worker_incrementally(topology_task.worker_key)\n                elif topology_task.task_type == \"add_dependency\":\n                    self._add_dependency_incrementally(topology_task.worker_key, topology_task.dependency)\n\n        def _set_worker_run_finished(worker_key: str):\n            for kickoff_info in self._current_kickoff_workers:\n                if kickoff_info.worker_key == worker_key:\n                    kickoff_info.run_finished = True\n                    break\n\n        def _check_and_normalize_interaction_params(\n            interaction_feedback: Optional[InteractionFeedback] = None,\n            interaction_feedbacks: Optional[List[InteractionFeedback]] = None,\n        ):\n            if interaction_feedback and interaction_feedbacks:\n                raise AutomaRuntimeError(\n                    f\"Only one of interaction_feedback or interaction_feedbacks can be used. \"\n                    f\"But received interaction_feedback={interaction_feedback} and \\n\"\n                    f\"interaction_feedbacks={interaction_feedbacks}\"\n                )\n            if interaction_feedback:\n                rx_feedbacks = [interaction_feedback]\n            else:\n                rx_feedbacks = interaction_feedbacks\n            return rx_feedbacks\n\n        def _match_ongoing_interaction_and_feedbacks(rx_feedbacks:List[InteractionFeedback]):\n            match_left_feedbacks = []\n            for feedback in rx_feedbacks:\n                matched = False\n                for interaction_and_feedbacks in self._ongoing_interactions.values():\n                    for interaction_and_feedback in interaction_and_feedbacks:\n                        if interaction_and_feedback.interaction.interaction_id == feedback.interaction_id:\n                            matched = True\n                            # Note: Only one feedback is allowed for each interaction. Here we assume that only the first feedback is valid, which is a choice of implementation.\n                            if interaction_and_feedback.feedback is None:\n                                # Set feedback to self._ongoing_interactions\n                                interaction_and_feedback.feedback = feedback\n                            break\n                    if matched:\n                        break\n                if not matched:\n                    match_left_feedbacks.append(feedback)\n            return match_left_feedbacks\n\n        running_options = self._get_top_running_options()\n\n        self._main_loop = asyncio.get_running_loop()\n        self._main_thread_id = threading.get_ident()\n\n        if self.thread_pool is None:\n            self.thread_pool = ThreadPoolExecutor(thread_name_prefix=\"bridgic-thread\")\n\n        if not self._automa_running:\n            # Here is the last chance to compile and check the DDG in the end of the [Initialization Phase] (phase 1 just before the first DS).\n            self._compile_graph_and_detect_risks()\n            self._automa_running = True\n\n        # An Automa needs to be re-run with _current_kickoff_workers reinitialized.\n        _reinit_current_kickoff_workers_if_needed()\n\n        rx_feedbacks = _check_and_normalize_interaction_params(interaction_feedback, interaction_feedbacks)\n        if rx_feedbacks:\n            rx_feedbacks = _match_ongoing_interaction_and_feedbacks(rx_feedbacks)\n\n        if running_options.debug:\n            printer.print(f\"\\n{type(self).__name__}-[{self.name}] is getting started.\", color=\"green\")\n\n        # Task loop divided into many dynamic steps (DS).\n        while self._current_kickoff_workers:\n            # A new DS started.\n            if running_options.debug:\n                kickoff_worker_keys = [kickoff_info.worker_key for kickoff_info in self._current_kickoff_workers]\n                printer.print(f\"[DS][Before Tasks Started] kickoff workers: {kickoff_worker_keys}\", color=\"purple\")\n\n            for kickoff_info in self._current_kickoff_workers:\n                if kickoff_info.run_finished:\n                    # Skip finished workers. Here is the case that the Automa is resumed after a human interaction.\n                    if running_options.debug:\n                        printer.print(f\"[{kickoff_info.worker_key}] will be skipped - run finished\", color=\"blue\")\n                    continue\n\n                if running_options.debug:\n                    kickoff_name = kickoff_info.last_kickoff\n                    if kickoff_name == \"__automa__\":\n                        kickoff_name = f\"{kickoff_name}:({self.name})\"\n                    printer.print(f\"[{kickoff_name}] will kick off [{kickoff_info.worker_key}]\", color=\"cyan\")\n\n                # First, Arguments Mapping:\n                if kickoff_info.last_kickoff == \"__automa__\":\n                    next_args, next_kwargs = self._input_buffer.args, {}\n                elif kickoff_info.from_ferry:\n                    next_args, next_kwargs = kickoff_info.args, kickoff_info.kwargs\n                else:\n                    next_args, next_kwargs = self._mapping_args(\n                        kickoff_worker_key=kickoff_info.last_kickoff,\n                        current_worker_key=kickoff_info.worker_key,\n                    )\n                # Second, Inputs Propagation:\n                next_args, next_kwargs = self._propagate_inputs(\n                    current_worker_key=kickoff_info.worker_key,\n                    next_args=next_args,\n                    next_kwargs=next_kwargs,\n                    input_kwargs=self._input_buffer.kwargs,\n                )\n                # Third, Resolve data injection.\n                worker_obj = self._workers[kickoff_info.worker_key]\n                next_args, next_kwargs = injector.inject(\n                    current_worker_key=kickoff_info.worker_key, \n                    current_worker_sig=worker_obj.get_input_param_names(), \n                    worker_dict=self._workers, \n                    worker_output=self._worker_output,\n                    next_args=next_args, \n                    next_kwargs=next_kwargs\n                )\n\n                # Schedule task for each kickoff worker.\n                if worker_obj.is_automa():\n                    coro = worker_obj.arun(\n                        *next_args, \n                        interaction_feedbacks=rx_feedbacks, \n                        **next_kwargs\n                    )\n                else:\n                    coro = worker_obj.arun(*next_args, **next_kwargs)\n\n                task = asyncio.create_task(\n                    # TODO1: arun() may need to be wrapped to support better interrupt...\n                    coro,\n                    name=f\"Task-{kickoff_info.worker_key}\"\n                )\n                self._running_tasks.append(_RunnningTask(\n                    worker_key=kickoff_info.worker_key,\n                    task=task,\n                ))\n\n            # Wait until all of the tasks are finished.\n            while True:\n                undone_tasks = [t.task for t in self._running_tasks if not t.task.done()]\n                if not undone_tasks:\n                    break\n                try:\n                    await undone_tasks[0]\n                except Exception as e:\n                    ...\n                    # The same exception will be raised again in the following task.result().\n                    # Note: A Task is done when the wrapped coroutine either returned a value, raised an exception, or the Task was cancelled.\n                    # Refer to: https://docs.python.org/3/library/asyncio-task.html#task-object\n\n            # Process graph topology change deferred tasks triggered by add_worker() and remove_worker().\n            _execute_topology_change_deferred_tasks(self._topology_change_deferred_tasks)\n\n            # Perform post-task follow-up operations.\n            interaction_exceptions: List[_InteractionEventException] = []\n            for task in self._running_tasks:\n                # task.task.result must be called here! It will raise an exception if task failed.\n                try:\n                    task_result = task.task.result()\n                    _set_worker_run_finished(task.worker_key)\n                    if task.worker_key in self._workers:\n                        # The current running worker may be removed.\n                        worker_obj = self._workers[task.worker_key]\n                        # Collect results of the finished tasks.\n                        self._worker_output[task.worker_key] = task_result\n                        # reset dynamic states of finished workers.\n                        self._workers_dynamic_states[task.worker_key].dependency_triggers = set(getattr(worker_obj, \"dependencies\", []))\n                        # Update the dynamic states of successor workers.\n                        for successor_key in self._worker_forwards.get(task.worker_key, []):\n                            self._workers_dynamic_states[successor_key].dependency_triggers.remove(task.worker_key)\n                        # Each time a worker is finished running, the ongoing interaction states should be cleared. Once it is re-run, the human interactions in the worker can be triggered again.\n                        if task.worker_key in self._worker_interaction_indices:\n                            del self._worker_interaction_indices[task.worker_key]\n                        if task.worker_key in self._ongoing_interactions:\n                            del self._ongoing_interactions[task.worker_key]\n\n                except _InteractionEventException as e:\n                    interaction_exceptions.append(e)\n                    if task.worker_key in self._workers and not self._workers[task.worker_key].is_automa():\n                        if task.worker_key not in self._ongoing_interactions:\n                            self._ongoing_interactions[task.worker_key] = []\n                        interaction=e.args[0]\n                        # Make sure the interaction_id is unique for each human interaction.\n                        found = False\n                        for iaf in self._ongoing_interactions[task.worker_key]:\n                            if iaf.interaction.interaction_id == interaction.interaction_id:\n                                found = True\n                                break\n                        if not found:\n                            self._ongoing_interactions[task.worker_key].append(_InteractionAndFeedback(\n                                interaction=interaction,\n                            ))\n\n            if len(self._topology_change_deferred_tasks) &gt; 0:\n                # Graph topology validation and risk detection. Only needed when topology changes.\n                # Guarantee the graph topology is valid and consistent after each DS.\n                # 1. Validate the canonical graph.\n                self._validate_canonical_graph()\n                # 2. Validate the DAG constraints.\n                GraphAutomaMeta.validate_dag_constraints(self._worker_forwards)\n                # TODO: more validations can be added here...\n\n            # Process the output_worker_key setting deferred task.\n            if self._set_output_worker_deferred_task and self._set_output_worker_deferred_task.output_worker_key in self._workers:\n                self._output_worker_key = self._set_output_worker_deferred_task.output_worker_key\n\n            # TODO: Ferry-related risk detection may be added here...\n\n            # Just after post-task operations (several deferred tasks) and before finding next kickoff workers, collect and process the interaction exceptions.\n            if len(interaction_exceptions) &gt; 0:\n                all_interactions: List[Interaction] = [interaction for e in interaction_exceptions for interaction in e.args]\n                if self.parent is None:\n                    # This is the top-level Automa. Serialize the Automa and raise InteractionException to the application layer.\n                    serialized_automa = msgpackx.dump_bytes(self)\n                    snapshot = Snapshot(\n                        serialized_bytes=serialized_automa,\n                        serialization_version=GraphAutoma.SERIALIZATION_VERSION,\n                    )\n                    raise InteractionException(\n                        interactions=all_interactions,\n                        snapshot=snapshot,\n                    )\n                else:\n                    # Continue raise exception to the upper level Automa.\n                    raise _InteractionEventException(*all_interactions)\n\n            # Find next kickoff workers and rebuild _current_kickoff_workers\n            run_finished_worker_keys: List[str] = [kickoff_info.worker_key for kickoff_info in self._current_kickoff_workers if kickoff_info.run_finished]\n            assert len(run_finished_worker_keys) == len(self._current_kickoff_workers)\n            self._current_kickoff_workers = []\n            # New kickoff workers can be triggered by two ways:\n            # 1. The ferry_to() operation is called during current worker execution.\n            # 2. The dependencies are eliminated after all predecessor workers are finished.\n            # So,\n            # First add kickoff workers triggered by ferry_to();\n            for ferry_task in self._ferry_deferred_tasks:\n                self._current_kickoff_workers.append(_KickoffInfo(\n                    worker_key=ferry_task.ferry_to_worker_key,\n                    last_kickoff=ferry_task.kickoff_worker_key,\n                    from_ferry=True,\n                    args=ferry_task.args,\n                    kwargs=ferry_task.kwargs,\n                ))\n            # Then add kickoff workers triggered by dependencies elimination.\n            # Merge successor keys of all finished tasks.\n            successor_keys = set()\n            for worker_key in run_finished_worker_keys:\n                # Note: The `worker_key` worker may have been removed from the Automa.\n                for successor_key in self._worker_forwards.get(worker_key, []):\n                    if successor_key not in successor_keys:\n                        dependency_triggers = self._workers_dynamic_states[successor_key].dependency_triggers\n                        if not dependency_triggers:\n                            self._current_kickoff_workers.append(_KickoffInfo(\n                                worker_key=successor_key,\n                                last_kickoff=worker_key,\n                            ))\n                        successor_keys.add(successor_key)\n            if running_options.debug:\n                deferred_ferrys = [ferry_task.ferry_to_worker_key for ferry_task in self._ferry_deferred_tasks]\n                printer.print(f\"[DS][After Tasks Finished] successor workers: {successor_keys}, deferred ferrys: {deferred_ferrys}\", color=\"purple\")\n\n            # Clear running tasks after all finished.\n            self._running_tasks.clear()\n            self._ferry_deferred_tasks.clear()\n            self._topology_change_deferred_tasks.clear()\n            self._set_output_worker_deferred_task = None\n\n        if running_options.debug:\n            printer.print(f\"{type(self).__name__}-[{self.name}] is finished.\", color=\"green\")\n\n        # After a complete run, reset all necessary states to allow the automa to re-run.\n        self._input_buffer = _AutomaInputBuffer()\n        if self.should_reset_local_space():\n            self._clean_all_worker_local_space()\n        self._ongoing_interactions.clear()\n        self._worker_interaction_indices.clear()\n        self._automa_running = False\n\n        # If the output-worker is specified, return its output as the return value of the automa.\n        if self._output_worker_key:\n            return self._worker_output[self._output_worker_key]\n        else:\n            return None\n\n    ###############################################################\n    ########## [Bridgic Event Handling Mechanism] starts ##########\n    ###############################################################\n\n    def register_event_handler(self, event_type: Optional[str], event_handler: EventHandlerType) -&gt; None:\n        \"\"\"\n        Register an event handler for the specified event type.\n\n        Note: Only event handlers registered on the top-level Automa will be invoked to handle events.\n\n        Parameters\n        ----------\n        event_type: Optional[str]\n            The type of event to be handled. If set to None, the event handler will be registered as the default handler and will be used to handle all event types.\n        event_handler: EventHandlerType\n            The event handler to be registered.\n        \"\"\"\n        if event_type is None:\n            self._default_event_handler = event_handler\n        else:\n            self._event_handlers[event_type] = event_handler\n\n    def unregister_event_handler(self, event_type: Optional[str]) -&gt; None:\n        \"\"\"\n        Unregister an event handler for the specified event type.\n\n        Parameters\n        ----------\n        event_type: Optional[str]\n            The type of event to be unregistered. If set to None, the default event handler will be unregistered.\n        \"\"\"\n        if event_type in self._event_handlers:\n            del self._event_handlers[event_type]\n        if event_type is None:\n            self._default_event_handler = None\n\n    def unregister_all_event_handlers(self) -&gt; None:\n        \"\"\"\n        Unregister all event handlers.\n        \"\"\"\n        self._event_handlers.clear()\n        self._default_event_handler = None\n\n    class _FeedbackSender(FeedbackSender):\n        def __init__(\n                self, \n                future: asyncio.Future[Feedback],\n                post_loop: AbstractEventLoop,\n                ):\n            self._future = future\n            self._post_loop = post_loop\n\n        def send(self, feedback: Feedback) -&gt; None:\n            try:\n                current_loop = asyncio.get_running_loop()\n            except Exception:\n                current_loop = None\n            try:\n                if current_loop is self._post_loop:\n                    self._future.set_result(feedback)\n                else:\n                    self._post_loop.call_soon_threadsafe(self._future.set_result, feedback)\n            except asyncio.InvalidStateError:\n                # Suppress the InvalidStateError to be raised, maybe due to timeout.\n                import warnings\n                warnings.warn(f\"Feedback future already set. feedback: {feedback}\", FutureWarning)\n\n    @override\n    def post_event(self, event: Event) -&gt; None:\n        \"\"\"\n        Post an event to the application layer outside the Automa.\n\n        The event handler implemented by the application layer will be called in the same thread as the worker (maybe the main thread or a new thread from the thread pool).\n\n        Note that `post_event` can be called in a non-async method or an async method.\n\n        The event will be bubbled up to the top-level Automa, where it will be processed by the event handler registered with the event type.\n\n        Parameters\n        ----------\n        event: Event\n            The event to be posted.\n        \"\"\"\n        def _handler_need_feedback_sender(handler: EventHandlerType):\n            positional_param_names = get_param_names_by_kind(handler, Parameter.POSITIONAL_ONLY) + get_param_names_by_kind(handler, Parameter.POSITIONAL_OR_KEYWORD)\n            var_positional_param_names = get_param_names_by_kind(handler, Parameter.VAR_POSITIONAL)\n            return len(var_positional_param_names) &gt; 0 or len(positional_param_names) &gt; 1\n\n        if self.parent is not None:\n            # Bubble up the event to the top-level Automa.\n            return self.parent.post_event(event)\n\n        # Here is the top-level Automa.\n        # Call event handlers\n        if event.event_type in self._event_handlers:\n            if _handler_need_feedback_sender(self._event_handlers[event.event_type]):\n                self._event_handlers[event.event_type](event, feedback_sender=None)\n            else:\n                self._event_handlers[event.event_type](event)\n        if self._default_event_handler is not None:\n            if _handler_need_feedback_sender(self._default_event_handler):\n                self._default_event_handler(event, feedback_sender=None)\n            else:\n                self._default_event_handler(event)\n\n    def request_feedback(\n        self, \n        event: Event,\n        timeout: Optional[float] = None\n    ) -&gt; Feedback:\n        \"\"\"\n        Request feedback for the specified event from the application layer outside the Automa. This method blocks the caller until the feedback is received.\n\n        Note that `post_event` should only be called from within a non-async method running in the new thread of the Automa thread pool.\n\n        Parameters\n        ----------\n        event: Event\n            The event to be posted to the event handler implemented by the application layer.\n        timeout: Optional[float]\n            A float or int number of seconds to wait for if the feedback is not received. If None, then there is no limit on the wait time.\n\n        Returns\n        -------\n        Feedback\n            The feedback received from the application layer.\n\n        Raises\n        ------\n        TimeoutError\n            If the feedback is not received before the timeout. Note that the raised exception is the built-in `TimeoutError` exception, instead of asyncio.TimeoutError or concurrent.futures.TimeoutError!\n        \"\"\"\n        if threading.get_ident() == self._main_thread_id:\n            raise AutomaRuntimeError(\n                f\"`request_feedback` should only be called in a different thread from the main thread of the {self.name}. \"\n            )\n        return asyncio.run_coroutine_threadsafe(\n            self.request_feedback_async(event, timeout),\n            self._main_loop\n        ).result()\n\n    async def request_feedback_async(\n        self, \n        event: Event,\n        timeout: Optional[float] = None\n    ) -&gt; Feedback:\n        \"\"\"\n        Request feedback for the specified event from the application layer outside the Automa. This method blocks the caller until the feedback is received.\n\n        The event handler implemented by the application layer will be called in the next event loop, in the main thread.\n\n        Parameters\n        ----------\n        event: Event\n            The event to be posted to the event handler implemented by the application layer.\n        timeout: Optional[float]\n            A float or int number of seconds to wait for if the feedback is not received. If None, then there is no limit on the wait time.\n\n        Returns\n        -------\n        Feedback\n            The feedback received from the application layer.\n\n        Raises\n        ------\n        TimeoutError\n            If the feedback is not received before the timeout. Note that the raised exception is the built-in `TimeoutError` exception, instead of asyncio.TimeoutError!\n        \"\"\"\n        if self.parent is not None:\n            # Bubble up the event to the top-level Automa.\n            return await self.parent.request_feedback_async(event, timeout)\n\n        # Here is the top-level Automa.\n        event_loop = asyncio.get_running_loop()\n        future = event_loop.create_future()\n        feedback_sender = self._FeedbackSender(future, event_loop)\n        # Call event handlers\n        if event.event_type in self._event_handlers:\n            self._event_handlers[event.event_type](event, feedback_sender)\n        if self._default_event_handler is not None:\n            self._default_event_handler(event, feedback_sender)\n\n        try:\n            return await asyncio.wait_for(future, timeout)\n        except TimeoutError as e:\n            # When python &gt;= 3.11 here.\n            raise TimeoutError(f\"No feedback is received before timeout in Automa[{self.name}]\") from e\n        except asyncio.TimeoutError as e:\n            # Version compatibility resolution: asyncio.wait_for raises asyncio.TimeoutError before python 3.11.\n            # https://docs.python.org/3/library/asyncio-task.html#asyncio.wait_for\n            raise TimeoutError(f\"No feedback is received before timeout in Automa[{self.name}]\") from e\n\n    ###############################################################\n    ########### [Bridgic Event Handling Mechanism] ends ###########\n    ###############################################################\n\n    ###############################################################\n    ######## [Bridgic Human Interaction Mechanism] starts #########\n    ###############################################################\n\n    def interact_with_human(self, event: Event) -&gt; InteractionFeedback:\n        kickoff_worker_key: str = self._trace_back_kickoff_worker_key_from_stack()\n        if kickoff_worker_key:\n            return self.interact_with_human_from_worker_key(event, kickoff_worker_key)\n        raise AutomaRuntimeError(\n            f\"Get kickoff worker failed in Automa[{self.name}] \"\n            f\"when trying to interact with human with event: {event}\"\n        )\n\n    def interact_with_human_from_worker(\n        self,\n        event: Event,\n        worker: Worker\n    ) -&gt; InteractionFeedback:\n        worker_key = self._loopup_worker_key_by_instance(worker)\n        if worker_key:\n            return self.interact_with_human_from_worker_key(event, worker_key)\n        raise AutomaRuntimeError(\n            f\"Not found worker[{worker}] in Automa[{self.name}] \"\n            f\"when trying to interact with human with event: {event}\"\n        )\n\n    def interact_with_human_from_worker_key(\n        self,\n        event: Event,\n        worker_key: str\n    ) -&gt; InteractionFeedback:\n        # Match interaction_feedback to see if it matches\n        matched_feedback: _InteractionAndFeedback = None\n        cur_interact_index = self._get_and_increment_interaction_index(worker_key)\n        if worker_key in self._ongoing_interactions:\n            interaction_and_feedbacks = self._ongoing_interactions[worker_key]\n            if cur_interact_index &lt; len(interaction_and_feedbacks):\n                matched_feedback = interaction_and_feedbacks[cur_interact_index]\n                # Check the event type\n                if event.event_type != matched_feedback.interaction.event.event_type:\n                    raise AutomaRuntimeError(\n                        f\"Event type mismatch! Automa[{self.name}-worker[{worker_key}]]. \"\n                        f\"interact_with_human passed-in event: {event}\\n\"\n                        f\"ongoing interaction &amp;&amp; feedback: {matched_feedback}\\n\"\n                    )\n        if matched_feedback is None or matched_feedback.feedback is None:\n            # Important: The interaction_id should be unique for each human interaction.\n            interaction_id = uuid.uuid4().hex if matched_feedback is None else matched_feedback.interaction.interaction_id\n            # Match interaction_feedback failed, raise an exception to go into the human interactioin process.\n            raise _InteractionEventException(Interaction(\n                interaction_id=interaction_id,\n                event=event,\n            ))\n        else:\n            # Match interaction_feedback succeeded, return it.\n            return matched_feedback.feedback\n\n    def _get_and_increment_interaction_index(self, worker_key: str) -&gt; int:\n        if worker_key not in self._worker_interaction_indices:\n            cur_index = 0\n            self._worker_interaction_indices[worker_key] = 0\n        else:\n            cur_index = self._worker_interaction_indices[worker_key]\n        self._worker_interaction_indices[worker_key] += 1\n        return cur_index\n\n    ###############################################################\n    ######### [Bridgic Human Interaction Mechanism] ends ##########\n    ###############################################################\n\n    def _get_worker_dependencies(self, worker_key: str) -&gt; List[str]:\n        \"\"\"\n        Get the worker keys of all dependencies of the worker.\n        \"\"\"\n        deps = self._workers[worker_key].dependencies\n        return [] if deps is None else deps\n\n    def _find_connected_components(self):\n        \"\"\"\n        Find all of the connected components in the whole automa graph described by self._workers.\n        \"\"\"\n        visited = set()\n        component_list = []\n        component_idx = {}\n\n        def dfs(worker: str, component: List[str]):\n            visited.add(worker)\n            component.append(worker)\n            for target in self._worker_forwards.get(worker, []):\n                if target not in visited:\n                    dfs(target, component)\n\n        for worker in self._workers.keys():\n            if worker not in visited:\n                component_list.append([])\n                current_idx = len(component_list) - 1\n                current_component = component_list[current_idx]\n\n                dfs(worker, current_component)\n\n                for worker in current_component:\n                    component_idx[worker] = current_idx\n\n        # self._component_list, self._component_idx = component_list, component_idx\n        # TODO: check how to use _component_list and _component_idx...\n\n    def _trace_back_kickoff_worker_key_from_stack(self) -&gt; Optional[str]:\n        worker = self._get_current_running_worker_instance_by_stacktrace()\n        if worker:\n            return self._loopup_worker_key_by_instance(worker)\n        return None\n\n    def _get_current_running_worker_instance_by_stacktrace(self) -&gt; Optional[Worker]:\n        for frame_info in inspect.stack():\n            frame = frame_info.frame\n            if 'self' in frame.f_locals:\n                self_obj = frame.f_locals['self']\n                if isinstance(self_obj, Worker) and (not isinstance(self_obj, Automa)) and (frame_info.function == \"arun\" or frame_info.function == \"run\"):\n                    return self_obj\n        return None\n\n    def _loopup_worker_key_by_instance(self, worker: Worker) -&gt; Optional[str]:\n        for worker_key, worker_obj in self._workers.items():\n            if worker_obj == worker:\n                # Note: _GraphAdaptedWorker.__eq__() is overridden to support the '==' operator.\n                return worker_key\n        return None\n\n    def _mapping_args(\n        self, \n        kickoff_worker_key: str,\n        current_worker_key: str,\n    ) -&gt; Tuple[Tuple[Any, ...], Dict[str, Any]]:\n        \"\"\"\n        Resolve arguments mapping between workers that have dependency relationships.\n\n        Parameters\n        ----------\n        kickoff_worker_key : str\n            The key of the kickoff worker.\n        current_worker_key : str\n            The key of the current worker.\n\n        Returns\n        -------\n        Tuple[Tuple[Any, ...], Dict[str, Any]]\n            The mapped positional arguments and keyword arguments.\n        \"\"\"\n        current_worker_obj = self._workers[current_worker_key]\n        args_mapping_rule = current_worker_obj.args_mapping_rule\n        dep_workers_keys = self._get_worker_dependencies(current_worker_key)\n        assert kickoff_worker_key in dep_workers_keys\n\n        def as_is_return_values(results: List[Any]) -&gt; Tuple[Tuple, Dict[str, Any]]:\n            next_args, next_kwargs = tuple(results), {}\n            return next_args, next_kwargs\n\n        def unpack_return_value(result: Any) -&gt; Tuple[Tuple, Dict[str, Any]]:\n            if dep_workers_keys != [kickoff_worker_key]:\n                raise WorkerArgsMappingError(\n                    f\"The worker \\\"{current_worker_key}\\\" must has exactly one dependency for the args_mapping_rule=\\\"{ArgsMappingRule.UNPACK}\\\", \"\n                    f\"but got {len(dep_workers_keys)} dependencies: {dep_workers_keys}\"\n                )\n            # result is not allowed to be None, since None can not be unpacked.\n            if isinstance(result, (List, Tuple)):\n                # Similar args mapping logic to as_is_return_values()\n                next_args, next_kwargs = tuple(result), {}\n            elif isinstance(result, Mapping):\n                next_args, next_kwargs = (), {**result}\n\n            else:\n                # Other types, including None, are not unpackable.\n                raise WorkerArgsMappingError(\n                    f\"args_mapping_rule=\\\"{ArgsMappingRule.UNPACK}\\\" is only valid for \"\n                    f\"tuple/list, or dict. But the worker \\\"{current_worker_key}\\\" got type \\\"{type(result)}\\\" from the kickoff worker \\\"{kickoff_worker_key}\\\".\"\n                )\n            return next_args, next_kwargs\n\n        def merge_return_values(results: List[Any]) -&gt; Tuple[Tuple, Dict[str, Any]]:\n            next_args, next_kwargs = tuple([results]), {}\n            return next_args, next_kwargs\n\n        if args_mapping_rule == ArgsMappingRule.AS_IS:\n            next_args, next_kwargs = as_is_return_values([self._worker_output[dep_worker_key] for dep_worker_key in dep_workers_keys])\n        elif args_mapping_rule == ArgsMappingRule.UNPACK:\n            next_args, next_kwargs = unpack_return_value(self._worker_output[kickoff_worker_key])\n        elif args_mapping_rule == ArgsMappingRule.MERGE:\n            next_args, next_kwargs = merge_return_values([self._worker_output[dep_worker_key] for dep_worker_key in dep_workers_keys])\n        elif args_mapping_rule == ArgsMappingRule.SUPPRESSED:\n            next_args, next_kwargs = (), {}\n\n        return next_args, next_kwargs\n\n    def _propagate_inputs(\n        self, \n        current_worker_key: str,\n        next_args: Tuple[Any, ...],\n        next_kwargs: Dict[str, Any],\n        input_kwargs: Dict[str, Any],\n    ) -&gt; Tuple[Tuple[Any, ...], Dict[str, Any]]:\n        \"\"\"\n        Resolve inputs propagation from the input buffer of the container Automa to every worker within the Automa.\n\n        Parameters\n        ----------\n        current_worker_key : str\n            The key of the current worker.\n        next_args : Tuple[Any, ...]\n            The positional arguments to be mapped.\n        next_kwargs : Dict[str, Any]\n            The keyword arguments to be mapped.\n        input_kwargs : Dict[str, Any]\n            The keyword arguments to be propagated from the input buffer of the container Automa.\n\n        Returns\n        -------\n        Tuple[Tuple[Any, ...], Dict[str, Any]]\n            The mapped positional arguments and keyword arguments.\n        \"\"\"\n        current_worker_obj = self._workers[current_worker_key]\n        input_kwargs = {k:v for k,v in input_kwargs.items() if k not in next_kwargs}\n        next_kwargs = {**next_kwargs, **input_kwargs}\n        rx_param_names_dict = current_worker_obj.get_input_param_names()\n        next_args, next_kwargs = safely_map_args(next_args, next_kwargs, rx_param_names_dict)\n        return next_args, next_kwargs\n\n    def __repr__(self) -&gt; str:\n        # TODO : It's good to make __repr__() of Automa compatible with eval().\n        # This feature depends on the implementation of __repr__() of workers.\n        class_name = self.__class__.__name__\n        workers_str = self._workers.__repr__()\n        return f\"{class_name}(workers={workers_str})\"\n\n    def __str__(self) -&gt; str:\n        d = {}\n        for k, v in self._workers.items():\n            d[k] = f\"{v} depends on {getattr(v, 'dependencies', [])}\"\n        return json.dumps(d, ensure_ascii=False, indent=4)\n</code></pre>"},{"location":"api/bridgic-core/bridgic/core/automa/graph_automa/#bridgic.core.automa.graph_automa.GraphAutoma.all_workers","title":"all_workers","text":"<pre><code>all_workers() -&gt; List[str]\n</code></pre> <p>Gets a list containing the keys of all workers registered in this Automa.</p> <p>Returns:</p> Type Description <code>List[str]</code> <p>A list of worker keys.</p> Source code in <code>bridgic/core/automa/graph_automa.py</code> <pre><code>def all_workers(self) -&gt; List[str]:\n    \"\"\"\n    Gets a list containing the keys of all workers registered in this Automa.\n\n    Returns\n    -------\n    List[str]\n        A list of worker keys.\n    \"\"\"\n    return list(self._workers.keys())\n</code></pre>"},{"location":"api/bridgic-core/bridgic/core/automa/graph_automa/#bridgic.core.automa.graph_automa.GraphAutoma.add_worker","title":"add_worker","text":"<pre><code>add_worker(\n    key: str,\n    worker: Worker,\n    *,\n    dependencies: List[str] = [],\n    is_start: bool = False,\n    args_mapping_rule: ArgsMappingRule = AS_IS\n) -&gt; None\n</code></pre> <p>This method is used to add a worker dynamically into the automa.</p> <p>If this method is called during the [Initialization Phase], the worker will be added immediately. If this method is called during the [Running Phase], the worker will be added as a deferred task which will be executed in the next DS.</p> <p>The dependencies can be added together with a worker. However, you can add a worker without any dependencies.</p> <p>Note: The args_mapping_rule can only be set together with adding a worker, even if the worker has no any dependencies.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>The key of the worker.</p> required <code>worker</code> <code>Worker</code> <p>The worker instance to be registered.</p> required <code>dependencies</code> <code>List[str]</code> <p>A list of worker keys that the worker depends on.</p> <code>[]</code> <code>is_start</code> <code>bool</code> <p>Whether the worker is a start worker.</p> <code>False</code> <code>args_mapping_rule</code> <code>ArgsMappingRule</code> <p>The rule of arguments mapping.</p> <code>AS_IS</code> Source code in <code>bridgic/core/automa/graph_automa.py</code> <pre><code>def add_worker(\n    self,\n    key: str,\n    worker: Worker,\n    *,\n    dependencies: List[str] = [],\n    is_start: bool = False,\n    args_mapping_rule: ArgsMappingRule = ArgsMappingRule.AS_IS,\n) -&gt; None:\n    \"\"\"\n    This method is used to add a worker dynamically into the automa.\n\n    If this method is called during the [Initialization Phase], the worker will be added immediately. If this method is called during the [Running Phase], the worker will be added as a deferred task which will be executed in the next DS.\n\n    The dependencies can be added together with a worker. However, you can add a worker without any dependencies.\n\n    Note: The args_mapping_rule can only be set together with adding a worker, even if the worker has no any dependencies.\n\n    Parameters\n    ----------\n    key : str\n        The key of the worker.\n    worker : Worker\n        The worker instance to be registered.\n    dependencies : List[str]\n        A list of worker keys that the worker depends on.\n    is_start : bool\n        Whether the worker is a start worker.\n    args_mapping_rule : ArgsMappingRule\n        The rule of arguments mapping.\n    \"\"\"\n    self._add_worker_internal(\n        key=key,\n        worker=worker,\n        dependencies=dependencies,\n        is_start=is_start,\n        args_mapping_rule=args_mapping_rule,\n    )\n</code></pre>"},{"location":"api/bridgic-core/bridgic/core/automa/graph_automa/#bridgic.core.automa.graph_automa.GraphAutoma.add_func_as_worker","title":"add_func_as_worker","text":"<pre><code>add_func_as_worker(\n    key: str,\n    func: Callable,\n    *,\n    dependencies: List[str] = [],\n    is_start: bool = False,\n    args_mapping_rule: ArgsMappingRule = AS_IS\n) -&gt; None\n</code></pre> <p>This method is used to add a function as a worker into the automa.</p> <p>The format of the parameters will follow that of the decorator @worker(...), so that the  behavior of the decorated function is consistent with that of normal CallableLandableWorker objects.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>The key of the function worker.</p> required <code>func</code> <code>Callable</code> <p>The function to be added as a worker to the automa.</p> required <code>dependencies</code> <code>List[str]</code> <p>A list of worker names that the decorated callable depends on.</p> <code>[]</code> <code>is_start</code> <code>bool</code> <p>Whether the decorated callable is a start worker. True means it is, while False means it is not.</p> <code>False</code> <code>args_mapping_rule</code> <code>ArgsMappingRule</code> <p>The rule of arguments mapping.</p> <code>AS_IS</code> Source code in <code>bridgic/core/automa/graph_automa.py</code> <pre><code>def add_func_as_worker(\n    self,\n    key: str,\n    func: Callable,\n    *,\n    dependencies: List[str] = [],\n    is_start: bool = False,\n    args_mapping_rule: ArgsMappingRule = ArgsMappingRule.AS_IS,\n) -&gt; None:\n    \"\"\"\n    This method is used to add a function as a worker into the automa.\n\n    The format of the parameters will follow that of the decorator @worker(...), so that the \n    behavior of the decorated function is consistent with that of normal CallableLandableWorker objects.\n\n    Parameters\n    ----------\n    key : str\n        The key of the function worker.\n    func : Callable\n        The function to be added as a worker to the automa.\n    dependencies : List[str]\n        A list of worker names that the decorated callable depends on.\n    is_start : bool\n        Whether the decorated callable is a start worker. True means it is, while False means it is not.\n    args_mapping_rule : ArgsMappingRule\n        The rule of arguments mapping.\n    \"\"\"\n    self._add_func_as_worker_internal(\n        key=key,\n        func=func,\n        dependencies=dependencies,\n        is_start=is_start,\n        args_mapping_rule=args_mapping_rule,\n    )\n</code></pre>"},{"location":"api/bridgic-core/bridgic/core/automa/graph_automa/#bridgic.core.automa.graph_automa.GraphAutoma.worker","title":"worker","text":"<pre><code>worker(\n    *,\n    key: Optional[str] = None,\n    dependencies: List[str] = [],\n    is_start: bool = False,\n    args_mapping_rule: ArgsMappingRule = AS_IS\n) -&gt; Callable\n</code></pre> <p>This is a decorator used to mark a function as an GraphAutoma detectable Worker. Dislike the  global decorator @worker(...), it is usally used after an GraphAutoma instance is initialized.</p> <p>The format of the parameters will follow that of the decorator @worker(...), so that the  behavior of the decorated function is consistent with that of normal CallableLandableWorker objects.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>The key of the worker. If not provided, the name of the decorated callable will be used.</p> <code>None</code> <code>dependencies</code> <code>List[str]</code> <p>A list of worker names that the decorated callable depends on.</p> <code>[]</code> <code>is_start</code> <code>bool</code> <p>Whether the decorated callable is a start worker. True means it is, while False means it is not.</p> <code>False</code> <code>args_mapping_rule</code> <code>str</code> <p>The rule of arguments mapping. The options are: \"auto\", \"as_list\", \"as_dict\", \"suppressed\".</p> <code>AS_IS</code> Source code in <code>bridgic/core/automa/graph_automa.py</code> <pre><code>def worker(\n    self,\n    *,\n    key: Optional[str] = None,\n    dependencies: List[str] = [],\n    is_start: bool = False,\n    args_mapping_rule: ArgsMappingRule = ArgsMappingRule.AS_IS,\n) -&gt; Callable:\n    \"\"\"\n    This is a decorator used to mark a function as an GraphAutoma detectable Worker. Dislike the \n    global decorator @worker(...), it is usally used after an GraphAutoma instance is initialized.\n\n    The format of the parameters will follow that of the decorator @worker(...), so that the \n    behavior of the decorated function is consistent with that of normal CallableLandableWorker objects.\n\n    Parameters\n    ----------\n    key : str\n        The key of the worker. If not provided, the name of the decorated callable will be used.\n    dependencies : List[str]\n        A list of worker names that the decorated callable depends on.\n    is_start : bool\n        Whether the decorated callable is a start worker. True means it is, while False means it is not.\n    args_mapping_rule : str\n        The rule of arguments mapping. The options are: \"auto\", \"as_list\", \"as_dict\", \"suppressed\".\n    \"\"\"\n    def wrapper(func: Callable):\n        self._add_func_as_worker_internal(\n            key=(key or func.__name__),\n            func=func,\n            dependencies=dependencies,\n            is_start=is_start,\n            args_mapping_rule=args_mapping_rule,\n        )\n\n    return wrapper\n</code></pre>"},{"location":"api/bridgic-core/bridgic/core/automa/graph_automa/#bridgic.core.automa.graph_automa.GraphAutoma.remove_worker","title":"remove_worker","text":"<pre><code>remove_worker(key: str) -&gt; None\n</code></pre> <p>Remove a worker from the Automa. This method can be called at any time to remove a worker from the Automa.</p> <p>When a worker is removed, all dependencies related to this worker, including all the dependencies of the worker itself and the dependencies between the worker and its successor workers, will be also removed.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>The key of the worker to be removed.</p> required <p>Returns:</p> Type Description <code>None</code> <p>Raises:</p> Type Description <code>AutomaDeclarationError</code> <p>If the worker specified by key does not exist in the Automa, this exception will be raised.</p> Source code in <code>bridgic/core/automa/graph_automa.py</code> <pre><code>def remove_worker(self, key: str) -&gt; None:\n    \"\"\"\n    Remove a worker from the Automa. This method can be called at any time to remove a worker from the Automa.\n\n    When a worker is removed, all dependencies related to this worker, including all the dependencies of the worker itself and the dependencies between the worker and its successor workers, will be also removed.\n\n    Parameters\n    ----------\n    key : str\n        The key of the worker to be removed.\n\n    Returns\n    -------\n    None\n\n    Raises\n    ------\n    AutomaDeclarationError\n        If the worker specified by key does not exist in the Automa, this exception will be raised.\n    \"\"\"\n    if not self._automa_running:\n        # remove immediately\n        self._remove_worker_incrementally(key)\n    else:\n        deferred_task = _RemoveWorkerDeferredTask(\n            worker_key=key,\n        )\n        # Note: the execution order of topology change deferred tasks is important and is determined by the order of the calls of add_worker(), remove_worker() and add_dependency() in one DS.\n        self._topology_change_deferred_tasks.append(deferred_task)\n</code></pre>"},{"location":"api/bridgic-core/bridgic/core/automa/graph_automa/#bridgic.core.automa.graph_automa.GraphAutoma.add_dependency","title":"add_dependency","text":"<pre><code>add_dependency(key: str, dependency: str) -&gt; None\n</code></pre> <p>This method is used to dynamically add a dependency from <code>key</code> to <code>dependency</code>.</p> <p>Note: args_mapping_rule is not allowed to be set by this method, instead it should be set together with add_worker() or add_func_as_worker().</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>The key of the worker that will depend on the worker with key <code>dependency</code>.</p> required <code>dependency</code> <code>str</code> <p>The key of the worker on which the worker with key <code>key</code> will depend.</p> required Source code in <code>bridgic/core/automa/graph_automa.py</code> <pre><code>def add_dependency(\n    self,\n    key: str,\n    dependency: str,\n) -&gt; None:\n    \"\"\"\n    This method is used to dynamically add a dependency from `key` to `dependency`.\n\n    Note: args_mapping_rule is not allowed to be set by this method, instead it should be set together with add_worker() or add_func_as_worker().\n\n    Parameters\n    ----------\n    key : str\n        The key of the worker that will depend on the worker with key `dependency`.\n    dependency : str\n        The key of the worker on which the worker with key `key` will depend.\n    \"\"\"\n    ...\n    if not self._automa_running:\n        # add the dependency immediately\n        self._add_dependency_incrementally(key, dependency)\n    else:\n        deferred_task = _AddDependencyDeferredTask(\n            worker_key=key,\n            dependency=dependency,\n        )\n        # Note: the execution order of topology change deferred tasks is important and is determined by the order of the calls of add_worker(), remove_worker() and add_dependency() in one DS.\n        self._topology_change_deferred_tasks.append(deferred_task)\n</code></pre>"},{"location":"api/bridgic-core/bridgic/core/automa/graph_automa/#bridgic.core.automa.graph_automa.GraphAutoma.ferry_to","title":"ferry_to","text":"<pre><code>ferry_to(worker_key: str, /, *args, **kwargs)\n</code></pre> <p>Defer the invocation to the specified worker, passing any provided arguments. This creates a  delayed call, ensuring the worker will be scheduled to run asynchronously in the next event loop,  independent of its dependencies.</p> <p>This primitive is commonly used for:</p> <ol> <li>Implementing dynamic branching based on runtime conditions.</li> <li>Creating logic that forms cyclic graphs.</li> </ol> <p>Parameters:</p> Name Type Description Default <code>worker_key</code> <code>str</code> <p>The key of the worker to run.</p> required <code>args</code> <code>optional</code> <p>Positional arguments to be passed.</p> <code>()</code> <code>kwargs</code> <code>optional</code> <p>Keyword arguments to be passed.</p> <code>{}</code> <p>Examples:</p> <pre><code>class MyGraphAutoma(GraphAutoma):\n    @worker(is_start=True)\n    def start_worker(self):\n        number = random.randint(0, 1)\n        if number == 0:\n            self.ferry_to(\"cond_1_worker\", number=number)\n        else:\n            self.ferry_to(\"cond_2_worker\")\n\n    @worker()\n    def cond_1_worker(self, number: int):\n        print(f'Got {{number}}!')\n\n    @worker()\n    def cond_2_worker(self):\n        self.ferry_to(\"start_worker\")\n\nautoma = MyGraphAutoma()\nawait automa.arun()\n\n# Output: Got 0!\n</code></pre> Source code in <code>bridgic/core/automa/graph_automa.py</code> <pre><code>def ferry_to(self, worker_key: str, /, *args, **kwargs):\n    \"\"\"\n    Defer the invocation to the specified worker, passing any provided arguments. This creates a \n    delayed call, ensuring the worker will be scheduled to run asynchronously in the next event loop, \n    independent of its dependencies.\n\n    This primitive is commonly used for:\n\n    1. Implementing dynamic branching based on runtime conditions.\n    2. Creating logic that forms cyclic graphs.\n\n    Parameters\n    ----------\n    worker_key : str\n        The key of the worker to run.\n    args : optional\n        Positional arguments to be passed.\n    kwargs : optional\n        Keyword arguments to be passed.\n\n    Examples\n    --------\n    ```python\n    class MyGraphAutoma(GraphAutoma):\n        @worker(is_start=True)\n        def start_worker(self):\n            number = random.randint(0, 1)\n            if number == 0:\n                self.ferry_to(\"cond_1_worker\", number=number)\n            else:\n                self.ferry_to(\"cond_2_worker\")\n\n        @worker()\n        def cond_1_worker(self, number: int):\n            print(f'Got {{number}}!')\n\n        @worker()\n        def cond_2_worker(self):\n            self.ferry_to(\"start_worker\")\n\n    automa = MyGraphAutoma()\n    await automa.arun()\n\n    # Output: Got 0!\n    ```\n    \"\"\"\n    # TODO: check worker_key is valid, maybe deferred check...\n    running_options = self._get_top_running_options()\n    # if debug is enabled, trace back the kickoff worker key from stacktrace.\n    kickoff_worker_key: str = self._trace_back_kickoff_worker_key_from_stack() if running_options.debug else None\n    deferred_task = _FerryDeferredTask(\n        ferry_to_worker_key=worker_key,\n        kickoff_worker_key=kickoff_worker_key,\n        args=args,\n        kwargs=kwargs,\n    )\n    # Note: ferry_to() may be called in a new thread.\n    # But _ferry_deferred_tasks is not necessary to be thread-safe due to Visibility Guarantees of the Bridgic Concurrency Model.\n    self._ferry_deferred_tasks.append(deferred_task)\n</code></pre>"},{"location":"api/bridgic-core/bridgic/core/automa/graph_automa/#bridgic.core.automa.graph_automa.GraphAutoma.get_local_space","title":"get_local_space","text":"<pre><code>get_local_space(\n    runtime_context: RuntimeContext,\n) -&gt; Dict[str, Any]\n</code></pre> <p>Get the local space, if you want to clean the local space after automa.arun(), you can override the should_reset_local_space() method.</p> <p>Parameters:</p> Name Type Description Default <code>runtime_context</code> <code>RuntimeContext</code> <p>The runtime context.</p> required <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>The local space.</p> Source code in <code>bridgic/core/automa/graph_automa.py</code> <pre><code>def get_local_space(self, runtime_context: RuntimeContext) -&gt; Dict[str, Any]:\n    \"\"\"\n    Get the local space, if you want to clean the local space after automa.arun(), you can override the should_reset_local_space() method.\n\n    Parameters\n    ----------\n    runtime_context : RuntimeContext\n        The runtime context.\n\n    Returns\n    -------\n    Dict[str, Any]\n        The local space.\n    \"\"\"\n    worker_key = runtime_context.worker_key\n    worker_obj = self._workers[worker_key]\n    return worker_obj.local_space\n</code></pre>"},{"location":"api/bridgic-core/bridgic/core/automa/graph_automa/#bridgic.core.automa.graph_automa.GraphAutoma.should_reset_local_space","title":"should_reset_local_space","text":"<pre><code>should_reset_local_space() -&gt; bool\n</code></pre> <p>This method indicates whether to reset the local space at the end of the arun method of GraphAutoma.  By default, it returns True, standing for resetting. Otherwise, it means doing nothing.</p> Examples: <pre><code>class MyAutoma(GraphAutoma):\n    def should_reset_local_space(self) -&gt; bool:\n        return False\n</code></pre> Source code in <code>bridgic/core/automa/graph_automa.py</code> <pre><code>def should_reset_local_space(self) -&gt; bool:\n    \"\"\"\n    This method indicates whether to reset the local space at the end of the arun method of GraphAutoma. \n    By default, it returns True, standing for resetting. Otherwise, it means doing nothing.\n\n    Examples:\n    --------\n    ```python\n    class MyAutoma(GraphAutoma):\n        def should_reset_local_space(self) -&gt; bool:\n            return False\n    ```\n    \"\"\"\n    return True\n</code></pre>"},{"location":"api/bridgic-core/bridgic/core/automa/graph_automa/#bridgic.core.automa.graph_automa.GraphAutoma.arun","title":"arun  <code>async</code>","text":"<pre><code>arun(\n    *args: Tuple[Any, ...],\n    interaction_feedback: Optional[\n        InteractionFeedback\n    ] = None,\n    interaction_feedbacks: Optional[\n        List[InteractionFeedback]\n    ] = None,\n    **kwargs: Dict[str, Any]\n) -&gt; Any\n</code></pre> <p>The entry point for running the constructed <code>GraphAutoma</code> instance.</p> <p>Parameters:</p> Name Type Description Default <code>args</code> <code>optional</code> <p>Positional arguments to be passed.</p> <code>()</code> <code>interaction_feedback</code> <code>Optional[InteractionFeedback]</code> <p>Feedback that is received from a human interaction. Only one of interaction_feedback or  interaction_feedbacks should be provided at a time.</p> <code>None</code> <code>interaction_feedbacks</code> <code>Optional[List[InteractionFeedback]]</code> <p>Feedbacks that are received from multiple interactions occurred simultaneously before the Automa  was paused. Only one of interaction_feedback or interaction_feedbacks should be provided at a time.</p> <code>None</code> <code>kwargs</code> <code>optional</code> <p>Keyword arguments to be passed.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Any</code> <p>The execution result of the output-worker if <code>output_worker_key</code> is specified, otherwise None.</p> <p>Raises:</p> Type Description <code>InteractionException</code> <p>If the Automa is the top-level Automa and the <code>interact_with_human()</code> method is called by  one or more workers, this exception will be raised to the application layer.</p> <code>_InteractionEventException</code> <p>If the Automa is not the top-level Automa and the <code>interact_with_human()</code> method is called by  one or more workers, this exception will be raised to the upper level Automa.</p> Source code in <code>bridgic/core/automa/graph_automa.py</code> <pre><code>async def arun(\n    self, \n    *args: Tuple[Any, ...],\n    interaction_feedback: Optional[InteractionFeedback] = None,\n    interaction_feedbacks: Optional[List[InteractionFeedback]] = None,\n    **kwargs: Dict[str, Any]\n) -&gt; Any:\n    \"\"\"\n    The entry point for running the constructed `GraphAutoma` instance.\n\n    Parameters\n    ----------\n    args : optional\n        Positional arguments to be passed.\n\n    interaction_feedback : Optional[InteractionFeedback]\n        Feedback that is received from a human interaction. Only one of interaction_feedback or \n        interaction_feedbacks should be provided at a time.\n\n    interaction_feedbacks : Optional[List[InteractionFeedback]]\n        Feedbacks that are received from multiple interactions occurred simultaneously before the Automa \n        was paused. Only one of interaction_feedback or interaction_feedbacks should be provided at a time.\n\n    kwargs : optional\n        Keyword arguments to be passed.\n\n    Returns\n    -------\n    Any\n        The execution result of the output-worker if `output_worker_key` is specified, otherwise None.\n\n    Raises\n    ------\n    InteractionException\n        If the Automa is the top-level Automa and the `interact_with_human()` method is called by \n        one or more workers, this exception will be raised to the application layer.\n\n    _InteractionEventException\n        If the Automa is not the top-level Automa and the `interact_with_human()` method is called by \n        one or more workers, this exception will be raised to the upper level Automa.\n    \"\"\"\n\n    def _reinit_current_kickoff_workers_if_needed():\n        # Note: After deserialization, the _current_kickoff_workers must not be empty!\n        # Therefore, _current_kickoff_workers will only be reinitialized when the Automa is run for the first time or rerun.\n        # It is guaranteed that _current_kickoff_workers will not be reinitialized when the Automa is resumed after deserialization.\n        if not self._current_kickoff_workers:\n            self._current_kickoff_workers = [\n                _KickoffInfo(\n                    worker_key=worker_key,\n                    last_kickoff=\"__automa__\"\n                ) for worker_key, worker_obj in self._workers.items()\n                if getattr(worker_obj, \"is_start\", False)\n            ]\n            # Each time the Automa re-runs, buffer the input arguments here.\n            self._input_buffer.args = args\n            self._input_buffer.kwargs = kwargs\n\n    def _execute_topology_change_deferred_tasks(tc_tasks: List[Union[_AddWorkerDeferredTask, _RemoveWorkerDeferredTask, _AddDependencyDeferredTask]]):\n        for topology_task in tc_tasks:\n            if topology_task.task_type == \"add_worker\":\n                self._add_worker_incrementally(\n                    key=topology_task.worker_key,\n                    worker=topology_task.worker_obj,\n                    dependencies=topology_task.dependencies,\n                    is_start=topology_task.is_start,\n                    args_mapping_rule=topology_task.args_mapping_rule,\n                )\n            elif topology_task.task_type == \"remove_worker\":\n                self._remove_worker_incrementally(topology_task.worker_key)\n            elif topology_task.task_type == \"add_dependency\":\n                self._add_dependency_incrementally(topology_task.worker_key, topology_task.dependency)\n\n    def _set_worker_run_finished(worker_key: str):\n        for kickoff_info in self._current_kickoff_workers:\n            if kickoff_info.worker_key == worker_key:\n                kickoff_info.run_finished = True\n                break\n\n    def _check_and_normalize_interaction_params(\n        interaction_feedback: Optional[InteractionFeedback] = None,\n        interaction_feedbacks: Optional[List[InteractionFeedback]] = None,\n    ):\n        if interaction_feedback and interaction_feedbacks:\n            raise AutomaRuntimeError(\n                f\"Only one of interaction_feedback or interaction_feedbacks can be used. \"\n                f\"But received interaction_feedback={interaction_feedback} and \\n\"\n                f\"interaction_feedbacks={interaction_feedbacks}\"\n            )\n        if interaction_feedback:\n            rx_feedbacks = [interaction_feedback]\n        else:\n            rx_feedbacks = interaction_feedbacks\n        return rx_feedbacks\n\n    def _match_ongoing_interaction_and_feedbacks(rx_feedbacks:List[InteractionFeedback]):\n        match_left_feedbacks = []\n        for feedback in rx_feedbacks:\n            matched = False\n            for interaction_and_feedbacks in self._ongoing_interactions.values():\n                for interaction_and_feedback in interaction_and_feedbacks:\n                    if interaction_and_feedback.interaction.interaction_id == feedback.interaction_id:\n                        matched = True\n                        # Note: Only one feedback is allowed for each interaction. Here we assume that only the first feedback is valid, which is a choice of implementation.\n                        if interaction_and_feedback.feedback is None:\n                            # Set feedback to self._ongoing_interactions\n                            interaction_and_feedback.feedback = feedback\n                        break\n                if matched:\n                    break\n            if not matched:\n                match_left_feedbacks.append(feedback)\n        return match_left_feedbacks\n\n    running_options = self._get_top_running_options()\n\n    self._main_loop = asyncio.get_running_loop()\n    self._main_thread_id = threading.get_ident()\n\n    if self.thread_pool is None:\n        self.thread_pool = ThreadPoolExecutor(thread_name_prefix=\"bridgic-thread\")\n\n    if not self._automa_running:\n        # Here is the last chance to compile and check the DDG in the end of the [Initialization Phase] (phase 1 just before the first DS).\n        self._compile_graph_and_detect_risks()\n        self._automa_running = True\n\n    # An Automa needs to be re-run with _current_kickoff_workers reinitialized.\n    _reinit_current_kickoff_workers_if_needed()\n\n    rx_feedbacks = _check_and_normalize_interaction_params(interaction_feedback, interaction_feedbacks)\n    if rx_feedbacks:\n        rx_feedbacks = _match_ongoing_interaction_and_feedbacks(rx_feedbacks)\n\n    if running_options.debug:\n        printer.print(f\"\\n{type(self).__name__}-[{self.name}] is getting started.\", color=\"green\")\n\n    # Task loop divided into many dynamic steps (DS).\n    while self._current_kickoff_workers:\n        # A new DS started.\n        if running_options.debug:\n            kickoff_worker_keys = [kickoff_info.worker_key for kickoff_info in self._current_kickoff_workers]\n            printer.print(f\"[DS][Before Tasks Started] kickoff workers: {kickoff_worker_keys}\", color=\"purple\")\n\n        for kickoff_info in self._current_kickoff_workers:\n            if kickoff_info.run_finished:\n                # Skip finished workers. Here is the case that the Automa is resumed after a human interaction.\n                if running_options.debug:\n                    printer.print(f\"[{kickoff_info.worker_key}] will be skipped - run finished\", color=\"blue\")\n                continue\n\n            if running_options.debug:\n                kickoff_name = kickoff_info.last_kickoff\n                if kickoff_name == \"__automa__\":\n                    kickoff_name = f\"{kickoff_name}:({self.name})\"\n                printer.print(f\"[{kickoff_name}] will kick off [{kickoff_info.worker_key}]\", color=\"cyan\")\n\n            # First, Arguments Mapping:\n            if kickoff_info.last_kickoff == \"__automa__\":\n                next_args, next_kwargs = self._input_buffer.args, {}\n            elif kickoff_info.from_ferry:\n                next_args, next_kwargs = kickoff_info.args, kickoff_info.kwargs\n            else:\n                next_args, next_kwargs = self._mapping_args(\n                    kickoff_worker_key=kickoff_info.last_kickoff,\n                    current_worker_key=kickoff_info.worker_key,\n                )\n            # Second, Inputs Propagation:\n            next_args, next_kwargs = self._propagate_inputs(\n                current_worker_key=kickoff_info.worker_key,\n                next_args=next_args,\n                next_kwargs=next_kwargs,\n                input_kwargs=self._input_buffer.kwargs,\n            )\n            # Third, Resolve data injection.\n            worker_obj = self._workers[kickoff_info.worker_key]\n            next_args, next_kwargs = injector.inject(\n                current_worker_key=kickoff_info.worker_key, \n                current_worker_sig=worker_obj.get_input_param_names(), \n                worker_dict=self._workers, \n                worker_output=self._worker_output,\n                next_args=next_args, \n                next_kwargs=next_kwargs\n            )\n\n            # Schedule task for each kickoff worker.\n            if worker_obj.is_automa():\n                coro = worker_obj.arun(\n                    *next_args, \n                    interaction_feedbacks=rx_feedbacks, \n                    **next_kwargs\n                )\n            else:\n                coro = worker_obj.arun(*next_args, **next_kwargs)\n\n            task = asyncio.create_task(\n                # TODO1: arun() may need to be wrapped to support better interrupt...\n                coro,\n                name=f\"Task-{kickoff_info.worker_key}\"\n            )\n            self._running_tasks.append(_RunnningTask(\n                worker_key=kickoff_info.worker_key,\n                task=task,\n            ))\n\n        # Wait until all of the tasks are finished.\n        while True:\n            undone_tasks = [t.task for t in self._running_tasks if not t.task.done()]\n            if not undone_tasks:\n                break\n            try:\n                await undone_tasks[0]\n            except Exception as e:\n                ...\n                # The same exception will be raised again in the following task.result().\n                # Note: A Task is done when the wrapped coroutine either returned a value, raised an exception, or the Task was cancelled.\n                # Refer to: https://docs.python.org/3/library/asyncio-task.html#task-object\n\n        # Process graph topology change deferred tasks triggered by add_worker() and remove_worker().\n        _execute_topology_change_deferred_tasks(self._topology_change_deferred_tasks)\n\n        # Perform post-task follow-up operations.\n        interaction_exceptions: List[_InteractionEventException] = []\n        for task in self._running_tasks:\n            # task.task.result must be called here! It will raise an exception if task failed.\n            try:\n                task_result = task.task.result()\n                _set_worker_run_finished(task.worker_key)\n                if task.worker_key in self._workers:\n                    # The current running worker may be removed.\n                    worker_obj = self._workers[task.worker_key]\n                    # Collect results of the finished tasks.\n                    self._worker_output[task.worker_key] = task_result\n                    # reset dynamic states of finished workers.\n                    self._workers_dynamic_states[task.worker_key].dependency_triggers = set(getattr(worker_obj, \"dependencies\", []))\n                    # Update the dynamic states of successor workers.\n                    for successor_key in self._worker_forwards.get(task.worker_key, []):\n                        self._workers_dynamic_states[successor_key].dependency_triggers.remove(task.worker_key)\n                    # Each time a worker is finished running, the ongoing interaction states should be cleared. Once it is re-run, the human interactions in the worker can be triggered again.\n                    if task.worker_key in self._worker_interaction_indices:\n                        del self._worker_interaction_indices[task.worker_key]\n                    if task.worker_key in self._ongoing_interactions:\n                        del self._ongoing_interactions[task.worker_key]\n\n            except _InteractionEventException as e:\n                interaction_exceptions.append(e)\n                if task.worker_key in self._workers and not self._workers[task.worker_key].is_automa():\n                    if task.worker_key not in self._ongoing_interactions:\n                        self._ongoing_interactions[task.worker_key] = []\n                    interaction=e.args[0]\n                    # Make sure the interaction_id is unique for each human interaction.\n                    found = False\n                    for iaf in self._ongoing_interactions[task.worker_key]:\n                        if iaf.interaction.interaction_id == interaction.interaction_id:\n                            found = True\n                            break\n                    if not found:\n                        self._ongoing_interactions[task.worker_key].append(_InteractionAndFeedback(\n                            interaction=interaction,\n                        ))\n\n        if len(self._topology_change_deferred_tasks) &gt; 0:\n            # Graph topology validation and risk detection. Only needed when topology changes.\n            # Guarantee the graph topology is valid and consistent after each DS.\n            # 1. Validate the canonical graph.\n            self._validate_canonical_graph()\n            # 2. Validate the DAG constraints.\n            GraphAutomaMeta.validate_dag_constraints(self._worker_forwards)\n            # TODO: more validations can be added here...\n\n        # Process the output_worker_key setting deferred task.\n        if self._set_output_worker_deferred_task and self._set_output_worker_deferred_task.output_worker_key in self._workers:\n            self._output_worker_key = self._set_output_worker_deferred_task.output_worker_key\n\n        # TODO: Ferry-related risk detection may be added here...\n\n        # Just after post-task operations (several deferred tasks) and before finding next kickoff workers, collect and process the interaction exceptions.\n        if len(interaction_exceptions) &gt; 0:\n            all_interactions: List[Interaction] = [interaction for e in interaction_exceptions for interaction in e.args]\n            if self.parent is None:\n                # This is the top-level Automa. Serialize the Automa and raise InteractionException to the application layer.\n                serialized_automa = msgpackx.dump_bytes(self)\n                snapshot = Snapshot(\n                    serialized_bytes=serialized_automa,\n                    serialization_version=GraphAutoma.SERIALIZATION_VERSION,\n                )\n                raise InteractionException(\n                    interactions=all_interactions,\n                    snapshot=snapshot,\n                )\n            else:\n                # Continue raise exception to the upper level Automa.\n                raise _InteractionEventException(*all_interactions)\n\n        # Find next kickoff workers and rebuild _current_kickoff_workers\n        run_finished_worker_keys: List[str] = [kickoff_info.worker_key for kickoff_info in self._current_kickoff_workers if kickoff_info.run_finished]\n        assert len(run_finished_worker_keys) == len(self._current_kickoff_workers)\n        self._current_kickoff_workers = []\n        # New kickoff workers can be triggered by two ways:\n        # 1. The ferry_to() operation is called during current worker execution.\n        # 2. The dependencies are eliminated after all predecessor workers are finished.\n        # So,\n        # First add kickoff workers triggered by ferry_to();\n        for ferry_task in self._ferry_deferred_tasks:\n            self._current_kickoff_workers.append(_KickoffInfo(\n                worker_key=ferry_task.ferry_to_worker_key,\n                last_kickoff=ferry_task.kickoff_worker_key,\n                from_ferry=True,\n                args=ferry_task.args,\n                kwargs=ferry_task.kwargs,\n            ))\n        # Then add kickoff workers triggered by dependencies elimination.\n        # Merge successor keys of all finished tasks.\n        successor_keys = set()\n        for worker_key in run_finished_worker_keys:\n            # Note: The `worker_key` worker may have been removed from the Automa.\n            for successor_key in self._worker_forwards.get(worker_key, []):\n                if successor_key not in successor_keys:\n                    dependency_triggers = self._workers_dynamic_states[successor_key].dependency_triggers\n                    if not dependency_triggers:\n                        self._current_kickoff_workers.append(_KickoffInfo(\n                            worker_key=successor_key,\n                            last_kickoff=worker_key,\n                        ))\n                    successor_keys.add(successor_key)\n        if running_options.debug:\n            deferred_ferrys = [ferry_task.ferry_to_worker_key for ferry_task in self._ferry_deferred_tasks]\n            printer.print(f\"[DS][After Tasks Finished] successor workers: {successor_keys}, deferred ferrys: {deferred_ferrys}\", color=\"purple\")\n\n        # Clear running tasks after all finished.\n        self._running_tasks.clear()\n        self._ferry_deferred_tasks.clear()\n        self._topology_change_deferred_tasks.clear()\n        self._set_output_worker_deferred_task = None\n\n    if running_options.debug:\n        printer.print(f\"{type(self).__name__}-[{self.name}] is finished.\", color=\"green\")\n\n    # After a complete run, reset all necessary states to allow the automa to re-run.\n    self._input_buffer = _AutomaInputBuffer()\n    if self.should_reset_local_space():\n        self._clean_all_worker_local_space()\n    self._ongoing_interactions.clear()\n    self._worker_interaction_indices.clear()\n    self._automa_running = False\n\n    # If the output-worker is specified, return its output as the return value of the automa.\n    if self._output_worker_key:\n        return self._worker_output[self._output_worker_key]\n    else:\n        return None\n</code></pre>"},{"location":"api/bridgic-core/bridgic/core/automa/graph_automa/#bridgic.core.automa.graph_automa.GraphAutoma.register_event_handler","title":"register_event_handler","text":"<pre><code>register_event_handler(\n    event_type: Optional[str],\n    event_handler: EventHandlerType,\n) -&gt; None\n</code></pre> <p>Register an event handler for the specified event type.</p> <p>Note: Only event handlers registered on the top-level Automa will be invoked to handle events.</p> <p>Parameters:</p> Name Type Description Default <code>event_type</code> <code>Optional[str]</code> <p>The type of event to be handled. If set to None, the event handler will be registered as the default handler and will be used to handle all event types.</p> required <code>event_handler</code> <code>EventHandlerType</code> <p>The event handler to be registered.</p> required Source code in <code>bridgic/core/automa/graph_automa.py</code> <pre><code>def register_event_handler(self, event_type: Optional[str], event_handler: EventHandlerType) -&gt; None:\n    \"\"\"\n    Register an event handler for the specified event type.\n\n    Note: Only event handlers registered on the top-level Automa will be invoked to handle events.\n\n    Parameters\n    ----------\n    event_type: Optional[str]\n        The type of event to be handled. If set to None, the event handler will be registered as the default handler and will be used to handle all event types.\n    event_handler: EventHandlerType\n        The event handler to be registered.\n    \"\"\"\n    if event_type is None:\n        self._default_event_handler = event_handler\n    else:\n        self._event_handlers[event_type] = event_handler\n</code></pre>"},{"location":"api/bridgic-core/bridgic/core/automa/graph_automa/#bridgic.core.automa.graph_automa.GraphAutoma.unregister_event_handler","title":"unregister_event_handler","text":"<pre><code>unregister_event_handler(event_type: Optional[str]) -&gt; None\n</code></pre> <p>Unregister an event handler for the specified event type.</p> <p>Parameters:</p> Name Type Description Default <code>event_type</code> <code>Optional[str]</code> <p>The type of event to be unregistered. If set to None, the default event handler will be unregistered.</p> required Source code in <code>bridgic/core/automa/graph_automa.py</code> <pre><code>def unregister_event_handler(self, event_type: Optional[str]) -&gt; None:\n    \"\"\"\n    Unregister an event handler for the specified event type.\n\n    Parameters\n    ----------\n    event_type: Optional[str]\n        The type of event to be unregistered. If set to None, the default event handler will be unregistered.\n    \"\"\"\n    if event_type in self._event_handlers:\n        del self._event_handlers[event_type]\n    if event_type is None:\n        self._default_event_handler = None\n</code></pre>"},{"location":"api/bridgic-core/bridgic/core/automa/graph_automa/#bridgic.core.automa.graph_automa.GraphAutoma.unregister_all_event_handlers","title":"unregister_all_event_handlers","text":"<pre><code>unregister_all_event_handlers() -&gt; None\n</code></pre> <p>Unregister all event handlers.</p> Source code in <code>bridgic/core/automa/graph_automa.py</code> <pre><code>def unregister_all_event_handlers(self) -&gt; None:\n    \"\"\"\n    Unregister all event handlers.\n    \"\"\"\n    self._event_handlers.clear()\n    self._default_event_handler = None\n</code></pre>"},{"location":"api/bridgic-core/bridgic/core/automa/graph_automa/#bridgic.core.automa.graph_automa.GraphAutoma.post_event","title":"post_event","text":"<pre><code>post_event(event: Event) -&gt; None\n</code></pre> <p>Post an event to the application layer outside the Automa.</p> <p>The event handler implemented by the application layer will be called in the same thread as the worker (maybe the main thread or a new thread from the thread pool).</p> <p>Note that <code>post_event</code> can be called in a non-async method or an async method.</p> <p>The event will be bubbled up to the top-level Automa, where it will be processed by the event handler registered with the event type.</p> <p>Parameters:</p> Name Type Description Default <code>event</code> <code>Event</code> <p>The event to be posted.</p> required Source code in <code>bridgic/core/automa/graph_automa.py</code> <pre><code>@override\ndef post_event(self, event: Event) -&gt; None:\n    \"\"\"\n    Post an event to the application layer outside the Automa.\n\n    The event handler implemented by the application layer will be called in the same thread as the worker (maybe the main thread or a new thread from the thread pool).\n\n    Note that `post_event` can be called in a non-async method or an async method.\n\n    The event will be bubbled up to the top-level Automa, where it will be processed by the event handler registered with the event type.\n\n    Parameters\n    ----------\n    event: Event\n        The event to be posted.\n    \"\"\"\n    def _handler_need_feedback_sender(handler: EventHandlerType):\n        positional_param_names = get_param_names_by_kind(handler, Parameter.POSITIONAL_ONLY) + get_param_names_by_kind(handler, Parameter.POSITIONAL_OR_KEYWORD)\n        var_positional_param_names = get_param_names_by_kind(handler, Parameter.VAR_POSITIONAL)\n        return len(var_positional_param_names) &gt; 0 or len(positional_param_names) &gt; 1\n\n    if self.parent is not None:\n        # Bubble up the event to the top-level Automa.\n        return self.parent.post_event(event)\n\n    # Here is the top-level Automa.\n    # Call event handlers\n    if event.event_type in self._event_handlers:\n        if _handler_need_feedback_sender(self._event_handlers[event.event_type]):\n            self._event_handlers[event.event_type](event, feedback_sender=None)\n        else:\n            self._event_handlers[event.event_type](event)\n    if self._default_event_handler is not None:\n        if _handler_need_feedback_sender(self._default_event_handler):\n            self._default_event_handler(event, feedback_sender=None)\n        else:\n            self._default_event_handler(event)\n</code></pre>"},{"location":"api/bridgic-core/bridgic/core/automa/graph_automa/#bridgic.core.automa.graph_automa.GraphAutoma.request_feedback","title":"request_feedback","text":"<pre><code>request_feedback(\n    event: Event, timeout: Optional[float] = None\n) -&gt; Feedback\n</code></pre> <p>Request feedback for the specified event from the application layer outside the Automa. This method blocks the caller until the feedback is received.</p> <p>Note that <code>post_event</code> should only be called from within a non-async method running in the new thread of the Automa thread pool.</p> <p>Parameters:</p> Name Type Description Default <code>event</code> <code>Event</code> <p>The event to be posted to the event handler implemented by the application layer.</p> required <code>timeout</code> <code>Optional[float]</code> <p>A float or int number of seconds to wait for if the feedback is not received. If None, then there is no limit on the wait time.</p> <code>None</code> <p>Returns:</p> Type Description <code>Feedback</code> <p>The feedback received from the application layer.</p> <p>Raises:</p> Type Description <code>TimeoutError</code> <p>If the feedback is not received before the timeout. Note that the raised exception is the built-in <code>TimeoutError</code> exception, instead of asyncio.TimeoutError or concurrent.futures.TimeoutError!</p> Source code in <code>bridgic/core/automa/graph_automa.py</code> <pre><code>def request_feedback(\n    self, \n    event: Event,\n    timeout: Optional[float] = None\n) -&gt; Feedback:\n    \"\"\"\n    Request feedback for the specified event from the application layer outside the Automa. This method blocks the caller until the feedback is received.\n\n    Note that `post_event` should only be called from within a non-async method running in the new thread of the Automa thread pool.\n\n    Parameters\n    ----------\n    event: Event\n        The event to be posted to the event handler implemented by the application layer.\n    timeout: Optional[float]\n        A float or int number of seconds to wait for if the feedback is not received. If None, then there is no limit on the wait time.\n\n    Returns\n    -------\n    Feedback\n        The feedback received from the application layer.\n\n    Raises\n    ------\n    TimeoutError\n        If the feedback is not received before the timeout. Note that the raised exception is the built-in `TimeoutError` exception, instead of asyncio.TimeoutError or concurrent.futures.TimeoutError!\n    \"\"\"\n    if threading.get_ident() == self._main_thread_id:\n        raise AutomaRuntimeError(\n            f\"`request_feedback` should only be called in a different thread from the main thread of the {self.name}. \"\n        )\n    return asyncio.run_coroutine_threadsafe(\n        self.request_feedback_async(event, timeout),\n        self._main_loop\n    ).result()\n</code></pre>"},{"location":"api/bridgic-core/bridgic/core/automa/graph_automa/#bridgic.core.automa.graph_automa.GraphAutoma.request_feedback_async","title":"request_feedback_async  <code>async</code>","text":"<pre><code>request_feedback_async(\n    event: Event, timeout: Optional[float] = None\n) -&gt; Feedback\n</code></pre> <p>Request feedback for the specified event from the application layer outside the Automa. This method blocks the caller until the feedback is received.</p> <p>The event handler implemented by the application layer will be called in the next event loop, in the main thread.</p> <p>Parameters:</p> Name Type Description Default <code>event</code> <code>Event</code> <p>The event to be posted to the event handler implemented by the application layer.</p> required <code>timeout</code> <code>Optional[float]</code> <p>A float or int number of seconds to wait for if the feedback is not received. If None, then there is no limit on the wait time.</p> <code>None</code> <p>Returns:</p> Type Description <code>Feedback</code> <p>The feedback received from the application layer.</p> <p>Raises:</p> Type Description <code>TimeoutError</code> <p>If the feedback is not received before the timeout. Note that the raised exception is the built-in <code>TimeoutError</code> exception, instead of asyncio.TimeoutError!</p> Source code in <code>bridgic/core/automa/graph_automa.py</code> <pre><code>async def request_feedback_async(\n    self, \n    event: Event,\n    timeout: Optional[float] = None\n) -&gt; Feedback:\n    \"\"\"\n    Request feedback for the specified event from the application layer outside the Automa. This method blocks the caller until the feedback is received.\n\n    The event handler implemented by the application layer will be called in the next event loop, in the main thread.\n\n    Parameters\n    ----------\n    event: Event\n        The event to be posted to the event handler implemented by the application layer.\n    timeout: Optional[float]\n        A float or int number of seconds to wait for if the feedback is not received. If None, then there is no limit on the wait time.\n\n    Returns\n    -------\n    Feedback\n        The feedback received from the application layer.\n\n    Raises\n    ------\n    TimeoutError\n        If the feedback is not received before the timeout. Note that the raised exception is the built-in `TimeoutError` exception, instead of asyncio.TimeoutError!\n    \"\"\"\n    if self.parent is not None:\n        # Bubble up the event to the top-level Automa.\n        return await self.parent.request_feedback_async(event, timeout)\n\n    # Here is the top-level Automa.\n    event_loop = asyncio.get_running_loop()\n    future = event_loop.create_future()\n    feedback_sender = self._FeedbackSender(future, event_loop)\n    # Call event handlers\n    if event.event_type in self._event_handlers:\n        self._event_handlers[event.event_type](event, feedback_sender)\n    if self._default_event_handler is not None:\n        self._default_event_handler(event, feedback_sender)\n\n    try:\n        return await asyncio.wait_for(future, timeout)\n    except TimeoutError as e:\n        # When python &gt;= 3.11 here.\n        raise TimeoutError(f\"No feedback is received before timeout in Automa[{self.name}]\") from e\n    except asyncio.TimeoutError as e:\n        # Version compatibility resolution: asyncio.wait_for raises asyncio.TimeoutError before python 3.11.\n        # https://docs.python.org/3/library/asyncio-task.html#asyncio.wait_for\n        raise TimeoutError(f\"No feedback is received before timeout in Automa[{self.name}]\") from e\n</code></pre>"},{"location":"api/bridgic-core/bridgic/core/automa/interaction/event_handling/","title":"EventHandling","text":"<p>               Bases: <code>BaseModel</code></p> <p>An event is a message that is sent from one worker inside the Automa to the application layer outside the Automa.</p> Fields <p>event_type: Optional[str]     The type of the event. The type of the event is used to identify the event handler registered to handle the event. timestamp: datetime     The timestamp of the event. data: Optional[Any]     The data attached to the event.</p> Source code in <code>bridgic/core/automa/interaction/event_handling.py</code> <pre><code>class Event(BaseModel):\n    \"\"\"\n    An event is a message that is sent from one worker inside the Automa to the application layer outside the Automa.\n\n    Fields\n    ------\n    event_type: Optional[str]\n        The type of the event. The type of the event is used to identify the event handler registered to handle the event.\n    timestamp: datetime\n        The timestamp of the event.\n    data: Optional[Any]\n        The data attached to the event.\n    \"\"\"\n    event_type: Optional[str] = None\n    timestamp: datetime = datetime.now()\n    data: Optional[Any] = None\n</code></pre>"},{"location":"api/bridgic-core/bridgic/core/intelligence/base_llm/","title":"BaseLlm","text":"<p>               Bases: <code>ABC</code></p> <p>Base class for Large Language Model implementations.</p> Source code in <code>bridgic/core/intelligence/base_llm.py</code> <pre><code>class BaseLlm(ABC):\n    \"\"\"\n    Base class for Large Language Model implementations.\n    \"\"\"\n\n    @abstractmethod\n    def chat(self, messages: List[Message], **kwargs) -&gt; Response:\n        ...\n\n    @abstractmethod\n    def stream(self, messages: List[Message], **kwargs) -&gt; StreamResponse:\n        ...\n\n    @abstractmethod\n    async def achat(self, messages: List[Message], **kwargs) -&gt; Response:\n        ...\n\n    @abstractmethod\n    async def astream(self, messages: List[Message], **kwargs) -&gt; AsyncStreamResponse:\n        ...\n</code></pre> <p>               Bases: <code>BaseModel</code></p> <p>LLM message.</p> Source code in <code>bridgic/core/intelligence/base_llm.py</code> <pre><code>class Message(BaseModel):\n    \"\"\"\n    LLM message.\n    \"\"\"\n    role: Role = Field(default=Role.USER)\n    blocks: List[ContentBlock] = Field(default=[])\n    extras: Dict[str, Any] = Field(default={})\n\n    @classmethod\n    def from_text(\n        cls,\n        text: str,\n        role: Union[Role, str] = Role.USER,\n        extras: Optional[Dict[str, Any]] = {},\n    ) -&gt; \"Message\":\n        if isinstance(role, str):\n            role = Role(role)\n        return cls(role=role, blocks=[TextBlock(text=text)], extras=extras)\n\n    @property\n    def content(self) -&gt; str:\n        return \"\\n\\n\".join([block.text for block in self.blocks if isinstance(block, TextBlock)])\n\n    @content.setter\n    def content(self, text: str):\n        if not self.blocks:\n            self.blocks = [TextBlock(text=text)]\n        elif len(self.blocks) == 1 and isinstance(self.blocks[0], TextBlock):\n            self.blocks = [TextBlock(text=text)]\n        else:\n            raise ValueError(\n                \"Message contains multiple blocks or contains a non-text block, thus it could not be \"\n                \"easily set by the property \\\"Message.content\\\". Use \\\"Message.blocks\\\" instead.\"\n            )\n</code></pre> <p>               Bases: <code>BaseModel</code></p> <p>LLM response.</p> Source code in <code>bridgic/core/intelligence/base_llm.py</code> <pre><code>class Response(BaseModel):\n    \"\"\"\n    LLM response.\n    \"\"\"\n    message: Optional[Message] = None\n    raw: Optional[Any] = None\n</code></pre>"},{"location":"home/","title":"Welcome to the world of Bridgic \ud83c\udf09!","text":""},{"location":"home/#what-does-bridgic-mean","title":"What does Bridgic mean?","text":"<p>Bridgic means bridging logic and magic together.</p> <ul> <li>Logic stands for deterministic execution flows;</li> <li>Magic stands for highly autonomous AI capabilities.</li> </ul> <p>Bridgic provides a programming paradigm that enables a more effective integration of the two.</p>"},{"location":"home/concepts/","title":"Concepts","text":""},{"location":"home/concepts/#automa-and-worker","title":"Automa and Worker","text":""},{"location":"home/concepts/#worker","title":"Worker","text":"<p>A worker is the basic execution unit in the Bridgic framework, representing a specific task node.</p> <p>Each worker typically corresponds to a function (which can be synchronous or asynchronous) that performs an independent business logic or processing step. Workers are automatically linked through <code>dependencies</code> to form a complete workflow. The design of Workers makes task decomposition, reuse, and composition simple and flexible, serving as the core foundation for implementing automated processes and complex business orchestration.</p> <p>There are two ways to define worker:</p> <ol> <li>Use the <code>@worker</code> decorator to decorate member methods (regular or asynchronous) of a <code>GraphAutoma</code> class, thereby registering them as Workers.</li> <li>Inherit from the <code>Worker</code> base class and implement its <code>run</code> or <code>arun</code> method.</li> </ol>"},{"location":"home/concepts/#graphautoma","title":"GraphAutoma","text":"<p><code>GraphAutoma</code> is the core class in the Bridgic framework for constructing and executing Directed Dependency Graphs (abbreviated as DDG). It is not just a simple task scheduler, but a highly abstracted workflow engine that helps developers organize, manage, and run complex asynchronous or synchronous task flows in a declarative manner. Compared to traditional flow control or manual orchestration, <code>GraphAutoma</code> offers the following significant advantages:</p> <ol> <li> <p>Declarative Dependency Modeling    With the <code>@worker</code> decorator, developers can define each node (worker) and its dependencies as if building blocks, without writing tedious scheduling logic. Each worker can be a synchronous or asynchronous function, and GraphAutoma will automatically recognize dependencies and construct the complete task graph.</p> </li> <li> <p>Call-based Branch Management    In addition to automatic scheduling based on dependencies, GraphAutoma also supports \"active jump\" or \"call-based branching\" control via the <code>ferry_to</code> method. Developers can call <code>ferry_to(worker_key, *args, **kwargs)</code> inside any worker to directly switch the control flow to the specified worker and pass parameters. This mechanism is similar to \"goto\" or \"event-driven jump\", but operates at the granularity of complete workers, while still maintaining asynchronous safety and context consistency. This feature is suitable for complex scenarios requiring dynamic branching, making it easy for developers to create logic orchestration programs that depend on runtime conditions. The call to <code>ferry_to</code> is not constrained by dependencies; the target worker will be scheduled for asynchronous execution in the next dynamic step, greatly enhancing the flexibility and controllability of the workflow.</p> </li> <li> <p>Automatic Driving and Scheduling    Once dependencies are defined, GraphAutoma will automatically drive the execution of each worker according to the dependency topology. As long as dependencies are satisfied, workers will be automatically scheduled, eliminating the need for developers to manually manage execution order or state transitions, thus greatly reducing error rates and maintenance costs.</p> </li> <li> <p>Asynchronous and Concurrent Support    GraphAutoma natively supports asynchronous workers, fully leveraging Python's async features for efficient concurrent execution. For I/O-intensive tasks, it can also be combined with thread pools to achieve true parallel processing and improve overall throughput.</p> </li> <li> <p>Human Interaction and Event Mechanism    In complex business scenarios, some nodes may need to wait for manual input or external events. GraphAutoma has a built-in human interaction mechanism, supporting task pausing and resuming after receiving user feedback. At the same time, it integrates Bridgic's event system, enabling real-time communication between workers and the application layer, greatly enhancing the system's interactivity and flexibility.</p> </li> <li> <p>Serialization and Persistence    GraphAutoma supports complete serialization and deserialization, allowing the current workflow state to be persisted to disk or a database, enabling advanced features such as checkpointing and fault recovery. This is especially important for long-running or highly reliable business processes.</p> </li> <li> <p>Dynamic Topology Changes    Supports dynamically adding or removing workers at runtime, flexibly adapting to changes and expansion needs in business processes. There is no need to restart or refactor the entire workflow, greatly improving system maintainability and scalability.</p> </li> <li> <p>Layerizable and Editable    GraphAutoma supports deep customization through inheritance and secondary development. Developers can flexibly add new worker nodes based on existing GraphAutoma subclasses to extend their behavior and functionality, enabling personalized customization of business processes. By inheriting, you can reuse existing dependencies and scheduling logic, and then stack new business nodes on top, quickly building more complex automated workflows. This layered and editable design allows GraphAutoma to be continuously expanded and evolved like Lego blocks, greatly enhancing the system's maintainability and evolutionary capability.</p> </li> <li> <p>Nestable and Reusable    GraphAutoma is not just a top-level workflow engine; it can itself act as a \"super worker\" and be nested within another, larger GraphAutoma. Each GraphAutoma instance can be scheduled, passed parameters, and reused just like a regular worker, enabling recursive composition and hierarchical management of workflows. This nesting mechanism allows developers to use the composition pattern to orchestrate even larger and more complex business processes.</p> </li> </ol> <p>In summary, <code>GraphAutoma</code> enables developers to quickly build robust, flexible, interactive, and persistent complex workflow systems with minimal mental overhead, making it a powerful cornerstone for modern automation and intelligent application development.</p>"},{"location":"home/examples/","title":"Examples","text":""},{"location":"home/installation/","title":"Installation","text":"<pre><code>pip install bridgic\n</code></pre>"},{"location":"tutorials/","title":"Tutorial","text":"<p>Welcome to the Bridgic tutorial! Here, we offer a complete learning path from the basics to advanced levels, helping you quickly master the core concepts and best practices of the framework.</p> <p>In this tutorial, we\u2019ll assume that Bridgic is already installed on your system. If that\u2019s not the case, see Installation guide: Bridgic Install.</p> <p>This tutorial will walk you through these tasks:</p> <ol> <li>Parameter Passing: Three ways of data transmission in Bridgic, including Args-Mapping, Args-Injection,  Inputs-Propagation, </li> </ol> <p>Start your Bridgic journey! \ud83c\udf89</p>"},{"location":"tutorials/parameter_passing/","title":"Parameter Passing","text":"In\u00a0[\u00a0]: Copied! <pre># Setting environment variables in the terminal:\nexport VLLM_SERVER_API_BASE=\"your-api-url\"\nexport VLLM_SERVER_API_KEY=\"your-api-key\"\nexport VLLM_SERVER_MODEL_NAME=\"your-model-name\"\n</pre> # Setting environment variables in the terminal: export VLLM_SERVER_API_BASE=\"your-api-url\" export VLLM_SERVER_API_KEY=\"your-api-key\" export VLLM_SERVER_MODEL_NAME=\"your-model-name\" <p>Get the environment variables.</p> In\u00a0[\u00a0]: Copied! <pre>import os\nfrom dotenv import load_dotenv\nload_dotenv()\n\n_api_base = os.environ.get(\"VLLM_SERVER_API_BASE\")\n_api_key = os.environ.get(\"VLLM_SERVER_API_KEY\")\n_model_name = os.environ.get(\"VLLM_SERVER_MODEL_NAME\")\n</pre> import os from dotenv import load_dotenv load_dotenv()  _api_base = os.environ.get(\"VLLM_SERVER_API_BASE\") _api_key = os.environ.get(\"VLLM_SERVER_API_KEY\") _model_name = os.environ.get(\"VLLM_SERVER_MODEL_NAME\")  <p>Import the necessary packages.</p> In\u00a0[\u00a0]: Copied! <pre>from pydantic import BaseModel, Field\nfrom typing import List, Dict, Tuple\nfrom bridgic.core.automa import GraphAutoma, worker, ArgsMappingRule\nfrom bridgic.llms.vllm.vllm_server_llm import VllmServerLlm, Message, Role, PydanticModel\n</pre> from pydantic import BaseModel, Field from typing import List, Dict, Tuple from bridgic.core.automa import GraphAutoma, worker, ArgsMappingRule from bridgic.llms.vllm.vllm_server_llm import VllmServerLlm, Message, Role, PydanticModel <p>Now, let's implement this query expansion. We assume that the user query we receive is in json format. It contain two keys:</p> <ol> <li><code>query</code>: A string in the form of <code>Q: user_query</code> representing the question input by the user.</li> <li><code>date</code>: The time when the user entered the query.</li> </ol> In\u00a0[18]: Copied! <pre>query_obj = {\n    \"query\": \"Q: What new developments have there been in RAG in the past year?\",\n    \"date\": \"2025-09-30\"\n}\n</pre> query_obj = {     \"query\": \"Q: What new developments have there been in RAG in the past year?\",     \"date\": \"2025-09-30\" } <p>We initialize the model to facilitate our subsequent convenient invocation of it to complete tasks.</p> In\u00a0[7]: Copied! <pre>llm = VllmServerLlm(  # the llm instance\n    api_base=_api_base,\n    api_key=_api_key,\n    timeout=5,\n)\n</pre> llm = VllmServerLlm(  # the llm instance     api_base=_api_base,     api_key=_api_key,     timeout=5, ) <p>Furthermore, we define that when the model completes entity extraction and query expansion, it returns the result in a Pandatic data structure.</p> In\u00a0[8]: Copied! <pre>class EntityList(BaseModel):  # The expected format of the model output in the extract_entity worker\n    entities: List[str] = Field(description=\"All entities in the input.\")\n\nclass QueryList(BaseModel):  # The expected format of the model output in the expand_query worker\n    queries: List[str] = Field(description=\"All queries in the input.\")\n</pre> class EntityList(BaseModel):  # The expected format of the model output in the extract_entity worker     entities: List[str] = Field(description=\"All entities in the input.\")  class QueryList(BaseModel):  # The expected format of the model output in the expand_query worker     queries: List[str] = Field(description=\"All queries in the input.\") <p>Next, let's outline the three steps of query expension to achieve our goal:</p> <ol> <li>Receive the user's input and perform preprocessing, get original question.</li> <li>Extract the entity information from the question, get entity information.</li> <li>Expand and obtain multiple queries.</li> </ol> In\u00a0[19]: Copied! <pre>class QueryExpansion(GraphAutoma):\n    @worker(is_start=True)\n    async def pre_query(self, query_obj: Dict):  # Receive the user's input and preprocess query\n        query = query_obj[\"query\"].split(\":\")[1].strip()  \n        return query\n\n    @worker(is_start=True)\n    async def pre_date(self, query_obj: Dict):  # Receive the user's input and preprocess date\n        date = query_obj[\"date\"]\n        return date\n\n    @worker(dependencies=[\"pre_query\", \"pre_date\"], args_mapping_rule=ArgsMappingRule.AS_IS)\n    async def extract_entity(self, query: str, date: str):  # Extract the entity information from the question, get entity information.\n        response: EntityList = llm.structured_output(  \n            model=_model_name,\n            constraint=PydanticModel(model=EntityList),\n            messages=[\n                Message.from_text(text=\"extract the entity information from the given query\", role=Role.SYSTEM),\n                Message.from_text(text=query, role=Role.USER,),\n            ]\n        )\n        return query, response.entities, date\n\n    @worker(dependencies=[\"extract_entity\"], args_mapping_rule=ArgsMappingRule.AS_IS)\n    async def expand_query(self, query_meta: Tuple[str, List[str]]):  # Expand and obtain multiple queries.\n        query, entities, date = query_meta\n        task_input = f\"Query: {query}\\nEntities: {entities}\"\n        response: str = llm.structured_output(\n            model=_model_name,\n            constraint=PydanticModel(model=QueryList),\n            messages=[\n                Message.from_text(text=f\"Centered around the given entities and given date {date}, expand the query to obtain multiple queries\", role=Role.SYSTEM),\n                Message.from_text(text=task_input, role=Role.USER,),\n            ]\n        )\n        return response.queries\n</pre> class QueryExpansion(GraphAutoma):     @worker(is_start=True)     async def pre_query(self, query_obj: Dict):  # Receive the user's input and preprocess query         query = query_obj[\"query\"].split(\":\")[1].strip()           return query      @worker(is_start=True)     async def pre_date(self, query_obj: Dict):  # Receive the user's input and preprocess date         date = query_obj[\"date\"]         return date      @worker(dependencies=[\"pre_query\", \"pre_date\"], args_mapping_rule=ArgsMappingRule.AS_IS)     async def extract_entity(self, query: str, date: str):  # Extract the entity information from the question, get entity information.         response: EntityList = llm.structured_output(               model=_model_name,             constraint=PydanticModel(model=EntityList),             messages=[                 Message.from_text(text=\"extract the entity information from the given query\", role=Role.SYSTEM),                 Message.from_text(text=query, role=Role.USER,),             ]         )         return query, response.entities, date      @worker(dependencies=[\"extract_entity\"], args_mapping_rule=ArgsMappingRule.AS_IS)     async def expand_query(self, query_meta: Tuple[str, List[str]]):  # Expand and obtain multiple queries.         query, entities, date = query_meta         task_input = f\"Query: {query}\\nEntities: {entities}\"         response: str = llm.structured_output(             model=_model_name,             constraint=PydanticModel(model=QueryList),             messages=[                 Message.from_text(text=f\"Centered around the given entities and given date {date}, expand the query to obtain multiple queries\", role=Role.SYSTEM),                 Message.from_text(text=task_input, role=Role.USER,),             ]         )         return response.queries <p>Let's run it!</p> In\u00a0[20]: Copied! <pre>query_expansion = QueryExpansion(output_worker_key=\"expand_query\")\nawait query_expansion.arun(query_obj)\n</pre> query_expansion = QueryExpansion(output_worker_key=\"expand_query\") await query_expansion.arun(query_obj) Out[20]: <pre>['What are the latest advancements in Retrieval-Augmented Generation (RAG) technologies as of 2025?',\n 'What new developments have emerged in RAG systems over the past 12 months?',\n 'How have RAG implementations evolved in the last year in terms of performance and scalability?',\n 'What innovative techniques have been introduced in RAG models since 2024?',\n 'What are the key breakthroughs in RAG technology reported in 2025?',\n 'How have recent improvements in RAG affected real-world applications in 2025?',\n 'What new tools and frameworks have been launched for RAG development in the past year?',\n 'What are the most significant updates in RAG research published between 2024 and 2025?',\n 'How have retrieval and generation components in RAG been optimized in the last year?',\n 'What are the major trends in RAG development as of September 2025?']</pre> <p>Get it! We have successfully completed the small module for query expansion. Reviewing the code, we find that each <code>@worker</code> decorator has an <code>args_mapping_rule</code> parameter. Let's understand what it does.</p> In\u00a0[\u00a0]: Copied! <pre>@worker(dependencies=[\"extract_entity\"], args_mapping_rule=ArgsMappingRule.AS_IS)\nasync def expand_query(self, query_meta: Tuple[str, List[str]]):  # Expand and obtain multiple queries.\n    query, entities, date = query_meta\n    ...\n</pre> @worker(dependencies=[\"extract_entity\"], args_mapping_rule=ArgsMappingRule.AS_IS) async def expand_query(self, query_meta: Tuple[str, List[str]]):  # Expand and obtain multiple queries.     query, entities, date = query_meta     ... <p>This operation requires to know what the parameters of <code>query_meta</code> as a whole contain, which might seem inconvenient. Could we complete the unpacking operation and fill in the corresponding parameters when returning? At this point, the <code>UNPACK</code> mode comes in handy.</p> <p>Let's modify the <code>expand_query</code> in the above example and add some print message.</p> In\u00a0[21]: Copied! <pre>class QueryExpansion(GraphAutoma):\n    @worker(is_start=True)\n    async def pre_query(self, query_obj: Dict):  # Receive the user's input and preprocess query\n        query = query_obj[\"query\"].split(\":\")[1].strip()  \n        return query\n\n    @worker(is_start=True)\n    async def pre_date(self, query_obj: Dict):  # Receive the user's input and preprocess date\n        date = query_obj[\"date\"]\n        return date\n\n    @worker(dependencies=[\"pre_query\", \"pre_date\"], args_mapping_rule=ArgsMappingRule.AS_IS)\n    async def extract_entity(self, query: str, date: str):  # Extract the entity information from the question, get entity information.\n        response: EntityList = llm.structured_output(  \n            model=_model_name,\n            constraint=PydanticModel(model=EntityList),\n            messages=[\n                Message.from_text(text=\"extract the entity information from the given query\", role=Role.SYSTEM),\n                Message.from_text(text=query, role=Role.USER,),\n            ]\n        )\n        return query, response.entities, date\n\n    @worker(dependencies=[\"extract_entity\"], args_mapping_rule=ArgsMappingRule.UNPACK)\n    async def expand_query(self, query: str, entities: List[str], date: str):  # Expand and obtain multiple queries.\n        print(f\"query: {query}, entities: {entities}, date: {date}\")\n        task_input = f\"Query: {query}\\nEntities: {entities}\"\n        response: str = llm.structured_output(\n            model=_model_name,\n            constraint=PydanticModel(model=QueryList),\n            messages=[\n                Message.from_text(text=f\"Centered around the given entities and given date {date}, expand the query to obtain multiple queries\", role=Role.SYSTEM),\n                Message.from_text(text=task_input, role=Role.USER,),\n            ]\n        )\n        return response.queries\n</pre> class QueryExpansion(GraphAutoma):     @worker(is_start=True)     async def pre_query(self, query_obj: Dict):  # Receive the user's input and preprocess query         query = query_obj[\"query\"].split(\":\")[1].strip()           return query      @worker(is_start=True)     async def pre_date(self, query_obj: Dict):  # Receive the user's input and preprocess date         date = query_obj[\"date\"]         return date      @worker(dependencies=[\"pre_query\", \"pre_date\"], args_mapping_rule=ArgsMappingRule.AS_IS)     async def extract_entity(self, query: str, date: str):  # Extract the entity information from the question, get entity information.         response: EntityList = llm.structured_output(               model=_model_name,             constraint=PydanticModel(model=EntityList),             messages=[                 Message.from_text(text=\"extract the entity information from the given query\", role=Role.SYSTEM),                 Message.from_text(text=query, role=Role.USER,),             ]         )         return query, response.entities, date      @worker(dependencies=[\"extract_entity\"], args_mapping_rule=ArgsMappingRule.UNPACK)     async def expand_query(self, query: str, entities: List[str], date: str):  # Expand and obtain multiple queries.         print(f\"query: {query}, entities: {entities}, date: {date}\")         task_input = f\"Query: {query}\\nEntities: {entities}\"         response: str = llm.structured_output(             model=_model_name,             constraint=PydanticModel(model=QueryList),             messages=[                 Message.from_text(text=f\"Centered around the given entities and given date {date}, expand the query to obtain multiple queries\", role=Role.SYSTEM),                 Message.from_text(text=task_input, role=Role.USER,),             ]         )         return response.queries <p>Let's run it!</p> In\u00a0[22]: Copied! <pre>query_expansion = QueryExpansion(output_worker_key=\"expand_query\")\nawait query_expansion.arun(query_obj)\n</pre> query_expansion = QueryExpansion(output_worker_key=\"expand_query\") await query_expansion.arun(query_obj) <pre>query: What new developments have there been in RAG in the past year?, entities: ['RAG', 'new developments', 'past year'], date: 2025-09-30\n</pre> Out[22]: <pre>['What are the latest advancements in Retrieval-Augmented Generation (RAG) technologies as of 2025?',\n 'What new developments have emerged in RAG systems over the past 12 months?',\n 'How has RAG evolved in the last year with regard to accuracy, scalability, and real-time performance?',\n 'What are the key innovations in RAG models reported in 2024 and 2025?',\n 'What new tools and frameworks have been introduced for RAG implementation in the past year?',\n 'What breakthroughs in RAG have improved retrieval precision and context handling in 2025?',\n 'How have privacy and security features been enhanced in RAG systems over the past year?',\n 'What new applications of RAG have been developed in the last 12 months across industries?',\n 'What challenges and solutions have been proposed in RAG research during 2024\u20132025?',\n 'What role has RAG played in the evolution of large language models in the past year?']</pre> <p>Get it! All the parameters were unpacked and accepted. It can be seen that the <code>unpack</code> mode makes our task flow clearer!</p> <p>However, it should be noted that the UNPACK mechanism requires that the current worker can only directly depend on one worker; otherwise, the results of multiple workers will be confused when unpacking!</p> In\u00a0[23]: Copied! <pre>class QueryExpansion(GraphAutoma):\n    @worker(is_start=True)\n    async def pre_query(self, query_obj: Dict):  # Receive the user's input and preprocess query\n        query = query_obj[\"query\"].split(\":\")[1].strip()  \n        return query\n\n    @worker(is_start=True)\n    async def pre_date(self, query_obj: Dict):  # Receive the user's input and preprocess date\n        date = query_obj[\"date\"]\n        return date\n\n    @worker(dependencies=[\"pre_query\", \"pre_date\"], args_mapping_rule=ArgsMappingRule.MERGE)\n    async def extract_entity(self, query_meta: Tuple[str, str]):  # Extract the entity information from the question, get entity information.\n        print(f\"query_meta: {query_meta}\")\n        query, date = query_meta\n        response: EntityList = llm.structured_output(  \n            model=_model_name,\n            constraint=PydanticModel(model=EntityList),\n            messages=[\n                Message.from_text(text=\"extract the entity information from the given query\", role=Role.SYSTEM),\n                Message.from_text(text=query, role=Role.USER,),\n            ]\n        )\n        return query, response.entities, date\n\n    @worker(dependencies=[\"extract_entity\"], args_mapping_rule=ArgsMappingRule.UNPACK)\n    async def expand_query(self, query: str, entities: List[str], date: str):  # Expand and obtain multiple queries.\n        task_input = f\"Query: {query}\\nEntities: {entities}\"\n        response: str = llm.structured_output(\n            model=_model_name,\n            constraint=PydanticModel(model=QueryList),\n            messages=[\n                Message.from_text(text=f\"Centered around the given entities and given date {date}, expand the query to obtain multiple queries\", role=Role.SYSTEM),\n                Message.from_text(text=task_input, role=Role.USER,),\n            ]\n        )\n        return response.queries\n</pre> class QueryExpansion(GraphAutoma):     @worker(is_start=True)     async def pre_query(self, query_obj: Dict):  # Receive the user's input and preprocess query         query = query_obj[\"query\"].split(\":\")[1].strip()           return query      @worker(is_start=True)     async def pre_date(self, query_obj: Dict):  # Receive the user's input and preprocess date         date = query_obj[\"date\"]         return date      @worker(dependencies=[\"pre_query\", \"pre_date\"], args_mapping_rule=ArgsMappingRule.MERGE)     async def extract_entity(self, query_meta: Tuple[str, str]):  # Extract the entity information from the question, get entity information.         print(f\"query_meta: {query_meta}\")         query, date = query_meta         response: EntityList = llm.structured_output(               model=_model_name,             constraint=PydanticModel(model=EntityList),             messages=[                 Message.from_text(text=\"extract the entity information from the given query\", role=Role.SYSTEM),                 Message.from_text(text=query, role=Role.USER,),             ]         )         return query, response.entities, date      @worker(dependencies=[\"extract_entity\"], args_mapping_rule=ArgsMappingRule.UNPACK)     async def expand_query(self, query: str, entities: List[str], date: str):  # Expand and obtain multiple queries.         task_input = f\"Query: {query}\\nEntities: {entities}\"         response: str = llm.structured_output(             model=_model_name,             constraint=PydanticModel(model=QueryList),             messages=[                 Message.from_text(text=f\"Centered around the given entities and given date {date}, expand the query to obtain multiple queries\", role=Role.SYSTEM),                 Message.from_text(text=task_input, role=Role.USER,),             ]         )         return response.queries <p>Let's run it!</p> In\u00a0[24]: Copied! <pre>query_expansion = QueryExpansion(output_worker_key=\"expand_query\")\nawait query_expansion.arun(query_obj)\n</pre> query_expansion = QueryExpansion(output_worker_key=\"expand_query\") await query_expansion.arun(query_obj) <pre>query_meta: ['What new developments have there been in RAG in the past year?', '2025-09-30']\n</pre> Out[24]: <pre>['What are the latest advancements in Retrieval-Augmented Generation (RAG) technologies as of 2025?',\n 'What new developments have emerged in RAG systems over the past 12 months?',\n 'How has RAG evolved in the last year with recent innovations in AI and natural language processing?',\n 'What are the key updates and improvements in RAG models from 2024 to 2025?',\n 'What new features and techniques have been introduced in RAG solutions in the past year?',\n 'What recent breakthroughs in RAG have been reported in 2025?',\n 'What are the most significant developments in RAG technology between 2024 and 2025?',\n 'How have retrieval and generation components in RAG been enhanced in the last year?',\n 'What new applications of RAG have been introduced in the past year?',\n 'What research papers or industry reports highlight new developments in RAG from 2024 to 2025?']</pre> <p>Get it! The results that <code>extract_entity</code> depends on from the workers have all been collected into a list and passed to its parameters.</p>"},{"location":"tutorials/parameter_passing/#parameter-passing","title":"Parameter Passing\u00b6","text":"<p>Bridgic can orchestrate workflows composed of workers. There are three ways to pass data among workers, including Args-Mapping, Args-Injection, Inputs-Propagation. Now let's understand them with a sample example.</p> <p>Query expansion is a common step in RAG and can enhance the quality of RAG. To enhance the quality of query expansion, developers often first extract the entity information from the question and use it to assist the model in expanding the original question.</p> <p>Now let's take it. The user inputs the original question, and then we expand the question to obtain more sub-questions. There are three steps to complete the query extension:</p> <ol> <li>Receive the user's input and perform preprocessing, get original question.</li> <li>Extract the entity information from the question, get entity information.</li> <li>Expand and obtain multiple queries.</li> </ol> <p>Before we start, let's prepare the running environment.</p> <p>We use the <code>export</code> command to set up the API information for model invocation.</p>"},{"location":"tutorials/parameter_passing/#1-args-mapping","title":"1. Args-Mapping\u00b6","text":"<p>The <code>args_mapping_rule</code> defines the way data is passed between directly dependent workers, that is, how the result of the previous worker is mapped to the parameter of the next worker. Its value can only be specified through the properties of <code>ArgsMappingRule</code>.</p>"},{"location":"tutorials/parameter_passing/#as_is-mode","title":"AS_IS mode\u00b6","text":"<p>In the AS_IS mode, a worker will receive the output of all its directly dependent workers as input parameters in the order declared by the dependencies.</p> <p>In the above example, <code>extract_entity</code> declares dependencies: <code>dependencies=[\"pre_query\", \"pre_date\"]</code>, so the results of the two preceding workers will be mapped to the first and second parameters of <code>extract_entity</code> in the order specified by the dependencies declaration, the result of <code>pre_query</code> is mapped to <code>query</code> pramameter and the result of <code>pre_date</code> is mapped to <code>date</code> parameter.</p> <p>Note: The declaration order in dependencies only affects the order of parameter mapping, but does not influence the execution order of the dependent workers.</p> <p>Additionally, if the previous worker returns a result with multiple values, such as <code>return x, y</code>, then all the results will be passed as a tuple result. So in the above example, the parameter <code>query_meta</code> of <code>expand_query</code> received all the result values from <code>extract_entity</code>.</p>"},{"location":"tutorials/parameter_passing/#unpack-mode","title":"UNPACK mode\u00b6","text":"<p>Let's go back to the previous example. In the <code>expand_query</code>, we receive the parameters from the previous worker in the <code>AS_IS</code> mode and manually unpack them as a whole, like this:</p>"},{"location":"tutorials/parameter_passing/#merge","title":"MERGE\u00b6","text":"<p>At the same time, conversely, since there is an UNPACK mechanism, is there also a mechanism that can aggregate multiple results for receiving? This is particularly useful when a worker collects the results of multiple dependent workers. At this point, the <code>MERGE</code> mode comes in handy.</p> <p>Still referring to the example above, <code>extract_entity</code> actually received the results from two workers. Now let's try to make <code>extract_entity</code> receive all these results in a single parameter for use, instead of receiving two parameters.</p> <p>Let's modify the <code>extract_entity</code> in the above example and add some print message.</p>"},{"location":"tutorials/parameter_passing/#2-args-injection","title":"2. Args-Injection\u00b6","text":"<p>Looking back at the example above, we actually find that the <code>date</code> information is passed through <code>pre_date</code>, <code>extract_entity</code>, and finally reaches <code>expand_query</code>. However, in reality, <code>extract_entity</code> doesn't use this information at all. Thus, passing <code>date</code> here seems redundant. And The use of <code>date</code> in <code>expand_query</code> essentially only means that the data depends on it, but whether it is executed or not, this control dependency does not directly rely on it.</p> <p>Bridgic emphasizes the separation of data dependency and control dependency. This is beneficial for the future construction of complex execution graphs, as it allows for decoupling and avoids the need to adjust the entire orchestration map due to changes in data dependency. If you want to know Bridgic's in-depth thoughts on this aspect, you can read this document.</p> <p>Now</p>"},{"location":"tutorials/parameter_passing/","title":"\u00b6","text":""}]}