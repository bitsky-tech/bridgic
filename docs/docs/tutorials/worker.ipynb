{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0812860a",
   "metadata": {},
   "source": [
    "# Worker\n",
    "\n",
    "Bridgic is an innovative framework for agent programming and dynamic task orchestration. It offers developers a brand-new programming paradigm. Whether it's for rapid experimentation, complex system orchestration, or building intelligent agents with long lifecycles, Bridgic provides concise and powerful support.\n",
    "\n",
    "In this tutorial, let's build a sample word learning assistant. Input a word, output its derivational variations and make sentences using these variations. At the same time, we will understand the sample usage of a bridgic project along with the word learning assistant.\n",
    "\n",
    "## Getting Started\n",
    "\n",
    "Before we start, let's prepare the running environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9da5cf46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the environment variables.\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "_api_key = os.environ.get(\"OPENAI_API_KEY\")\n",
    "_model_name = os.environ.get(\"OPENAI_MODEL_NAME\")  # Using gpt-4.1-mini in this example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "539fa068",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necessary packages.\n",
    "from bridgic.llms.openai.openai_llm import OpenAILlm, Message, Role\n",
    "from bridgic.core.automa import GraphAutoma, worker"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ea6b1e6",
   "metadata": {},
   "source": [
    "### 1. Initialize a model.\n",
    "\n",
    "Bridgic provides a powerful encapsulation for model usage. Here, we will simply use it first. If you want to know more details, you can refer to this tutorial: [Llm](llm.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7468d809",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = OpenAILlm(\n",
    "    api_key=_api_key,\n",
    "    timeout=20,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92d70238",
   "metadata": {},
   "source": [
    "### 2. Complete chatbot\n",
    "\n",
    "There are two steps to complete the word learning assistant:\n",
    "\n",
    "1. Generate derivatives of the input word.\n",
    "2. Make sentences with derivatives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "da260e53",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WordLearningAssistant(GraphAutoma):\n",
    "    @worker(is_start=True)\n",
    "    async def generate_derivatives(self, word: str):\n",
    "        print(f\"------Generating derivatives for {word}------\")\n",
    "        response = await llm.achat(\n",
    "            model=_model_name,\n",
    "            messages=[\n",
    "                Message.from_text(text=\"You are a word learning assistant. Generate derivatives of the input word in a list.\", role=Role.SYSTEM),\n",
    "                Message.from_text(text=word, role=Role.USER),\n",
    "            ]\n",
    "        )\n",
    "        print(response.message.content)\n",
    "        print(f\"------End of generating derivatives------\\n\")\n",
    "        return response.message.content\n",
    "\n",
    "    @worker(dependencies=[\"generate_derivatives\"], is_output=True)\n",
    "    async def make_sentences(self, derivatives):\n",
    "        print(f\"------Making sentences with------\")\n",
    "        response = await llm.achat(\n",
    "            model=_model_name,\n",
    "            messages=[\n",
    "                Message.from_text(text=\"You are a word learning assistant. Make sentences with the input derivatives in a list.\", role=Role.SYSTEM),\n",
    "                Message.from_text(text=derivatives, role=Role.USER),\n",
    "            ]\n",
    "        )\n",
    "        print(response.message.content)\n",
    "        print(f\"------End of making sentences------\\n\")\n",
    "        return response.message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81cf951d",
   "metadata": {},
   "source": [
    "Let's run it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a6fbb8d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------Generating derivatives for happy------\n",
      "1. happiness  \n",
      "2. happily  \n",
      "3. happier  \n",
      "4. happiest  \n",
      "5. unhappiness  \n",
      "6. unhappily  \n",
      "7. unhappier  \n",
      "8. unhappiest\n",
      "------End of generating derivatives------\n",
      "\n",
      "------Making sentences with------\n",
      "1. Happiness is the key to a fulfilling life.  \n",
      "2. She happily accepted the invitation to the party.  \n",
      "3. I'm happier now that I've started a new job.  \n",
      "4. That was the happiest day of my childhood.  \n",
      "5. His unhappiness became apparent after the breakup.  \n",
      "6. They lived unhappily together for many years.  \n",
      "7. After the storm, the plants looked unhappier than before.  \n",
      "8. She felt the unhappiest when she lost her favorite book.\n",
      "------End of making sentences------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "word_learning_assistant = WordLearningAssistant()\n",
    "res = await word_learning_assistant.arun(word=\"happy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a176cd0c",
   "metadata": {},
   "source": [
    "> Note: Ensure you have set up your .env file to store your OPENAI_API_KEY or set up your terminal environment variable. This key is necessary for authenticating requests to the OpenAI API.\n",
    "\n",
    "Get it! We successfully completed the word learning assistant. It correctly completed the task as per our requirements.\n",
    "\n",
    "## Understand\n",
    "\n",
    "The above example is the typical usage of writing an agent application with Bridgic. Now let's understand some of its components.\n",
    "\n",
    "### Worker\n",
    "\n",
    "Any callable object (such as functions, methods, etc.) when used by the framework, will be converted into a worker object and serve as the **smallest execution unit** for scheduling and orchestration. \n",
    "\n",
    "> Note: From the perspective of the Bridgic framework, a worker must be placed in an automa for scheduling before it can be executed. Of course, even after packaging it as a worker, you can directly call `worker.arun()` or `worker.run()` to run it, but this is not within the purview of bridgic.\n",
    "\n",
    "There are two ways to convert a callable object into a Worker object. There are two ways to convert a callable object into a worker object. The first is to use the `@worker` decorator, and the second is to add it through the `add_worker()` interface.\n",
    "\n",
    "#### 1. @worker\n",
    "\n",
    "Just as in the example of the word learning assistant, we can use decorator syntax to wrap functions and methods into a worker object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d507c1eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyAutoma(GraphAutoma):\n",
    "    @worker(is_start=True)\n",
    "    async def start(self, x: int):\n",
    "        return x    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82ed3b6b",
   "metadata": {},
   "source": [
    "Or, you can also use it like thisï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "614c5d49",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyAutoma(GraphAutoma): ...\n",
    "my_automa = MyAutoma()\n",
    "\n",
    "@my_automa.worker(is_start=True)\n",
    "async def start(automa, x: int):\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "179b434b",
   "metadata": {},
   "source": [
    "Both of these two ways can correctly add the `start` to `MyAutoma`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfff50c5",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "### Automa\n",
    "\n",
    "Automa is a specific worker. Automa serves as the **orchestration engine**. Developers can entrust multiple Workers to Automa for management, and it is responsible for unified scheduling and operation, acting as the entry point for the entire process."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
