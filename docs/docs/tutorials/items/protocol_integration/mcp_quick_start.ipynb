{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MCP\n",
    "\n",
    "This tutorial will demonstrate how to integrate [Model Context Protocol (MCP)](https://modelcontextprotocol.io/) with Bridgic to enhance your development in building agentic applications.\n",
    "\n",
    "## Introduction\n",
    "\n",
    "**Model Context Protocol (MCP)** is an open protocol that enables AI applications to securely access external resources and tools. By integrating MCP with Bridgic, you can:\n",
    "\n",
    "- **Connect to MCP Servers**: Access a wide range of external services and resources through standardized MCP servers\n",
    "- **Get and Use MCP Tools**: Leverage tools provided by MCP servers as workers in your Bridgic workflows\n",
    "- **Get and Use MCP Prompts**: Utilize pre-configured prompt templates from MCP servers\n",
    "- **Build Intelligent Agents**: Enable LLM-driven agents to autonomously select and use MCP tools\n",
    "\n",
    "This tutorial will walk you through the essentials of integrating MCP with Bridgic, from basic installation to advanced usage, along with easy-to-understand examples.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation\n",
    "\n",
    "First, install the `bridgic-protocols-mcp` package. Since the MCP Python SDK requires Python 3.12 or newer. Please ensure you are using a compatible Python version before installation.\n",
    "\n",
    "```shell\n",
    "pip install bridgic-protocols-mcp\n",
    "```\n",
    "\n",
    "Let's verify the installation by running:\n",
    "\n",
    "```shell\n",
    "python -c \"from bridgic.protocols.mcp import __version__; print(__version__)\"\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Usage\n",
    "\n",
    "### Connecting to an MCP Server\n",
    "\n",
    "MCP servers can be connected via different transport. The most common is **stdio** transport, which runs the MCP server as a subprocess. Let's connect to a filesystem MCP server as an example and see what tools it offers:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using temporary directory: /private/var/folders/9t/5r9fms9s5q33p6xty_0_k1mw0000gn/T/tmpcr218hp9\n",
      "‚úì Connected to MCP server: connection-filesystem-stdio\n",
      "  Connection status: True\n",
      "\n",
      "‚úì Found 14 available tools:\n",
      "  - read_file: Read the complete contents of a file as text. DEPR...\n",
      "  - read_text_file: Read the complete contents of a file from the file...\n",
      "  - read_media_file: Read an image or audio file. Returns the base64 en...\n",
      "  - read_multiple_files: Read the contents of multiple files simultaneously...\n",
      "  - write_file: Create a new file or completely overwrite an exist...\n",
      "  - edit_file: Make line-based edits to a text file. Each edit re...\n",
      "  - create_directory: Create a new directory or ensure a directory exist...\n",
      "  - list_directory: Get a detailed listing of all files and directorie...\n",
      "  - list_directory_with_sizes: Get a detailed listing of all files and directorie...\n",
      "  - directory_tree: Get a recursive tree view of files and directories...\n",
      "  - move_file: Move or rename files and directories. Can move fil...\n",
      "  - search_files: Recursively search for files and directories match...\n",
      "  - get_file_info: Retrieve detailed metadata about a file or directo...\n",
      "  - list_allowed_directories: Returns the list of directories that this server i...\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import tempfile\n",
    "\n",
    "from bridgic.protocols.mcp import McpServerConnectionStdio\n",
    "\n",
    "# Create a temporary directory for the filesystem MCP server\n",
    "temp_dir = os.path.realpath(tempfile.mkdtemp())\n",
    "print(f\"Using temporary directory: {temp_dir}\")\n",
    "\n",
    "# Create a connection to a filesystem MCP server\n",
    "# Note: This requires Node.js and npx to be installed\n",
    "filesystem_connection = McpServerConnectionStdio(\n",
    "    name=\"connection-filesystem-stdio\",\n",
    "    command=\"npx\",\n",
    "    args=[\"-y\", \"@modelcontextprotocol/server-filesystem\", temp_dir],\n",
    ")\n",
    "\n",
    "# Establish the connection\n",
    "filesystem_connection.connect()\n",
    "\n",
    "# Verify connection\n",
    "print(f\"‚úì Connected to MCP server: {filesystem_connection.name}\")\n",
    "print(f\"  Connection status: {filesystem_connection.is_connected}\")\n",
    "\n",
    "# List available tools\n",
    "tools = filesystem_connection.list_tools()\n",
    "print(f\"\\n‚úì Found {len(tools)} available tools:\")\n",
    "for tool in tools:\n",
    "    print(f\"  - {tool.tool_name}: {tool.tool_description[:50]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also connect to an MCP server via streamable HTTP transport. Below is an example of how to connect to a remote Github MCP Server and view the tools it supports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Connected to MCP server: connection-github-streamable-http\n",
      "  Connection status: True\n",
      "\n",
      "‚úì Found 40 available tools:\n",
      "  - add_comment_to_pending_review: Add review comment to the requester's latest pendi...\n",
      "  - add_issue_comment: Add a comment to a specific issue in a GitHub repo...\n",
      "  - assign_copilot_to_issue: Assign Copilot to a specific issue in a GitHub rep...\n",
      "  - create_branch: Create a new branch in a GitHub repository...\n",
      "  - create_or_update_file: Create or update a single file in a GitHub reposit...\n",
      "  - create_pull_request: Create a new pull request in a GitHub repository....\n",
      "  - create_repository: Create a new GitHub repository in your account or ...\n",
      "  - delete_file: Delete a file from a GitHub repository...\n",
      "  - fork_repository: Fork a GitHub repository to your account or specif...\n",
      "  - get_commit: Get details for a commit from a GitHub repository...\n",
      "  - get_file_contents: Get the contents of a file or directory from a Git...\n",
      "  - get_label: Get a specific label from a repository....\n",
      "  - get_latest_release: Get the latest release in a GitHub repository...\n",
      "  - get_me: Get details of the authenticated GitHub user. Use ...\n",
      "  - get_release_by_tag: Get a specific release by its tag name in a GitHub...\n",
      "  - get_tag: Get details about a specific git tag in a GitHub r...\n",
      "  - get_team_members: Get member usernames of a specific team in an orga...\n",
      "  - get_teams: Get details of the teams the user is a member of. ...\n",
      "  - issue_read: Get information about a specific issue in a GitHub...\n",
      "  - issue_write: Create a new or update an existing issue in a GitH...\n",
      "  - list_branches: List branches in a GitHub repository...\n",
      "  - list_commits: Get list of commits of a branch in a GitHub reposi...\n",
      "  - list_issue_types: List supported issue types for repository owner (o...\n",
      "  - list_issues: List issues in a GitHub repository. For pagination...\n",
      "  - list_pull_requests: List pull requests in a GitHub repository. If the ...\n",
      "  - list_releases: List releases in a GitHub repository...\n",
      "  - list_tags: List git tags in a GitHub repository...\n",
      "  - merge_pull_request: Merge a pull request in a GitHub repository....\n",
      "  - pull_request_read: Get information on a specific pull request in GitH...\n",
      "  - pull_request_review_write: Create and/or submit, delete review of a pull requ...\n",
      "  - push_files: Push multiple files to a GitHub repository in a si...\n",
      "  - request_copilot_review: Request a GitHub Copilot code review for a pull re...\n",
      "  - search_code: Fast and precise code search across ALL GitHub rep...\n",
      "  - search_issues: Search for issues in GitHub repositories using iss...\n",
      "  - search_pull_requests: Search for pull requests in GitHub repositories us...\n",
      "  - search_repositories: Find GitHub repositories by name, description, rea...\n",
      "  - search_users: Find GitHub users by username, real name, or other...\n",
      "  - sub_issue_write: Add a sub-issue to a parent issue in a GitHub repo...\n",
      "  - update_pull_request: Update an existing pull request in a GitHub reposi...\n",
      "  - update_pull_request_branch: Update the branch of a pull request with the lates...\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import dotenv\n",
    "\n",
    "from mcp.shared._httpx_utils import create_mcp_http_client\n",
    "from bridgic.protocols.mcp import McpServerConnectionStreamableHttp\n",
    "\n",
    "dotenv.load_dotenv()\n",
    "\n",
    "github_mcp_url = os.environ.get(\"GITHUB_MCP_HTTP_URL\", \"https://api.githubcopilot.com/mcp/\")\n",
    "github_token = os.environ.get(\"GITHUB_TOKEN\")\n",
    "\n",
    "http_client = create_mcp_http_client(\n",
    "    headers={\"Authorization\": f\"Bearer {github_token}\"},\n",
    ")\n",
    "\n",
    "github_connection = McpServerConnectionStreamableHttp(\n",
    "    name=\"connection-github-streamable-http\",\n",
    "    url=github_mcp_url,\n",
    "    http_client=http_client,\n",
    "    request_timeout=15,\n",
    ")\n",
    "\n",
    "github_connection.connect()\n",
    "\n",
    "# Verify connection\n",
    "print(f\"‚úì Connected to MCP server: {github_connection.name}\")\n",
    "print(f\"  Connection status: {github_connection.is_connected}\")\n",
    "\n",
    "# List available tools\n",
    "tools = github_connection.list_tools()\n",
    "print(f\"\\n‚úì Found {len(tools)} available tools:\")\n",
    "for tool in tools:\n",
    "    print(f\"  - {tool.tool_name}: {tool.tool_description[:50]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using a MCP Tool as a Worker\n",
    "\n",
    "MCP tools can be converted to Bridgic workers and integrated into `GraphAutoma` building. This allows you to orchestrate MCP tool calls alongside other workers in your application.\n",
    "\n",
    "Why don't we just run the tool directly, but instead execute it as a worker? In Bridgic's view, every execution process in a workflow program (or an even more agentic system) can be decomposed into fine-grained workers, which can then be orchestrated and scheduled. Standardizing the execution process in this way simplifies development and debugging, and enhances observability during execution. Currently, too many frameworks separate agent operation from programmable orchestration, causing tools and developer-defined work units to hold unequal positions. This leads to two very different development and debugging experiences.\n",
    "\n",
    "In the world of Bridgic, a tool is a worker: both are work units with clearly defined inputs and expected outputs, which are connected together, either due to predefined conditions or runtime ones, to accomplish more complex and bigger tasks.\n",
    "\n",
    "Let's create a simple workflow that uses MCP tools to read and write files:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úì Finnished writting!\n",
      "File path: /private/var/folders/9t/5r9fms9s5q33p6xty_0_k1mw0000gn/T/tmp13yvj1mx/1.txt.txt\n",
      "size: 51\n",
      "created: Wed Jan 21 2026 19:17:10 GMT+0800 (China Standard Time)\n",
      "modified: Wed Jan 21 2026 19:17:10 GMT+0800 (China Standard Time)\n",
      "accessed: Wed Jan 21 2026 19:17:10 GMT+0800 (China Standard Time)\n",
      "isDirectory: false\n",
      "isFile: true\n",
      "permissions: 644\n",
      "\n",
      "‚úì Finnished writting!\n",
      "File path: /private/var/folders/9t/5r9fms9s5q33p6xty_0_k1mw0000gn/T/tmp13yvj1mx/2.txt.txt\n",
      "size: 47\n",
      "created: Wed Jan 21 2026 19:17:10 GMT+0800 (China Standard Time)\n",
      "modified: Wed Jan 21 2026 19:17:10 GMT+0800 (China Standard Time)\n",
      "accessed: Wed Jan 21 2026 19:17:10 GMT+0800 (China Standard Time)\n",
      "isDirectory: false\n",
      "isFile: true\n",
      "permissions: 644\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "import mcp\n",
    "\n",
    "from bridgic.core.automa import GraphAutoma, RunningOptions, worker\n",
    "from bridgic.core.automa.args import System\n",
    "\n",
    "# List the tools via the server connection\n",
    "tools = filesystem_connection.list_tools()\n",
    "\n",
    "# Filter the needed one which will create the real worker\n",
    "write_tool = next(t for t in tools if t.tool_name == \"write_file\")\n",
    "read_tool = next(t for t in tools if t.tool_name == \"read_file\")\n",
    "meta_tool = next(t for t in tools if t.tool_name == \"get_file_info\")\n",
    "\n",
    "class FileWriter(GraphAutoma):\n",
    "    def __init__(self, name: str, running_options: RunningOptions = None):\n",
    "        super().__init__(name=name, running_options=running_options)\n",
    "        self.add_worker(\"write\", write_tool.create_worker())\n",
    "        self.add_worker(\"read\", read_tool.create_worker())\n",
    "        self.add_worker(\"meta\", meta_tool.create_worker())\n",
    "\n",
    "    @worker(is_start=True)\n",
    "    def start(self, title: str, content: str, rtx = System(\"runtime_context\")):\n",
    "        # Get the current time\n",
    "        now_time = datetime.datetime.now()\n",
    "\n",
    "        # Get the content and path of the file to be written\n",
    "        file_path = f\"{temp_dir}/{title}.txt\"\n",
    "        file_content = (\n",
    "            f\"Time: {now_time.strftime('%Y-%m-%d %H:%M:%S')}\\n\"\n",
    "            f\"Content: {content}\\n\"\n",
    "        )\n",
    "\n",
    "        # Write the file at the next step\n",
    "        self.ferry_to(\"write\", content=file_content, path=file_path)\n",
    "\n",
    "        return file_path\n",
    "\n",
    "    @worker(dependencies=[\"start\", \"write\"])\n",
    "    def after_write(self, file_path: str, write_info: mcp.types.CallToolResult):\n",
    "        self.ferry_to(\"read\", path=file_path)\n",
    "        self.ferry_to(\"meta\", path=file_path)\n",
    "\n",
    "    @worker(is_output=True, dependencies=[\"start\", \"read\", \"meta\"])\n",
    "    def output(self, file_path: str, read_info: mcp.types.CallToolResult, meta_info: mcp.types.CallToolResult) -> str:\n",
    "        return (\n",
    "            f\"‚úì Finnished writting!\"\n",
    "            f\"\\nFile path: {file_path}\"\n",
    "            f\"\\n{meta_info.content[0].text}\"\n",
    "        )\n",
    "\n",
    "file_processor = FileWriter(name=\"file-processor\")\n",
    "\n",
    "for content in [\n",
    "    (\"1.txt\", \"Hello, Bridgic!\"),\n",
    "    (\"2.txt\", \"Hello, MCP!\"),\n",
    "]:\n",
    "    result = await file_processor.arun(title=content[0], content=content[1])\n",
    "    print(f\"\\n{result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using MCP Tools in an agentic Automa\n",
    "\n",
    "In more scenarios, you may prefer to use an LLM-powered automa that can determine which MCP tools to utilize, adapting its choices to a specific goal and the evolving context during execution.\n",
    "\n",
    "`ReCentAutoma` is such an agentic automa. By passing MCP tools into it, you can:\n",
    "\n",
    "- Keep the orchestration logic in Bridgic while delegating decisions to the LLM.\n",
    "- Let the LLM select appropriate tools at each step\n",
    "- Collect the results of tool calls and incorporate them as part of a dynamic process\n",
    "\n",
    "Below is a minimal example that uses the weather MCP tools inside a `ReCentAutoma`.\n",
    "\n",
    "First of all, you have to connect to an MCP server that contains the weather tool."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bridgic.protocols.mcp import McpServerConnectionStdio\n",
    "\n",
    "weather_connection = McpServerConnectionStdio(\n",
    "    name=\"connection-weather-stdio\",\n",
    "    command=\"npx\",\n",
    "    args=[\"-y\", \"@mariox/weather-mcp-server\"],\n",
    ")\n",
    "weather_connection.connect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then you have to initialize an agentic automa which utilizes the weather tool(s) to answer the weather questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;46m\n",
      "[ReCentAutoma]-[ReCentAutoma-07228b9a] is started.\u001b[0m\n",
      "\u001b[38;5;129m[ReCentAutoma]-[ReCentAutoma-07228b9a] [__dynamic_step__] driving [initialize_task_goal]\u001b[0m\n",
      "\u001b[38;5;51m[ReCentAutoma]-[ReCentAutoma-07228b9a] [__automa__] triggers [initialize_task_goal]\u001b[0m\n",
      "[ReCentAutoma]-[ReCentAutoma-07228b9a] üéØ Task Goal\n",
      "Get the weather in Shanghai.\n",
      "\u001b[38;5;129m[ReCentAutoma]-[ReCentAutoma-07228b9a] [__dynamic_step__] driving [observe]\u001b[0m\n",
      "\u001b[38;5;51m[ReCentAutoma]-[ReCentAutoma-07228b9a] [initialize_task_goal] triggers [observe]\u001b[0m\n",
      "\u001b[38;5;244m[ReCentAutoma]-[ReCentAutoma-07228b9a] üëÄ Observation\n",
      "    Iteration: 1\n",
      "    Achieved: False\n",
      "    Thinking: The task goal was to get the weather in Shanghai, but the conversation history does not indicate that any weather information has been provided yet. Thus, there are significant gaps remaining as the goal has not been addressed. Therefore, the goal has not been achieved.\u001b[0m\n",
      "\u001b[38;5;129m[ReCentAutoma]-[ReCentAutoma-07228b9a] [__dynamic_step__] driving [select_tools, compress_memory]\u001b[0m\n",
      "\u001b[38;5;51m[ReCentAutoma]-[ReCentAutoma-07228b9a] [observe] triggers [select_tools]\u001b[0m\n",
      "\u001b[38;5;51m[ReCentAutoma]-[ReCentAutoma-07228b9a] [observe] triggers [compress_memory]\u001b[0m\n",
      "\u001b[38;5;100m[ReCentAutoma]-[ReCentAutoma-07228b9a] üß≠ Memory Check\n",
      "    Compression Needed: False\u001b[0m\n",
      "\u001b[38;5;208m[ReCentAutoma]-[ReCentAutoma-07228b9a] üîß Tool Selection\n",
      "    Tool 1: get_weather\n",
      "      id: tool_e400cec223544e4ea161ef38e\n",
      "      arguments: {'location': '‰∏äÊµ∑'}\u001b[0m\n",
      "\u001b[38;5;129m[ReCentAutoma]-[ReCentAutoma-07228b9a] [__dynamic_step__] driving [tool-<get_weather>-<tool_e400cec223544e4ea161ef38e>]\u001b[0m\n",
      "\u001b[38;5;51m[ReCentAutoma]-[ReCentAutoma-07228b9a] [select_tools] triggers [tool-<get_weather>-<tool_e400cec223544e4ea161ef38e>]\u001b[0m\n",
      "\u001b[38;5;129m[ReCentAutoma]-[ReCentAutoma-07228b9a] [__dynamic_step__] driving [collect_results-<f6e28bdd>]\u001b[0m\n",
      "\u001b[38;5;51m[ReCentAutoma]-[ReCentAutoma-07228b9a] [tool-<get_weather>-<tool_e400cec223544e4ea161ef38e>] triggers [collect_results-<f6e28bdd>]\u001b[0m\n",
      "\u001b[38;5;46m[ReCentAutoma]-[ReCentAutoma-07228b9a] üö© Tool Results\n",
      "    Tool 1: get_weather\n",
      "      id: tool_e400cec223544e4ea161ef38e\n",
      "      result: meta=None content=[TextContent(type='text', text='üå§Ô∏è **‰∏äÊµ∑ Â§©Ê∞î‰ø°ÊÅØ**\\n\\nüìç **‰ΩçÁΩÆ‰ø°ÊÅØ:**\\nüó∫Ô∏è Âú∞ÁÇπ: ‰∏äÊµ∑Â∏Ç, ‰∏≠ÂõΩ\\nüìä ÂùêÊ†á: 31.2304, 121.4737\\n\\nüå§Ô∏è **Â§©Ê∞î‰ø°ÊÅØ:**\\nüå°Ô∏è Ê∏©Â∫¶: -0.5¬∞C\\n‚òÅÔ∏è Â§©Ê∞î: Èò¥Â§©\\nüí® È£éÈÄü: 8.9 km/h\\nüß≠ È£éÂêë: 346¬∞\\nüíß ÊπøÂ∫¶: 56%\\nüìä Ê∞îÂéã: 1034.3 hPa\\nüïê Êó∂Èó¥: 2026-01-21T21:45\\n\\nüì° Êï∞ÊçÆÊ∫ê: Open-Meteo (ÂÖçË¥πAPI)', annotations=None, meta=None)] structuredContent=None isError=False\u001b[0m\n",
      "\u001b[38;5;129m[ReCentAutoma]-[ReCentAutoma-07228b9a] [__dynamic_step__] driving [observe]\u001b[0m\n",
      "\u001b[38;5;51m[ReCentAutoma]-[ReCentAutoma-07228b9a] [collect_results-<f6e28bdd>] triggers [observe]\u001b[0m\n",
      "\u001b[38;5;244m[ReCentAutoma]-[ReCentAutoma-07228b9a] üëÄ Observation\n",
      "    Iteration: 2\n",
      "    Achieved: True\n",
      "    Thinking: The task goal was to obtain the weather information for Shanghai. This has been successfully accomplished, as the weather conditions including temperature, weather description, wind speed, humidity, and pressure have been retrieved. There are no remaining gaps or missing information regarding the goal.\u001b[0m\n",
      "\u001b[38;5;129m[ReCentAutoma]-[ReCentAutoma-07228b9a] [__dynamic_step__] driving [finalize_answer]\u001b[0m\n",
      "\u001b[38;5;51m[ReCentAutoma]-[ReCentAutoma-07228b9a] [observe] triggers [finalize_answer]\u001b[0m\n",
      "\u001b[38;5;46m[ReCentAutoma]-[ReCentAutoma-07228b9a] is finished.\u001b[0m\n",
      "### Weather Information for Shanghai\n",
      "\n",
      "- **Location:** Shanghai, China\n",
      "- **Coordinates:** 31.2304¬∞ N, 121.4737¬∞ E\n",
      "\n",
      "#### Current Weather Conditions\n",
      "- **Temperature:** -0.5¬∞C\n",
      "- **Weather:** Overcast\n",
      "- **Wind Speed:** 8.9 km/h\n",
      "- **Wind Direction:** 346¬∞\n",
      "- **Humidity:** 56%\n",
      "- **Atmospheric Pressure:** 1034.3 hPa\n",
      "\n",
      "#### Data Source\n",
      "- This weather information is sourced from Open-Meteo (free API).\n",
      "\n",
      "This summary presents the current weather conditions in Shanghai, providing a snapshot of the local atmosphere including temperatures, wind, humidity, and more.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import dotenv\n",
    "\n",
    "from bridgic.llms.openai import OpenAILlm, OpenAIConfiguration\n",
    "from bridgic.core.automa import RunningOptions\n",
    "from bridgic.core.agentic.recent import ReCentAutoma, StopCondition\n",
    "\n",
    "dotenv.load_dotenv()\n",
    "\n",
    "# Prepare the LLM (set these env vars before running this cell)\n",
    "_api_key = os.environ.get(\"OPENAI_API_KEY\")\n",
    "_api_base = os.environ.get(\"OPENAI_API_BASE\", \"https://api.openai.com/v1\")\n",
    "_model_name = os.environ.get(\"OPENAI_MODEL_NAME\", \"gpt-4o-mini\")\n",
    "\n",
    "# Initialize LLM instance\n",
    "llm = OpenAILlm(\n",
    "    api_key=_api_key,\n",
    "    api_base=_api_base,\n",
    "    configuration=OpenAIConfiguration(model=_model_name),\n",
    "    timeout=30,\n",
    ")\n",
    "\n",
    "# Pass weather tools in directly to build an agentic automa as a weather agent\n",
    "weather_agent = ReCentAutoma(\n",
    "    llm=llm,\n",
    "    tools=weather_connection.list_tools(),\n",
    "    stop_condition=StopCondition(max_iteration=3),\n",
    "    running_options=RunningOptions(debug=True),\n",
    ")\n",
    "\n",
    "# Ask the weather agent for the weather in Shanghai\n",
    "result = await weather_agent.arun(goal=\"Get the weather in Shanghai.\")\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using MCP Prompts to render your context\n",
    "\n",
    "MCP servers can also provide prompt templates that can be used to render context for your LLM applications. These prompts are useful for standardizing how you format messages before sending them to an LLM.\n",
    "\n",
    "You can check the available prompt templates from the server by running:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name: AssignCodingAgent:\n",
      "description: Assign GitHub Coding Agent to multiple tasks in a GitHub repository.\n",
      "parameters: [\n",
      "  \"[required=True] repo: The repository to assign tasks in (owner/repo).\"\n",
      "]\n",
      "\n",
      "name: issue_to_fix_workflow:\n",
      "description: Create an issue for a problem and then generate a pull request to fix it\n",
      "parameters: [\n",
      "  \"[required=True] owner: Repository owner\",\n",
      "  \"[required=True] repo: Repository name\",\n",
      "  \"[required=True] title: Issue title\",\n",
      "  \"[required=True] description: Issue description\",\n",
      "  \"[required=None] labels: Comma-separated list of labels to apply (optional)\",\n",
      "  \"[required=None] assignees: Comma-separated list of assignees (optional)\"\n",
      "]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "from bridgic.protocols.mcp import McpPromptTemplate\n",
    "\n",
    "prompts: list[McpPromptTemplate] = github_connection.list_prompts()\n",
    "\n",
    "for prompt in prompts:\n",
    "    description = prompt.prompt_info.description\n",
    "    arguments = [f\"[required={arg.required}] {arg.name}: {arg.description}\" for arg in prompt.prompt_info.arguments]\n",
    "    print(\n",
    "        f\"name: {prompt.prompt_name}:\\n\"\n",
    "        f\"description: {description}\\n\"\n",
    "        f\"parameters: {json.dumps(arguments, indent=2)}\\n\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now know that there is a prompt template named \"issue_to_fix_workflow\" available. This template is designed to generate instructions for LLM to help it to use tools to create an issue and a pull request on GitHub. It requires the following parameters: `owner`, `repo`, `title`, and `description`.\n",
    "\n",
    "Let's fill in these arguments to render the prompt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Message(role=<Role.USER: 'user'>, blocks=[TextBlock(block_type='text', text='You are a development workflow assistant helping to create GitHub issues and generate corresponding pull requests to fix them. You should: 1) Create a well-structured issue with clear problem description, 2) Assign it to Copilot coding agent to generate a solution, and 3) Monitor the PR creation process.')], extras={}), Message(role=<Role.USER: 'user'>, blocks=[TextBlock(block_type='text', text=\"I need to create an issue titled 'A New bug' in somebody/awesome-project and then have a PR generated to fix it. The issue description is: The bug is really annoying and it have to be fixed.\")], extras={}), Message(role=<Role.AI: 'assistant'>, blocks=[TextBlock(block_type='text', text=\"I'll help you create the issue 'A New bug' in somebody/awesome-project and then coordinate with Copilot to generate a fix. Let me start by creating the issue with the provided details.\")], extras={}), Message(role=<Role.USER: 'user'>, blocks=[TextBlock(block_type='text', text='Perfect! Please:\\n1. Create the issue with the title, description, labels, and assignees\\n2. Once created, assign it to Copilot coding agent to generate a solution\\n3. Monitor the process and let me know when the PR is ready for review')], extras={}), Message(role=<Role.AI: 'assistant'>, blocks=[TextBlock(block_type='text', text=\"Excellent plan! Here's what I'll do:\\n\\n1. ‚úÖ Create the issue with all specified details\\n2. ü§ñ Assign to Copilot coding agent for automated fix\\n3. üìã Monitor progress and notify when PR is created\\n4. üîç Provide PR details for your review\\n\\nLet me start by creating the issue.\")], extras={})]\n"
     ]
    }
   ],
   "source": [
    "fix_issue_template = next(p for p in prompts if p.prompt_name == \"issue_to_fix_workflow\")\n",
    "\n",
    "messages = fix_issue_template.format_messages(\n",
    "    owner=\"somebody\",\n",
    "    repo=\"awesome-project\",\n",
    "    title=\"A New bug\",\n",
    "    description=\"The bug is really annoying and it have to be fixed.\",\n",
    ")\n",
    "print(messages)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_bridgic",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
