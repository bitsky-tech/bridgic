{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# MCP\n",
        "\n",
        "This tutorial will demonstrate how to integrate [Model Context Protocol (MCP)](https://modelcontextprotocol.io/) with Bridgic to enhance your development in building agentic applications.\n",
        "\n",
        "## Introduction\n",
        "\n",
        "**Model Context Protocol (MCP)** enables AI applications to access external resources and tools. By integrating MCP with Bridgic, you can:\n",
        "\n",
        "- **Connect to MCP Servers**: Access a wide range of external services and resources through standardized MCP servers\n",
        "- **Get and Use MCP Tools**: Leverage tools provided by MCP servers as workers in your Bridgic workflows\n",
        "- **Get and Use MCP Prompts**: Utilize pre-configured prompt templates from MCP servers\n",
        "- **Greatly Enhance your Agentic Module**: Enable LLM-driven agents to autonomously select and use MCP tools\n",
        "\n",
        "This tutorial will walk you through the essentials of integrating MCP with Bridgic, from basic installation to advanced usage, along with easy-to-understand examples.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Installation\n",
        "\n",
        "First, install the `bridgic-protocols-mcp` package. Since the MCP Python SDK requires Python 3.12 or newer. Please ensure you are using a compatible Python version before installation.\n",
        "\n",
        "```shell\n",
        "pip install bridgic-protocols-mcp\n",
        "```\n",
        "\n",
        "Let's verify the installation by running:\n",
        "\n",
        "```shell\n",
        "python -c \"from bridgic.protocols.mcp import __version__; print(__version__)\"\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Basic Usage\n",
        "\n",
        "### Connecting to an MCP Server\n",
        "\n",
        "MCP servers can be connected via different transport. The most common is **stdio** transport, which runs the MCP server as a subprocess. Let's connect to a filesystem MCP server as an example and see what tools it offers:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using temporary directory: /private/var/folders/9t/5r9fms9s5q33p6xty_0_k1mw0000gn/T/tmpeuov9ggi\n",
            "‚úì Connected to MCP server: connection-filesystem-stdio\n",
            "  Connection status: True\n",
            "\n",
            "‚úì Found 14 available tools:\n",
            "  - read_file: Read the complete contents of a file as text. DEPR...\n",
            "  - read_text_file: Read the complete contents of a file from the file...\n",
            "  - read_media_file: Read an image or audio file. Returns the base64 en...\n",
            "  - read_multiple_files: Read the contents of multiple files simultaneously...\n",
            "  - write_file: Create a new file or completely overwrite an exist...\n",
            "  - edit_file: Make line-based edits to a text file. Each edit re...\n",
            "  - create_directory: Create a new directory or ensure a directory exist...\n",
            "  - list_directory: Get a detailed listing of all files and directorie...\n",
            "  - list_directory_with_sizes: Get a detailed listing of all files and directorie...\n",
            "  - directory_tree: Get a recursive tree view of files and directories...\n",
            "  - move_file: Move or rename files and directories. Can move fil...\n",
            "  - search_files: Recursively search for files and directories match...\n",
            "  - get_file_info: Retrieve detailed metadata about a file or directo...\n",
            "  - list_allowed_directories: Returns the list of directories that this server i...\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import tempfile\n",
        "\n",
        "from bridgic.protocols.mcp import McpServerConnectionStdio\n",
        "\n",
        "# Create a temporary directory for the filesystem MCP server\n",
        "temp_dir = os.path.realpath(tempfile.mkdtemp())\n",
        "print(f\"Using temporary directory: {temp_dir}\")\n",
        "\n",
        "# Create a connection to a filesystem MCP server\n",
        "# Note: This requires Node.js and npx to be installed\n",
        "filesystem_connection = McpServerConnectionStdio(\n",
        "    name=\"connection-filesystem-stdio\",\n",
        "    command=\"npx\",\n",
        "    args=[\"-y\", \"@modelcontextprotocol/server-filesystem\", temp_dir],\n",
        ")\n",
        "\n",
        "# Establish the connection\n",
        "filesystem_connection.connect()\n",
        "\n",
        "# Verify connection\n",
        "print(f\"‚úì Connected to MCP server: {filesystem_connection.name}\")\n",
        "print(f\"  Connection status: {filesystem_connection.is_connected}\")\n",
        "\n",
        "# List available tools\n",
        "tools = filesystem_connection.list_tools()\n",
        "print(f\"\\n‚úì Found {len(tools)} available tools:\")\n",
        "for tool in tools:\n",
        "    print(f\"  - {tool.tool_name}: {tool.tool_description[:50]}...\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "You can also connect to an MCP server via streamable HTTP transport. Below is an example of how to connect to a remote Github MCP Server and view the tools it supports:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úì Connected to MCP server: connection-github-streamable-http\n",
            "  Connection status: True\n",
            "\n",
            "‚úì Found 40 available tools:\n",
            "  - add_comment_to_pending_review: Add review comment to the requester's latest pendi...\n",
            "  - add_issue_comment: Add a comment to a specific issue in a GitHub repo...\n",
            "  - assign_copilot_to_issue: Assign Copilot to a specific issue in a GitHub rep...\n",
            "  - create_branch: Create a new branch in a GitHub repository...\n",
            "  - create_or_update_file: Create or update a single file in a GitHub reposit...\n",
            "  - create_pull_request: Create a new pull request in a GitHub repository....\n",
            "  - create_repository: Create a new GitHub repository in your account or ...\n",
            "  - delete_file: Delete a file from a GitHub repository...\n",
            "  - fork_repository: Fork a GitHub repository to your account or specif...\n",
            "  - get_commit: Get details for a commit from a GitHub repository...\n",
            "  - get_file_contents: Get the contents of a file or directory from a Git...\n",
            "  - get_label: Get a specific label from a repository....\n",
            "  - get_latest_release: Get the latest release in a GitHub repository...\n",
            "  - get_me: Get details of the authenticated GitHub user. Use ...\n",
            "  - get_release_by_tag: Get a specific release by its tag name in a GitHub...\n",
            "  - get_tag: Get details about a specific git tag in a GitHub r...\n",
            "  - get_team_members: Get member usernames of a specific team in an orga...\n",
            "  - get_teams: Get details of the teams the user is a member of. ...\n",
            "  - issue_read: Get information about a specific issue in a GitHub...\n",
            "  - issue_write: Create a new or update an existing issue in a GitH...\n",
            "  - list_branches: List branches in a GitHub repository...\n",
            "  - list_commits: Get list of commits of a branch in a GitHub reposi...\n",
            "  - list_issue_types: List supported issue types for repository owner (o...\n",
            "  - list_issues: List issues in a GitHub repository. For pagination...\n",
            "  - list_pull_requests: List pull requests in a GitHub repository. If the ...\n",
            "  - list_releases: List releases in a GitHub repository...\n",
            "  - list_tags: List git tags in a GitHub repository...\n",
            "  - merge_pull_request: Merge a pull request in a GitHub repository....\n",
            "  - pull_request_read: Get information on a specific pull request in GitH...\n",
            "  - pull_request_review_write: Create and/or submit, delete review of a pull requ...\n",
            "  - push_files: Push multiple files to a GitHub repository in a si...\n",
            "  - request_copilot_review: Request a GitHub Copilot code review for a pull re...\n",
            "  - search_code: Fast and precise code search across ALL GitHub rep...\n",
            "  - search_issues: Search for issues in GitHub repositories using iss...\n",
            "  - search_pull_requests: Search for pull requests in GitHub repositories us...\n",
            "  - search_repositories: Find GitHub repositories by name, description, rea...\n",
            "  - search_users: Find GitHub users by username, real name, or other...\n",
            "  - sub_issue_write: Add a sub-issue to a parent issue in a GitHub repo...\n",
            "  - update_pull_request: Update an existing pull request in a GitHub reposi...\n",
            "  - update_pull_request_branch: Update the branch of a pull request with the lates...\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import dotenv\n",
        "\n",
        "from mcp.shared._httpx_utils import create_mcp_http_client\n",
        "from bridgic.protocols.mcp import McpServerConnectionStreamableHttp\n",
        "\n",
        "dotenv.load_dotenv()\n",
        "\n",
        "github_mcp_url = os.environ.get(\"GITHUB_MCP_HTTP_URL\", \"https://api.githubcopilot.com/mcp/\")\n",
        "github_token = os.environ.get(\"GITHUB_TOKEN\")\n",
        "\n",
        "http_client = create_mcp_http_client(\n",
        "    headers={\"Authorization\": f\"Bearer {github_token}\"},\n",
        ")\n",
        "\n",
        "github_connection = McpServerConnectionStreamableHttp(\n",
        "    name=\"connection-github-streamable-http\",\n",
        "    url=github_mcp_url,\n",
        "    http_client=http_client,\n",
        "    request_timeout=15,\n",
        ")\n",
        "\n",
        "github_connection.connect()\n",
        "\n",
        "# Verify connection\n",
        "print(f\"‚úì Connected to MCP server: {github_connection.name}\")\n",
        "print(f\"  Connection status: {github_connection.is_connected}\")\n",
        "\n",
        "# List available tools\n",
        "tools = github_connection.list_tools()\n",
        "print(f\"\\n‚úì Found {len(tools)} available tools:\")\n",
        "for tool in tools:\n",
        "    print(f\"  - {tool.tool_name}: {tool.tool_description[:50]}...\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Using a MCP Tool as a Worker\n",
        "\n",
        "MCP tools can be converted to Bridgic workers and integrated into `GraphAutoma` building. This allows you to orchestrate MCP tool calls alongside other workers in your application.\n",
        "\n",
        "Why don't we just run the tool directly, but instead execute it as a worker? In Bridgic's view, every execution process in a workflow program (or an even more agentic system) can be decomposed into fine-grained workers, which can then be orchestrated and scheduled. Standardizing the execution process in this way simplifies development and debugging, and enhances observability during execution. Currently, too many frameworks separate agent operation from programmable orchestration, causing tools and developer-defined work units to hold unequal positions. This leads to two very different development and debugging experiences.\n",
        "\n",
        "In Bridgic, tools have distinct specifications, but all tool executions are carried out by converting them into workers. This approach standardizes different kinds of tools as uniform and orchestratable units, facilitating their integration and scheduling alongside other workers to accomplish more complex tasks.\n",
        "\n",
        "Let's create a simple workflow that uses MCP tools to read and write files:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "‚úì Finnished writting!\n",
            "File path: /private/var/folders/9t/5r9fms9s5q33p6xty_0_k1mw0000gn/T/tmpeuov9ggi/1.txt.txt\n",
            "size: 51\n",
            "created: Sun Jan 25 2026 23:09:15 GMT+0800 (China Standard Time)\n",
            "modified: Sun Jan 25 2026 23:09:15 GMT+0800 (China Standard Time)\n",
            "accessed: Sun Jan 25 2026 23:09:15 GMT+0800 (China Standard Time)\n",
            "isDirectory: false\n",
            "isFile: true\n",
            "permissions: 644\n",
            "\n",
            "‚úì Finnished writting!\n",
            "File path: /private/var/folders/9t/5r9fms9s5q33p6xty_0_k1mw0000gn/T/tmpeuov9ggi/2.txt.txt\n",
            "size: 47\n",
            "created: Sun Jan 25 2026 23:09:15 GMT+0800 (China Standard Time)\n",
            "modified: Sun Jan 25 2026 23:09:15 GMT+0800 (China Standard Time)\n",
            "accessed: Sun Jan 25 2026 23:09:15 GMT+0800 (China Standard Time)\n",
            "isDirectory: false\n",
            "isFile: true\n",
            "permissions: 644\n"
          ]
        }
      ],
      "source": [
        "import datetime\n",
        "import mcp\n",
        "\n",
        "from bridgic.core.automa import GraphAutoma, RunningOptions, worker\n",
        "from bridgic.core.automa.args import System\n",
        "\n",
        "# List the tools via the server connection\n",
        "tools = filesystem_connection.list_tools()\n",
        "\n",
        "# Filter the needed one which will create the real worker\n",
        "write_tool = next(t for t in tools if t.tool_name == \"write_file\")\n",
        "read_tool = next(t for t in tools if t.tool_name == \"read_file\")\n",
        "meta_tool = next(t for t in tools if t.tool_name == \"get_file_info\")\n",
        "\n",
        "class FileWriter(GraphAutoma):\n",
        "    def __init__(self, name: str, running_options: RunningOptions = None):\n",
        "        super().__init__(name=name, running_options=running_options)\n",
        "        self.add_worker(\"write\", write_tool.create_worker())\n",
        "        self.add_worker(\"read\", read_tool.create_worker())\n",
        "        self.add_worker(\"meta\", meta_tool.create_worker())\n",
        "\n",
        "    @worker(is_start=True)\n",
        "    def start(self, title: str, content: str, rtx = System(\"runtime_context\")):\n",
        "        # Get the current time\n",
        "        now_time = datetime.datetime.now()\n",
        "\n",
        "        # Get the content and path of the file to be written\n",
        "        file_path = f\"{temp_dir}/{title}.txt\"\n",
        "        file_content = (\n",
        "            f\"Time: {now_time.strftime('%Y-%m-%d %H:%M:%S')}\\n\"\n",
        "            f\"Content: {content}\\n\"\n",
        "        )\n",
        "\n",
        "        # Write the file at the next step\n",
        "        self.ferry_to(\"write\", content=file_content, path=file_path)\n",
        "\n",
        "        return file_path\n",
        "\n",
        "    @worker(dependencies=[\"start\", \"write\"])\n",
        "    def after_write(self, file_path: str, write_info: mcp.types.CallToolResult):\n",
        "        self.ferry_to(\"read\", path=file_path)\n",
        "        self.ferry_to(\"meta\", path=file_path)\n",
        "\n",
        "    @worker(is_output=True, dependencies=[\"start\", \"read\", \"meta\"])\n",
        "    def output(self, file_path: str, read_info: mcp.types.CallToolResult, meta_info: mcp.types.CallToolResult) -> str:\n",
        "        return (\n",
        "            f\"‚úì Finnished writting!\"\n",
        "            f\"\\nFile path: {file_path}\"\n",
        "            f\"\\n{meta_info.content[0].text}\"\n",
        "        )\n",
        "\n",
        "file_processor = FileWriter(name=\"file-processor\")\n",
        "\n",
        "for content in [\n",
        "    (\"1.txt\", \"Hello, Bridgic!\"),\n",
        "    (\"2.txt\", \"Hello, MCP!\"),\n",
        "]:\n",
        "    result = await file_processor.arun(title=content[0], content=content[1])\n",
        "    print(f\"\\n{result}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Using MCP Tools in an agentic Automa\n",
        "\n",
        "In more scenarios, you may prefer to use an LLM-powered automa that can determine which MCP tools to utilize, adapting its choices to a specific goal and the evolving context during execution.\n",
        "\n",
        "`ReCentAutoma` is such an agentic automa. By passing MCP tools into it, you can:\n",
        "\n",
        "- Keep the orchestration logic in Bridgic while delegating decisions to the LLM.\n",
        "- Let the LLM select appropriate tools at each step\n",
        "- Collect the results of tool calls and incorporate them as part of a dynamic process\n",
        "\n",
        "Below is a minimal example that uses the weather MCP tools inside a `ReCentAutoma`.\n",
        "\n",
        "First of all, you have to connect to an MCP server that contains the weather tool."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "from bridgic.protocols.mcp import McpServerConnectionStdio\n",
        "\n",
        "weather_connection = McpServerConnectionStdio(\n",
        "    name=\"connection-weather-stdio\",\n",
        "    command=\"npx\",\n",
        "    args=[\"-y\", \"@mariox/weather-mcp-server\"],\n",
        ")\n",
        "weather_connection.connect()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Then you can initialize an agentic automa which utilizes the weather tool(s) to answer the weather questions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import dotenv\n",
        "\n",
        "from bridgic.llms.openai import OpenAILlm, OpenAIConfiguration\n",
        "\n",
        "dotenv.load_dotenv()\n",
        "\n",
        "# Prepare the LLM (set these env vars before running this cell)\n",
        "_api_key = os.environ.get(\"OPENAI_API_KEY\")\n",
        "_api_base = os.environ.get(\"OPENAI_API_BASE\", \"https://api.openai.com/v1\")\n",
        "_model_name = os.environ.get(\"OPENAI_MODEL_NAME\", \"gpt-4o-mini\")\n",
        "\n",
        "# Initialize LLM instance\n",
        "llm = OpenAILlm(\n",
        "    api_key=_api_key,\n",
        "    api_base=_api_base,\n",
        "    configuration=OpenAIConfiguration(model=_model_name),\n",
        "    timeout=180,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[38;5;46m\n",
            "[ReCentAutoma]-[ReCentAutoma-171bd997] is started.\u001b[0m\n",
            "\u001b[38;5;129m[ReCentAutoma]-[ReCentAutoma-171bd997] [__dynamic_step__] driving [initialize_task_goal]\u001b[0m\n",
            "\u001b[38;5;51m[ReCentAutoma]-[ReCentAutoma-171bd997] [__automa__] triggers [initialize_task_goal]\u001b[0m\n",
            "[ReCentAutoma]-[ReCentAutoma-171bd997] üéØ Task Goal\n",
            "Get the weather in Shanghai.\n",
            "\u001b[38;5;129m[ReCentAutoma]-[ReCentAutoma-171bd997] [__dynamic_step__] driving [observe]\u001b[0m\n",
            "\u001b[38;5;51m[ReCentAutoma]-[ReCentAutoma-171bd997] [initialize_task_goal] triggers [observe]\u001b[0m\n",
            "\u001b[38;5;244m[ReCentAutoma]-[ReCentAutoma-171bd997] üëÄ Observation\n",
            "    Iteration: 1\n",
            "    Achieved: False\n",
            "    Thinking: The task goal is to get the weather in Shanghai. However, the conversation history does not show that any information regarding the current weather in Shanghai has been provided or gathered so far. Therefore, there is a significant gap because the specific weather details are still missing. The goal has not been achieved yet.\u001b[0m\n",
            "\u001b[38;5;129m[ReCentAutoma]-[ReCentAutoma-171bd997] [__dynamic_step__] driving [select_tools, compress_memory]\u001b[0m\n",
            "\u001b[38;5;51m[ReCentAutoma]-[ReCentAutoma-171bd997] [observe] triggers [select_tools]\u001b[0m\n",
            "\u001b[38;5;51m[ReCentAutoma]-[ReCentAutoma-171bd997] [observe] triggers [compress_memory]\u001b[0m\n",
            "\u001b[38;5;100m[ReCentAutoma]-[ReCentAutoma-171bd997] üß≠ Memory Check\n",
            "    Compression Needed: False\u001b[0m\n",
            "\u001b[38;5;208m[ReCentAutoma]-[ReCentAutoma-171bd997] üîß Tool Selection\n",
            "    (No tools selected)\n",
            "\n",
            "    LLM Response: To achieve the task goal of getting the weather in Shanghai, the most appropriate tool to use is the `get_weather` function, specifying \"Shanghai\" as the location.\n",
            "\n",
            "I will proceed to execute this function to retrieve the current weather information for Shanghai. \n",
            "\n",
            "Here we go!\u001b[0m\n",
            "\u001b[38;5;129m[ReCentAutoma]-[ReCentAutoma-171bd997] [__dynamic_step__] driving [observe]\u001b[0m\n",
            "\u001b[38;5;51m[ReCentAutoma]-[ReCentAutoma-171bd997] [select_tools] triggers [observe]\u001b[0m\n",
            "\u001b[38;5;244m[ReCentAutoma]-[ReCentAutoma-171bd997] üëÄ Observation\n",
            "    Iteration: 2\n",
            "    Achieved: False\n",
            "    Thinking: The task goal is to obtain the weather information for Shanghai. As of now, there have been no updates or details regarding the weather in Shanghai provided. Consequently, there remains a critical gap as the necessary weather information has not been collected or presented. Therefore, the goal has not been achieved.\u001b[0m\n",
            "\u001b[38;5;129m[ReCentAutoma]-[ReCentAutoma-171bd997] [__dynamic_step__] driving [select_tools, compress_memory]\u001b[0m\n",
            "\u001b[38;5;51m[ReCentAutoma]-[ReCentAutoma-171bd997] [observe] triggers [select_tools]\u001b[0m\n",
            "\u001b[38;5;51m[ReCentAutoma]-[ReCentAutoma-171bd997] [observe] triggers [compress_memory]\u001b[0m\n",
            "\u001b[38;5;100m[ReCentAutoma]-[ReCentAutoma-171bd997] üß≠ Memory Check\n",
            "    Compression Needed: False\u001b[0m\n",
            "\u001b[38;5;208m[ReCentAutoma]-[ReCentAutoma-171bd997] üîß Tool Selection\n",
            "    Tool 1: get_weather\n",
            "      id: tool_bfecfd4db550446d8631f200e\n",
            "      arguments: {'location': '‰∏äÊµ∑'}\u001b[0m\n",
            "\u001b[38;5;129m[ReCentAutoma]-[ReCentAutoma-171bd997] [__dynamic_step__] driving [tool-<get_weather>-<tool_bfecfd4db550446d8631f200e>]\u001b[0m\n",
            "\u001b[38;5;51m[ReCentAutoma]-[ReCentAutoma-171bd997] [select_tools] triggers [tool-<get_weather>-<tool_bfecfd4db550446d8631f200e>]\u001b[0m\n",
            "\u001b[38;5;129m[ReCentAutoma]-[ReCentAutoma-171bd997] [__dynamic_step__] driving [collect_results-<07038e9e>]\u001b[0m\n",
            "\u001b[38;5;51m[ReCentAutoma]-[ReCentAutoma-171bd997] [tool-<get_weather>-<tool_bfecfd4db550446d8631f200e>] triggers [collect_results-<07038e9e>]\u001b[0m\n",
            "\u001b[38;5;46m[ReCentAutoma]-[ReCentAutoma-171bd997] üö© Tool Results\n",
            "    Tool 1: get_weather\n",
            "      id: tool_bfecfd4db550446d8631f200e\n",
            "      result: meta=None content=[TextContent(type='text', text='üå§Ô∏è **‰∏äÊµ∑ Â§©Ê∞î‰ø°ÊÅØ**\\n\\nüìç **‰ΩçÁΩÆ‰ø°ÊÅØ:**\\nüó∫Ô∏è Âú∞ÁÇπ: ‰∏äÊµ∑Â∏Ç, ‰∏≠ÂõΩ\\nüìä ÂùêÊ†á: 31.2304, 121.4737\\n\\nüå§Ô∏è **Â§©Ê∞î‰ø°ÊÅØ:**\\nüå°Ô∏è Ê∏©Â∫¶: 8.6¬∞C\\n‚òÅÔ∏è Â§©Ê∞î: Èò¥Â§©\\nüí® È£éÈÄü: 12.3 km/h\\nüß≠ È£éÂêë: 93¬∞\\nüíß ÊπøÂ∫¶: 77%\\nüìä Ê∞îÂéã: 1026.7 hPa\\nüïê Êó∂Èó¥: 2026-01-25T23:00\\n\\nüì° Êï∞ÊçÆÊ∫ê: Open-Meteo (ÂÖçË¥πAPI)', annotations=None, meta=None)] structuredContent=None isError=False\u001b[0m\n",
            "\u001b[38;5;129m[ReCentAutoma]-[ReCentAutoma-171bd997] [__dynamic_step__] driving [observe]\u001b[0m\n",
            "\u001b[38;5;51m[ReCentAutoma]-[ReCentAutoma-171bd997] [collect_results-<07038e9e>] triggers [observe]\u001b[0m\n",
            "\u001b[38;5;244m[ReCentAutoma]-[ReCentAutoma-171bd997] üëÄ Observation\n",
            "    Iteration: 3\n",
            "    Achieved: True\n",
            "    Thinking: The current goal was to get the weather information for Shanghai. The weather data has now been successfully retrieved, including temperature, weather condition, wind speed, humidity, and pressure. There are no remaining gaps as the goal has been fully achieved with all necessary details provided.\u001b[0m\n",
            "\u001b[38;5;129m[ReCentAutoma]-[ReCentAutoma-171bd997] [__dynamic_step__] driving [finalize_answer]\u001b[0m\n",
            "\u001b[38;5;51m[ReCentAutoma]-[ReCentAutoma-171bd997] [observe] triggers [finalize_answer]\u001b[0m\n",
            "\u001b[38;5;46m[ReCentAutoma]-[ReCentAutoma-171bd997] is finished.\u001b[0m\n",
            "### Current Weather in Shanghai\n",
            "\n",
            "- **Location:** Shanghai, China\n",
            "- **Coordinates:** 31.2304¬∞ N, 121.4737¬∞ E\n",
            "\n",
            "#### Weather Information:\n",
            "- **Temperature:** 8.6¬∞C\n",
            "- **Condition:** Overcast\n",
            "- **Wind Speed:** 12.3 km/h\n",
            "- **Wind Direction:** 93¬∞ (East)\n",
            "- **Humidity:** 77%\n",
            "- **Pressure:** 1026.7 hPa\n",
            "\n",
            "#### Data Source:\n",
            "- The information is sourced from Open-Meteo (free API).\n",
            "\n",
            "### Summary\n",
            "The current weather in Shanghai indicates an overcast day with a temperature of 8.6¬∞C, moderate winds from the east, and high humidity levels.\n"
          ]
        }
      ],
      "source": [
        "from bridgic.core.automa import RunningOptions\n",
        "from bridgic.core.agentic.recent import ReCentAutoma, StopCondition\n",
        "\n",
        "# Pass weather tools in directly to build an agentic automa as a weather agent\n",
        "weather_agent = ReCentAutoma(\n",
        "    llm=llm,\n",
        "    tools=weather_connection.list_tools(),\n",
        "    stop_condition=StopCondition(max_iteration=5),\n",
        "    running_options=RunningOptions(debug=True),\n",
        ")\n",
        "\n",
        "# Ask the weather agent for the weather in Shanghai\n",
        "result = await weather_agent.arun(goal=\"Get the weather in Shanghai.\")\n",
        "print(result)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Using MCP Prompts to render your context\n",
        "\n",
        "MCP servers can also provide prompt templates that can be used to render context for your LLM applications. These prompts are useful for standardizing how you format messages before sending them to an LLM.\n",
        "\n",
        "You can check the available prompt templates from the server by running:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "name: AssignCodingAgent:\n",
            "description: Assign GitHub Coding Agent to multiple tasks in a GitHub repository.\n",
            "parameters: [\n",
            "  \"[required=True] repo: The repository to assign tasks in (owner/repo).\"\n",
            "]\n",
            "\n",
            "name: issue_to_fix_workflow:\n",
            "description: Create an issue for a problem and then generate a pull request to fix it\n",
            "parameters: [\n",
            "  \"[required=True] owner: Repository owner\",\n",
            "  \"[required=True] repo: Repository name\",\n",
            "  \"[required=True] title: Issue title\",\n",
            "  \"[required=True] description: Issue description\",\n",
            "  \"[required=None] labels: Comma-separated list of labels to apply (optional)\",\n",
            "  \"[required=None] assignees: Comma-separated list of assignees (optional)\"\n",
            "]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "\n",
        "from bridgic.protocols.mcp import McpPromptTemplate\n",
        "\n",
        "prompts: list[McpPromptTemplate] = github_connection.list_prompts()\n",
        "\n",
        "for prompt in prompts:\n",
        "    description = prompt.prompt_info.description\n",
        "    arguments = [f\"[required={arg.required}] {arg.name}: {arg.description}\" for arg in prompt.prompt_info.arguments]\n",
        "    print(\n",
        "        f\"name: {prompt.prompt_name}:\\n\"\n",
        "        f\"description: {description}\\n\"\n",
        "        f\"parameters: {json.dumps(arguments, indent=2)}\\n\"\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We now know that there is a prompt template named \"issue_to_fix_workflow\" available. This template is designed to generate instructions for LLM to help it to use tools to create an issue and a pull request on GitHub. It requires the following parameters: `owner`, `repo`, `title`, and `description`.\n",
        "\n",
        "Let's fill in these arguments to render the prompt:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Message(role=<Role.USER: 'user'>, blocks=[TextBlock(block_type='text', text='You are a development workflow assistant helping to create GitHub issues and generate corresponding pull requests to fix them. You should: 1) Create a well-structured issue with clear problem description, 2) Assign it to Copilot coding agent to generate a solution, and 3) Monitor the PR creation process.')], extras={}), Message(role=<Role.USER: 'user'>, blocks=[TextBlock(block_type='text', text=\"I need to create an issue titled 'A New bug' in somebody/awesome-project and then have a PR generated to fix it. The issue description is: The bug is really annoying and it have to be fixed.\")], extras={}), Message(role=<Role.AI: 'assistant'>, blocks=[TextBlock(block_type='text', text=\"I'll help you create the issue 'A New bug' in somebody/awesome-project and then coordinate with Copilot to generate a fix. Let me start by creating the issue with the provided details.\")], extras={}), Message(role=<Role.USER: 'user'>, blocks=[TextBlock(block_type='text', text='Perfect! Please:\\n1. Create the issue with the title, description, labels, and assignees\\n2. Once created, assign it to Copilot coding agent to generate a solution\\n3. Monitor the process and let me know when the PR is ready for review')], extras={}), Message(role=<Role.AI: 'assistant'>, blocks=[TextBlock(block_type='text', text=\"Excellent plan! Here's what I'll do:\\n\\n1. ‚úÖ Create the issue with all specified details\\n2. ü§ñ Assign to Copilot coding agent for automated fix\\n3. üìã Monitor progress and notify when PR is created\\n4. üîç Provide PR details for your review\\n\\nLet me start by creating the issue.\")], extras={})]\n"
          ]
        }
      ],
      "source": [
        "fix_issue_template = next(p for p in prompts if p.prompt_name == \"issue_to_fix_workflow\")\n",
        "\n",
        "messages = fix_issue_template.format_messages(\n",
        "    owner=\"somebody\",\n",
        "    repo=\"awesome-project\",\n",
        "    title=\"A New bug\",\n",
        "    description=\"The bug is really annoying and it have to be fixed.\",\n",
        ")\n",
        "print(messages)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The rendered messages can now be used with your LLM. You can pass them directly to the LLM's chat method or tool-selection method, or even use them as part of a larger conversation context.\n",
        "\n",
        "The following example shows "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ToolCall(id='tool_7d890ee2619842b0add42624e', name='issue_write', arguments={'title': 'A New bug', 'body': 'The bug is really annoying and it have to be fixed.', 'method': 'create', 'owner': 'somebody', 'repo': 'awesome-project'})]\n"
          ]
        }
      ],
      "source": [
        "# Convert MCP tool specifications to standard tool objects\n",
        "model_tools = [tool.to_tool() for tool in github_connection.list_tools()]\n",
        "\n",
        "# Use the rendered messages to help select tool(s)\n",
        "tool_calls, _ = await llm.aselect_tool(\n",
        "    messages=messages,\n",
        "    tools=model_tools,\n",
        ")\n",
        "print(tool_calls)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Advanced Usage\n",
        "\n",
        "### Multiple Server Connection Management\n",
        "\n",
        "When building complex applications, you may need to connect to multiple MCP servers simultaneously. Bridgic provides `McpServerConnectionManager` to help you manage multiple connections efficiently.\n",
        "\n",
        "A connection manager:\n",
        "\n",
        "- Shares a common event loop across for the connections registered in it\n",
        "- Handles the lifecycle of connections within the same event loop\n",
        "- Allows to retrieve any connection by its name across your application\n",
        "\n",
        "When you call the `connect()` method on a connection, it is actually automatically registered with the default manager. All operations on an MCP server connection, such as `list_tools()`, `list_prompt()` or their asynchronous peers, are internally managed by the connection manager.\n",
        "\n",
        "If you want more control, you can explicitly choose which manager to register your connection(s) with by calling the `register_connection()`. This is particularly useful when it's necessary to isolate connections and their operations of certain MCP servers that contain some time-consuming tools. The execution isolation is at the thread level.\n",
        "\n",
        "For example, browser and terminal usage are relatively time-consuming, so it's necessary to use a separate manager for connection management to prevent their execution from blocking the use of other MCP tools.\n",
        "\n",
        "The following example demonstrates:\n",
        " \n",
        "1. Connecting to both a Cli MCP server and a Playwright MCP server simultaneously.\n",
        "2. Assigning each connection to a separate manager to keep their operations isolated."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cli MCP server connected: True\n",
            "Playwright MCP server connected: True\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import tempfile\n",
        "\n",
        "from bridgic.protocols.mcp import (\n",
        "    McpServerConnectionStdio,\n",
        "    McpServerConnectionManager,\n",
        ")\n",
        "\n",
        "temp_dir = os.path.realpath(tempfile.mkdtemp())\n",
        "\n",
        "# Create a file with written content\n",
        "with open(os.path.join(temp_dir, \"dream.txt\"), \"w\", encoding=\"utf-8\") as f:\n",
        "    f.write(\"Bridging Logic and Magic\")\n",
        "\n",
        "cli_connection = McpServerConnectionStdio(\n",
        "    name=\"connection-cli-stdio\",\n",
        "    command=\"uvx\",\n",
        "    args=[\"cli-mcp-server\"],\n",
        "    env={\n",
        "        \"ALLOWED_DIR\": temp_dir,\n",
        "        \"ALLOWED_COMMANDS\": \"ls,cat,wc,pwd,echo\",\n",
        "        \"ALLOWED_FLAGS\": \"all\",\n",
        "        \"ALLOW_SHELL_OPERATORS\": \"true\",\n",
        "    },\n",
        ")\n",
        "\n",
        "playwright_connection = McpServerConnectionStdio(\n",
        "    name=\"connection-playwright-stdio\",\n",
        "    command=\"npx\",\n",
        "    args=[\n",
        "        \"@playwright/mcp@latest\",\n",
        "    ],\n",
        "    request_timeout=60,\n",
        ")\n",
        "\n",
        "# Register the two connection in different connection manager\n",
        "# In this way, their operations will never block each others\n",
        "McpServerConnectionManager.get_instance(\"terminal-use\").register_connection(cli_connection)\n",
        "McpServerConnectionManager.get_instance(\"browser-use\").register_connection(playwright_connection)\n",
        "\n",
        "# Note: registration have be done before calling `connect()` method\n",
        "cli_connection.connect()\n",
        "playwright_connection.connect()\n",
        "\n",
        "# Retrieve connections by their names\n",
        "print(\"Cli MCP server connected:\", McpServerConnectionManager.get_connection(\"connection-cli-stdio\").is_connected)\n",
        "print(\"Playwright MCP server connected:\", McpServerConnectionManager.get_connection(\"connection-playwright-stdio\").is_connected)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Pay attention to the Connection Lifecycle\n",
        "\n",
        "The lifecycle of an MCP server connection is **independent** from the execution of an automa: neither `interact_with_human()` (which pauses and raises `InteractionException`) nor `arun()` / `arun(feedback_data=...)` (which runs or resumes the automa) affects the connection. Once a connection is established and managed by a connection manager, it remains open until you close it.\n",
        "\n",
        "A practical implication is that **one connection can serve many executions**, which is important for the development of application. The automa may pause at `interact_with_human()` and be resumed later with `arun(feedback_data=...)`; each cycle can use MCP tools over the **same** connection without reconnecting.\n",
        "\n",
        "The following example demonstrates a simple CLI loop: in each turn, the automa requests a human command (interrupt), the application provides the command as feedback (resume), the automa executes the CLI MCP tool, and then requests for the next command‚Äîrepeating this process to simulate user's multi-turn input. Across all these turns, the connection to the CLI MCP server is created only once (in the previous cell) and reused each time.\n",
        "\n",
        "Please note that this example specifically simulates multi-turn human‚Äìcomputer interactions by mimicking user command input; in real-world development, developers are free to customize their own human-in-the-loop interaction flow as needed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[38;5;244mWelcome to the example CLI Automa.\u001b[0m\n",
            "> pwd\n",
            "\u001b[38;5;244m/private/var/folders/9t/5r9fms9s5q33p6xty_0_k1mw0000gn/T/tmpr7ghhwn0\n",
            "\u001b[0m\n",
            "> ls -l\n",
            "\u001b[38;5;244mtotal 8\n",
            "-rw-r--r--  1 xushili  staff  24 Jan 25 23:09 dream.txt\n",
            "\u001b[0m\n",
            "> wc -l dream.txt\n",
            "\u001b[38;5;244m0 dream.txt\n",
            "\u001b[0m\n",
            "> cat dream.txt\n",
            "\u001b[38;5;244mBridging Logic and Magic\n",
            "\u001b[0m\n",
            "> exit\n",
            "\u001b[38;5;244mSee you again.\n",
            "\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "import uuid\n",
        "\n",
        "from bridgic.core.automa import GraphAutoma, worker, RunningOptions\n",
        "from bridgic.core.automa.interaction import Event, InteractionFeedback, InteractionException\n",
        "from bridgic.core.utils._console import printer\n",
        "\n",
        "# One MCP connection across multiple interrupt‚Äìresume cycles. Specifically, \n",
        "# calling interact_with_human() pauses the automa and alling arun(feedback_data=...) \n",
        "# resumes it. The same connection (established in the previous cell) is reused\n",
        "# on every turn to run the CLI tool.\n",
        "\n",
        "# Define an Automa which supports human-interaction\n",
        "class CliAutoma(GraphAutoma):\n",
        "    @worker(is_start=True)\n",
        "    def start(self):\n",
        "        printer.print(f\"Welcome to the example CLI Automa.\", color=\"gray\")\n",
        "        self.ferry_to(\"human_input\")\n",
        "\n",
        "    @worker()\n",
        "    def human_input(self):\n",
        "        # Interrupt‚Äìresume:\n",
        "        # - on first run this pauses (raising InteractionException);\n",
        "        # - on resume we receive feedback (the human command) and continue.\n",
        "        event = Event(event_type=\"get_human_command\")\n",
        "        feedback: InteractionFeedback = self.interact_with_human(event)\n",
        "        human_command = feedback.data\n",
        "\n",
        "        printer.print(f\"> {human_command}\")\n",
        "\n",
        "        if human_command in [\"quit\", \"exit\"]:\n",
        "            self.ferry_to(\"end\")\n",
        "        else:\n",
        "            tool_key = f\"tool-<{uuid.uuid4().hex[:8]}>\"\n",
        "            collect_key = f\"collect-<{uuid.uuid4().hex[:8]}>\"\n",
        "\n",
        "            async def _collect_command_result(command_result: mcp.types.CallToolResult):\n",
        "                printer.print(f\"{command_result.content[0].text.strip()}\\n\", color=\"gray\")\n",
        "                self.ferry_to(\"human_input\")\n",
        "\n",
        "            # Reuse the same connection across all interrupt‚Äìresume cycles.\n",
        "            # It was established once (previous cell) and stays open.\n",
        "            # Each turn we fetch it here and it outlives cycle of running.\n",
        "            real_connection = McpServerConnectionManager.get_connection(\"connection-cli-stdio\")\n",
        "\n",
        "            # Filter the \"run_command\" tool spec from cli-mcp-server.\n",
        "            command_tool = next(t for t in real_connection.list_tools() if t.tool_name == \"run_command\")\n",
        "\n",
        "            # Use the tool specification to create worker instance and then add it dynamically.\n",
        "            self.add_worker(tool_key, command_tool.create_worker())\n",
        "            self.add_func_as_worker(collect_key, _collect_command_result, dependencies=[tool_key])\n",
        "            self.ferry_to(tool_key, command=human_command)\n",
        "\n",
        "    @worker(is_output=True)\n",
        "    def end(self):\n",
        "        printer.print(f\"See you again.\\n\", color=\"gray\")\n",
        "\n",
        "hi_automa = CliAutoma(name=\"human-interaction-automa\", running_options=RunningOptions(debug=False))\n",
        "\n",
        "interaction_id = None\n",
        "interaction_feedback = None\n",
        "\n",
        "async def continue_automa(feedback_data = None) -> str:\n",
        "    try:\n",
        "        await hi_automa.arun(feedback_data=feedback_data)\n",
        "    except InteractionException as e:\n",
        "        interaction_id = e.interactions[0].interaction_id\n",
        "        return interaction_id\n",
        "\n",
        "# First run: automa reaches human_input, calls interact_with_human, pauses (InteractionException).\n",
        "# We obtain interaction_id for the next resume.\n",
        "interaction_id = await continue_automa()\n",
        "\n",
        "# Each iteration we send the human command as feedback to resume the execution.\n",
        "commands = [\n",
        "    \"pwd\",\n",
        "    \"ls -l\",\n",
        "    \"wc -l dream.txt\",\n",
        "    \"cat dream.txt\",\n",
        "    \"exit\",\n",
        "]\n",
        "for command in commands:\n",
        "    interaction_feedback = InteractionFeedback(\n",
        "        interaction_id=interaction_id,\n",
        "        data=command\n",
        "    )\n",
        "    interaction_id = await continue_automa(interaction_feedback)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Before shutting down your application, make sure to properly close all your connections. Finally, let's close all the connections we've created to conclude this tutorial:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "All connections closed: True\n"
          ]
        }
      ],
      "source": [
        "filesystem_connection.close()\n",
        "weather_connection.close()\n",
        "github_connection.close()\n",
        "cli_connection.close()\n",
        "playwright_connection.close()\n",
        "\n",
        "all_closed = all([\n",
        "    not filesystem_connection.is_connected,\n",
        "    not weather_connection.is_connected,\n",
        "    not github_connection.is_connected,\n",
        "    not cli_connection.is_connected,\n",
        "    not playwright_connection.is_connected,\n",
        "])\n",
        "print(\"All connections closed:\", all_closed)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "env_bridgic",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.19"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
