{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "25379a8b",
   "metadata": {},
   "source": [
    "# Human-in-the-loop\n",
    "\n",
    "When building workflows or agents with Bridgic, developers can seamlessly integrate human-in-the-loop interactions into the execution flow. At any point, the system can pause its automated process to request human input â€” such as approval, verification, or additional instructions â€” and wait for a response. Once the human feedback is provided, the workflow or agent resumes execution from the point of interruption, adapting its behavior based on the new input. Bridgic ensures that the entire process, including pause and resume states, can be reliably serialized and deserialized for persistence and recovery.\n",
    "\n",
    "## Interaction Scenarios\n",
    "\n",
    "Let's go through a few simple examples to understand this process. Before that, let's set up the running environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e32e34c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Set the API base and key.\n",
    "_api_key = os.environ.get(\"OPENAI_API_KEY\")\n",
    "_api_base = os.environ.get(\"OPENAI_API_BASE\")\n",
    "_model_name = os.environ.get(\"OPENAI_MODEL_NAME\")\n",
    "\n",
    "from pydantic import BaseModel, Field\n",
    "from bridgic.core.automa import GraphAutoma, worker, Snapshot\n",
    "from bridgic.core.automa.args import From\n",
    "from bridgic.core.automa.interaction import Event, Feedback, FeedbackSender, InteractionFeedback, InteractionException\n",
    "from bridgic.core.model.types import Message, Role\n",
    "from bridgic.core.model.protocols import PydanticModel\n",
    "from bridgic.llms.openai import OpenAILlm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44f15da9",
   "metadata": {},
   "source": [
    "### Programming assistant\n",
    "\n",
    "During the development of a programming assistant, it can be designed to automatically execute and verify the code it generates. However, since program execution consumes system resources, the user must decide whether to grant permission for the assistant to run the code.\n",
    "\n",
    "Let's achieve it with Bridgic. The steps are as follows:\n",
    "\n",
    "1. Generate code based on user requirements.\n",
    "2. Ask the user if it is allowed to execute the generated code.\n",
    "3. Output result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99108f38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the LLM\n",
    "llm = OpenAILlm(api_base=_api_base, api_key=_api_key, timeout=10)\n",
    "\n",
    "class CodeBlock(BaseModel):\n",
    "    code: str = Field(description=\"The code to be executed.\")\n",
    "\n",
    "class CodeAssistant(GraphAutoma):\n",
    "    @worker(is_start=True)\n",
    "    async def generate_code(self, user_requirement: str):\n",
    "        response = await llm.astructured_output(\n",
    "            model=_model_name,\n",
    "            messages=[\n",
    "                Message.from_text(text=f\"You are a programming assistant. Please generate code according to the user's requirements.\", role=Role.SYSTEM),\n",
    "                Message.from_text(text=user_requirement, role=Role.USER),\n",
    "            ],\n",
    "            constraint=PydanticModel(model=CodeBlock)\n",
    "        )\n",
    "        return response.code\n",
    "\n",
    "    @worker(dependencies=[\"generate_code\"])\n",
    "    async def ask_to_run_code(self, code: str):\n",
    "        event = Event(event_type=\"can_run_code\", data=code)\n",
    "        feedback = await self.request_feedback_async(event)\n",
    "        return feedback.data\n",
    "        \n",
    "    @worker(dependencies=[\"ask_to_run_code\"])\n",
    "    async def output_result(self, feedback: str, code: str = From(\"generate_code\")):\n",
    "        code = code.strip(\"```python\").strip(\"```\")\n",
    "        if feedback == \"yes\":\n",
    "            print(f\"- - - - - - Result - - - - - -\")\n",
    "            exec(code)\n",
    "            print(f\"- - - - - - End - - - - - -\")\n",
    "        else:\n",
    "            print(f\"This code was rejected for execution. In response to the requirements, I have generated the following code:\\n {code}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5136fc6c",
   "metadata": {},
   "source": [
    "In the `ask_to_run_code()` method of `CodeAssistant`, we use [`request_feedback_async()`](../../../../reference/bridgic-core/bridgic/core/automa/#bridgic.core.automa.Automa.request_feedback_async) to send an Event to the human user and expect to receive a feedback. To handle this Event, the corresponding logic needs to be registered with the automa, like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "23256d8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "\n",
    "# Handle can_run_code event\n",
    "def can_run_code_handler(event: Event, feedback_sender: FeedbackSender):\n",
    "    print(f\"Can I run this code now to verify if it's correct?\")\n",
    "    print(event.data)\n",
    "    res = input(\"Please input your answer (yes/no): \")\n",
    "    print(f\"\\nPlease input your answer (yes/no): {res}\\n\")  # print the input\n",
    "    if res in [\"yes\", \"no\"]:\n",
    "        feedback_sender.send(Feedback(data=res))\n",
    "    else:\n",
    "        print(\"Invalid input. Please input yes or no.\")\n",
    "        feedback_sender.send(Feedback(data=\"no\"))\n",
    "\n",
    "# register can_run_code event handler to `CodeAssistant` automa\n",
    "code_assistant = CodeAssistant()\n",
    "code_assistant.register_event_handler(\"can_run_code\", can_run_code_handler)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be93ace5",
   "metadata": {},
   "source": [
    "Now let's use it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3f87eada",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Can I run this code now to verify if it's correct?\n",
      "```python\n",
      "def print_hello_world():\n",
      "    print('Hello, World!')\n",
      "\n",
      "# Call the function to print 'Hello, World!'\n",
      "print_hello_world()\n",
      "```\n",
      "\n",
      "Please input your answer (yes/no): yes\n",
      "\n",
      "- - - - - - Result - - - - - -\n",
      "Hello, World!\n",
      "- - - - - - End - - - - - -\n"
     ]
    }
   ],
   "source": [
    "await code_assistant.arun(user_requirement=\"Please write a function to print 'Hello, World!'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5674e729",
   "metadata": {},
   "source": [
    "In the above example, Bridgic wrap the message sent to the human user in an [`Event`](../../../../reference/bridgic-core/bridgic/core/automa/interaction/#bridgic.core.automa.interaction.Event) and he message received from the user in a [`FeedBack`](../../../../reference/bridgic-core/bridgic/core/automa/interaction/#bridgic.core.automa.interaction.Feedback). \n",
    "\n",
    "- `Event` contains three fields:\n",
    "    - `event_type`: A string. The event type is used to identify the registered event handler.\n",
    "    - `timestamp`: A Python datetime object. The timestamp of the event. The default is `datetime.now()`.\n",
    "    - `data`: The data attached to the event.\n",
    "- `FeedBack` contains one field:\n",
    "    - `data`: The data attached to the feedback.\n",
    "\n",
    "`request_feedback_async()` send an event to the user and request for a feedback. This method call will block the caller until the feedback is received. However, thanks to Pythonâ€™s asynchronous event loop mechanism, other automas running on the same main thread will not be blocked. \n",
    "\n",
    "The registered event handler must be defined as type of [`EventHandlerType`](../../../../reference/bridgic-core/bridgic/core/automa/interaction/#bridgic.core.automa.interaction.EventHandlerType).  Here it should be a function that takes an [`Event`](../../../../reference/bridgic-core/bridgic/core/automa/interaction/#bridgic.core.automa.interaction.Event) and a [`FeedbackSender`](../../../../reference/bridgic-core/bridgic/core/automa/interaction/#bridgic.core.automa.interaction.FeedbackSender) as arguments.\n",
    "\n",
    "### Counting notifier\n",
    "\n",
    "Sometimes, it may be necessary to post an event without expecting any feedback, for example, message notifications or progress updates. At this point, we call the [`post_event()`](../../../../reference/bridgic-core/bridgic/core/automa/#bridgic.core.automa.Automa.post_event) method and register a event handler of type [`EventHandlerType`](../../../../reference/bridgic-core/bridgic/core/automa/interaction/#bridgic.core.automa.interaction.EventHandlerType) to process the event. Here the event handler should be a function that takes only an `Event` as an argument\n",
    "\n",
    "For example, a counting notifier is implemented to count from 1 up to the number specified by the `user_input` argument. The user can also specify which number (`notify_int`) should trigger a reminder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58ba2ee0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loop from 1 to 10\n",
      "!! Now count to Loop 5 times. !!\n"
     ]
    }
   ],
   "source": [
    "class MessageNotifier(GraphAutoma):\n",
    "    @worker(is_start=True)\n",
    "    async def notify(self, user_input: int, notify_int: int):\n",
    "        print(f\"Loop from 1 to {user_input}\")\n",
    "        for i in range(1, user_input + 1):\n",
    "            if i == notify_int:\n",
    "                event = Event(event_type=\"message_notification\", data=f\"Loop {i} times\")\n",
    "                self.post_event(event)\n",
    "\n",
    "def message_notification_handler(event: Event):\n",
    "    print(f'!! Now count to {event.data}. !!')\n",
    "\n",
    "message_notifier = MessageNotifier()\n",
    "message_notifier.register_event_handler(\"message_notification\", message_notification_handler)\n",
    "await message_notifier.arun(user_input=10, notify_int=5)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed7fb65d",
   "metadata": {},
   "source": [
    "### Message assistant\n",
    "\n",
    "In certain scenarios, it may be necessary to wait for feedback after triggering an event. However, since the feedback could take a long time to arrive, keeping the system in a waiting state would result in unnecessary resource consumption.\n",
    "\n",
    "Bridgic provides a powerful [`interact_with_human`](../../../../reference/bridgic-core/bridgic/core/automa/#bridgic.core.automa.Automa.interact_with_human) mechanism for interruption recovery in this situation. This allows the program to pause and save its current execution state when such events occur, wait for feedback, and then resume execution.\n",
    "\n",
    "Let's implement a message assistant that receives a message from user \"A\" and replies to it, but doesn't know how long it might have to wait for the reply."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f969e43c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MessageAssistant(GraphAutoma):\n",
    "    @worker(is_start=True)\n",
    "    async def receive_message(self, message: str):\n",
    "        print(f'- - - - - - Received message - - - - - -')\n",
    "        print(message)\n",
    "        print(f'- - - - - - End - - - - - -\\n')\n",
    "        return message\n",
    "    \n",
    "    @worker(dependencies=[\"receive_message\"])\n",
    "    async def reply_message_and_wait_reply(self, message: str):\n",
    "        print(f'- - - - - - Reply Message - - - - - -')\n",
    "        response = await llm.achat(\n",
    "            model=_model_name,\n",
    "            messages=[\n",
    "                Message.from_text(text=f\"You are a message assistant. Please reply to the following message.\", role=Role.SYSTEM),\n",
    "                Message.from_text(text=message, role=Role.USER),\n",
    "            ]\n",
    "        )\n",
    "        print(response.message.content)\n",
    "        print(f'- - - - - - End - - - - - -\\n')\n",
    "\n",
    "        if \"Bye!\" in message:  # if the message contains \"Bye!\", the reply is complete\n",
    "            return \n",
    "\n",
    "        # wait for reply            \n",
    "        event = Event(event_type=\"wait_for_reply\")\n",
    "        feedback: InteractionFeedback = self.interact_with_human(event)  # interrupt to wait for reply and resume when feedback is received\n",
    "        self.ferry_to(\"receive_message\", feedback.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4365edb",
   "metadata": {},
   "source": [
    "Calling [`interact_with_human()`](../../../../reference/bridgic-core/bridgic/core/automa/#bridgic.core.automa.Automa.interact_with_human) posts an event that might take an unpredictable amount of time to finish. Consequently, an [`InteractionException`](../../../../reference/bridgic-core/bridgic/core/automa/interaction/#bridgic.core.automa.interaction.InteractionException) is thrown by the [`arun`](../../../../reference/bridgic-core/bridgic/core/automa/#bridgic.core.automa.GraphAutoma.arun) method of the automa.\n",
    "\n",
    "The `InteractionException` contains two fields:\n",
    "\n",
    "- `interactions`: A list of [`Interaction`](../../../../reference/bridgic-core/bridgic/core/automa/interaction/#bridgic.core.automa.interaction.Interaction)s, each `Interaction` containing an `interaction_id` and an `event`.\n",
    "- `snapshot`: a snapshot of the Automa's current state, of type [`Snapshot`](../../../../reference/bridgic-core/bridgic/core/automa/#bridgic.core.automa.Snapshot).\n",
    "\n",
    "The snapshot corresponding to the interaction can be persisted in the database, and the execution can be resumed when needed in the future."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "301c3e55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- - - - - - Received message - - - - - -\n",
      "Hello, how are you?\n",
      "- - - - - - End - - - - - -\n",
      "\n",
      "- - - - - - Reply Message - - - - - -\n",
      "Hello! I'm functioning well, thank you for asking. I'm always excited to chat and help out! ðŸ˜Š How can I assist you today?\n",
      "- - - - - - End - - - - - -\n",
      "\n",
      "! ! ! State has been saved and can be resumed later. ! ! !\n"
     ]
    }
   ],
   "source": [
    "import tempfile\n",
    "import os\n",
    "\n",
    "# Use a temporary directory to achieve the purpose of persistent storage.\n",
    "temp_dir = tempfile.TemporaryDirectory()\n",
    "\n",
    "# Use a dictionary to record the interaction id and the corresponding snapshot path.\n",
    "cache_dict = {}\n",
    "\n",
    "# deal with delayed interaction\n",
    "message_assistant = MessageAssistant()\n",
    "try:\n",
    "    await message_assistant.arun(message=\"Hello, how are you?\")\n",
    "except InteractionException as e:\n",
    "    interaction_id = e.interactions[0].interaction_id\n",
    "    bytes_file = os.path.join(temp_dir.name, f\"message_assistant_{interaction_id}.bytes\")\n",
    "    version_file = os.path.join(temp_dir.name, f\"message_assistant_{interaction_id}.version\")\n",
    "    with open(bytes_file, \"wb\") as f:\n",
    "        f.write(e.snapshot.serialized_bytes)\n",
    "    with open(version_file, \"w\") as f:\n",
    "        f.write(e.snapshot.serialization_version)\n",
    "\n",
    "    cache_dict[\"A\"] = {\n",
    "        \"interaction_id\": interaction_id,\n",
    "        \"bytes_file\": bytes_file,\n",
    "        \"version_file\": version_file\n",
    "    }\n",
    "    print(f\"! ! ! State has been saved and can be resumed later. ! ! !\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "589db18c",
   "metadata": {},
   "source": [
    "Suppose after quite a long time, there was finally a reply."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf642f11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- - - - - - Reply Message - - - - - -\n",
      "Hello! I'm functioning well, thank you for asking. I'm always excited to chat and help out! ðŸ˜Š How can I assist you today?\n",
      "- - - - - - End - - - - - -\n",
      "\n",
      "- - - - - - Received message - - - - - -\n",
      "I really enjoy taking with you. Bye!\n",
      "- - - - - - End - - - - - -\n",
      "\n",
      "- - - - - - Reply Message - - - - - -\n",
      "Thank you for enjoying our conversation! I'm glad I could help. Have a wonderful day, and take care! ðŸ˜Šâœ¨\n",
      "- - - - - - End - - - - - -\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# build feedback\n",
    "user = \"A\"\n",
    "reply_message = \"I really enjoy talking with you. Bye!\"\n",
    "interaction_id = cache_dict[user][\"interaction_id\"]\n",
    "feedback = InteractionFeedback(\n",
    "    interaction_id=interaction_id,\n",
    "    data=reply_message\n",
    ")\n",
    "\n",
    "# load snapshot\n",
    "bytes_file = cache_dict[user][\"bytes_file\"]\n",
    "version_file = cache_dict[user][\"version_file\"]\n",
    "with open(bytes_file, \"rb\") as f:\n",
    "    serialized_bytes = f.read()\n",
    "with open(version_file, \"r\") as f:\n",
    "    serialization_version = f.read()\n",
    "snapshot = Snapshot(\n",
    "    serialized_bytes=serialized_bytes, \n",
    "    serialization_version=serialization_version\n",
    ")\n",
    "\n",
    "# automa resumes from the snapshot\n",
    "message_assistant = MessageAssistant.load_from_snapshot(snapshot)\n",
    "await message_assistant.arun(interaction_feedback=feedback)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4716b28",
   "metadata": {},
   "source": [
    "When facing a situation that requires feedback but the waiting time is uncertain, this mechanism saves the current state and re-enters when the right moment comes in the future. This not only enables the system to release resources that would otherwise be occupied for a long time, but also allows it to be awakened at an appropriate time.\n",
    "\n",
    "## What have we done\n",
    "\n",
    "No matter which form of human-in-the-loop it is, Bridgic provides flexible support.\n",
    "\n",
    "- [`request_feedback_async`](../../../../reference/bridgic-core/bridgic/core/automa/#bridgic.core.automa.Automa.request_feedback_async): Used when the event must return feedback before the program can proceed. The program remains blocked until feedback is received.\n",
    "- [`post_event`](../../../../reference/bridgic-core/bridgic/core/automa/#bridgic.core.automa.Automa.post_event): Used when you just want to notify or trigger an event without expecting any feedback. The main program never blocks.\n",
    "- [`interact_with_human`](../../../../reference/bridgic-core/bridgic/core/automa/#bridgic.core.automa.Automa.interact_with_human): Used when feedback is required but may arrive much later. The program is suspended and persisted, and resumes immediately when feedback becomes available."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
