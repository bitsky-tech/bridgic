{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "12eeea60",
   "metadata": {},
   "source": [
    "# Dynamic DAG\n",
    "\n",
    "Bridgic can dynamic revise arrange and execute logic at runtime. Now let's understand them with a sample example.\n",
    "\n",
    "<br>\n",
    "<div style=\"text-align: center;\">\n",
    "<img src=\"../imgs/dynamic_dag.png\" alt=\"Parameter Passing\" width=\"400\" height=\"250\">\n",
    "</div>\n",
    "<br>\n",
    "\n",
    "Take the worker dynamically added at runtime as an example. In the context of article writing, the common practice is to split multiple sub-topics based on user input and then generate text around each topic. During this process, we cannot know in advance how many sub-problems the original input will be split into. We can only determine at runtime how many subsequent writing tasks there will be. \n",
    "\n",
    "> Of course, we can specify that only a certain number of sub-topics be split out, but it cannot guarantee that it will be applicable to all user inputs. In user inputs from different fields, it might be necessary to stipulate multiple splitting quantities.\n",
    "\n",
    "Now, we receive user input, break down subtopics, and then generate text. There are four steps:\n",
    "\n",
    "1. Receive user input\n",
    "2. Break down subtopics and add worker\n",
    "3. Write around subtopics\n",
    "4. Summary writing results\n",
    "\n",
    "Before we start, let's prepare the running environment.\n",
    "\n",
    "Use the `export` command to set up the API information for model invocation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3b38776",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# Setting environment variables in the terminal:\n",
    "export VLLM_SERVER_API_BASE=\"your-api-url\"\n",
    "export VLLM_SERVER_API_KEY=\"your-api-key\"\n",
    "export VLLM_SERVER_MODEL_NAME=\"your-model-name\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c113655f",
   "metadata": {},
   "source": [
    "Get the environment variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf74e15d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "_api_base = os.environ.get(\"VLLM_SERVER_API_BASE\")\n",
    "_api_key = os.environ.get(\"VLLM_SERVER_API_KEY\")\n",
    "_model_name = os.environ.get(\"VLLM_SERVER_MODEL_NAME\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8544f1d7",
   "metadata": {},
   "source": [
    "Import the necessary packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d84528b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from typing import List\n",
    "from bridgic.core.automa import GraphAutoma, worker, ArgsMappingRule\n",
    "from bridgic.llms.vllm.vllm_server_llm import VllmServerLlm, Message, Role, PydanticModel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25767744",
   "metadata": {},
   "source": [
    "We initialize the model to facilitate our subsequent convenient invocation of it to complete tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d282b20",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = VllmServerLlm(  # the llm instance\n",
    "    api_base=_api_base,\n",
    "    api_key=_api_key,\n",
    "    timeout=20,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce518390",
   "metadata": {},
   "source": [
    "Furthermore, we define that when the model completes entity extraction and query expansion, it returns the result in a Pydantic data structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a20433b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SubTopics(BaseModel):  # The expected format of the model output in the topic_split worker\n",
    "    sub_topics: List[str] = Field(description=\"All sub-topics in the input.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cc047f9",
   "metadata": {},
   "source": [
    "Next, let's complete the three steps of query expansion to achieve our goal:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee5d84b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ArticleWriter(GraphAutoma):\n",
    "    @worker(is_start=True)\n",
    "    async def pre_query(self, query: str):  # Receive the user's input and preprocess query\n",
    "        return query\n",
    "\n",
    "    @worker(dependencies=[\"pre_query\"], is_output=True)\n",
    "    async def topic_split(self, query: str):\n",
    "        response: SubTopics = await llm.astructured_output(\n",
    "            model=_model_name,\n",
    "            constraint=PydanticModel(model=SubTopics),\n",
    "            messages=[\n",
    "                Message.from_text(text=\"split the input into multiple sub-topics\", role=Role.SYSTEM),\n",
    "                Message.from_text(text=query, role=Role.USER,),\n",
    "            ]\n",
    "        )\n",
    "        print(response.model_dump_json(indent=4))\n",
    "        res = []\n",
    "        for i, sub_topic in enumerate(response.sub_topics):\n",
    "            res.append(self.write_around_subtopic(sub_topic))\n",
    "        return res\n",
    "\n",
    "    async def write_around_subtopic(self, sub_topic: str):\n",
    "        response: str = await llm.achat(\n",
    "            model=_model_name,\n",
    "            messages=[\n",
    "                Message.from_text(text=f\"write around the sub-topic: {sub_topic}\", role=Role.SYSTEM),\n",
    "                Message.from_text(text=sub_topic, role=Role.USER,),\n",
    "            ]\n",
    "        )\n",
    "        text = response.message.content\n",
    "        self.ferry_to('summary', text)\n",
    "        return text\n",
    "\n",
    "    @worker()\n",
    "    def summary(self, text: str):\n",
    "        return text"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
