{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "85375722",
   "metadata": {},
   "source": [
    "# Model Usage\n",
    "\n",
    "Bridgic provides a concise and rich encapsulation for model usage. Including:\n",
    "\n",
    "- Support for model integration with multiple model providers: `OpenAI LLM`, `OpenAI Like LLM`, `Vllm Server LLM`.\n",
    "- Support multiple methods of model invocation: `chat`, `stream`, `structured output`.\n",
    "\n",
    "The information passed to the model during these model invocation processes are all `Message` objects in Bridgic.The return results of model invocation vary depending on the invocation method used.\n",
    "\n",
    "## Model Integration\n",
    "\n",
    "In the actual production process, many models may be used, and these models come from different providers. For ease of using, Bridgic also provides corresponding usage methods.\n",
    "\n",
    "### OpenAI LLM\n",
    "\n",
    "Common usage for OpenAI model. Initialize an instance of an OpenAI model using `OpenAILlm` for invocation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34b4c5f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello! How can I assist you today?\n"
     ]
    }
   ],
   "source": [
    "# Get the environment variables.\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "from bridgic.core.model.types import Message, Role\n",
    "from bridgic.llms.openai.openai_llm import OpenAILlm\n",
    "\n",
    "# Set the OpenAI Key\n",
    "_api_key = os.environ.get(\"OPENAI_API_KEY\")\n",
    "_model_name = \"gpt-4.1-mini\"\n",
    "\n",
    "# Initialize the OpenAI Llm.\n",
    "llm = OpenAILlm(api_key=_api_key, timeout=10)\n",
    "\n",
    "# Chat.\n",
    "response = llm.chat(\n",
    "    model=_model_name,\n",
    "    messages=[\n",
    "        Message.from_text(text=\"You are a helpful assistant.\", role=Role.SYSTEM),\n",
    "        Message.from_text(text=\"Hello!\", role=Role.USER),\n",
    "    ]\n",
    ")\n",
    "print(response.message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00df4885",
   "metadata": {},
   "source": [
    "### OpenAI Like LLM\n",
    "\n",
    "Some LLM providers, not OpenAI's models, offer interfaces compatible with OpenAI models. Initialize an instance using `OpenAILikeLlm` for invocation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad285152",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello! How can I assist you today?\n"
     ]
    }
   ],
   "source": [
    "# Get the environment variables.\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "from Bridgic.core.intelligence import Message, Role\n",
    "from Bridgic.llms.openai_like.openai_like_llm import OpenAILikeLlm\n",
    "\n",
    "# Set the API base and key.\n",
    "_api_base = os.environ.get(\"OPENAI_LIKE_API_BASE\")\n",
    "_api_key = os.environ.get(\"OPENAI_LIKE_API_KEY\")\n",
    "_model_name = os.environ.get(\"OPENAI_LIKE_MODEL_NAME\")\n",
    "\n",
    "# Initialize the OpenAI Llm.\n",
    "llm = OpenAILikeLlm(api_base=_api_base, api_key=_api_key, timeout=10)\n",
    "\n",
    "# Chat.\n",
    "response = llm.chat(\n",
    "    model=_model_name,\n",
    "    messages=[\n",
    "        Message.from_text(text=\"You are a helpful assistant.\", role=Role.SYSTEM),\n",
    "        Message.from_text(text=\"Hello!\", role=Role.USER),\n",
    "    ]\n",
    ")\n",
    "print(response.message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e084acf0",
   "metadata": {},
   "source": [
    "### Vllm Server LLM\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d48832d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello! How can I assist you today? ðŸ˜Š\n"
     ]
    }
   ],
   "source": [
    "# Get the environment variables.\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "from Bridgic.core.intelligence import Message, Role\n",
    "from Bridgic.llms.vllm.vllm_server_llm import VllmServerLlm\n",
    "\n",
    "# Set the API base and key.\n",
    "_api_base = os.environ.get(\"VLLM_SERVER_API_BASE\")\n",
    "_api_key = os.environ.get(\"VLLM_SERVER_API_KEY\")\n",
    "_model_name = os.environ.get(\"VLLM_SERVER_MODEL_NAME\")\n",
    "\n",
    "# Initialize the OpenAI Llm.\n",
    "llm = VllmServerLlm(api_base=_api_base, api_key=_api_key, timeout=10)\n",
    "\n",
    "# Chat.\n",
    "response = llm.chat(\n",
    "    model=_model_name,\n",
    "    messages=[\n",
    "        Message.from_text(text=\"You are a helpful assistant.\", role=Role.SYSTEM),\n",
    "        Message.from_text(text=\"Hello!\", role=Role.USER),\n",
    "    ]\n",
    ")\n",
    "print(response.message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5886f6cd",
   "metadata": {},
   "source": [
    "## Model Invocation\n",
    "\n",
    "When invoking the model, depending on the different scenarios, we may expect it to have different behaviors. For instance, in a chatbot, we expect it to return a stream, while in a fixed pipeline scenario, we expect it to return a fixed data structure. Bridgic has designed relevant calling methods for these needs.\n",
    "\n",
    "> Not all invocation methods can be executed for sure. Some invocation methods can run correctly only when the model supports the relevant protocols at the same time.\n",
    "\n",
    "Let's take VllmServerLlm as an example to introduce each calling method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d433c0ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the environment variables.\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "# Import the necessary modules.\n",
    "from Bridgic.core.intelligence import Message, Role, Response, StreamResponse, AsyncStreamResponse\n",
    "from Bridgic.llms.vllm.vllm_server_llm import VllmServerLlm\n",
    "\n",
    "# Set the API base and key.\n",
    "_api_base = os.environ.get(\"VLLM_SERVER_API_BASE\")\n",
    "_api_key = os.environ.get(\"VLLM_SERVER_API_KEY\")\n",
    "_model_name = os.environ.get(\"VLLM_SERVER_MODEL_NAME\")\n",
    "\n",
    "# Initialize the OpenAI Llm.\n",
    "vllm_llm = VllmServerLlm(api_base=_api_base, api_key=_api_key, timeout=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4623a30",
   "metadata": {},
   "source": [
    "### Chat\n",
    "\n",
    "`chat()` and `achat()` directly invoke the model and return a `Response` object, which encapsulates the model's output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f63850bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response: message=Message(role=<Role.AI: 'assistant'>, blocks=[TextBlock(block_type='text', text='Hello! ðŸ˜Š How can I assist you today?')], extras={}) raw=ChatCompletion(id='chatcmpl-92b13da2cd0347aaa90f2171eb424843', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='Hello! ðŸ˜Š How can I assist you today?', refusal=None, role='assistant', annotations=None, audio=None, function_call=None, tool_calls=[], reasoning_content=None), stop_reason=None)], created=1760338633, model='Qwen/Qwen3-4B-Instruct-2507', object='chat.completion', service_tier=None, system_fingerprint=None, usage=CompletionUsage(completion_tokens=12, prompt_tokens=21, total_tokens=33, completion_tokens_details=None, prompt_tokens_details=None), prompt_logprobs=None, kv_transfer_params=None)\n",
      "content: Hello! ðŸ˜Š How can I assist you today?\n"
     ]
    }
   ],
   "source": [
    "response: Response = vllm_llm.chat(\n",
    "    model=_model_name,\n",
    "    messages=[\n",
    "        Message.from_text(text=\"You are a helpful assistant.\", role=Role.SYSTEM),\n",
    "        Message.from_text(text=\"Hello!\", role=Role.USER),\n",
    "    ]\n",
    ")\n",
    "print(f\"response: {response}\")\n",
    "print(f\"content: {response.message.content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8988d878",
   "metadata": {},
   "source": [
    "### Stream\n",
    "\n",
    "`stream()` return a generator `StreamResponse` objects, which encapsulates the model's streaming output. Each chunk of the stream is encapsulated in a `MessageChunk` object. `astream()` returns an asynchronous generator `AsyncStreamResponse` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6960be3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'generator'>\n",
      "------use the stream------\n",
      "\n",
      "Help\n",
      "ful\n",
      ",\n",
      " thoughtful\n",
      "\n",
      "------end of the stream------\n",
      "\n",
      "<class 'async_generator'>\n",
      "------use the async stream------\n",
      "\n",
      "Help\n",
      "ful\n",
      ",\n",
      " thoughtful\n",
      ".\n",
      "\n",
      "------end of the async stream------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "response: StreamResponse = vllm_llm.stream(\n",
    "    model=_model_name,\n",
    "    messages=[\n",
    "        Message.from_text(text=\"You are a helpful assistant.\", role=Role.SYSTEM),\n",
    "        Message.from_text(text=\"Describe yourself with two words.\", role=Role.USER),\n",
    "    ]\n",
    ")\n",
    "print(type(response))\n",
    "print(f\"------use the stream------\")\n",
    "for chunk in response:\n",
    "    print(chunk.delta)\n",
    "print(f\"------end of the stream------\\n\")\n",
    "\n",
    "async_response: AsyncStreamResponse = vllm_llm.astream(\n",
    "    model=_model_name,\n",
    "    messages=[\n",
    "        Message.from_text(text=\"You are a helpful assistant.\", role=Role.SYSTEM),\n",
    "        Message.from_text(text=\"Describe yourself with two words.\", role=Role.USER),\n",
    "    ]\n",
    ")\n",
    "print(type(async_response))\n",
    "print(f\"------use the async stream------\")\n",
    "async for chunk in async_response:\n",
    "    print(chunk.delta)\n",
    "print(f\"------end of the async stream------\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6be78877",
   "metadata": {},
   "source": [
    "### Structured Output\n",
    "\n",
    "\n",
    "\n",
    "`structured_output()` and `astructured_output()` are used to invoke the model to return the output in the specified format. The format can be pydantic basemodel, JSON schema, regular expression, EBNF grammar, etc. \n",
    "\n",
    "> Note: The prerequisite for this method to operate correctly is that the model service used supports the corresponding structured output. For instance, models deployed through Vllm support a relatively wide variety of structured outputs: [vLLM Structured Output](https://docs.vllm.ai/en/latest/features/structured_outputs.html) and OpenAI natively only supports pydantic BaseModel and some JSON schemas: [OpenAI Structured Output](https://platform.openai.com/docs/guides/structured-outputs)\n",
    "\n",
    "For example, invoke the model to extract names from a given text and return the `Names`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c534db1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class '__main__.Names'>\n",
      "names=['John Doe', 'Jane Smith', 'Bob Johnson']\n"
     ]
    }
   ],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from typing import List\n",
    "from Bridgic.core.intelligence import PydanticModel\n",
    "\n",
    "class Names(BaseModel):\n",
    "    names: List[str] = Field(description=\"The names extracted from the text.\")\n",
    "\n",
    "response: Names = vllm_llm.structured_output(\n",
    "    model=_model_name,\n",
    "    messages=[\n",
    "        Message.from_text(text=\"Given a text, extract the names from it.\", role=Role.SYSTEM),\n",
    "        Message.from_text(text=\"The text is: 'John Doe, Jane Smith, and Bob Johnson.'\", role=Role.USER),\n",
    "    ],\n",
    "    constraint=PydanticModel(model=Names)\n",
    ")\n",
    "print(type(response))\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dba6dd0c",
   "metadata": {},
   "source": [
    "Or, we can also make it return a JSON schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06f0308d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict'>\n",
      "{'names': ['John Doe', 'Jane Smith', 'Bob Johnson']}\n"
     ]
    }
   ],
   "source": [
    "from typing import Dict\n",
    "from Bridgic.core.intelligence import JsonSchema\n",
    "\n",
    "schema = {\n",
    "    \"type\": \"object\",\n",
    "    \"properties\": {\n",
    "        \"names\": {\"type\": \"array\", \"description\": \"The names extracted from the text.\", \"items\": {\"type\": \"string\"}},\n",
    "    },\n",
    "    \"required\": [\"names\"],\n",
    "}\n",
    "response: Dict = vllm_llm.structured_output(\n",
    "    model=_model_name,\n",
    "    messages=[\n",
    "        Message.from_text(text=\"Given a text, extract the names from it.\", role=Role.SYSTEM),\n",
    "        Message.from_text(text=\"The text is: 'John Doe, Jane Smith, and Bob Johnson.'\", role=Role.USER),\n",
    "    ],\n",
    "    constraint=JsonSchema(name=\"ExtractNames\", schema_dict=schema)\n",
    ")\n",
    "print(type(response))\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbde2f9f",
   "metadata": {},
   "source": [
    "Structured output also supports outputting strings described by the given regular expression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "130d6ac5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-12-25\n"
     ]
    }
   ],
   "source": [
    "from Bridgic.core.intelligence import Regex\n",
    "\n",
    "response: str = vllm_llm.structured_output(\n",
    "    model=_model_name,\n",
    "    messages=[\n",
    "        Message.from_text(text=\"You are a helpful assistant.\", role=Role.SYSTEM),\n",
    "        Message.from_text(text=\"This year is 2025. Please tell me the date of this year's Christmas.\", role=Role.USER),\n",
    "    ],\n",
    "    constraint=Regex(pattern=r\"\\d{4}-\\d{2}-\\d{2}\", description=\"The date should be in the format of YYYY-MM-DD.\")\n",
    ")\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
