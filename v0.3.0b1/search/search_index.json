{"config":{"lang":["en"],"separator":"[\\s\\-,:!=\\[\\]()\"`/]+|\\.(?!\\d)|&[lg]t;|(?!\\b)(?=[A-Z][a-z])","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":""},{"location":"#bridgic-next-generation-agent-development","title":"Bridgic: Next-Generation Agent Development","text":"<p>The name Bridgic embodies our core philosophy \"Bridging Logic and Magic\" where:</p> <ul> <li>Logic represents structured and predictable execution flows, forming the foundation of reliable systems.</li> <li>Magic refers to the autonomous parts that can make dynamic decisions and solve problems creatively.</li> </ul>"},{"location":"#installation","title":"Installation","text":"<p>Bridgic requires Python 3.9 or newer. Make sure your environment meets this requirement before installing.</p> pipuv <pre><code>pip install bridgic\n</code></pre> <pre><code>uv add bridgic\n</code></pre> <p>After installation, verify that Bridgic is installed correctly:</p> pipuv <pre><code>python -c \"from bridgic.core import __version__; print(f'Bridgic version: {__version__}')\"\n</code></pre> <pre><code>uv run python -c \"from bridgic.core import __version__; print(f'Bridgic version: {__version__}')\"\n</code></pre>"},{"location":"#what-is-the-vision","title":"What is the Vision","text":"<p>Our vision is to make intelligent systems easy to build for everyone.</p> <p>Bridgic isn't just a framework\u2014it's defining how the next generation of intelligent systems will be built.</p> <p>By fundamentally rethinking the relationship between deterministic workflows and autonomous agents and establishing a unified runtime foundation, we\u2019re setting a new standard for the development of intelligent systems.</p> <p>This is not merely a design choice\u2014it is a fundamental redefinition of what is possible in intelligent system development. We are creating a world where developers can seamlessly transition from deterministic workflows to autonomous agents within a single, unified framework.</p>"},{"location":"#how-we-achieve-it","title":"How We Achieve It","text":"<p>To realize our vision of making intelligent systems easy to build for everyone, Bridgic is built on four fundamental technological innovations that collectively redefine what is possible in intelligent system development:</p> <p>\ud83c\udfaf Innovative Dynamic Directed Graph Engine: At the core lies Dynamic Directed Graph (DDG)\u2014an innovative runtime model that fundamentally breaks down the artificial barriers between traditional static graph systems and autonomous agent construction. Unlike static graph systems, DDG operates as a dynamic runtime model that enables intelligent systems to adapt their execution structure in real-time, providing developers with a unified development experience that seamlessly spans the entire spectrum from structured workflows to fully autonomous systems.</p> <p>\ud83e\udd1d Powerful Human-in-the-Loop APIs: Bridgic provides powerful human-in-the-loop APIs that enable sophisticated interaction patterns between intelligent systems and human operators. These APIs support interrupting execution units at any point in the workflow, allowing systems to pause, request external feedback, and seamlessly resume execution with the acquired feedback, which supports an intelligent system to leverage human's input when needed.</p> <p>\ud83d\udd0c Seamless Third-Party Integration: Bridgic establishes a comprehensive integration architecture through well-designed internal abstractions, including:</p> <ul> <li>technology-neutral model integration that enables seamless integration with any LLM provider</li> <li>systematic MCP integration that transforms tool integration into a composition opportunity</li> <li>seamless enterprise observability integration that ensures intelligent systems are transparent and optimizable.</li> </ul> <p>\ud83c\udfa8 High-Information-Density Agent Structure Language: Bridgic introduces Agent Structure Language (ASL)\u2014a Python-native DSL that enables developers to express sophisticated agentic structures within limited code footprint. This code organization paradigm is optimized for the future of AI-assisted development, making it particularly well-suited for AI code generation.</p> <p>Ready to bridge logic and magic? Explore the documentation to continue your journey:</p> <ul> <li>Tutorials \u2014 Learn through hands-on examples</li> <li>Understanding \u2014 Deep dive into core concepts</li> <li>API Reference \u2014 Complete API documentation</li> </ul>"},{"location":"extras/llms/","title":"llms","text":""},{"location":"extras/llms/#llm-integration","title":"LLM Integration","text":""},{"location":"extras/llms/#overview","title":"Overview","text":"<p>Bridgic's model integration module (<code>bridgic.llms</code>) employs a Provider-centric integration strategy, adapting different model service providers and inference frameworks as independent integration packages. Each package adheres to the same interface standards, ensuring model neutrality and consistent interface definitions.</p>"},{"location":"extras/llms/#design-philosophy","title":"Design Philosophy","text":""},{"location":"extras/llms/#provider-centric-integration","title":"Provider-Centric Integration","text":"<p>Bridgic designs model integration with Providers as the fundamental unit:</p> <ul> <li>Independent Packaging: Each model provider (such as OpenAI, vLLM, other model vendors, or inference frameworks) has its own dedicated integration package</li> <li>Unified Interface: All packages implement the same core interface to ensure consistent user experience</li> <li>On-Demand Installation: Developers only need to install the required Provider packages, avoiding unnecessary dependencies</li> </ul>"},{"location":"extras/llms/#model-neutrality","title":"Model Neutrality","text":"<p>Through Provider-centric design, Bridgic achieves true model neutrality:</p> <ul> <li>No Vendor Lock-in: Application code is not tied to specific model providers, enabling easy switching</li> <li>Seamless Switching: When changing model providers, application code requires minimal changes\u2014simply swap the model object</li> </ul>"},{"location":"extras/llms/#protocol-driven-design","title":"Protocol-Driven Design","text":"<p>Each Provider declares its capabilities by implementing predefined protocols:</p> <ul> <li>Capability Declaration: Protocol based implementations clearly declare the supported functional features</li> <li>Progressive Enhancement: Providers can implement partial protocols to support incremental enhancement</li> <li>Extensibility: New Providers can be seamlessly integrated by implementing the appropriate protocols</li> </ul>"},{"location":"extras/llms/#available-integrations","title":"Available Integrations","text":"Provider Package Description OpenAI <code>bridgic-llms-openai</code> Official OpenAI API integration for GPT-4, GPT-3.5, and other models vLLM <code>bridgic-llms-vllm</code> Self-hosted large language model inference engine - <code>bridgic-llms-openai-like</code> Thin wrapper for OpenAI compatible model services"},{"location":"extras/protocols/","title":"protocols","text":""},{"location":"extras/protocols/#protocol-integration","title":"Protocol Integration","text":""},{"location":"extras/protocols/#overview","title":"Overview","text":"<p>Bridgic's protocol integration module (<code>bridgic.protocols</code>) provides seamless integration with third-party protocols, enabling AI applications to leverage external tools, resources, and services through standardized interfaces.</p>"},{"location":"extras/protocols/#design-philosophy","title":"Design Philosophy","text":"<p>Bridgic integrates third-party protocols and SDKs through core abstractions tailored to its architecture, allowing developers to effortlessly use familiar development abilities and features.</p> <ul> <li>Bridgic-Centric Abstractions: Third-party protocols are integrated through Bridgic's core abstractions (like tools, prompts, workers), allowing developers to work with familiar Bridgic concepts</li> <li>Seamless Development Experience: Developers can use protocol tools and resources through the same patterns they already know from Bridgic, without needing to learn so many specific APIs</li> </ul> <p>The protocol integration architecture is designed for extensibility:</p> <ul> <li>Modular Design: Each protocol has its own dedicated integration package</li> <li>Future-Proof: New protocol is not difficult to be addded</li> </ul>"},{"location":"extras/protocols/#available-integrations","title":"Available Integrations","text":"Protocol Package Description MCP <code>bridgic-protocols-mcp</code> Model Context Protocol integration"},{"location":"extras/traces/","title":"traces","text":""},{"location":"extras/traces/#trace-integration","title":"Trace Integration","text":""},{"location":"extras/traces/#overview","title":"Overview","text":"<p>Any agentic system needs a reliable observability layer to stay debuggable and trustworthy. The full picture of observability spans two complementary consists of these two parts:</p> <ul> <li>Passive Tracing: To Keep watching over every worker lifecycle event without code intrusion.</li> <li>Active Tracing: To enable application to emit custom spans or metrics whenever richer context is needed.</li> </ul> <p>Together they form a holistic observability surface.</p> <p>Bridgic fully supports passive tracing based on Worker Callback Mechanism, and provides seamless integration with several third-party platforms. More convient programming tools that support active tracing will also be introduced in the future.</p>"},{"location":"extras/traces/#available-integrations","title":"Available Integrations","text":"Package Description <code>bridgic-traces-opik</code> Opik-powered observability and runtime tracing adapter. <code>bridgic-traces-langwatch</code> LangWatch-powered observability and runtime tracing adapter."},{"location":"reference/bridgic-asl/bridgic/asl/","title":"asl","text":""},{"location":"reference/bridgic-asl/bridgic/asl/#asl","title":"asl","text":""},{"location":"reference/bridgic-asl/bridgic/asl/#bridgic.asl.ASLAutoma","title":"ASLAutoma","text":"<p>               Bases: <code>GraphAutoma</code></p> <p>An automa that builds agent structures from ASL (Agent Structured Language) definitions.</p> <p>This class extends <code>GraphAutoma</code> and uses a declarative syntax to define workflows. It automatically builds the graph structure from definitions during initialization, handling  both static and dynamic worker registration.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from bridgic.asl import graph, ASLAutoma\n&gt;&gt;&gt; \n&gt;&gt;&gt; def add_one(x: int):\n...     return x + 1\n&gt;&gt;&gt; \n&gt;&gt;&gt; def add_two(x: int):\n...     return x + 2\n&gt;&gt;&gt; \n&gt;&gt;&gt; class MyGraph(ASLAutoma):\n...     with graph as g:\n...         a = add_one\n...         b = add_two\n...         +a &gt;&gt; b  # a is the start worker, b depends on a and is the output worker.\n&gt;&gt;&gt; \n&gt;&gt;&gt; graph = MyGraph()\n&gt;&gt;&gt; result = await graph.arun(x=1)  # result: 4 (1+1+2)\n</code></pre> Source code in <code>bridgic/asl/_asl_automa.py</code> <pre><code>class ASLAutoma(GraphAutoma, metaclass=ASLAutomaMeta):\n    \"\"\"\n    An automa that builds agent structures from ASL (Agent Structured Language) definitions.\n\n    This class extends `GraphAutoma` and uses a declarative syntax to define workflows. It\n    automatically builds the graph structure from definitions during initialization, handling \n    both static and dynamic worker registration.\n\n    Examples\n    --------\n    &gt;&gt;&gt; from bridgic.asl import graph, ASLAutoma\n    &gt;&gt;&gt; \n    &gt;&gt;&gt; def add_one(x: int):\n    ...     return x + 1\n    &gt;&gt;&gt; \n    &gt;&gt;&gt; def add_two(x: int):\n    ...     return x + 2\n    &gt;&gt;&gt; \n    &gt;&gt;&gt; class MyGraph(ASLAutoma):\n    ...     with graph as g:\n    ...         a = add_one\n    ...         b = add_two\n    ...         +a &gt;&gt; b  # a is the start worker, b depends on a and is the output worker.\n    &gt;&gt;&gt; \n    &gt;&gt;&gt; graph = MyGraph()\n    &gt;&gt;&gt; result = await graph.arun(x=1)  # result: 4 (1+1+2)\n    \"\"\"\n    # The canvases of the automa (stored in bottom-up order).\n    _top_canvas: _Canvas = None\n\n    def __init__(\n        self, \n        name: str = None, \n        thread_pool: Optional[ThreadPoolExecutor] = None, \n        running_options: Optional[RunningOptions] = None\n    ):\n        \"\"\"\n        Initialize the ASLAutoma instance.\n\n        Parameters\n        ----------\n        name : str, optional\n            The name of the automa. If None, a default name will be assigned.\n        thread_pool : ThreadPoolExecutor, optional\n            The thread pool for parallel running of I/O-bound tasks. If None, a default thread pool will be used.\n        running_options : RunningOptions, optional\n            The running options for the automa. If None, a default running options will be used.\n        \"\"\"\n        self.running_options = running_options or RunningOptions()\n        super().__init__(name=name, thread_pool=thread_pool, running_options=running_options)\n        self._dynamic_workers = {}\n        if not self._top_canvas:\n            self.automa = None\n        else:\n            top_canvas = self._top_canvas[-1]\n            self.automa: GraphAutoma = self._build_graph(top_canvas)\n\n    def _build_graph(self, canvas: _Canvas) -&gt; GraphAutoma:\n        \"\"\"\n        Build the graph structure from all canvases.\n\n        This method iterates through all canvases in bottom-up order and builds the graph\n        structure for each canvas. It separates static and dynamic elements and delegates\n        the actual building to _inner_build_graph.\n        \"\"\"\n        static_elements = {\n            key: value \n            for key, value in canvas.elements.items() \n            if (isinstance(value, _Element) and not value.is_lambda) or isinstance(value, _Canvas)\n        }\n        dynamic_elements = {\n            key: value \n            for key, value in canvas.elements.items() \n            if (isinstance(value, _Element) and value.is_lambda)\n        }\n        return self._inner_build_graph(canvas, static_elements, dynamic_elements)\n\n    def _inner_build_graph(\n        self, \n        canvas: _Canvas, \n        static_elements: Dict[str, \"_Element\"],\n        dynamic_elements: Dict[str, \"_Element\"]\n    ) -&gt; GraphAutoma:\n        \"\"\"\n        Build the graph structure for a specific canvas.\n\n        This method handles the construction of both dynamic and static logic flows. For dynamic\n        elements (lambda functions), it sets up callbacks that will add workers at runtime and remove \n        them when the execution completes. For static elements, it immediately adds them to the \n        graph with their dependencies and settings.\n\n        Parameters\n        ----------\n        canvas : _Canvas\n            The canvas to build the graph for.\n        static_elements : Dict[str, \"_Element\"]\n            Dictionary of static elements (non-lambda workers) to add to the graph.\n        dynamic_elements : Dict[str, \"_Element\"]\n            Dictionary of dynamic elements (lambda functions) that will generate workers at runtime.\n        \"\"\"\n        automa = None\n        current_canvas_key = canvas.key\n\n        ###############################\n        # build the dynamic logic flow\n        ###############################\n        running_options_callback = []\n        for _, element in dynamic_elements.items():\n            worker_material = element.worker_material\n            params_names = element.cached_param_names\n\n            # If the canvas is top level, use `RunningOptions` to add callback.\n            if canvas.is_top_level():\n                running_options_callback.append(\n                    WorkerCallbackBuilder(\n                        AsTopLevelDynamicCallback, \n                        init_kwargs={\"__dynamic_lambda_func__\": worker_material, \"__param_names__\": simplify_param_names(params_names)}\n                    )\n                )\n\n            # Otherwise, delegate parent automa to add callback during building graph.\n            else:\n                parent_key = canvas.parent_canvas.key\n                if parent_key not in self._dynamic_workers:\n                    self._dynamic_workers[parent_key] = {}\n                if current_canvas_key not in self._dynamic_workers[parent_key]:\n                    self._dynamic_workers[parent_key][current_canvas_key] = []\n                self._dynamic_workers[parent_key][current_canvas_key].append(element)\n\n        # Make the automa.\n\n        canvas.make_automa(running_options=RunningOptions(\n            debug=self.running_options.debug,\n            verbose=self.running_options.verbose,\n            callback_builders=self.running_options.callback_builders + running_options_callback,\n            model_config=self.running_options.model_config\n        ))\n        automa = canvas.worker_material\n        if canvas.is_top_level():\n            params_data = canvas.worker_material.get_input_param_names()\n            set_method_signature(self.arun, params_data)\n\n\n        ###############################\n        # build the static logic flow\n        ###############################\n        for _, element in static_elements.items():\n            key = element.key\n            parent_key = element.parent_canvas.key\n            worker_material = element.worker_material\n            is_start = element.is_start\n            is_output = element.is_output\n            dependencies = [item.key for item in element.dependencies]\n            args_mapping_rule = element.args_mapping_rule\n            result_dispatching_rule = element.result_dispatching_rule\n\n            # If the object is an instance of an object instance, it must be ensured that each time\n            # an instance of the current ASLAutoma is created, it is an independent one of this object\n            # instance. Here, the object has these forms:\n            #   1. Canvas:\n            #     a. graph (exactly is GraphAutoma) or concurrent (exactly is ConcurrentAutoma) etc.\n            #   2. Element:\n            #     a. Callable\n            #     b. ASLAutoma\n            #     c. GraphAutoma etc.\n            #     d. Worker\n            if isinstance(element, _Canvas):\n                worker_material = self._build_graph(element)\n            elif isinstance(element, _Element):\n                if isinstance(worker_material, ASLAutoma):\n                    asl_automa_class = type(worker_material)\n                    running_options_callback = (\n                        getattr(worker_material, \"running_options\", None).callback_builders\n                        if getattr(worker_material, \"running_options\", None) \n                        else []\n                    ) + self.running_options.callback_builders\n                    worker_material = asl_automa_class(\n                        name=getattr(worker_material, \"name\", None), \n                        thread_pool=getattr(worker_material, \"thread_pool\", None), \n                        running_options=RunningOptions(\n                            debug=self.running_options.debug,\n                            verbose=self.running_options.verbose,\n                            callback_builders=running_options_callback,\n                            model_config=self.running_options.model_config\n                        )\n                    )\n                elif isinstance(worker_material, GraphAutoma):\n                    graph_automa_class = type(worker_material)\n                    running_options_callback = (\n                        getattr(worker_material, \"running_options\", None).callback_builders\n                        if getattr(worker_material, \"running_options\", None) \n                        else []\n                    ) + self.running_options.callback_builders\n                    worker_material = graph_automa_class(\n                        name=getattr(worker_material, \"name\", None), \n                        thread_pool=getattr(worker_material, \"thread_pool\", None), \n                        running_options=RunningOptions(\n                            debug=self.running_options.debug,\n                            verbose=self.running_options.verbose,\n                            callback_builders=running_options_callback,\n                            model_config=self.running_options.model_config\n                        )\n                    )\n                elif isinstance(worker_material, Worker):\n                    worker_material = _copy_worker_safely(worker_material)\n                elif isinstance(worker_material, Callable):\n                    pass\n            else:\n                raise ValueError(f\"Invalid worker material type: {type(worker_material)}.\")\n\n            # Prepare the callback builders.\n            # If current element delegated dynamic workers to be added in current canvas.\n            callback_builders = []\n            if current_canvas_key in self._dynamic_workers and key in self._dynamic_workers[current_canvas_key]:\n                delegated_dynamic_workers = self._dynamic_workers[current_canvas_key][key]\n                for delegated_dynamic_element in delegated_dynamic_workers:\n                    delegated_dynamic_func = delegated_dynamic_element.worker_material\n                    delegated_dynamic_params_names = delegated_dynamic_element.cached_param_names\n                    callback_builders.append(WorkerCallbackBuilder(\n                        AsWorkerDynamicCallback,\n                        init_kwargs={\"__dynamic_lambda_func__\": delegated_dynamic_func, \"__param_names__\": simplify_param_names(delegated_dynamic_params_names)}\n                    ))\n\n            # Update the signature in Data to set the __cached_param_names_of_arun or __cached_param_names_of_run.\n            # Note: Python's name mangling mechanism\n            # In Python, attributes that start with double underscores `__` but don't end with `__`\n            # are subject to name mangling. The actual attribute name becomes `_ClassName__attribute_name`:\n            # - __cached_param_names_of_arun in Worker class \u2192 _Worker__cached_param_names_of_arun\n            # - __cached_param_names_of_run in Worker class \u2192 _Worker__cached_param_names_of_run\n            # - __cached_param_names_of_callable in CallableWorker class \u2192 _CallableWorker__cached_param_names_of_callable\n            # If we directly write `worker_material.__cached_param_names_of_xxx`, Python will interpret it\n            # as a private attribute of the current module or class, not as an attribute of Worker or\n            # CallableWorker. Therefore, we must use setattr() with the correct mangled attribute name.\n            def set_cached_param_names(worker_material: Worker, override_params: Dict):\n                if isinstance(worker_material, CallableWorker):\n                    setattr(worker_material, '_CallableWorker__cached_param_names_of_callable', override_params)\n                else:\n                    if worker_material._is_arun_overridden():\n                        setattr(worker_material, '_Worker__cached_param_names_of_arun', override_params)\n                    else:\n                        setattr(worker_material, '_Worker__cached_param_names_of_run', override_params)\n\n            def get_param_names_dict(sig: inspect.Signature, exclude_default: bool = False) -&gt; Dict:\n                param_names_dict = {}\n                for name, param in sig.parameters.items():\n                    if exclude_default and param.default is not inspect.Parameter.empty:\n                        continue\n                    if param.kind not in param_names_dict:\n                        param_names_dict[param.kind] = []\n\n                    if param.default is inspect.Parameter.empty:\n                        param_names_dict[param.kind].append((name, inspect._empty))\n                    else:\n                        param_names_dict[param.kind].append((name, param.default))\n                return param_names_dict\n\n            signature_name = f\"{parent_key}.{key}\" if parent_key else f\"__TOP__.{key}\"\n            override_signature = getattr(worker_material, \"__signature_overrides__\", {}).get(signature_name, None)\n            if override_signature:\n                override_params = get_param_names_dict(override_signature)\n                if isinstance(worker_material, Callable):\n                    worker_material = CallableWorker(func_or_method=worker_material)\n                set_cached_param_names(worker_material, override_params)\n\n            # Build the automa.\n            if isinstance(automa, ConcurrentAutoma):\n                build_concurrent(\n                    automa=automa,\n                    key=key,\n                    worker_material=worker_material,\n                    callback_builders=callback_builders\n                )\n            elif isinstance(automa, GraphAutoma):\n                build_graph(\n                    automa=automa,\n                    key=key,\n                    worker_material=worker_material,\n                    is_start=is_start,\n                    is_output=is_output,\n                    dependencies=dependencies,\n                    args_mapping_rule=args_mapping_rule,\n                    result_dispatching_rule=result_dispatching_rule,\n                    callback_builders=callback_builders\n                )\n            else:\n                raise ValueError(f\"Invalid automa type: {type(automa)}.\")\n        return automa\n\n    def dump_to_dict(self) -&gt; Dict[str, Any]:\n        \"\"\"\n        Dump the ASLAutoma instance to a dictionary.\n\n        Returns\n        -------\n        Dict[str, Any]\n            A dictionary containing the serialized state of the ASLAutoma instance.\n        \"\"\"\n        state_dict = super().dump_to_dict()\n        state_dict[\"automa\"] = self.automa.dump_to_dict()\n        return state_dict\n\n    def load_from_dict(self, state_dict: Dict[str, Any]) -&gt; None:\n        \"\"\"\n        Load the ASLAutoma instance from a dictionary.\n\n        Parameters\n        ----------\n        state_dict : Dict[str, Any]\n            A dictionary containing the serialized state of the ASLAutoma instance.\n        \"\"\"\n        super().load_from_dict(state_dict)\n        self.automa = state_dict[\"automa\"]\n\n    async def arun(\n        self,\n        *args: Tuple[Any, ...],\n        feedback_data = None,\n        **kwargs: Dict[str, Any]\n    ) -&gt; Any:\n        \"\"\"\n        Run the automa asynchronously.\n\n        Parameters\n        ----------\n        *args : Tuple[Any, ...]\n            Positional arguments to pass to the automa.\n        feedback_data : Any, optional\n            Feedback data for the execution (default: None).\n        **kwargs : Dict[str, Any]\n            Keyword arguments to pass to the automa.\n\n        Returns\n        -------\n        Any\n            The result of the automa execution.\n        \"\"\"\n        if not self.automa:\n            return super().arun(*args, feedback_data=feedback_data, **kwargs)\n\n        res = await self.automa.arun(*args, feedback_data=feedback_data, **kwargs)\n        return res\n\n    def __str__(self) -&gt; str:\n        return f\"ASLAutoma(automa={self.automa})\"\n\n    def __repr__(self) -&gt; str:\n        return self.__str__()\n</code></pre>"},{"location":"reference/bridgic-asl/bridgic/asl/#bridgic.asl.ASLAutoma.dump_to_dict","title":"dump_to_dict","text":"<pre><code>dump_to_dict() -&gt; Dict[str, Any]\n</code></pre> <p>Dump the ASLAutoma instance to a dictionary.</p> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>A dictionary containing the serialized state of the ASLAutoma instance.</p> Source code in <code>bridgic/asl/_asl_automa.py</code> <pre><code>def dump_to_dict(self) -&gt; Dict[str, Any]:\n    \"\"\"\n    Dump the ASLAutoma instance to a dictionary.\n\n    Returns\n    -------\n    Dict[str, Any]\n        A dictionary containing the serialized state of the ASLAutoma instance.\n    \"\"\"\n    state_dict = super().dump_to_dict()\n    state_dict[\"automa\"] = self.automa.dump_to_dict()\n    return state_dict\n</code></pre>"},{"location":"reference/bridgic-asl/bridgic/asl/#bridgic.asl.ASLAutoma.load_from_dict","title":"load_from_dict","text":"<pre><code>load_from_dict(state_dict: Dict[str, Any]) -&gt; None\n</code></pre> <p>Load the ASLAutoma instance from a dictionary.</p> <p>Parameters:</p> Name Type Description Default <code>state_dict</code> <code>Dict[str, Any]</code> <p>A dictionary containing the serialized state of the ASLAutoma instance.</p> required Source code in <code>bridgic/asl/_asl_automa.py</code> <pre><code>def load_from_dict(self, state_dict: Dict[str, Any]) -&gt; None:\n    \"\"\"\n    Load the ASLAutoma instance from a dictionary.\n\n    Parameters\n    ----------\n    state_dict : Dict[str, Any]\n        A dictionary containing the serialized state of the ASLAutoma instance.\n    \"\"\"\n    super().load_from_dict(state_dict)\n    self.automa = state_dict[\"automa\"]\n</code></pre>"},{"location":"reference/bridgic-asl/bridgic/asl/#bridgic.asl.ASLAutoma.arun","title":"arun","text":"<code>async</code> <pre><code>arun(\n    *args: Tuple[Any, ...],\n    feedback_data=None,\n    **kwargs: Dict[str, Any]\n) -&gt; Any\n</code></pre> <p>Run the automa asynchronously.</p> <p>Parameters:</p> Name Type Description Default <code>*args</code> <code>Tuple[Any, ...]</code> <p>Positional arguments to pass to the automa.</p> <code>()</code> <code>feedback_data</code> <code>Any</code> <p>Feedback data for the execution (default: None).</p> <code>None</code> <code>**kwargs</code> <code>Dict[str, Any]</code> <p>Keyword arguments to pass to the automa.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Any</code> <p>The result of the automa execution.</p> Source code in <code>bridgic/asl/_asl_automa.py</code> <pre><code>async def arun(\n    self,\n    *args: Tuple[Any, ...],\n    feedback_data = None,\n    **kwargs: Dict[str, Any]\n) -&gt; Any:\n    \"\"\"\n    Run the automa asynchronously.\n\n    Parameters\n    ----------\n    *args : Tuple[Any, ...]\n        Positional arguments to pass to the automa.\n    feedback_data : Any, optional\n        Feedback data for the execution (default: None).\n    **kwargs : Dict[str, Any]\n        Keyword arguments to pass to the automa.\n\n    Returns\n    -------\n    Any\n        The result of the automa execution.\n    \"\"\"\n    if not self.automa:\n        return super().arun(*args, feedback_data=feedback_data, **kwargs)\n\n    res = await self.automa.arun(*args, feedback_data=feedback_data, **kwargs)\n    return res\n</code></pre>"},{"location":"reference/bridgic-asl/bridgic/asl/#bridgic.asl.Data","title":"Data","text":"<p>Container for parameter data configuration.</p> <p>This class stores type and default value information for function parameters, allowing dynamic modification of function signatures at declaration time. It can be attached to workers or callables using the left-multiplication operator (*).</p> Source code in <code>bridgic/asl/_canvas_object.py</code> <pre><code>class Data:\n    \"\"\"\n    Container for parameter data configuration.\n\n    This class stores type and default value information for function parameters,\n    allowing dynamic modification of function signatures at declaration time. It can be\n    attached to workers or callables using the left-multiplication operator (*).\n    \"\"\"\n    def __init__(self, **kwargs: Any) -&gt; None:\n        \"\"\"\n        Initialize the Data container with parameter configurations.\n\n        Parameters\n        ----------\n        **kwargs : Any\n            Keyword arguments where each key is a parameter name and each value\n            is the default value for that parameter. The type is set to Any by default.\n        \"\"\"\n        data = {\n            Parameter.POSITIONAL_OR_KEYWORD: [],\n        }\n        for key, value in kwargs.items():\n            data[Parameter.POSITIONAL_OR_KEYWORD].append((key, value))\n        self.data = data\n\n    def __rmul__(self, other: Union[Callable, Worker]):\n        \"\"\"\n        Attach this Data configuration to a worker or callable using right-multiplication.\n\n        This method allows syntax like `other * Data(param1=value1)` to attach parameter\n        configuration to a worker or callable function.\n\n        Parameters\n        ----------\n        other : Union[Callable, Worker]\n            The worker or callable to attach the data configuration to.\n\n        Returns\n        -------\n        Union[Callable, Worker]\n            The same object with the `__data__` attribute set.\n        \"\"\"\n        setattr(other, \"__data__\", self)\n        return other\n</code></pre>"},{"location":"reference/bridgic-asl/bridgic/asl/#bridgic.asl.Settings","title":"Settings  <code>dataclass</code>","text":"<p>Configuration settings for canvas objects (workers, elements, and canvases).</p> <p>This dataclass stores metadata about how a canvas object should be configured in the graph, including its key, start/output status, dependencies, and argument mapping rules.</p> <p>Attributes:</p> Name Type Description <code>key</code> <code>str</code> <p>The unique identifier for the canvas object. Defaults to KeyUnDifined if not set.</p> <code>args_mapping_rule</code> <code>ArgsMappingRule</code> <p>The rule for mapping arguments to this object. Defaults to ArgsMappingRule.AS_IS.</p> <code>result_dispatching_rule</code> <code>ResultDispatchingRule</code> <p>The rule for dispatching the result of this object to the next object. Defaults to ResultDispatchingRule.AS_IS.</p> Source code in <code>bridgic/asl/_canvas_object.py</code> <pre><code>@dataclass\nclass Settings:\n    \"\"\"\n    Configuration settings for canvas objects (workers, elements, and canvases).\n\n    This dataclass stores metadata about how a canvas object should be configured\n    in the graph, including its key, start/output status, dependencies, and argument\n    mapping rules.\n\n    Attributes\n    ----------\n    key : str\n        The unique identifier for the canvas object. Defaults to KeyUnDifined if not set.\n    args_mapping_rule : ArgsMappingRule\n        The rule for mapping arguments to this object. Defaults to ArgsMappingRule.AS_IS.\n    result_dispatching_rule : ResultDispatchingRule\n        The rule for dispatching the result of this object to the next object. Defaults to ResultDispatchingRule.AS_IS.\n    \"\"\"\n    key: str = None\n    args_mapping_rule: ArgsMappingRule = None\n    result_dispatching_rule: ResultDispatchingRule = None\n\n    def __post_init__(self):\n        \"\"\"\n        Initialize default values for Settings attributes after dataclass initialization.\n\n        This method is automatically called by the dataclass decorator and ensures that\n        all attributes have appropriate default values if they were not explicitly set.\n        \"\"\"\n        if not self.key:\n            self.key = KeyUnDifined()\n        if not self.args_mapping_rule:\n            self.args_mapping_rule = ArgsMappingRule.AS_IS\n        if not self.result_dispatching_rule:\n            self.result_dispatching_rule = ResultDispatchingRule.AS_IS\n\n    def __rmul__(self, other: Union[Callable, Worker]):\n        \"\"\"\n        Attach this Settings configuration to a worker or callable using right-multiplication.\n\n        This method allows syntax like `other * Settings(key=\"worker1\", result_dispatching_rule=ResultDispatchingRule.IN_ORDER)` to\n        attach configuration settings to a worker or callable function.\n\n        Parameters\n        ----------\n        other : Union[Callable, Worker]\n            The worker or callable to attach the settings to.\n\n        Returns\n        -------\n        Union[Callable, Worker]\n            The same object with the `__settings__` attribute set.\n        \"\"\"\n        setattr(other, \"__settings__\", self)\n        return other\n</code></pre>"},{"location":"reference/bridgic-asl/bridgic/asl/#bridgic.asl.ASLField","title":"ASLField","text":"<p>               Bases: <code>FieldInfo</code></p> <p>A custom Field class that extends Pydantic's FieldInfo with support for storing default type information.</p> <p>The <code>default_type</code> parameter is stored as metadata and does not automatically generate default values. You must explicitly provide a <code>default</code> value if you want a default.</p> Source code in <code>bridgic/asl/_canvas_object.py</code> <pre><code>class ASLField(FieldInfo):\n    \"\"\"\n    A custom Field class that extends Pydantic's FieldInfo with support for storing default type information.\n\n    The `default_type` parameter is stored as metadata and does not automatically generate default values.\n    You must explicitly provide a `default` value if you want a default.\n    \"\"\"\n\n    def __init__(\n        self,\n        type: Type[Any] = Any,\n        *,\n        default: Any = ...,\n        dispatching_rule: ResultDispatchingRule = ResultDispatchingRule.AS_IS,\n        **kwargs: Any\n    ):\n        \"\"\"\n        Initialize ASLField with optional default_type metadata.\n\n        Parameters\n        ----------\n        default : Any\n            Explicit default value. Must be provided if you want a default value.\n        type : Type[Any]\n            Type information stored as metadata. Does not automatically generate default values.\n        dispatching_rule : ResultDispatchingRule\n            The rule for dispatching the data to multiple workers.\n        **kwargs : Any\n            Other Field parameters (description, ge, le, etc.)\n        \"\"\"\n        super().__init__(default=default, **kwargs)\n        self.type = type\n        self.dispatching_rule = dispatching_rule\n</code></pre>"},{"location":"reference/bridgic-asl/bridgic/asl/#bridgic.asl.ASLCompilationError","title":"ASLCompilationError","text":"<p>               Bases: <code>Exception</code></p> <p>ASL code compilation error.</p> Source code in <code>bridgic/asl/_error.py</code> <pre><code>class ASLCompilationError(Exception):\n    \"\"\"\n    ASL code compilation error.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/bridgic-core/bridgic/core/","title":"core","text":""},{"location":"reference/bridgic-core/bridgic/core/#core","title":"core","text":""},{"location":"reference/bridgic-core/bridgic/core/agentic/","title":"agentic","text":""},{"location":"reference/bridgic-core/bridgic/core/agentic/#agentic","title":"agentic","text":"<p>The Agentic module provides core components for building intelligent agent systems.</p> <p>This module contains various Automa implementations for orchestrating and executing  LLM-based workflows or agents. These Automa implementations are typically  composed together to build complex intelligent agents with advanced capabilities.</p>"},{"location":"reference/bridgic-core/bridgic/core/agentic/#bridgic.core.agentic.ConcurrentAutoma","title":"ConcurrentAutoma","text":"<p>               Bases: <code>GraphAutoma</code></p> <p>This class is to provide concurrent execution of multiple workers.</p> <p>In accordance with the defined \"Concurrency Model of Worker\", each worker within  a ConcurrentAutoma can be configured to operate in one of two concurrency modes:</p> <ol> <li>Async Mode: Workers execute concurrently in an asynchronous fashion, driven  by the event loop of the main thread. This execution mode corresponds to the <code>arun()</code>  method of the Worker.</li> <li>Parallel Mode: Workers execute synchronously, each running in a dedicated  thread within a thread pool managed by the ConcurrentAutoma. This execution mode  corresponds to the <code>run()</code> method of the Worker.</li> </ol> <p>Upon completion of all worker tasks, the concurrent automa instance aggregates  the result outputs from each worker into a single list, which is then returned  to the caller.</p> Source code in <code>bridgic/core/agentic/_concurrent_automa.py</code> <pre><code>class ConcurrentAutoma(GraphAutoma):\n    \"\"\"\n    This class is to provide concurrent execution of multiple workers.\n\n    In accordance with the defined \"Concurrency Model of Worker\", each worker within \n    a ConcurrentAutoma can be configured to operate in one of two concurrency modes:\n\n    1. **Async Mode**: Workers execute concurrently in an asynchronous fashion, driven \n    by the event loop of the main thread. This execution mode corresponds to the `arun()` \n    method of the Worker.\n    2. **Parallel Mode**: Workers execute synchronously, each running in a dedicated \n    thread within a thread pool managed by the ConcurrentAutoma. This execution mode \n    corresponds to the `run()` method of the Worker.\n\n    Upon completion of all worker tasks, the concurrent automa instance aggregates \n    the result outputs from each worker into a single list, which is then returned \n    to the caller.\n    \"\"\"\n\n    # Automa type.\n    AUTOMA_TYPE: ClassVar[AutomaType] = AutomaType.Concurrent\n\n    _MERGER_WORKER_KEY: Final[str] = \"__merger__\"\n\n    def __init__(\n        self,\n        name: Optional[str] = None,\n        thread_pool: Optional[ThreadPoolExecutor] = None,\n        running_options: Optional[RunningOptions] = None,\n    ):\n        super().__init__(name=name, thread_pool=thread_pool, running_options=running_options)\n\n        # Implementation notes:\n        # There are two types of workers in the concurrent automa:\n        # 1. Concurrent workers: These workers will be concurrently executed with each other.\n        # 2. The Merger worker: This worker will merge the results of all the concurrent workers.\n\n        cls = type(self)\n        if cls.AUTOMA_TYPE == AutomaType.Concurrent:\n            # The _registered_worker_funcs data are from @worker decorators.\n            # Initialize the decorated concurrent workers.\n            for worker_key, worker_func in self._registered_worker_funcs.items():\n                super().add_func_as_worker(\n                    key=worker_key,\n                    func=worker_func,\n                    is_start=True,\n                )\n\n        # Add a hidden worker as the merger worker, which will merge the results of all the start workers.\n        super().add_func_as_worker(\n            key=self._MERGER_WORKER_KEY,\n            func=self._merge_workers_results,\n            dependencies=super().all_workers(),\n            is_output=True,\n            args_mapping_rule=ArgsMappingRule.MERGE,\n        )\n\n    def _merge_workers_results(self, results: List[Any]) -&gt; List[Any]:\n        return results\n\n    @override\n    def add_worker(\n        self,\n        key: str,\n        worker: Worker,\n        args_mapping_rule: ArgsMappingRule = ArgsMappingRule.AS_IS,\n        callback_builders: List[WorkerCallbackBuilder] = [],\n    ) -&gt; None:\n        \"\"\"\n        Add a concurrent worker to the concurrent automa. This worker will be concurrently executed with other concurrent workers.\n\n        Parameters\n        ----------\n        key : str\n            The key of the worker.\n        worker : Worker\n            The worker instance to be registered.\n        args_mapping_rule : ArgsMappingRule, default ArgsMappingRule.AS_IS\n            The rule for mapping input arguments to the worker.\n        callback_builders : List[WorkerCallbackBuilder], default []\n            The list of callback builders to be registered for the worker.\n        \"\"\"\n        if key == self._MERGER_WORKER_KEY:\n            raise AutomaRuntimeError(f\"the reserved key `{key}` is not allowed to be used by `add_worker()`\")\n        # Implementation notes:\n        # Concurrent workers are implemented as start workers in the underlying graph automa.\n        super().add_worker(key=key, worker=worker, is_start=True, args_mapping_rule=args_mapping_rule, callback_builders=callback_builders)\n        super().add_dependency(self._MERGER_WORKER_KEY, key)\n\n    @override\n    def add_func_as_worker(\n        self,\n        key: str,\n        func: Callable,\n        args_mapping_rule: ArgsMappingRule = ArgsMappingRule.AS_IS,\n        callback_builders: List[WorkerCallbackBuilder] = [],\n    ) -&gt; None:\n        \"\"\"\n        Add a function or method as a concurrent worker to the concurrent automa. This worker will be concurrently executed with other concurrent workers.\n\n        Parameters\n        ----------\n        key : str\n            The key of the function worker.\n        func : Callable\n            The function to be added as a concurrent worker to the automa.\n        args_mapping_rule : ArgsMappingRule, default ArgsMappingRule.AS_IS\n            The rule for mapping input arguments to the worker.\n        callback_builders : List[WorkerCallbackBuilder], default []\n            The list of callback builders to be registered for the worker.\n        \"\"\"\n        if key == self._MERGER_WORKER_KEY:\n            raise AutomaRuntimeError(f\"the reserved key `{key}` is not allowed to be used by `add_func_as_worker()`\")\n        # Implementation notes:\n        # Concurrent workers are implemented as start workers in the underlying graph automa.\n        super().add_func_as_worker(key=key, func=func, is_start=True, args_mapping_rule=args_mapping_rule, callback_builders=callback_builders)\n        super().add_dependency(self._MERGER_WORKER_KEY, key)\n\n    @override\n    def worker(\n        self,\n        *,\n        key: Optional[str] = None,\n        args_mapping_rule: ArgsMappingRule = ArgsMappingRule.AS_IS,\n        callback_builders: List[WorkerCallbackBuilder] = [],\n    ) -&gt; Callable:\n        \"\"\"\n        This is a decorator to mark a function or method as a concurrent worker of the concurrent automa. This worker will be concurrently executed with other concurrent workers.\n\n        Parameters\n        ----------\n        key : str\n            The key of the worker. If not provided, the name of the decorated callable will be used.\n        args_mapping_rule : ArgsMappingRule, default ArgsMappingRule.AS_IS\n            The rule for mapping input arguments to the worker.\n        callback_builders : List[WorkerCallbackBuilder], default []\n            The list of callback builders to be registered for the worker.\n        \"\"\"\n        if key == self._MERGER_WORKER_KEY:\n            raise AutomaRuntimeError(f\"the reserved key `{key}` is not allowed to be used by `automa.worker()`\")\n\n        super_automa = super()\n        def wrapper(func: Callable):\n            super_automa.add_func_as_worker(key=key, func=func, is_start=True, args_mapping_rule=args_mapping_rule, callback_builders=callback_builders)\n            super_automa.add_dependency(self._MERGER_WORKER_KEY, key)\n\n        return wrapper\n\n    @override\n    def remove_worker(self, key: str) -&gt; None:\n        \"\"\"\n        Remove a concurrent worker from the concurrent automa.\n\n        Parameters\n        ----------\n        key : str\n            The key of the worker to be removed.\n        \"\"\"\n        if key == self._MERGER_WORKER_KEY:\n            raise AutomaRuntimeError(f\"the merge worker is not allowed to be removed from the concurrent automa\")\n        super().remove_worker(key=key)\n\n    @override\n    def add_dependency(\n        self,\n        key: str,\n        dependency: str,\n    ) -&gt; None:\n        raise AutomaRuntimeError(f\"add_dependency() is not allowed to be called on a concurrent automa\")\n\n    def all_workers(self) -&gt; List[str]:\n        \"\"\"\n        Gets a list containing the keys of all concurrent workers registered in this concurrent automa.\n\n        Returns\n        -------\n        List[str]\n            A list of concurrent worker keys.\n        \"\"\"\n        keys_list = super().all_workers()\n        # Implementation notes:\n        # Hide the merger worker from the list of concurrent workers.\n        return list(filter(lambda key: key != self._MERGER_WORKER_KEY, keys_list))\n\n    def ferry_to(self, worker_key: str, /, *args, **kwargs):\n        raise AutomaRuntimeError(f\"ferry_to() is not allowed to be called on a concurrent automa\")\n\n    async def arun(\n        self, \n        *args: Tuple[Any, ...],\n        feedback_data: Optional[Union[InteractionFeedback, List[InteractionFeedback]]] = None,\n        **kwargs: Dict[str, Any]\n    ) -&gt; List[Any]:\n        result = await super().arun(\n            *args,\n            feedback_data=feedback_data,\n            **kwargs\n        )\n        return cast(List[Any], result)\n</code></pre>"},{"location":"reference/bridgic-core/bridgic/core/agentic/#bridgic.core.agentic.ConcurrentAutoma.add_worker","title":"add_worker","text":"<pre><code>add_worker(\n    key: str,\n    worker: Worker,\n    args_mapping_rule: ArgsMappingRule = AS_IS,\n    callback_builders: List[WorkerCallbackBuilder] = [],\n) -&gt; None\n</code></pre> <p>Add a concurrent worker to the concurrent automa. This worker will be concurrently executed with other concurrent workers.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>The key of the worker.</p> required <code>worker</code> <code>Worker</code> <p>The worker instance to be registered.</p> required <code>args_mapping_rule</code> <code>ArgsMappingRule</code> <p>The rule for mapping input arguments to the worker.</p> <code>ArgsMappingRule.AS_IS</code> <code>callback_builders</code> <code>List[WorkerCallbackBuilder]</code> <p>The list of callback builders to be registered for the worker.</p> <code>[]</code> Source code in <code>bridgic/core/agentic/_concurrent_automa.py</code> <pre><code>@override\ndef add_worker(\n    self,\n    key: str,\n    worker: Worker,\n    args_mapping_rule: ArgsMappingRule = ArgsMappingRule.AS_IS,\n    callback_builders: List[WorkerCallbackBuilder] = [],\n) -&gt; None:\n    \"\"\"\n    Add a concurrent worker to the concurrent automa. This worker will be concurrently executed with other concurrent workers.\n\n    Parameters\n    ----------\n    key : str\n        The key of the worker.\n    worker : Worker\n        The worker instance to be registered.\n    args_mapping_rule : ArgsMappingRule, default ArgsMappingRule.AS_IS\n        The rule for mapping input arguments to the worker.\n    callback_builders : List[WorkerCallbackBuilder], default []\n        The list of callback builders to be registered for the worker.\n    \"\"\"\n    if key == self._MERGER_WORKER_KEY:\n        raise AutomaRuntimeError(f\"the reserved key `{key}` is not allowed to be used by `add_worker()`\")\n    # Implementation notes:\n    # Concurrent workers are implemented as start workers in the underlying graph automa.\n    super().add_worker(key=key, worker=worker, is_start=True, args_mapping_rule=args_mapping_rule, callback_builders=callback_builders)\n    super().add_dependency(self._MERGER_WORKER_KEY, key)\n</code></pre>"},{"location":"reference/bridgic-core/bridgic/core/agentic/#bridgic.core.agentic.ConcurrentAutoma.add_func_as_worker","title":"add_func_as_worker","text":"<pre><code>add_func_as_worker(\n    key: str,\n    func: Callable,\n    args_mapping_rule: ArgsMappingRule = AS_IS,\n    callback_builders: List[WorkerCallbackBuilder] = [],\n) -&gt; None\n</code></pre> <p>Add a function or method as a concurrent worker to the concurrent automa. This worker will be concurrently executed with other concurrent workers.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>The key of the function worker.</p> required <code>func</code> <code>Callable</code> <p>The function to be added as a concurrent worker to the automa.</p> required <code>args_mapping_rule</code> <code>ArgsMappingRule</code> <p>The rule for mapping input arguments to the worker.</p> <code>ArgsMappingRule.AS_IS</code> <code>callback_builders</code> <code>List[WorkerCallbackBuilder]</code> <p>The list of callback builders to be registered for the worker.</p> <code>[]</code> Source code in <code>bridgic/core/agentic/_concurrent_automa.py</code> <pre><code>@override\ndef add_func_as_worker(\n    self,\n    key: str,\n    func: Callable,\n    args_mapping_rule: ArgsMappingRule = ArgsMappingRule.AS_IS,\n    callback_builders: List[WorkerCallbackBuilder] = [],\n) -&gt; None:\n    \"\"\"\n    Add a function or method as a concurrent worker to the concurrent automa. This worker will be concurrently executed with other concurrent workers.\n\n    Parameters\n    ----------\n    key : str\n        The key of the function worker.\n    func : Callable\n        The function to be added as a concurrent worker to the automa.\n    args_mapping_rule : ArgsMappingRule, default ArgsMappingRule.AS_IS\n        The rule for mapping input arguments to the worker.\n    callback_builders : List[WorkerCallbackBuilder], default []\n        The list of callback builders to be registered for the worker.\n    \"\"\"\n    if key == self._MERGER_WORKER_KEY:\n        raise AutomaRuntimeError(f\"the reserved key `{key}` is not allowed to be used by `add_func_as_worker()`\")\n    # Implementation notes:\n    # Concurrent workers are implemented as start workers in the underlying graph automa.\n    super().add_func_as_worker(key=key, func=func, is_start=True, args_mapping_rule=args_mapping_rule, callback_builders=callback_builders)\n    super().add_dependency(self._MERGER_WORKER_KEY, key)\n</code></pre>"},{"location":"reference/bridgic-core/bridgic/core/agentic/#bridgic.core.agentic.ConcurrentAutoma.worker","title":"worker","text":"<pre><code>worker(\n    *,\n    key: Optional[str] = None,\n    args_mapping_rule: ArgsMappingRule = AS_IS,\n    callback_builders: List[WorkerCallbackBuilder] = []\n) -&gt; Callable\n</code></pre> <p>This is a decorator to mark a function or method as a concurrent worker of the concurrent automa. This worker will be concurrently executed with other concurrent workers.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>The key of the worker. If not provided, the name of the decorated callable will be used.</p> <code>None</code> <code>args_mapping_rule</code> <code>ArgsMappingRule</code> <p>The rule for mapping input arguments to the worker.</p> <code>ArgsMappingRule.AS_IS</code> <code>callback_builders</code> <code>List[WorkerCallbackBuilder]</code> <p>The list of callback builders to be registered for the worker.</p> <code>[]</code> Source code in <code>bridgic/core/agentic/_concurrent_automa.py</code> <pre><code>@override\ndef worker(\n    self,\n    *,\n    key: Optional[str] = None,\n    args_mapping_rule: ArgsMappingRule = ArgsMappingRule.AS_IS,\n    callback_builders: List[WorkerCallbackBuilder] = [],\n) -&gt; Callable:\n    \"\"\"\n    This is a decorator to mark a function or method as a concurrent worker of the concurrent automa. This worker will be concurrently executed with other concurrent workers.\n\n    Parameters\n    ----------\n    key : str\n        The key of the worker. If not provided, the name of the decorated callable will be used.\n    args_mapping_rule : ArgsMappingRule, default ArgsMappingRule.AS_IS\n        The rule for mapping input arguments to the worker.\n    callback_builders : List[WorkerCallbackBuilder], default []\n        The list of callback builders to be registered for the worker.\n    \"\"\"\n    if key == self._MERGER_WORKER_KEY:\n        raise AutomaRuntimeError(f\"the reserved key `{key}` is not allowed to be used by `automa.worker()`\")\n\n    super_automa = super()\n    def wrapper(func: Callable):\n        super_automa.add_func_as_worker(key=key, func=func, is_start=True, args_mapping_rule=args_mapping_rule, callback_builders=callback_builders)\n        super_automa.add_dependency(self._MERGER_WORKER_KEY, key)\n\n    return wrapper\n</code></pre>"},{"location":"reference/bridgic-core/bridgic/core/agentic/#bridgic.core.agentic.ConcurrentAutoma.remove_worker","title":"remove_worker","text":"<pre><code>remove_worker(key: str) -&gt; None\n</code></pre> <p>Remove a concurrent worker from the concurrent automa.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>The key of the worker to be removed.</p> required Source code in <code>bridgic/core/agentic/_concurrent_automa.py</code> <pre><code>@override\ndef remove_worker(self, key: str) -&gt; None:\n    \"\"\"\n    Remove a concurrent worker from the concurrent automa.\n\n    Parameters\n    ----------\n    key : str\n        The key of the worker to be removed.\n    \"\"\"\n    if key == self._MERGER_WORKER_KEY:\n        raise AutomaRuntimeError(f\"the merge worker is not allowed to be removed from the concurrent automa\")\n    super().remove_worker(key=key)\n</code></pre>"},{"location":"reference/bridgic-core/bridgic/core/agentic/#bridgic.core.agentic.ConcurrentAutoma.all_workers","title":"all_workers","text":"<pre><code>all_workers() -&gt; List[str]\n</code></pre> <p>Gets a list containing the keys of all concurrent workers registered in this concurrent automa.</p> <p>Returns:</p> Type Description <code>List[str]</code> <p>A list of concurrent worker keys.</p> Source code in <code>bridgic/core/agentic/_concurrent_automa.py</code> <pre><code>def all_workers(self) -&gt; List[str]:\n    \"\"\"\n    Gets a list containing the keys of all concurrent workers registered in this concurrent automa.\n\n    Returns\n    -------\n    List[str]\n        A list of concurrent worker keys.\n    \"\"\"\n    keys_list = super().all_workers()\n    # Implementation notes:\n    # Hide the merger worker from the list of concurrent workers.\n    return list(filter(lambda key: key != self._MERGER_WORKER_KEY, keys_list))\n</code></pre>"},{"location":"reference/bridgic-core/bridgic/core/agentic/#bridgic.core.agentic.SequentialAutoma","title":"SequentialAutoma","text":"<p>               Bases: <code>GraphAutoma</code></p> <p>This class is to provide an easy way to orchestrate workers in a strictly  sequential manner.</p> <p>Each worker within the SequentialAutoma is invoked in the precise order determined  by their positional index, ensuring a linear workflow where the output of one worker  can serve as the input to the next.</p> <p>Upon the completion of all registered workers, the SequentialAutoma returns the output  produced by the final worker in the sequence as the overall result to the caller. This  design enforces ordered, step-wise processing, making the SequentialAutoma particularly  suitable for use cases that require strict procedural dependencies among constituent tasks.</p> Source code in <code>bridgic/core/agentic/_sequential_automa.py</code> <pre><code>class SequentialAutoma(GraphAutoma):\n    \"\"\"\n    This class is to provide an easy way to orchestrate workers in a strictly \n    sequential manner.\n\n    Each worker within the SequentialAutoma is invoked in the precise order determined \n    by their positional index, ensuring a linear workflow where the output of one worker \n    can serve as the input to the next.\n\n    Upon the completion of all registered workers, the SequentialAutoma returns the output \n    produced by the final worker in the sequence as the overall result to the caller. This \n    design enforces ordered, step-wise processing, making the SequentialAutoma particularly \n    suitable for use cases that require strict procedural dependencies among constituent tasks.\n    \"\"\"\n\n    # Automa type.\n    AUTOMA_TYPE: ClassVar[AutomaType] = AutomaType.Sequential\n\n    _TAIL_WORKER_KEY: Final[str] = \"__tail__\"\n    _last_worker_key: Optional[str]\n\n    def __init__(\n        self,\n        name: Optional[str] = None,\n        thread_pool: Optional[ThreadPoolExecutor] = None,\n        running_options: Optional[RunningOptions] = None,\n    ):\n        super().__init__(name=name, thread_pool=thread_pool, running_options=running_options)\n\n        cls = type(self)\n        self._last_worker_key = None\n        if cls.AUTOMA_TYPE == AutomaType.Sequential:\n            # The _registered_worker_funcs data are from @worker decorators.\n            # Initialize the decorated sequential workers.\n            for worker_key, worker_func in self._registered_worker_funcs.items():\n                is_start = self._last_worker_key is None\n                dependencies = [] if self._last_worker_key is None else [self._last_worker_key]\n                super().add_func_as_worker(\n                    key=worker_key,\n                    func=worker_func,\n                    dependencies=dependencies,\n                    is_start=is_start,\n                    args_mapping_rule=worker_func.__args_mapping_rule__,\n                )\n                self._last_worker_key = worker_key\n\n        if self._last_worker_key is not None:\n            # Add a hidden worker as the tail worker.\n            super().add_func_as_worker(\n                key=self._TAIL_WORKER_KEY,\n                func=self._tail_worker,\n                dependencies=[self._last_worker_key],\n                is_output=True,\n                args_mapping_rule=ArgsMappingRule.AS_IS,\n            )\n\n    def _tail_worker(self, result: Any) -&gt; Any:\n        # Return the result of the last worker without any modification.\n        return result\n\n    @override\n    def dump_to_dict(self) -&gt; Dict[str, Any]:\n        state_dict = super().dump_to_dict()\n        state_dict[\"last_worker_key\"] = self._last_worker_key\n        return state_dict\n\n    @override\n    def load_from_dict(self, state_dict: Dict[str, Any]) -&gt; None:\n        super().load_from_dict(state_dict)\n        self._last_worker_key = state_dict[\"last_worker_key\"]\n\n    def __add_worker_internal(\n        self,\n        key: str,\n        func_or_worker: Union[Callable, Worker],\n        *,\n        args_mapping_rule: ArgsMappingRule = ArgsMappingRule.AS_IS,\n    ) -&gt; None:\n        is_start = self._last_worker_key is None\n        dependencies = [] if self._last_worker_key is None else [self._last_worker_key]\n        if isinstance(func_or_worker, Callable):\n            super().add_func_as_worker(\n                key=key, \n                func=func_or_worker,\n                dependencies=dependencies,\n                is_start=is_start,\n                args_mapping_rule=args_mapping_rule,\n            )\n        else:\n            super().add_worker(\n                key=key, \n                worker=func_or_worker,\n                dependencies=dependencies,\n                is_start=is_start,\n                args_mapping_rule=args_mapping_rule,\n            )\n        if self._last_worker_key is not None:\n            # Remove the old hidden tail worker.\n            super().remove_worker(self._TAIL_WORKER_KEY)\n\n        # Add a new hidden tail worker.\n        self._last_worker_key = key\n        super().add_func_as_worker(\n            key=self._TAIL_WORKER_KEY,\n            func=self._tail_worker,\n            dependencies=[self._last_worker_key],\n            is_output=True,\n            args_mapping_rule=ArgsMappingRule.AS_IS,\n        )\n\n    @override\n    def add_worker(\n        self,\n        key: str,\n        worker: Worker,\n        *,\n        args_mapping_rule: ArgsMappingRule = ArgsMappingRule.AS_IS,\n    ) -&gt; None:\n        \"\"\"\n        Add a sequential worker to the sequential automa at the end of the automa.\n\n        Parameters\n        ----------\n        key : str\n            The key of the worker.\n        worker : Worker\n            The worker instance to be registered.\n        args_mapping_rule : ArgsMappingRule\n            The rule of arguments mapping.\n        \"\"\"\n        if key == self._TAIL_WORKER_KEY:\n            raise AutomaRuntimeError(f\"the reserved key `{key}` is not allowed to be used by `add_worker()`\")\n\n        self.__add_worker_internal(\n            key, \n            worker, \n            args_mapping_rule=args_mapping_rule\n        )\n\n    @override\n    def add_func_as_worker(\n        self,\n        key: str,\n        func: Callable,\n        *,\n        args_mapping_rule: ArgsMappingRule = ArgsMappingRule.AS_IS,\n    ) -&gt; None:\n        \"\"\"\n        Add a function or method as a sequential worker to the sequential automa at the end of the automa.\n\n        Parameters\n        ----------\n        key : str\n            The key of the worker.\n        func : Callable\n            The function to be added as a sequential worker to the automa.\n        args_mapping_rule : ArgsMappingRule\n            The rule of arguments mapping.\n        \"\"\"\n        if key == self._TAIL_WORKER_KEY:\n            raise AutomaRuntimeError(f\"the reserved key `{key}` is not allowed to be used by `add_func_as_worker()`\")\n\n        self.__add_worker_internal(\n            key, \n            func, \n            args_mapping_rule=args_mapping_rule\n        )\n\n    @override\n    def worker(\n        self,\n        *,\n        key: Optional[str] = None,\n        args_mapping_rule: ArgsMappingRule = ArgsMappingRule.AS_IS,\n    ) -&gt; Callable:\n        \"\"\"\n        This is a decorator to mark a function or method as a sequential worker of the sequential automa, at the end of the automa.\n\n        Parameters\n        ----------\n        key : str\n            The key of the worker. If not provided, the name of the decorated callable will be used.\n        args_mapping_rule: ArgsMappingRule\n            The rule of arguments mapping.\n        \"\"\"\n        if key == self._TAIL_WORKER_KEY:\n            raise AutomaRuntimeError(f\"the reserved key `{key}` is not allowed to be used by `automa.worker()`\")\n\n        def wrapper(func: Callable):\n            self.__add_worker_internal(\n                key, \n                func, \n                args_mapping_rule=args_mapping_rule\n            )\n\n        return wrapper\n\n    @override\n    def remove_worker(self, key: str) -&gt; None:\n        raise AutomaRuntimeError(f\"remove_worker() is not allowed to be called on a sequential automa\")\n\n    @override\n    def add_dependency(\n        self,\n        key: str,\n        depends: str,\n    ) -&gt; None:\n        raise AutomaRuntimeError(f\"add_dependency() is not allowed to be called on a sequential automa\")\n\n    def ferry_to(self, worker_key: str, /, *args, **kwargs):\n        raise AutomaRuntimeError(f\"ferry_to() is not allowed to be called on a sequential automa\")\n</code></pre>"},{"location":"reference/bridgic-core/bridgic/core/agentic/#bridgic.core.agentic.SequentialAutoma.add_worker","title":"add_worker","text":"<pre><code>add_worker(\n    key: str,\n    worker: Worker,\n    *,\n    args_mapping_rule: ArgsMappingRule = AS_IS\n) -&gt; None\n</code></pre> <p>Add a sequential worker to the sequential automa at the end of the automa.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>The key of the worker.</p> required <code>worker</code> <code>Worker</code> <p>The worker instance to be registered.</p> required <code>args_mapping_rule</code> <code>ArgsMappingRule</code> <p>The rule of arguments mapping.</p> <code>AS_IS</code> Source code in <code>bridgic/core/agentic/_sequential_automa.py</code> <pre><code>@override\ndef add_worker(\n    self,\n    key: str,\n    worker: Worker,\n    *,\n    args_mapping_rule: ArgsMappingRule = ArgsMappingRule.AS_IS,\n) -&gt; None:\n    \"\"\"\n    Add a sequential worker to the sequential automa at the end of the automa.\n\n    Parameters\n    ----------\n    key : str\n        The key of the worker.\n    worker : Worker\n        The worker instance to be registered.\n    args_mapping_rule : ArgsMappingRule\n        The rule of arguments mapping.\n    \"\"\"\n    if key == self._TAIL_WORKER_KEY:\n        raise AutomaRuntimeError(f\"the reserved key `{key}` is not allowed to be used by `add_worker()`\")\n\n    self.__add_worker_internal(\n        key, \n        worker, \n        args_mapping_rule=args_mapping_rule\n    )\n</code></pre>"},{"location":"reference/bridgic-core/bridgic/core/agentic/#bridgic.core.agentic.SequentialAutoma.add_func_as_worker","title":"add_func_as_worker","text":"<pre><code>add_func_as_worker(\n    key: str,\n    func: Callable,\n    *,\n    args_mapping_rule: ArgsMappingRule = AS_IS\n) -&gt; None\n</code></pre> <p>Add a function or method as a sequential worker to the sequential automa at the end of the automa.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>The key of the worker.</p> required <code>func</code> <code>Callable</code> <p>The function to be added as a sequential worker to the automa.</p> required <code>args_mapping_rule</code> <code>ArgsMappingRule</code> <p>The rule of arguments mapping.</p> <code>AS_IS</code> Source code in <code>bridgic/core/agentic/_sequential_automa.py</code> <pre><code>@override\ndef add_func_as_worker(\n    self,\n    key: str,\n    func: Callable,\n    *,\n    args_mapping_rule: ArgsMappingRule = ArgsMappingRule.AS_IS,\n) -&gt; None:\n    \"\"\"\n    Add a function or method as a sequential worker to the sequential automa at the end of the automa.\n\n    Parameters\n    ----------\n    key : str\n        The key of the worker.\n    func : Callable\n        The function to be added as a sequential worker to the automa.\n    args_mapping_rule : ArgsMappingRule\n        The rule of arguments mapping.\n    \"\"\"\n    if key == self._TAIL_WORKER_KEY:\n        raise AutomaRuntimeError(f\"the reserved key `{key}` is not allowed to be used by `add_func_as_worker()`\")\n\n    self.__add_worker_internal(\n        key, \n        func, \n        args_mapping_rule=args_mapping_rule\n    )\n</code></pre>"},{"location":"reference/bridgic-core/bridgic/core/agentic/#bridgic.core.agentic.SequentialAutoma.worker","title":"worker","text":"<pre><code>worker(\n    *,\n    key: Optional[str] = None,\n    args_mapping_rule: ArgsMappingRule = AS_IS\n) -&gt; Callable\n</code></pre> <p>This is a decorator to mark a function or method as a sequential worker of the sequential automa, at the end of the automa.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>The key of the worker. If not provided, the name of the decorated callable will be used.</p> <code>None</code> <code>args_mapping_rule</code> <code>ArgsMappingRule</code> <p>The rule of arguments mapping.</p> <code>AS_IS</code> Source code in <code>bridgic/core/agentic/_sequential_automa.py</code> <pre><code>@override\ndef worker(\n    self,\n    *,\n    key: Optional[str] = None,\n    args_mapping_rule: ArgsMappingRule = ArgsMappingRule.AS_IS,\n) -&gt; Callable:\n    \"\"\"\n    This is a decorator to mark a function or method as a sequential worker of the sequential automa, at the end of the automa.\n\n    Parameters\n    ----------\n    key : str\n        The key of the worker. If not provided, the name of the decorated callable will be used.\n    args_mapping_rule: ArgsMappingRule\n        The rule of arguments mapping.\n    \"\"\"\n    if key == self._TAIL_WORKER_KEY:\n        raise AutomaRuntimeError(f\"the reserved key `{key}` is not allowed to be used by `automa.worker()`\")\n\n    def wrapper(func: Callable):\n        self.__add_worker_internal(\n            key, \n            func, \n            args_mapping_rule=args_mapping_rule\n        )\n\n    return wrapper\n</code></pre>"},{"location":"reference/bridgic-core/bridgic/core/agentic/recent/","title":"recent","text":""},{"location":"reference/bridgic-core/bridgic/core/agentic/recent/#recent","title":"recent","text":"<p>The module provides core components for the ReCENT memory management Algorithm.</p> <p>ReCENT Algorithm (Recursive Compressed Episodic Node Tree Algorithm) is an algorithm  designed to address issues such as context explosion and goal drift, by employing a recursive  memory compression mechanism to compress the memory when necessary.</p> <p>This module provides an agentic automa and its corresponding memory and task configurations:</p> <ul> <li><code>ReCentAutoma</code>: The main automaton that implements the ReCENT algorithm.</li> <li><code>ReCentMemoryConfig</code>: Configuration for ReCENT memory management.</li> <li><code>ObservationTaskConfig</code>: Configuration for the observation task.</li> <li><code>ToolTaskConfig</code>: Configuration for the tool selection task.</li> <li><code>AnswerTaskConfig</code>: Configuration for the answer generation task.</li> <li><code>StopCondition</code>: Stop condition configuration for ReCentAutoma.</li> </ul> <p>The core data structures are:</p> <ul> <li><code>EpisodicNodeTree</code>: Tree of episodic nodes which is the core data structure of ReCENT.</li> <li><code>BaseEpisodicNode</code>: Base class for all episodic nodes. It is inherited by:<ul> <li><code>GoalEpisodicNode</code>: A goal node that represents the goal of the agent.</li> <li><code>LeafEpisodicNode</code>: A leaf node that represents a sequence of messages.</li> <li><code>CompressionEpisodicNode</code>: A compression node that summarizes a sequence of episodic nodes.</li> </ul> </li> </ul>"},{"location":"reference/bridgic-core/bridgic/core/agentic/recent/#bridgic.core.agentic.recent.ReCentAutoma","title":"ReCentAutoma","text":"<p>               Bases: <code>GraphAutoma</code></p> <p>ReCentAutoma is an automa that implements a ReAct-like process, leveraging the ReCENT memory  algorithm to support stronger autonomous next-step planning, thus better achieving the pre-set goal.</p> <p>This automa extends GraphAutoma to provide a memory-aware agentic automa that: - Maintains episodic memory with compression capabilities - Supports goal-oriented task execution - Dynamically creates tool workers based on LLM decisions - Manages memory compression to prevent context explosion</p> <p>Parameters:</p> Name Type Description Default <code>llm</code> <code>BaseLlm</code> <p>The LLM that serves as the default LLM for all tasks (if a dedicated LLM is not configured for a specific task).</p> required <code>tools</code> <code>Optional[List[Union[Callable, Automa, ToolSpec]]]</code> <p>List of tools available to the automa. Can be functions, Automa instances, or ToolSpec instances.</p> <code>None</code> <code>tools_builders</code> <code>Optional[List[ToolSetBuilder]]</code> <p>List of <code>ToolSetBuilder</code> instances used to dynamically create <code>ToolSpec</code> instances at initialization. This is useful when the automa needs to exclusive access to a resource-like tool set, like browser, terminal, etc.</p> <code>None</code> <code>stop_condition</code> <code>Optional[StopCondition]</code> <p>Stop condition configuration. If None, uses default configuration: - max_iteration: -1 - max_consecutive_no_tool_selected: 3</p> <code>None</code> <code>memory_config</code> <code>Optional[ReCentMemoryConfig]</code> <p>Memory configuration for ReCent memory management. If None, a default config will be created using the provided llm.</p> <code>None</code> <code>observation_task_config</code> <code>Optional[ObservationTaskConfig]</code> <p>Configuration for the observation task. If None, uses default config with the provided <code>llm</code>. If provided but system_template or instruction_template is None, will use default templates.</p> <code>None</code> <code>tool_task_config</code> <code>Optional[ToolTaskConfig]</code> <p>Configuration for the tool selection task. If None, uses default config with the provided <code>llm</code>. If provided but system_template or instruction_template is None, will use default templates.</p> <code>None</code> <code>answer_task_config</code> <code>Optional[AnswerTaskConfig]</code> <p>Configuration for the answer generation task. If None, uses default config with the provided <code>llm</code>. If provided but system_template or instruction_template is None, will use default templates.</p> <code>None</code> <code>name</code> <code>Optional[str]</code> <p>The name of the automa instance.</p> <code>None</code> <code>thread_pool</code> <code>Optional[ThreadPoolExecutor]</code> <p>The thread pool for parallel execution of I/O-bound or CPU-bound tasks.</p> <code>None</code> <code>running_options</code> <code>Optional[RunningOptions]</code> <p>The running options for the automa instance.</p> <code>None</code> Source code in <code>bridgic/core/agentic/recent/_recent_automa.py</code> <pre><code>class ReCentAutoma(GraphAutoma):\n    \"\"\"\n    ReCentAutoma is an automa that implements a ReAct-like process, leveraging the ReCENT memory \n    algorithm to support stronger autonomous next-step planning, thus better achieving the pre-set goal.\n\n    This automa extends GraphAutoma to provide a memory-aware agentic automa that:\n    - Maintains episodic memory with compression capabilities\n    - Supports goal-oriented task execution\n    - Dynamically creates tool workers based on LLM decisions\n    - Manages memory compression to prevent context explosion\n\n    Parameters\n    ----------\n    llm : BaseLlm\n        The LLM that serves as the default LLM for all tasks (if a dedicated LLM is not configured for a specific task).\n    tools : Optional[List[Union[Callable, Automa, ToolSpec]]]\n        List of tools available to the automa. Can be functions, Automa instances, or ToolSpec instances.\n    tools_builders : Optional[List[ToolSetBuilder]]\n        List of `ToolSetBuilder` instances used to dynamically create `ToolSpec` instances at initialization.\n        This is useful when the automa needs to exclusive access to a resource-like tool set, like browser, terminal, etc.\n    stop_condition : Optional[StopCondition]\n        Stop condition configuration. If None, uses default configuration:\n        - max_iteration: -1\n        - max_consecutive_no_tool_selected: 3\n    memory_config : Optional[ReCentMemoryConfig]\n        Memory configuration for ReCent memory management. If None, a default config will be created using the provided llm.\n    observation_task_config : Optional[ObservationTaskConfig]\n        Configuration for the observation task. If None, uses default config with the provided `llm`.\n        If provided but system_template or instruction_template is None, will use default templates.\n    tool_task_config : Optional[ToolTaskConfig]\n        Configuration for the tool selection task. If None, uses default config with the provided `llm`.\n        If provided but system_template or instruction_template is None, will use default templates.\n    answer_task_config : Optional[AnswerTaskConfig]\n        Configuration for the answer generation task. If None, uses default config with the provided `llm`.\n        If provided but system_template or instruction_template is None, will use default templates.\n    name : Optional[str]\n        The name of the automa instance.\n    thread_pool : Optional[ThreadPoolExecutor]\n        The thread pool for parallel execution of I/O-bound or CPU-bound tasks.\n    running_options : Optional[RunningOptions]\n        The running options for the automa instance.\n    \"\"\"\n\n    _llm: BaseLlm\n    \"\"\"The main LLM which is used as fallback for the essential tasks.\"\"\"\n\n    _tool_specs: Optional[List[ToolSpec]]\n    \"\"\"List of tool specifications available to the automa.\"\"\"\n\n    _tools_builders: Optional[List[ToolSetBuilder]]\n    \"\"\"List of builders used to dynamically create ToolSpec instances.\"\"\"\n\n    _memory_manager: ReCentMemoryManager\n    \"\"\"Memory manager instance for managing episodic memory.\"\"\"\n\n    _observation_task_config: LlmTaskConfig\n    \"\"\"Configuration for the observation task.\"\"\"\n\n    _tool_task_config: LlmTaskConfig\n    \"\"\"Configuration for the tool selection task.\"\"\"\n\n    _answer_task_config: LlmTaskConfig\n    \"\"\"Configuration for the answer generation task.\"\"\"\n\n    _stop_condition: StopCondition\n    \"\"\"Stop condition configuration.\"\"\"\n\n    def __init__(\n        self,\n        llm: BaseLlm,\n        tools: Optional[List[Union[Callable, Automa, ToolSpec]]] = None,\n        tools_builders: Optional[List[ToolSetBuilder]] = None,\n        stop_condition: Optional[StopCondition] = None,\n        memory_config: Optional[ReCentMemoryConfig] = None,\n        observation_task_config: Optional[ObservationTaskConfig] = None,\n        tool_task_config: Optional[ToolTaskConfig] = None,\n        answer_task_config: Optional[AnswerTaskConfig] = None,\n        name: Optional[str] = None,\n        thread_pool: Optional[ThreadPoolExecutor] = None,\n        running_options: Optional[RunningOptions] = None,\n    ):\n        super().__init__(name=name, thread_pool=thread_pool, running_options=running_options)\n        self._llm = llm\n\n        # Initialize tool specs from direct tools.\n        self._tool_specs = [self._ensure_tool_spec(tool) for tool in tools or []]\n\n        # Initialize tools_builders and create ToolSpec instances from them.\n        self._tools_builders = tools_builders or []\n        if self._tools_builders:\n            for builder in self._tools_builders:\n                response = builder.build()\n                self._tool_specs.extend(response.get(\"tool_specs\", []))\n\n        # Initialize memory manager with memory config.\n        memory_config = memory_config or ReCentMemoryConfig(llm=llm)\n        self._memory_manager = ReCentMemoryManager(compression_config=memory_config)\n\n        # Initialize task-specific configs with defaults if not provided.\n        if observation_task_config is None:\n            observation_task_config = ObservationTaskConfig(llm=self._llm)\n        self._observation_task_config = observation_task_config.to_llm_task_config()\n\n        if tool_task_config is None:\n            tool_task_config = ToolTaskConfig(llm=self._llm)\n        self._tool_task_config = tool_task_config.to_llm_task_config()\n\n        if answer_task_config is None:\n            answer_task_config = AnswerTaskConfig(llm=self._llm)\n        self._answer_task_config = answer_task_config.to_llm_task_config()\n\n        # Initialize stop condition with defaults if not provided.\n        self._stop_condition = stop_condition or StopCondition()\n\n    def _ensure_tool_spec(self, tool: Union[Callable, Automa, ToolSpec]) -&gt; ToolSpec:\n        if isinstance(tool, ToolSpec):\n            return tool\n        elif isinstance(tool, type) and issubclass(tool, Automa):\n            return AutomaToolSpec.from_raw(tool)\n        elif isinstance(tool, Callable):\n            # Note: this test against `Callable` should be placed at last.\n            return FunctionToolSpec.from_raw(tool)\n        else:\n            raise TypeError(f\"Invalid tool type: {type(tool)} detected, expected `Callable`, `Automa`, or `ToolSpec`.\")\n\n    @worker(is_start=True)\n    async def initialize_task_goal(\n        self,\n        goal: str,\n        guidance: Optional[str] = None,\n    ):\n        \"\"\"\n        Initialize the goal of the task and start the automa.\n\n        This worker is the entry point of the automa. It creates a goal node as the first episodic \n        node in the memory sequence and optionally pushes initial user messages.\n\n        Parameters\n        ----------\n        goal : str\n            The task goal.\n        guidance : Optional[str]\n            The guidance for achieving the task goal.\n        \"\"\"\n        if not goal:\n            raise ValueError(\"Goal cannot be empty.\")\n        self._memory_manager.create_goal(goal, guidance)\n\n        # Log task goal initialization in debug mode.\n        top_options = self._get_top_running_options()\n        if top_options.debug:\n            msg = (\n                f\"[{type(self).__name__}]-[{self.name}] \ud83c\udfaf Task Goal\\n\"\n                f\"{goal}\" + (f\"\\n\\n{guidance}\" if guidance else '')\n            )\n            printer.print(msg)\n\n        self.ferry_to(\"observe\")\n\n    @worker()\n    async def observe(self, rtx = System(\"runtime_context\")):\n        \"\"\"\n        Observe the current state and determine if the goal has been achieved.\n\n        This worker builds context from memory, uses LLM (with `StructuredOutput` protocol) to \n        determine if the goal has been achieved, and routes accordingly.\n        \"\"\"\n        local_space = self.get_local_space(rtx)\n\n        # 0. If iteration count is bigger than max_iteration, route to finalize_answer and stop.\n        iteration_cnt = local_space.get(\"iteration_cnt\", 0) + 1\n        local_space[\"iteration_cnt\"] = iteration_cnt\n\n        if self._stop_condition.max_iteration &gt;= 0:\n            if iteration_cnt &gt; self._stop_condition.max_iteration:\n                self.ferry_to(\"finalize_answer\")\n                return\n\n        # 1. Build context from memory.\n        context: ReCentContext = await self._memory_manager.abuild_context()\n\n        # 2. Build LLM message list for goal status evaluation.\n        messages: List[Message] = []\n\n        # Add system message for goal evaluation (if configured).\n        if self._observation_task_config.system_template is not None:\n            system_message = self._observation_task_config.system_template.format_message(\n                role=Role.SYSTEM,\n                goal_content=context[\"goal_content\"],\n                goal_guidance=context[\"goal_guidance\"],\n            )\n            messages.append(system_message)\n\n        # Add memory messages (observation history).\n        if context[\"memory_messages\"]:\n            messages.extend(context[\"memory_messages\"])\n\n        # Add instruction message (if configured).\n        if self._observation_task_config.instruction_template is not None:\n            instruction_message = self._observation_task_config.instruction_template.format_message(role=Role.USER)\n            messages.append(instruction_message)\n\n        # 3. Call LLM with StructuredOutput to get goal status.\n        observe_llm = self._observation_task_config.llm\n        if not isinstance(observe_llm, StructuredOutput):\n            raise TypeError(f\"LLM must support StructuredOutput protocol, but {type(observe_llm)} does not.\")\n\n        goal_status: GoalStatus = await observe_llm.astructured_output(\n            messages=messages,\n            constraint=PydanticModel(model=GoalStatus),\n        )\n        observation_template = EjinjaPromptTemplate(\n            \"Goal Status:\\n\"\n            \"- Achieved: {%- if goal_status.goal_achieved %}goal_status.goal_achieved{% else %}No{% endif %}\\n\"\n            \"- Thinking: {%- if goal_status.brief_thinking %}{{ goal_status.brief_thinking }}{% else %}(No thinking){% endif %}\"\n        )\n        observation_message = observation_template.format_message(\n            role=Role.AI,\n            goal_status=goal_status,\n        )\n        self._memory_manager.push_messages([observation_message])\n\n        # Log observation result in debug mode.\n        top_options = self._get_top_running_options()\n        if top_options.debug:\n            msg = (\n                f\"[{type(self).__name__}]-[{self.name}] \ud83d\udc40 Observation\\n\"\n                f\"    Iteration: {iteration_cnt}\\n\"\n                f\"    Achieved: {goal_status.goal_achieved}\\n\"\n                f\"    Thinking: {goal_status.brief_thinking}\"\n            )\n            printer.print(msg, color=\"gray\")\n\n        # 4. Dynamic routing based on goal status.\n        if goal_status.goal_achieved or not self._tool_specs:\n            # Goal achieved, route to finalize_answer.\n            self.ferry_to(\"finalize_answer\")\n        else:\n            # Goal not achieved, prepare messages and tools, then route to select_tools.\n            tool_select_messages = []\n\n            # Add system message (if configured)\n            if self._tool_task_config.system_template is not None:\n                tool_system_message = self._tool_task_config.system_template.format_message(\n                    role=Role.SYSTEM,\n                    goal_content=context[\"goal_content\"],\n                    goal_guidance=context[\"goal_guidance\"],\n                )\n                tool_select_messages.append(tool_system_message)\n\n            # Add memory messages\n            tool_select_messages.extend(context[\"memory_messages\"])\n\n            # Add instruction message (if configured).\n            if self._tool_task_config.instruction_template is not None:\n                tool_instruction_message = self._tool_task_config.instruction_template.format_message(role=Role.USER)\n                tool_select_messages.append(tool_instruction_message)\n\n            tools = [tool_spec.to_tool() for tool_spec in self._tool_specs]\n\n            # Concurrently select tools and compress memory.\n            self.ferry_to(\"select_tools\", messages=tool_select_messages, tools=tools)\n            self.ferry_to(\"compress_memory\")\n\n    @worker()\n    async def select_tools(\n        self,\n        rtx = System(\"runtime_context\"),\n        *,\n        messages: List[Message],\n        tools: List[Tool],\n    ) -&gt; Tuple[List[ToolCall], Optional[str]]:\n        \"\"\"\n        Select tools using LLM's tool selection capability.\n\n        This method calls the LLM's aselect_tool method to select appropriate tools\n        based on the conversation context.\n\n        Parameters\n        ----------\n        messages : List[Message]\n            The conversation history and current context.\n        tools : List[Tool]\n            Available tools that can be selected for use.\n\n        Returns\n        -------\n        Tuple[List[ToolCall], Optional[str]]\n            A tuple containing:\n            - List of selected tool calls with determined parameters\n            - Optional response text from the LLM\n        \"\"\"\n        # Check if LLM supports ToolSelection protocol.\n        tool_select_llm = self._tool_task_config.llm\n        if not isinstance(tool_select_llm, ToolSelection):\n            raise TypeError(f\"LLM must support ToolSelection protocol, but {type(tool_select_llm)} does not.\")\n\n        # Call tool selection method.\n        tool_calls, tool_response = await tool_select_llm.aselect_tool(\n            messages=messages,\n            tools=tools,\n        )\n\n        # Log selected tools in debug mode.\n        top_options = self._get_top_running_options()\n        if top_options.debug:\n            tool_info_lines = []\n\n            if tool_calls:\n                for i, tool_call in enumerate(tool_calls, 1):\n                    tool_info_lines.append(f\"    Tool {i}: {tool_call.name}\")\n                    tool_info_lines.append(f\"      id: {tool_call.id}\")\n                    tool_info_lines.append(f\"      arguments: {tool_call.arguments}\")\n            else:\n                tool_info_lines.append(\"    (No tools selected)\")\n\n            if tool_response:\n                tool_info_lines.append(f\"\\n    LLM Response: {tool_response}\")\n\n            tool_info_lines_str = \"\\n\".join(tool_info_lines)\n\n            msg = (\n                f\"[{type(self).__name__}]-[{self.name}] \ud83d\udd27 Tool Selection\\n\"\n                f\"{tool_info_lines_str}\"\n            )\n            printer.print(msg, color=\"orange\")\n\n        # Match tool calls with tool specs.\n        if tool_calls and self._tool_specs:\n            matched_list = self._match_tool_calls_and_tool_specs(tool_calls, self._tool_specs)\n\n            if matched_list:\n                # Create tool workers dynamically.\n                tool_worker_keys = []\n                matched_tool_calls = []\n\n                for tool_call, tool_spec in matched_list:\n                    # Create the tool worker.\n                    tool_worker_key = f\"tool-&lt;{tool_call.name}&gt;-&lt;{tool_call.id}&gt;\"\n                    tool_worker_obj = tool_spec.create_worker()\n\n                    # Register the tool worker.\n                    self.add_worker(key=tool_worker_key, worker=tool_worker_obj)\n                    tool_worker_keys.append(tool_worker_key)\n                    matched_tool_calls.append(tool_call)\n\n                    # Execute the tool worker in the next dynamic step (via ferry_to).\n                    self.ferry_to(tool_worker_key, **tool_call.arguments)\n\n                # Create collect_results worker dynamically.\n                # After collecting, the tool calls and their results will be pushed to memory.\n                def collect_wrapper(compression_timestep_and_tool_results: List[Any]) -&gt; None:\n                    tool_results = compression_timestep_and_tool_results[1:]\n                    return self._collect_tools_results(tool_response, matched_tool_calls, tool_results)\n\n                # To ensure that compression is performed based on the memory prior to tool selection, \n                # the collect_results worker must be started after the compress_memory worker.\n                self.add_func_as_worker(\n                    key=f\"collect_results-&lt;{uuid.uuid4().hex[:8]}&gt;\",\n                    func=collect_wrapper,\n                    dependencies=[\"compress_memory\"] + tool_worker_keys,\n                    args_mapping_rule=ArgsMappingRule.MERGE,\n                    callback_builders=[WorkerCallbackBuilder(CollectResultsCleanupCallback)],\n                )\n\n                tool_selected = True\n            else:\n                tool_selected = False\n        else:\n            tool_selected = False\n\n        local_space = self.get_local_space(rtx)\n\n        # Count the number of times no tool is selected.\n        if not tool_selected:\n            no_tool_selected_cnt = local_space.get(\"no_tool_selected_cnt\", 0)\n            no_tool_selected_cnt = no_tool_selected_cnt + 1 if not tool_selected else 0\n            local_space[\"no_tool_selected_cnt\"] = no_tool_selected_cnt\n\n            if no_tool_selected_cnt &lt; self._stop_condition.max_consecutive_no_tool_selected:\n                # If the limit is not exceeded, try one more time.\n                self.ferry_to(\"observe\")\n            else:\n                # If the limit is exceeded, finalize the answer.\n                self.ferry_to(\"finalize_answer\")\n\n    @worker()\n    async def compress_memory(self) -&gt; Optional[int]:\n        \"\"\"\n        Compress memory if necessary.\n\n        Returns\n        -------\n        Optional[int]\n            The timestep of the compression node if compression is needed, otherwise None.\n        \"\"\"\n        top_options = self._get_top_running_options()\n        should_compress = self._memory_manager.should_trigger_compression()\n\n        if top_options.debug:\n            msg = (\n                f\"[{type(self).__name__}]-[{self.name}] \ud83e\udded Memory Check\\n\"\n                f\"    Compression Needed: {should_compress}\"\n            )\n            printer.print(msg, color=\"olive\")\n\n        if should_compress:\n            # Create the compression node. The summary is now complete.\n            timestep = await self._memory_manager.acreate_compression()\n\n            if top_options.debug:\n                node: CompressionEpisodicNode = self._memory_manager.get_specified_memory_node(timestep)\n                summary = await asyncio.wrap_future(node.summary)\n                msg = (\n                    f\"[{type(self).__name__}]-[{self.name}] \ud83d\udcc2 Memory Compression\\n\"\n                    f\"{summary}\"\n                )\n                printer.print(msg, color=\"blue\")\n\n            return timestep\n        else:\n            return None\n\n    @worker(is_output=True)\n    async def finalize_answer(self) -&gt; str:\n        \"\"\"\n        Generate the final answer based on memory and goal using LLM.\n\n        This worker is the output node of the automa. It builds context from memory,\n        calls LLM to generate a comprehensive final answer based on the goal and\n        conversation history.\n\n        Returns\n        -------\n        str\n            The final answer string.\n        \"\"\"\n        # 1. Build context from memory.\n        context: ReCentContext = await self._memory_manager.abuild_context()\n\n        # 2. Build LLM message list for final answer generation.\n        messages: List[Message] = []\n\n        # Add system prompt for final answer generation (if configured).\n        if self._answer_task_config.system_template is not None:\n            system_message = self._answer_task_config.system_template.format_message(\n                role=Role.SYSTEM,\n                goal_content=context[\"goal_content\"],\n                goal_guidance=context[\"goal_guidance\"],\n            )\n            messages.append(system_message)\n\n        # Add memory messages (observation history).\n        if context[\"memory_messages\"]:\n            messages.extend(context[\"memory_messages\"])\n\n        # Add instruction to generate final answer (if configured).\n        if self._answer_task_config.instruction_template is not None:\n            instruction_message = self._answer_task_config.instruction_template.format_message(role=Role.USER)\n            messages.append(instruction_message)\n\n        # 3. Call LLM to generate final answer.\n        answer_llm = self._answer_task_config.llm\n        response = await answer_llm.achat(messages=messages)\n        final_answer = response.message.content\n\n        # 4. Push final answer as a Message into the memory\n        final_answer_msg = Message.from_text(text=final_answer, role=Role.AI)\n        self._memory_manager.push_messages([final_answer_msg])\n\n        return final_answer\n\n    def _collect_tools_results(\n        self,\n        tool_response: Optional[str],\n        tool_calls: Optional[List[ToolCall]],\n        tool_results: List[Any],\n    ) -&gt; None:\n        \"\"\"\n        Collect results from tool executions and push to memory.\n\n        This method is used as a worker function to collect results from multiple tool workers. \n        It converts tool results to ToolResult messages and pushes them to memory.\n\n        Parameters\n        ----------\n        tool_select_messages : List[Message]\n            The input messages from the tool selection task.\n        tool_response : Optional[str]\n            The output response from the tool selection task, if any.\n        tool_calls : Optional[List[ToolCall]]\n            List of tool calls (optional, for building ToolResult messages).\n        tool_results : List[Any]\n            List of tool execution results (merged via ArgsMappingRule.MERGE).\n        \"\"\"\n        # Push assistant message with tool calls and tool response to memory.\n        if tool_calls or tool_response:\n            assistant_message = Message.from_tool_call(tool_calls=tool_calls, text=tool_response)\n            self._memory_manager.push_messages([assistant_message])\n\n        # Convert tool results to ToolResult messages.\n        tool_result_messages: List[Message] = []\n\n        for tool_call, tool_result in zip(tool_calls, tool_results):\n            # Convert tool result to string\n            result_content = str(tool_result)\n            # Create ToolResult message\n            tool_result_message = Message.from_tool_result(\n                tool_id=tool_call.id,\n                content=result_content,\n            )\n            tool_result_messages.append(tool_result_message)\n\n        # Push tool result messages to memory.\n        if tool_result_messages:\n            self._memory_manager.push_messages(tool_result_messages)\n\n        # Log tool execution results in debug mode.\n        top_options = self._get_top_running_options()\n        if top_options.debug:\n            result_lines = []\n            for i, (tool_call, tool_result) in enumerate(zip(tool_calls, tool_results), 1):\n                # Format tool result with MCP support\n                result_content = stringify_tool_result(tool_result, verbose=top_options.verbose)\n                result_content = result_content[:10000] + \"...\" if len(result_content) &gt; 10000 else result_content\n                result_lines.append(f\"    Tool {i}: {tool_call.name}\")\n                result_lines.append(f\"      id: {tool_call.id}\")\n                result_lines.append(f\"      result: {result_content}\")\n\n            result_lines_str = \"\\n\".join(result_lines)\n\n            msg = (\n                f\"[{type(self).__name__}]-[{self.name}] \ud83d\udea9 Tool Results\\n\"\n                f\"{result_lines_str}\"\n            )\n            printer.print(msg, color=\"green\")\n\n        # Route to observe.\n        self.ferry_to(\"observe\")\n\n    def _match_tool_calls_and_tool_specs(\n        self,\n        tool_calls: List[ToolCall],\n        tool_specs: List[ToolSpec],\n    ) -&gt; List[Tuple[ToolCall, ToolSpec]]:\n        matched = []\n        for tool_call in tool_calls:\n            for tool_spec in tool_specs:\n                if tool_spec.tool_name == tool_call.name:\n                    matched.append((tool_call, tool_spec))\n                    break\n        return matched\n\n    @override\n    def dump_to_dict(self) -&gt; Dict[str, Any]:\n        state_dict = super().dump_to_dict()\n        state_dict[\"llm\"] = self._llm\n\n        # Only serialize ToolSpec instances that are not from builders.\n        tool_specs_to_serialize = [\n            tool_spec for tool_spec in self._tool_specs if not tool_spec._from_builder\n        ]\n        state_dict[\"tool_specs\"] = tool_specs_to_serialize\n\n        # Serialize tools_builders so they can be used to recreate ToolSpec instances during deserialization.\n        state_dict[\"tools_builders\"] = self._tools_builders or []\n\n        state_dict[\"memory_manager\"] = self._memory_manager\n        state_dict[\"observation_task_config\"] = self._observation_task_config\n        state_dict[\"tool_task_config\"] = self._tool_task_config\n        state_dict[\"answer_task_config\"] = self._answer_task_config\n        state_dict[\"stop_condition\"] = self._stop_condition\n        return state_dict\n\n    @override\n    def load_from_dict(self, state_dict: Dict[str, Any]) -&gt; None:\n        super().load_from_dict(state_dict)\n        self._llm = state_dict[\"llm\"]\n\n        # Load ToolSpec instances that were directly serialized.\n        self._tool_specs = state_dict.get(\"tool_specs\") or []\n\n        # Load tools_builders.\n        self._tools_builders = state_dict.get(\"tools_builders\") or []\n\n        # Recreate ToolSpec instances from the deserialized builders.\n        if self._tools_builders:\n            for builder in self._tools_builders:\n                response = builder.build()\n                self._tool_specs.extend(response.get(\"tool_specs\", []))\n\n        self._stop_condition = state_dict[\"stop_condition\"]\n        self._memory_manager = state_dict[\"memory_manager\"]\n\n        self._observation_task_config = (\n            state_dict.get(\"observation_task_config\")\n            or ObservationTaskConfig(llm=self._llm).to_llm_task_config()\n        )\n        self._tool_task_config = (\n            state_dict.get(\"tool_task_config\")\n            or ToolTaskConfig(llm=self._llm).to_llm_task_config()\n        )\n        self._answer_task_config = (\n            state_dict.get(\"answer_task_config\")\n            or AnswerTaskConfig(llm=self._llm).to_llm_task_config()\n        )\n</code></pre>"},{"location":"reference/bridgic-core/bridgic/core/agentic/recent/#bridgic.core.agentic.recent.ReCentAutoma.initialize_task_goal","title":"initialize_task_goal","text":"<code>async</code> <pre><code>initialize_task_goal(\n    goal: str, guidance: Optional[str] = None\n)\n</code></pre> <p>Initialize the goal of the task and start the automa.</p> <p>This worker is the entry point of the automa. It creates a goal node as the first episodic  node in the memory sequence and optionally pushes initial user messages.</p> <p>Parameters:</p> Name Type Description Default <code>goal</code> <code>str</code> <p>The task goal.</p> required <code>guidance</code> <code>Optional[str]</code> <p>The guidance for achieving the task goal.</p> <code>None</code> Source code in <code>bridgic/core/agentic/recent/_recent_automa.py</code> <pre><code>@worker(is_start=True)\nasync def initialize_task_goal(\n    self,\n    goal: str,\n    guidance: Optional[str] = None,\n):\n    \"\"\"\n    Initialize the goal of the task and start the automa.\n\n    This worker is the entry point of the automa. It creates a goal node as the first episodic \n    node in the memory sequence and optionally pushes initial user messages.\n\n    Parameters\n    ----------\n    goal : str\n        The task goal.\n    guidance : Optional[str]\n        The guidance for achieving the task goal.\n    \"\"\"\n    if not goal:\n        raise ValueError(\"Goal cannot be empty.\")\n    self._memory_manager.create_goal(goal, guidance)\n\n    # Log task goal initialization in debug mode.\n    top_options = self._get_top_running_options()\n    if top_options.debug:\n        msg = (\n            f\"[{type(self).__name__}]-[{self.name}] \ud83c\udfaf Task Goal\\n\"\n            f\"{goal}\" + (f\"\\n\\n{guidance}\" if guidance else '')\n        )\n        printer.print(msg)\n\n    self.ferry_to(\"observe\")\n</code></pre>"},{"location":"reference/bridgic-core/bridgic/core/agentic/recent/#bridgic.core.agentic.recent.ReCentAutoma.observe","title":"observe","text":"<code>async</code> <pre><code>observe(rtx=System('runtime_context'))\n</code></pre> <p>Observe the current state and determine if the goal has been achieved.</p> <p>This worker builds context from memory, uses LLM (with <code>StructuredOutput</code> protocol) to  determine if the goal has been achieved, and routes accordingly.</p> Source code in <code>bridgic/core/agentic/recent/_recent_automa.py</code> <pre><code>@worker()\nasync def observe(self, rtx = System(\"runtime_context\")):\n    \"\"\"\n    Observe the current state and determine if the goal has been achieved.\n\n    This worker builds context from memory, uses LLM (with `StructuredOutput` protocol) to \n    determine if the goal has been achieved, and routes accordingly.\n    \"\"\"\n    local_space = self.get_local_space(rtx)\n\n    # 0. If iteration count is bigger than max_iteration, route to finalize_answer and stop.\n    iteration_cnt = local_space.get(\"iteration_cnt\", 0) + 1\n    local_space[\"iteration_cnt\"] = iteration_cnt\n\n    if self._stop_condition.max_iteration &gt;= 0:\n        if iteration_cnt &gt; self._stop_condition.max_iteration:\n            self.ferry_to(\"finalize_answer\")\n            return\n\n    # 1. Build context from memory.\n    context: ReCentContext = await self._memory_manager.abuild_context()\n\n    # 2. Build LLM message list for goal status evaluation.\n    messages: List[Message] = []\n\n    # Add system message for goal evaluation (if configured).\n    if self._observation_task_config.system_template is not None:\n        system_message = self._observation_task_config.system_template.format_message(\n            role=Role.SYSTEM,\n            goal_content=context[\"goal_content\"],\n            goal_guidance=context[\"goal_guidance\"],\n        )\n        messages.append(system_message)\n\n    # Add memory messages (observation history).\n    if context[\"memory_messages\"]:\n        messages.extend(context[\"memory_messages\"])\n\n    # Add instruction message (if configured).\n    if self._observation_task_config.instruction_template is not None:\n        instruction_message = self._observation_task_config.instruction_template.format_message(role=Role.USER)\n        messages.append(instruction_message)\n\n    # 3. Call LLM with StructuredOutput to get goal status.\n    observe_llm = self._observation_task_config.llm\n    if not isinstance(observe_llm, StructuredOutput):\n        raise TypeError(f\"LLM must support StructuredOutput protocol, but {type(observe_llm)} does not.\")\n\n    goal_status: GoalStatus = await observe_llm.astructured_output(\n        messages=messages,\n        constraint=PydanticModel(model=GoalStatus),\n    )\n    observation_template = EjinjaPromptTemplate(\n        \"Goal Status:\\n\"\n        \"- Achieved: {%- if goal_status.goal_achieved %}goal_status.goal_achieved{% else %}No{% endif %}\\n\"\n        \"- Thinking: {%- if goal_status.brief_thinking %}{{ goal_status.brief_thinking }}{% else %}(No thinking){% endif %}\"\n    )\n    observation_message = observation_template.format_message(\n        role=Role.AI,\n        goal_status=goal_status,\n    )\n    self._memory_manager.push_messages([observation_message])\n\n    # Log observation result in debug mode.\n    top_options = self._get_top_running_options()\n    if top_options.debug:\n        msg = (\n            f\"[{type(self).__name__}]-[{self.name}] \ud83d\udc40 Observation\\n\"\n            f\"    Iteration: {iteration_cnt}\\n\"\n            f\"    Achieved: {goal_status.goal_achieved}\\n\"\n            f\"    Thinking: {goal_status.brief_thinking}\"\n        )\n        printer.print(msg, color=\"gray\")\n\n    # 4. Dynamic routing based on goal status.\n    if goal_status.goal_achieved or not self._tool_specs:\n        # Goal achieved, route to finalize_answer.\n        self.ferry_to(\"finalize_answer\")\n    else:\n        # Goal not achieved, prepare messages and tools, then route to select_tools.\n        tool_select_messages = []\n\n        # Add system message (if configured)\n        if self._tool_task_config.system_template is not None:\n            tool_system_message = self._tool_task_config.system_template.format_message(\n                role=Role.SYSTEM,\n                goal_content=context[\"goal_content\"],\n                goal_guidance=context[\"goal_guidance\"],\n            )\n            tool_select_messages.append(tool_system_message)\n\n        # Add memory messages\n        tool_select_messages.extend(context[\"memory_messages\"])\n\n        # Add instruction message (if configured).\n        if self._tool_task_config.instruction_template is not None:\n            tool_instruction_message = self._tool_task_config.instruction_template.format_message(role=Role.USER)\n            tool_select_messages.append(tool_instruction_message)\n\n        tools = [tool_spec.to_tool() for tool_spec in self._tool_specs]\n\n        # Concurrently select tools and compress memory.\n        self.ferry_to(\"select_tools\", messages=tool_select_messages, tools=tools)\n        self.ferry_to(\"compress_memory\")\n</code></pre>"},{"location":"reference/bridgic-core/bridgic/core/agentic/recent/#bridgic.core.agentic.recent.ReCentAutoma.select_tools","title":"select_tools","text":"<code>async</code> <pre><code>select_tools(\n    rtx=System(\"runtime_context\"),\n    *,\n    messages: List[Message],\n    tools: List[Tool]\n) -&gt; Tuple[List[ToolCall], Optional[str]]\n</code></pre> <p>Select tools using LLM's tool selection capability.</p> <p>This method calls the LLM's aselect_tool method to select appropriate tools based on the conversation context.</p> <p>Parameters:</p> Name Type Description Default <code>messages</code> <code>List[Message]</code> <p>The conversation history and current context.</p> required <code>tools</code> <code>List[Tool]</code> <p>Available tools that can be selected for use.</p> required <p>Returns:</p> Type Description <code>Tuple[List[ToolCall], Optional[str]]</code> <p>A tuple containing: - List of selected tool calls with determined parameters - Optional response text from the LLM</p> Source code in <code>bridgic/core/agentic/recent/_recent_automa.py</code> <pre><code>@worker()\nasync def select_tools(\n    self,\n    rtx = System(\"runtime_context\"),\n    *,\n    messages: List[Message],\n    tools: List[Tool],\n) -&gt; Tuple[List[ToolCall], Optional[str]]:\n    \"\"\"\n    Select tools using LLM's tool selection capability.\n\n    This method calls the LLM's aselect_tool method to select appropriate tools\n    based on the conversation context.\n\n    Parameters\n    ----------\n    messages : List[Message]\n        The conversation history and current context.\n    tools : List[Tool]\n        Available tools that can be selected for use.\n\n    Returns\n    -------\n    Tuple[List[ToolCall], Optional[str]]\n        A tuple containing:\n        - List of selected tool calls with determined parameters\n        - Optional response text from the LLM\n    \"\"\"\n    # Check if LLM supports ToolSelection protocol.\n    tool_select_llm = self._tool_task_config.llm\n    if not isinstance(tool_select_llm, ToolSelection):\n        raise TypeError(f\"LLM must support ToolSelection protocol, but {type(tool_select_llm)} does not.\")\n\n    # Call tool selection method.\n    tool_calls, tool_response = await tool_select_llm.aselect_tool(\n        messages=messages,\n        tools=tools,\n    )\n\n    # Log selected tools in debug mode.\n    top_options = self._get_top_running_options()\n    if top_options.debug:\n        tool_info_lines = []\n\n        if tool_calls:\n            for i, tool_call in enumerate(tool_calls, 1):\n                tool_info_lines.append(f\"    Tool {i}: {tool_call.name}\")\n                tool_info_lines.append(f\"      id: {tool_call.id}\")\n                tool_info_lines.append(f\"      arguments: {tool_call.arguments}\")\n        else:\n            tool_info_lines.append(\"    (No tools selected)\")\n\n        if tool_response:\n            tool_info_lines.append(f\"\\n    LLM Response: {tool_response}\")\n\n        tool_info_lines_str = \"\\n\".join(tool_info_lines)\n\n        msg = (\n            f\"[{type(self).__name__}]-[{self.name}] \ud83d\udd27 Tool Selection\\n\"\n            f\"{tool_info_lines_str}\"\n        )\n        printer.print(msg, color=\"orange\")\n\n    # Match tool calls with tool specs.\n    if tool_calls and self._tool_specs:\n        matched_list = self._match_tool_calls_and_tool_specs(tool_calls, self._tool_specs)\n\n        if matched_list:\n            # Create tool workers dynamically.\n            tool_worker_keys = []\n            matched_tool_calls = []\n\n            for tool_call, tool_spec in matched_list:\n                # Create the tool worker.\n                tool_worker_key = f\"tool-&lt;{tool_call.name}&gt;-&lt;{tool_call.id}&gt;\"\n                tool_worker_obj = tool_spec.create_worker()\n\n                # Register the tool worker.\n                self.add_worker(key=tool_worker_key, worker=tool_worker_obj)\n                tool_worker_keys.append(tool_worker_key)\n                matched_tool_calls.append(tool_call)\n\n                # Execute the tool worker in the next dynamic step (via ferry_to).\n                self.ferry_to(tool_worker_key, **tool_call.arguments)\n\n            # Create collect_results worker dynamically.\n            # After collecting, the tool calls and their results will be pushed to memory.\n            def collect_wrapper(compression_timestep_and_tool_results: List[Any]) -&gt; None:\n                tool_results = compression_timestep_and_tool_results[1:]\n                return self._collect_tools_results(tool_response, matched_tool_calls, tool_results)\n\n            # To ensure that compression is performed based on the memory prior to tool selection, \n            # the collect_results worker must be started after the compress_memory worker.\n            self.add_func_as_worker(\n                key=f\"collect_results-&lt;{uuid.uuid4().hex[:8]}&gt;\",\n                func=collect_wrapper,\n                dependencies=[\"compress_memory\"] + tool_worker_keys,\n                args_mapping_rule=ArgsMappingRule.MERGE,\n                callback_builders=[WorkerCallbackBuilder(CollectResultsCleanupCallback)],\n            )\n\n            tool_selected = True\n        else:\n            tool_selected = False\n    else:\n        tool_selected = False\n\n    local_space = self.get_local_space(rtx)\n\n    # Count the number of times no tool is selected.\n    if not tool_selected:\n        no_tool_selected_cnt = local_space.get(\"no_tool_selected_cnt\", 0)\n        no_tool_selected_cnt = no_tool_selected_cnt + 1 if not tool_selected else 0\n        local_space[\"no_tool_selected_cnt\"] = no_tool_selected_cnt\n\n        if no_tool_selected_cnt &lt; self._stop_condition.max_consecutive_no_tool_selected:\n            # If the limit is not exceeded, try one more time.\n            self.ferry_to(\"observe\")\n        else:\n            # If the limit is exceeded, finalize the answer.\n            self.ferry_to(\"finalize_answer\")\n</code></pre>"},{"location":"reference/bridgic-core/bridgic/core/agentic/recent/#bridgic.core.agentic.recent.ReCentAutoma.compress_memory","title":"compress_memory","text":"<code>async</code> <pre><code>compress_memory() -&gt; Optional[int]\n</code></pre> <p>Compress memory if necessary.</p> <p>Returns:</p> Type Description <code>Optional[int]</code> <p>The timestep of the compression node if compression is needed, otherwise None.</p> Source code in <code>bridgic/core/agentic/recent/_recent_automa.py</code> <pre><code>@worker()\nasync def compress_memory(self) -&gt; Optional[int]:\n    \"\"\"\n    Compress memory if necessary.\n\n    Returns\n    -------\n    Optional[int]\n        The timestep of the compression node if compression is needed, otherwise None.\n    \"\"\"\n    top_options = self._get_top_running_options()\n    should_compress = self._memory_manager.should_trigger_compression()\n\n    if top_options.debug:\n        msg = (\n            f\"[{type(self).__name__}]-[{self.name}] \ud83e\udded Memory Check\\n\"\n            f\"    Compression Needed: {should_compress}\"\n        )\n        printer.print(msg, color=\"olive\")\n\n    if should_compress:\n        # Create the compression node. The summary is now complete.\n        timestep = await self._memory_manager.acreate_compression()\n\n        if top_options.debug:\n            node: CompressionEpisodicNode = self._memory_manager.get_specified_memory_node(timestep)\n            summary = await asyncio.wrap_future(node.summary)\n            msg = (\n                f\"[{type(self).__name__}]-[{self.name}] \ud83d\udcc2 Memory Compression\\n\"\n                f\"{summary}\"\n            )\n            printer.print(msg, color=\"blue\")\n\n        return timestep\n    else:\n        return None\n</code></pre>"},{"location":"reference/bridgic-core/bridgic/core/agentic/recent/#bridgic.core.agentic.recent.ReCentAutoma.finalize_answer","title":"finalize_answer","text":"<code>async</code> <pre><code>finalize_answer() -&gt; str\n</code></pre> <p>Generate the final answer based on memory and goal using LLM.</p> <p>This worker is the output node of the automa. It builds context from memory, calls LLM to generate a comprehensive final answer based on the goal and conversation history.</p> <p>Returns:</p> Type Description <code>str</code> <p>The final answer string.</p> Source code in <code>bridgic/core/agentic/recent/_recent_automa.py</code> <pre><code>@worker(is_output=True)\nasync def finalize_answer(self) -&gt; str:\n    \"\"\"\n    Generate the final answer based on memory and goal using LLM.\n\n    This worker is the output node of the automa. It builds context from memory,\n    calls LLM to generate a comprehensive final answer based on the goal and\n    conversation history.\n\n    Returns\n    -------\n    str\n        The final answer string.\n    \"\"\"\n    # 1. Build context from memory.\n    context: ReCentContext = await self._memory_manager.abuild_context()\n\n    # 2. Build LLM message list for final answer generation.\n    messages: List[Message] = []\n\n    # Add system prompt for final answer generation (if configured).\n    if self._answer_task_config.system_template is not None:\n        system_message = self._answer_task_config.system_template.format_message(\n            role=Role.SYSTEM,\n            goal_content=context[\"goal_content\"],\n            goal_guidance=context[\"goal_guidance\"],\n        )\n        messages.append(system_message)\n\n    # Add memory messages (observation history).\n    if context[\"memory_messages\"]:\n        messages.extend(context[\"memory_messages\"])\n\n    # Add instruction to generate final answer (if configured).\n    if self._answer_task_config.instruction_template is not None:\n        instruction_message = self._answer_task_config.instruction_template.format_message(role=Role.USER)\n        messages.append(instruction_message)\n\n    # 3. Call LLM to generate final answer.\n    answer_llm = self._answer_task_config.llm\n    response = await answer_llm.achat(messages=messages)\n    final_answer = response.message.content\n\n    # 4. Push final answer as a Message into the memory\n    final_answer_msg = Message.from_text(text=final_answer, role=Role.AI)\n    self._memory_manager.push_messages([final_answer_msg])\n\n    return final_answer\n</code></pre>"},{"location":"reference/bridgic-core/bridgic/core/agentic/recent/#bridgic.core.agentic.recent.StopCondition","title":"StopCondition","text":"<p>               Bases: <code>BaseModel</code></p> <p>Stop condition configuration for ReCentAutoma.</p> <p>The different stop conditions below are combined with logic \"or\". In other words, the process  will stop if any condition is met.</p> <p>Attributes:</p> Name Type Description <code>max_iteration</code> <code>int</code> <p>Maximum number of times to enter the observe node before finalizing the answer. Defaults to -1 which means there is no limit to the number of iterations.</p> <code>max_consecutive_no_tool_selected</code> <code>int</code> <p>Maximum number of consecutive times no tool is selected before finalizing the answer. Defaults to 3.</p> Source code in <code>bridgic/core/agentic/recent/_recent_automa.py</code> <pre><code>class StopCondition(BaseModel):\n    \"\"\"\n    Stop condition configuration for ReCentAutoma.\n\n    The different stop conditions below are combined with logic \"or\". In other words, the process \n    will stop if any condition is met.\n\n    Attributes\n    ----------\n    max_iteration : int\n        Maximum number of times to enter the observe node before finalizing the answer.\n        Defaults to -1 which means there is no limit to the number of iterations.\n    max_consecutive_no_tool_selected : int\n        Maximum number of consecutive times no tool is selected before finalizing the answer.\n        Defaults to 3.\n    \"\"\"\n\n    max_iteration: int = Field(default=-1)\n    \"\"\"Maximum number of times to enter the observe node before finalizing the answer.\"\"\"\n\n    max_consecutive_no_tool_selected: int = Field(default=3)\n    \"\"\"Maximum number of consecutive times no tool is selected before finalizing the answer.\"\"\"\n</code></pre>"},{"location":"reference/bridgic-core/bridgic/core/agentic/recent/#bridgic.core.agentic.recent.StopCondition.max_iteration","title":"max_iteration  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>max_iteration: int = Field(default=-1)\n</code></pre> <p>Maximum number of times to enter the observe node before finalizing the answer.</p>"},{"location":"reference/bridgic-core/bridgic/core/agentic/recent/#bridgic.core.agentic.recent.StopCondition.max_consecutive_no_tool_selected","title":"max_consecutive_no_tool_selected  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>max_consecutive_no_tool_selected: int = Field(default=3)\n</code></pre> <p>Maximum number of consecutive times no tool is selected before finalizing the answer.</p>"},{"location":"reference/bridgic-core/bridgic/core/agentic/recent/#bridgic.core.agentic.recent.ReCentMemoryConfig","title":"ReCentMemoryConfig","text":"<p>               Bases: <code>Serializable</code></p> <p>This configuration class defines the memory management strategy that will compress the conversation history when certain conditions are met.</p> <p>Attributes:</p> Name Type Description <code>llm</code> <code>BaseLlm</code> <p>The LLM instance used for memory compression operations.</p> <code>max_node_size</code> <code>int</code> <p>Maximum number of memory nodes before triggering compression. Defaults to 10.</p> <code>max_token_size</code> <code>int</code> <p>Maximum number of tokens before triggering compression. Defaults to 8192 (1024 * 8).</p> <code>system_template</code> <code>str</code> <p>Jinja2 prompt template for the system prompt used in memory compression, which accepts  parameters: <code>goal</code> and <code>guidance</code>.</p> <code>instruction_template</code> <code>str</code> <p>Jinja2 prompt template for the instruction prompt used in memory compression.</p> <code>token_count_callback</code> <code>Optional[Callable[[str], int]]</code> <p>Optional callback function to calculate token count from text. If None, defaults to <code>estimate_token_count</code> which uses a simple approximation (character_count / 4). The callback should accept a text string and return the token count.</p> Source code in <code>bridgic/core/agentic/recent/_recent_memory_config.py</code> <pre><code>class ReCentMemoryConfig(Serializable):\n    \"\"\"\n    This configuration class defines the memory management strategy that will compress\n    the conversation history when certain conditions are met.\n\n    Attributes\n    ----------\n    llm : BaseLlm\n        The LLM instance used for memory compression operations.\n    max_node_size : int\n        Maximum number of memory nodes before triggering compression.\n        Defaults to 10.\n    max_token_size : int\n        Maximum number of tokens before triggering compression.\n        Defaults to 8192 (1024 * 8).\n    system_template : str\n        Jinja2 prompt template for the system prompt used in memory compression, which accepts \n        parameters: `goal` and `guidance`.\n    instruction_template : str\n        Jinja2 prompt template for the instruction prompt used in memory compression.\n    token_count_callback : Optional[Callable[[str], int]]\n        Optional callback function to calculate token count from text.\n        If None, defaults to `estimate_token_count` which uses a simple approximation\n        (character_count / 4). The callback should accept a text string and return the token count.\n    \"\"\"\n\n    llm: BaseLlm\n    \"\"\"The LLM used for memory compression.\"\"\"\n\n    max_node_size: int\n    \"\"\"Threshold for the number of memory nodes to trigger memory compression.\"\"\"\n\n    max_token_size: int\n    \"\"\"Threshold for the number of tokens to trigger memory compression.\"\"\"\n\n    system_template: EjinjaPromptTemplate\n    \"\"\"Template for system prompt used in memory compression.\"\"\"\n\n    instruction_template: EjinjaPromptTemplate\n    \"\"\"Instruction prompt template used in memory compression.\"\"\"\n\n    token_count_callback: Callable[[str], int]\n    \"\"\"Callback function to calculate token count from text. Defaults to estimate_token_count.\"\"\"\n\n    def __init__(\n        self,\n        llm: BaseLlm,\n        max_node_size: int = 20,\n        max_token_size: int = 1024 * 16,\n        system_template: Optional[str] = DEFAULT_SYSTEM_PROMPT_TEMPLATE,\n        instruction_template: Optional[str] = DEFAULT_INSTRUCTION_PROMPT_TEMPLATE,\n        token_count_callback: Optional[Callable[[str], int]] = estimate_token_count,\n    ):\n        self.llm = llm\n        self.max_node_size = max_node_size\n        self.max_token_size = max_token_size\n        self.token_count_callback = token_count_callback if token_count_callback is not None else estimate_token_count\n\n        # Convert string templates to EjinjaPromptTemplate instances\n        # If None is explicitly passed, use default templates\n        if system_template is None:\n            system_template = DEFAULT_SYSTEM_PROMPT_TEMPLATE\n        self.system_template = EjinjaPromptTemplate(system_template)\n\n        if instruction_template is None:\n            instruction_template = DEFAULT_INSTRUCTION_PROMPT_TEMPLATE\n        self.instruction_template = EjinjaPromptTemplate(instruction_template)\n\n    @override\n    def dump_to_dict(self) -&gt; Dict[str, Any]:\n        state_dict = {}\n        state_dict[\"llm\"] = self.llm\n        state_dict[\"max_node_size\"] = self.max_node_size\n        state_dict[\"max_token_size\"] = self.max_token_size\n        state_dict[\"token_count_callback\"] = self.token_count_callback.__module__ + \".\" + self.token_count_callback.__qualname__\n        state_dict[\"system_template\"] = self.system_template\n        state_dict[\"instruction_template\"] = self.instruction_template\n        return state_dict\n\n    @override\n    def load_from_dict(self, state_dict: Dict[str, Any]) -&gt; None:\n        self.llm = state_dict[\"llm\"]\n        self.max_node_size = state_dict[\"max_node_size\"]\n        self.max_token_size = state_dict[\"max_token_size\"]\n        self.token_count_callback = load_qualified_class_or_func(state_dict[\"token_count_callback\"])\n        self.system_template = state_dict[\"system_template\"]\n        self.instruction_template = state_dict[\"instruction_template\"]\n</code></pre>"},{"location":"reference/bridgic-core/bridgic/core/agentic/recent/#bridgic.core.agentic.recent.ReCentMemoryConfig.llm","title":"llm  <code>instance-attribute</code>","text":"<pre><code>llm: BaseLlm = llm\n</code></pre> <p>The LLM used for memory compression.</p>"},{"location":"reference/bridgic-core/bridgic/core/agentic/recent/#bridgic.core.agentic.recent.ReCentMemoryConfig.max_node_size","title":"max_node_size  <code>instance-attribute</code>","text":"<pre><code>max_node_size: int = max_node_size\n</code></pre> <p>Threshold for the number of memory nodes to trigger memory compression.</p>"},{"location":"reference/bridgic-core/bridgic/core/agentic/recent/#bridgic.core.agentic.recent.ReCentMemoryConfig.max_token_size","title":"max_token_size  <code>instance-attribute</code>","text":"<pre><code>max_token_size: int = max_token_size\n</code></pre> <p>Threshold for the number of tokens to trigger memory compression.</p>"},{"location":"reference/bridgic-core/bridgic/core/agentic/recent/#bridgic.core.agentic.recent.ReCentMemoryConfig.system_template","title":"system_template  <code>instance-attribute</code>","text":"<pre><code>system_template: EjinjaPromptTemplate = (\n    EjinjaPromptTemplate(system_template)\n)\n</code></pre> <p>Template for system prompt used in memory compression.</p>"},{"location":"reference/bridgic-core/bridgic/core/agentic/recent/#bridgic.core.agentic.recent.ReCentMemoryConfig.instruction_template","title":"instruction_template  <code>instance-attribute</code>","text":"<pre><code>instruction_template: EjinjaPromptTemplate = (\n    EjinjaPromptTemplate(instruction_template)\n)\n</code></pre> <p>Instruction prompt template used in memory compression.</p>"},{"location":"reference/bridgic-core/bridgic/core/agentic/recent/#bridgic.core.agentic.recent.ReCentMemoryConfig.token_count_callback","title":"token_count_callback  <code>instance-attribute</code>","text":"<pre><code>token_count_callback: Callable[[str], int] = (\n    token_count_callback\n    if token_count_callback is not None\n    else estimate_token_count\n)\n</code></pre> <p>Callback function to calculate token count from text. Defaults to estimate_token_count.</p>"},{"location":"reference/bridgic-core/bridgic/core/agentic/recent/#bridgic.core.agentic.recent.ObservationTaskConfig","title":"ObservationTaskConfig","text":"<p>Configuration for the observation task in ReCentAutoma.</p> <p>This class allows configuring the LLM and prompt templates for the observation task. When system_template or instruction_template is None, the default template will be used.</p> <p>Attributes:</p> Name Type Description <code>llm</code> <code>BaseLlm</code> <p>The LLM instance to use for this task.</p> <code>system_template</code> <code>Optional[Union[str, EjinjaPromptTemplate]]</code> <p>System prompt template. If None, uses DEFAULT_OBSERVE_SYSTEM_TEMPLATE.</p> <code>instruction_template</code> <code>Optional[Union[str, EjinjaPromptTemplate]]</code> <p>Instruction prompt template. If None, uses DEFAULT_OBSERVE_INSTRUCTION_TEMPLATE.</p> Source code in <code>bridgic/core/agentic/recent/_recent_task_configs.py</code> <pre><code>class ObservationTaskConfig:\n    \"\"\"\n    Configuration for the observation task in ReCentAutoma.\n\n    This class allows configuring the LLM and prompt templates for the observation task.\n    When system_template or instruction_template is None, the default template will be used.\n\n    Attributes\n    ----------\n    llm : BaseLlm\n        The LLM instance to use for this task.\n    system_template : Optional[Union[str, EjinjaPromptTemplate]]\n        System prompt template. If None, uses DEFAULT_OBSERVE_SYSTEM_TEMPLATE.\n    instruction_template : Optional[Union[str, EjinjaPromptTemplate]]\n        Instruction prompt template. If None, uses DEFAULT_OBSERVE_INSTRUCTION_TEMPLATE.\n    \"\"\"\n\n    def __init__(\n        self,\n        llm: BaseLlm,\n        system_template: Optional[Union[str, EjinjaPromptTemplate]] = DEFAULT_OBSERVATION_SYSTEM_TEMPLATE,\n        instruction_template: Optional[Union[str, EjinjaPromptTemplate]] = DEFAULT_OBSERVATION_INSTRUCTION_TEMPLATE,\n    ):\n        self.llm = llm\n        self.system_template = system_template or DEFAULT_OBSERVATION_SYSTEM_TEMPLATE\n        self.instruction_template = instruction_template or DEFAULT_OBSERVATION_INSTRUCTION_TEMPLATE\n\n    def to_llm_task_config(self) -&gt; LlmTaskConfig:\n        return LlmTaskConfig(\n            llm=self.llm,\n            system_template=self.system_template,\n            instruction_template=self.instruction_template,\n        )\n</code></pre>"},{"location":"reference/bridgic-core/bridgic/core/agentic/recent/#bridgic.core.agentic.recent.ToolTaskConfig","title":"ToolTaskConfig","text":"<p>Configuration for the tool selection task in ReCentAutoma.</p> <p>This class allows configuring the LLM and prompt templates for the tool selection task. When system_template or instruction_template is None, the default template will be used.</p> <p>Attributes:</p> Name Type Description <code>llm</code> <code>BaseLlm</code> <p>The LLM instance to use for this task.</p> <code>system_template</code> <code>Optional[Union[str, EjinjaPromptTemplate]]</code> <p>System prompt template. If None, uses DEFAULT_TOOL_SELECTION_SYSTEM_TEMPLATE.</p> <code>instruction_template</code> <code>Optional[Union[str, EjinjaPromptTemplate]]</code> <p>Instruction prompt template. If None, uses DEFAULT_TOOL_SELECTION_INSTRUCTION_TEMPLATE.</p> Source code in <code>bridgic/core/agentic/recent/_recent_task_configs.py</code> <pre><code>class ToolTaskConfig:\n    \"\"\"\n    Configuration for the tool selection task in ReCentAutoma.\n\n    This class allows configuring the LLM and prompt templates for the tool selection task.\n    When system_template or instruction_template is None, the default template will be used.\n\n    Attributes\n    ----------\n    llm : BaseLlm\n        The LLM instance to use for this task.\n    system_template : Optional[Union[str, EjinjaPromptTemplate]]\n        System prompt template. If None, uses DEFAULT_TOOL_SELECTION_SYSTEM_TEMPLATE.\n    instruction_template : Optional[Union[str, EjinjaPromptTemplate]]\n        Instruction prompt template. If None, uses DEFAULT_TOOL_SELECTION_INSTRUCTION_TEMPLATE.\n    \"\"\"\n\n    def __init__(\n        self,\n        llm: BaseLlm,\n        system_template: Optional[Union[str, EjinjaPromptTemplate]] = DEFAULT_TOOL_SELECTION_SYSTEM_TEMPLATE,\n        instruction_template: Optional[Union[str, EjinjaPromptTemplate]] = DEFAULT_TOOL_SELECTION_INSTRUCTION_TEMPLATE,\n    ):\n        self.llm = llm\n        self.system_template = system_template or DEFAULT_TOOL_SELECTION_SYSTEM_TEMPLATE\n        self.instruction_template = instruction_template or DEFAULT_TOOL_SELECTION_INSTRUCTION_TEMPLATE\n\n    def to_llm_task_config(self) -&gt; LlmTaskConfig:\n        return LlmTaskConfig(\n            llm=self.llm,\n            system_template=self.system_template,\n            instruction_template=self.instruction_template,\n        )\n</code></pre>"},{"location":"reference/bridgic-core/bridgic/core/agentic/recent/#bridgic.core.agentic.recent.AnswerTaskConfig","title":"AnswerTaskConfig","text":"<p>Configuration for the answer generation task in ReCentAutoma.</p> <p>This class allows configuring the LLM and prompt templates for the answer generation task. When system_template or instruction_template is None, the default template will be used.</p> <p>Attributes:</p> Name Type Description <code>llm</code> <code>BaseLlm</code> <p>The LLM instance to use for this task.</p> <code>system_template</code> <code>Optional[Union[str, EjinjaPromptTemplate]]</code> <p>System prompt template. If None, uses DEFAULT_ANSWER_SYSTEM_TEMPLATE.</p> <code>instruction_template</code> <code>Optional[Union[str, EjinjaPromptTemplate]]</code> <p>Instruction prompt template. If None, uses DEFAULT_ANSWER_INSTRUCTION_TEMPLATE.</p> Source code in <code>bridgic/core/agentic/recent/_recent_task_configs.py</code> <pre><code>class AnswerTaskConfig:\n    \"\"\"\n    Configuration for the answer generation task in ReCentAutoma.\n\n    This class allows configuring the LLM and prompt templates for the answer generation task.\n    When system_template or instruction_template is None, the default template will be used.\n\n    Attributes\n    ----------\n    llm : BaseLlm\n        The LLM instance to use for this task.\n    system_template : Optional[Union[str, EjinjaPromptTemplate]]\n        System prompt template. If None, uses DEFAULT_ANSWER_SYSTEM_TEMPLATE.\n    instruction_template : Optional[Union[str, EjinjaPromptTemplate]]\n        Instruction prompt template. If None, uses DEFAULT_ANSWER_INSTRUCTION_TEMPLATE.\n    \"\"\"\n\n    def __init__(\n        self,\n        llm: BaseLlm,\n        system_template: Optional[Union[str, EjinjaPromptTemplate]] = DEFAULT_ANSWER_SYSTEM_TEMPLATE,\n        instruction_template: Optional[Union[str, EjinjaPromptTemplate]] = DEFAULT_ANSWER_INSTRUCTION_TEMPLATE,\n    ):\n        self.llm = llm\n        self.system_template = system_template or DEFAULT_ANSWER_SYSTEM_TEMPLATE\n        self.instruction_template = instruction_template or DEFAULT_ANSWER_INSTRUCTION_TEMPLATE\n\n    def to_llm_task_config(self) -&gt; LlmTaskConfig:\n        return LlmTaskConfig(\n            llm=self.llm,\n            system_template=self.system_template,\n            instruction_template=self.instruction_template,\n        )\n</code></pre>"},{"location":"reference/bridgic-core/bridgic/core/agentic/recent/#bridgic.core.agentic.recent.EpisodicNodeTree","title":"EpisodicNodeTree","text":"<p>               Bases: <code>Serializable</code></p> <p>EpisodicNodeTree is a data structure responsible for managing the sequence of episodic memory nodes,  which is the core data structure of the ReCENT Algorithm.</p> <p>ReCENT Algorithm (Recursive Compressed Episodic Node Tree Algorithm) is an algorithm designed to  address issues such as context explosion and goal drift, by employing a recursive memory compression  mechanism. In this algorithm, each episodic node will serve as a container of memory and could be  tightly organized together to form a more efficient and reliable memory for the higher agentic system.</p> Notes: <ul> <li>This data structure only supports appending new nodes; deletion or insertion is not allowed.</li> <li>All write operations are protected by a lock to ensure atomicity and preserve ordered nature of the structure.</li> <li>The data structure does not and should not perform any computationally expensive operations such as summarization.</li> </ul> Source code in <code>bridgic/core/agentic/recent/_episodic_node_tree.py</code> <pre><code>class EpisodicNodeTree(Serializable):\n    \"\"\"\n    EpisodicNodeTree is a data structure responsible for managing the sequence of episodic memory nodes, \n    which is the core data structure of the ReCENT Algorithm.\n\n    **ReCENT Algorithm** (Recursive Compressed Episodic Node Tree Algorithm) is an algorithm designed to \n    address issues such as context explosion and goal drift, by employing a recursive memory compression \n    mechanism. In this algorithm, each episodic node will serve as a container of memory and could be \n    tightly organized together to form a more efficient and reliable memory for the higher agentic system.\n\n    Notes:\n    ------\n    - This data structure only supports appending new nodes; deletion or insertion is not allowed.\n    - All write operations are protected by a lock to ensure atomicity and preserve ordered nature of the structure.\n    - The data structure does not and should not perform any computationally expensive operations such as summarization.\n    \"\"\"\n\n    _lock: RLock\n    \"\"\"Reentrant lock for thread-safe node write operations.\"\"\"\n\n    _node_sequence: List[BaseEpisodicNode]\n    \"\"\"The sequence of nodes in the episodic node tree.\"\"\"\n\n    _goal_node_timestep: int\n    \"\"\"The timestep of the current goal node. If no goal node exists, it will be -1.\"\"\"\n    _non_goal_node_timesteps: List[int]\n    \"\"\"The timesteps of the non-goal nodes.\"\"\"\n\n    def __init__(self):\n        self._node_sequence = []\n        self._goal_node_timestep = -1\n        self._non_goal_node_timesteps = []\n        self._lock = RLock()\n\n    def get_node(self, timestep: int) -&gt; Optional[BaseEpisodicNode]:\n        \"\"\"\n        Get a node by its timestep.\n\n        Parameters\n        ----------\n        timestep : int\n            The timestep of the node.\n\n        Returns\n        -------\n        Optional[BaseEpisodicNode]\n            The node with the given timestep, or None if not found.\n        \"\"\"\n        if timestep &lt; len(self._node_sequence):\n            return self._node_sequence[timestep]\n        return None\n\n    def get_goal_node(self) -&gt; Optional[GoalEpisodicNode]:\n        \"\"\"\n        Get the current goal node.\n\n        Returns\n        -------\n        Optional[GoalEpisodicNode]\n            The current goal node, or None if no goal node exists.\n        \"\"\"\n        if self._goal_node_timestep != -1:\n            return cast(GoalEpisodicNode, self.get_node(self._goal_node_timestep))\n        return None\n\n    def get_non_goal_nodes(self) -&gt; List[BaseEpisodicNode]:\n        \"\"\"\n        Get all directly accessible non-goal nodes (sorted by timestep).\n\n        Returns\n        -------\n        List[BaseEpisodicNode]\n            List of non-goal nodes sorted by timestep.\n        \"\"\"\n        nodes = []\n        for timestep in self._non_goal_node_timesteps:\n            nodes.append(self.get_node(timestep))\n        return nodes\n\n    def _get_next_timestep(self) -&gt; int:\n        \"\"\"\n        Get the next available timestep.\n\n        Returns\n        -------\n        int\n            The next available timestep.\n        \"\"\"\n        return len(self._node_sequence)\n\n    def _mark_tail_leaf_node_not_appendable(self) -&gt; None:\n        \"\"\"\n        Mark the tail appendable leaf node as not appendable.\n        \"\"\"\n        if self._node_sequence:\n            last_node = self._node_sequence[-1]\n            if isinstance(last_node, LeafEpisodicNode) and last_node.message_appendable:\n                last_node.message_appendable = False\n\n    def add_goal_node(self, goal: str, guidance: Optional[str] = None) -&gt; int:\n        \"\"\"\n        Add a new goal node.\n\n        If a previous goal node exists, its timestep will be linked in the new goal node.\n        The tail appendable leaf node (if exists) will be closed before adding the new goal node.\n\n        Parameters\n        ----------\n        goal : str\n            The goal content.\n        guidance : Optional[str]\n            Optional execution guidance.\n\n        Returns\n        -------\n        int\n            The timestep of the new goal node.\n        \"\"\"\n        with self._lock:\n            # Close the tail appendable leaf node.\n            self._mark_tail_leaf_node_not_appendable()\n\n            # Get the next timestep.\n            new_timestep = self._get_next_timestep()\n\n            # Create the new goal node and record the timestep of the previous goal node.\n            goal_node = GoalEpisodicNode(\n                timestep=new_timestep,\n                goal=goal,\n                guidance=guidance,\n                previous_goal_node_timestep=self._goal_node_timestep\n            )\n\n            # Add the new goal node to the sequence and update the goal timestep.\n            self._node_sequence.append(goal_node)\n            self._goal_node_timestep = new_timestep\n\n        return new_timestep\n\n    def add_leaf_node(self, messages: List[Message]) -&gt; int:\n        \"\"\"\n        Add a new leaf node that is appendable to new messages.\n\n        The tail appendable leaf node will be closed before adding the new node.\n\n        Parameters\n        ----------\n        messages : List[Message]\n            The original message sequence.\n\n        Returns\n        -------\n        int\n            The timestep of the new leaf node.\n        \"\"\"\n        with self._lock:\n            # Close the tail appendable leaf node.\n            self._mark_tail_leaf_node_not_appendable()\n\n            # Get the next timestep.\n            new_timestep = self._get_next_timestep()\n\n            # Create a new appendable leaf node.\n            leaf_node = LeafEpisodicNode(\n                timestep=new_timestep,\n                messages=messages\n            )\n\n            # Add the new leaf node to the sequence and update the non-goal node timesteps.\n            self._node_sequence.append(leaf_node)\n            self._non_goal_node_timesteps.append(new_timestep)\n\n        return new_timestep\n\n    def add_compression_node(self, compressed_timesteps: List[int], summary: Optional[str] = None) -&gt; int:\n        \"\"\"\n        Add a new compression node that summarizes the given non-goal nodes.\n\n        Before creating the compression node, close the last leaf node if it is still appendable.\n        The `compressed_timesteps` list tells which nodes to summarize. Those nodes are then removed \n        from the active list, and the new compression node replaces them in the active non-goal node list.\n\n        Parameters\n        ----------\n        compressed_timesteps : List[int]\n            List of timesteps of the compressed nodes.\n        summary : Optional[str]\n            The compression summary content. If not provided, an unset concurrent.futures.Future of summary will be created.\n\n        Returns\n        -------\n        int\n            The timestep of the new compression node.\n        \"\"\"\n        with self._lock:\n            # Close the tail appendable leaf node.\n            self._mark_tail_leaf_node_not_appendable()\n\n            # Get the next timestep.\n            new_timestep = self._get_next_timestep()\n\n            # Create the compression node.\n            compression_node = CompressionEpisodicNode(\n                timestep=new_timestep,\n                compressed_timesteps=compressed_timesteps,\n                summary=summary,\n            )\n\n            # Add the new compression node to the sequence and update the non-goal node timesteps.\n            self._node_sequence.append(compression_node)\n            self._non_goal_node_timesteps.append(new_timestep)\n\n            # Remove the timesteps of the compressed nodes from _non_goal_node_timesteps.\n            self._non_goal_node_timesteps = [\n                t for t in self._non_goal_node_timesteps \n                if t not in compressed_timesteps\n            ]\n\n        return new_timestep\n\n    def get_tail_appendable_leaf_node(self) -&gt; Optional[LeafEpisodicNode]:\n        \"\"\"\n        Get the tail appendable leaf node if it exists.\n\n        Returns\n        -------\n        Optional[LeafEpisodicNode]\n            The tail appendable leaf node, or None if not found.\n        \"\"\"\n        if self._node_sequence:\n            last_node = self._node_sequence[-1]\n            if isinstance(last_node, LeafEpisodicNode) and last_node.message_appendable:\n                return last_node\n        return None\n\n    def dump_to_dict(self) -&gt; Dict[str, Any]:\n        return {\n            \"node_sequence\": [node.dump_to_dict() for node in self._node_sequence],\n            \"goal_node_timestep\": self._goal_node_timestep,\n            \"non_goal_node_timesteps\": self._non_goal_node_timesteps,\n        }\n\n    def load_from_dict(self, state_dict: Dict[str, Any]) -&gt; None:\n        self._node_sequence = []\n        node_dicts = state_dict.get(\"node_sequence\", [])\n\n        for node_dict in node_dicts:\n            node_type = NodeType(node_dict[\"node_type\"])\n            node = None\n\n            if node_type == NodeType.GOAL:\n                node = GoalEpisodicNode(timestep=0, goal=\"\")\n            elif node_type == NodeType.LEAF:\n                node = LeafEpisodicNode(timestep=0)\n            elif node_type == NodeType.COMPRESSION:\n                node = CompressionEpisodicNode(timestep=0, compressed_timesteps=[])\n            else:\n                raise ValueError(f\"Invalid node type: {node_type}\")\n\n            node.load_from_dict(node_dict)\n            self._node_sequence.append(node)\n\n        self._goal_node_timestep = state_dict.get(\"goal_node_timestep\", -1)\n        self._non_goal_node_timesteps = state_dict.get(\"non_goal_node_timesteps\", [])\n</code></pre>"},{"location":"reference/bridgic-core/bridgic/core/agentic/recent/#bridgic.core.agentic.recent.EpisodicNodeTree.get_node","title":"get_node","text":"<pre><code>get_node(timestep: int) -&gt; Optional[BaseEpisodicNode]\n</code></pre> <p>Get a node by its timestep.</p> <p>Parameters:</p> Name Type Description Default <code>timestep</code> <code>int</code> <p>The timestep of the node.</p> required <p>Returns:</p> Type Description <code>Optional[BaseEpisodicNode]</code> <p>The node with the given timestep, or None if not found.</p> Source code in <code>bridgic/core/agentic/recent/_episodic_node_tree.py</code> <pre><code>def get_node(self, timestep: int) -&gt; Optional[BaseEpisodicNode]:\n    \"\"\"\n    Get a node by its timestep.\n\n    Parameters\n    ----------\n    timestep : int\n        The timestep of the node.\n\n    Returns\n    -------\n    Optional[BaseEpisodicNode]\n        The node with the given timestep, or None if not found.\n    \"\"\"\n    if timestep &lt; len(self._node_sequence):\n        return self._node_sequence[timestep]\n    return None\n</code></pre>"},{"location":"reference/bridgic-core/bridgic/core/agentic/recent/#bridgic.core.agentic.recent.EpisodicNodeTree.get_goal_node","title":"get_goal_node","text":"<pre><code>get_goal_node() -&gt; Optional[GoalEpisodicNode]\n</code></pre> <p>Get the current goal node.</p> <p>Returns:</p> Type Description <code>Optional[GoalEpisodicNode]</code> <p>The current goal node, or None if no goal node exists.</p> Source code in <code>bridgic/core/agentic/recent/_episodic_node_tree.py</code> <pre><code>def get_goal_node(self) -&gt; Optional[GoalEpisodicNode]:\n    \"\"\"\n    Get the current goal node.\n\n    Returns\n    -------\n    Optional[GoalEpisodicNode]\n        The current goal node, or None if no goal node exists.\n    \"\"\"\n    if self._goal_node_timestep != -1:\n        return cast(GoalEpisodicNode, self.get_node(self._goal_node_timestep))\n    return None\n</code></pre>"},{"location":"reference/bridgic-core/bridgic/core/agentic/recent/#bridgic.core.agentic.recent.EpisodicNodeTree.get_non_goal_nodes","title":"get_non_goal_nodes","text":"<pre><code>get_non_goal_nodes() -&gt; List[BaseEpisodicNode]\n</code></pre> <p>Get all directly accessible non-goal nodes (sorted by timestep).</p> <p>Returns:</p> Type Description <code>List[BaseEpisodicNode]</code> <p>List of non-goal nodes sorted by timestep.</p> Source code in <code>bridgic/core/agentic/recent/_episodic_node_tree.py</code> <pre><code>def get_non_goal_nodes(self) -&gt; List[BaseEpisodicNode]:\n    \"\"\"\n    Get all directly accessible non-goal nodes (sorted by timestep).\n\n    Returns\n    -------\n    List[BaseEpisodicNode]\n        List of non-goal nodes sorted by timestep.\n    \"\"\"\n    nodes = []\n    for timestep in self._non_goal_node_timesteps:\n        nodes.append(self.get_node(timestep))\n    return nodes\n</code></pre>"},{"location":"reference/bridgic-core/bridgic/core/agentic/recent/#bridgic.core.agentic.recent.EpisodicNodeTree.add_goal_node","title":"add_goal_node","text":"<pre><code>add_goal_node(\n    goal: str, guidance: Optional[str] = None\n) -&gt; int\n</code></pre> <p>Add a new goal node.</p> <p>If a previous goal node exists, its timestep will be linked in the new goal node. The tail appendable leaf node (if exists) will be closed before adding the new goal node.</p> <p>Parameters:</p> Name Type Description Default <code>goal</code> <code>str</code> <p>The goal content.</p> required <code>guidance</code> <code>Optional[str]</code> <p>Optional execution guidance.</p> <code>None</code> <p>Returns:</p> Type Description <code>int</code> <p>The timestep of the new goal node.</p> Source code in <code>bridgic/core/agentic/recent/_episodic_node_tree.py</code> <pre><code>def add_goal_node(self, goal: str, guidance: Optional[str] = None) -&gt; int:\n    \"\"\"\n    Add a new goal node.\n\n    If a previous goal node exists, its timestep will be linked in the new goal node.\n    The tail appendable leaf node (if exists) will be closed before adding the new goal node.\n\n    Parameters\n    ----------\n    goal : str\n        The goal content.\n    guidance : Optional[str]\n        Optional execution guidance.\n\n    Returns\n    -------\n    int\n        The timestep of the new goal node.\n    \"\"\"\n    with self._lock:\n        # Close the tail appendable leaf node.\n        self._mark_tail_leaf_node_not_appendable()\n\n        # Get the next timestep.\n        new_timestep = self._get_next_timestep()\n\n        # Create the new goal node and record the timestep of the previous goal node.\n        goal_node = GoalEpisodicNode(\n            timestep=new_timestep,\n            goal=goal,\n            guidance=guidance,\n            previous_goal_node_timestep=self._goal_node_timestep\n        )\n\n        # Add the new goal node to the sequence and update the goal timestep.\n        self._node_sequence.append(goal_node)\n        self._goal_node_timestep = new_timestep\n\n    return new_timestep\n</code></pre>"},{"location":"reference/bridgic-core/bridgic/core/agentic/recent/#bridgic.core.agentic.recent.EpisodicNodeTree.add_leaf_node","title":"add_leaf_node","text":"<pre><code>add_leaf_node(messages: List[Message]) -&gt; int\n</code></pre> <p>Add a new leaf node that is appendable to new messages.</p> <p>The tail appendable leaf node will be closed before adding the new node.</p> <p>Parameters:</p> Name Type Description Default <code>messages</code> <code>List[Message]</code> <p>The original message sequence.</p> required <p>Returns:</p> Type Description <code>int</code> <p>The timestep of the new leaf node.</p> Source code in <code>bridgic/core/agentic/recent/_episodic_node_tree.py</code> <pre><code>def add_leaf_node(self, messages: List[Message]) -&gt; int:\n    \"\"\"\n    Add a new leaf node that is appendable to new messages.\n\n    The tail appendable leaf node will be closed before adding the new node.\n\n    Parameters\n    ----------\n    messages : List[Message]\n        The original message sequence.\n\n    Returns\n    -------\n    int\n        The timestep of the new leaf node.\n    \"\"\"\n    with self._lock:\n        # Close the tail appendable leaf node.\n        self._mark_tail_leaf_node_not_appendable()\n\n        # Get the next timestep.\n        new_timestep = self._get_next_timestep()\n\n        # Create a new appendable leaf node.\n        leaf_node = LeafEpisodicNode(\n            timestep=new_timestep,\n            messages=messages\n        )\n\n        # Add the new leaf node to the sequence and update the non-goal node timesteps.\n        self._node_sequence.append(leaf_node)\n        self._non_goal_node_timesteps.append(new_timestep)\n\n    return new_timestep\n</code></pre>"},{"location":"reference/bridgic-core/bridgic/core/agentic/recent/#bridgic.core.agentic.recent.EpisodicNodeTree.add_compression_node","title":"add_compression_node","text":"<pre><code>add_compression_node(\n    compressed_timesteps: List[int],\n    summary: Optional[str] = None,\n) -&gt; int\n</code></pre> <p>Add a new compression node that summarizes the given non-goal nodes.</p> <p>Before creating the compression node, close the last leaf node if it is still appendable. The <code>compressed_timesteps</code> list tells which nodes to summarize. Those nodes are then removed  from the active list, and the new compression node replaces them in the active non-goal node list.</p> <p>Parameters:</p> Name Type Description Default <code>compressed_timesteps</code> <code>List[int]</code> <p>List of timesteps of the compressed nodes.</p> required <code>summary</code> <code>Optional[str]</code> <p>The compression summary content. If not provided, an unset concurrent.futures.Future of summary will be created.</p> <code>None</code> <p>Returns:</p> Type Description <code>int</code> <p>The timestep of the new compression node.</p> Source code in <code>bridgic/core/agentic/recent/_episodic_node_tree.py</code> <pre><code>def add_compression_node(self, compressed_timesteps: List[int], summary: Optional[str] = None) -&gt; int:\n    \"\"\"\n    Add a new compression node that summarizes the given non-goal nodes.\n\n    Before creating the compression node, close the last leaf node if it is still appendable.\n    The `compressed_timesteps` list tells which nodes to summarize. Those nodes are then removed \n    from the active list, and the new compression node replaces them in the active non-goal node list.\n\n    Parameters\n    ----------\n    compressed_timesteps : List[int]\n        List of timesteps of the compressed nodes.\n    summary : Optional[str]\n        The compression summary content. If not provided, an unset concurrent.futures.Future of summary will be created.\n\n    Returns\n    -------\n    int\n        The timestep of the new compression node.\n    \"\"\"\n    with self._lock:\n        # Close the tail appendable leaf node.\n        self._mark_tail_leaf_node_not_appendable()\n\n        # Get the next timestep.\n        new_timestep = self._get_next_timestep()\n\n        # Create the compression node.\n        compression_node = CompressionEpisodicNode(\n            timestep=new_timestep,\n            compressed_timesteps=compressed_timesteps,\n            summary=summary,\n        )\n\n        # Add the new compression node to the sequence and update the non-goal node timesteps.\n        self._node_sequence.append(compression_node)\n        self._non_goal_node_timesteps.append(new_timestep)\n\n        # Remove the timesteps of the compressed nodes from _non_goal_node_timesteps.\n        self._non_goal_node_timesteps = [\n            t for t in self._non_goal_node_timesteps \n            if t not in compressed_timesteps\n        ]\n\n    return new_timestep\n</code></pre>"},{"location":"reference/bridgic-core/bridgic/core/agentic/recent/#bridgic.core.agentic.recent.EpisodicNodeTree.get_tail_appendable_leaf_node","title":"get_tail_appendable_leaf_node","text":"<pre><code>get_tail_appendable_leaf_node() -&gt; (\n    Optional[LeafEpisodicNode]\n)\n</code></pre> <p>Get the tail appendable leaf node if it exists.</p> <p>Returns:</p> Type Description <code>Optional[LeafEpisodicNode]</code> <p>The tail appendable leaf node, or None if not found.</p> Source code in <code>bridgic/core/agentic/recent/_episodic_node_tree.py</code> <pre><code>def get_tail_appendable_leaf_node(self) -&gt; Optional[LeafEpisodicNode]:\n    \"\"\"\n    Get the tail appendable leaf node if it exists.\n\n    Returns\n    -------\n    Optional[LeafEpisodicNode]\n        The tail appendable leaf node, or None if not found.\n    \"\"\"\n    if self._node_sequence:\n        last_node = self._node_sequence[-1]\n        if isinstance(last_node, LeafEpisodicNode) and last_node.message_appendable:\n            return last_node\n    return None\n</code></pre>"},{"location":"reference/bridgic-core/bridgic/core/agentic/recent/#bridgic.core.agentic.recent.BaseEpisodicNode","title":"BaseEpisodicNode","text":"<p>               Bases: <code>Serializable</code>, <code>ABC</code></p> <p>BaseEpisodicNode represents a single memory unit in the memory sequence in the ReCENT Algorithm.</p> Source code in <code>bridgic/core/agentic/recent/_episodic_node.py</code> <pre><code>class BaseEpisodicNode(Serializable, ABC):\n    \"\"\"\n    BaseEpisodicNode represents a single memory unit in the memory sequence in the ReCENT Algorithm.\n    \"\"\"\n\n    node_type: NodeType\n    \"\"\"The type of the node.\"\"\"\n\n    timestep: int\n    \"\"\"The timestep of the node.\"\"\"\n    timestamp: str\n    \"\"\"The timestamp of the node.\"\"\"\n\n    def __init__(self, timestep: int):\n        self.timestep = timestep\n        self.timestamp = datetime.now().isoformat()\n\n    @override\n    def dump_to_dict(self) -&gt; Dict[str, Any]:\n        return {\n            \"node_type\": self.node_type.value,\n            \"timestep\": self.timestep,\n            \"timestamp\": self.timestamp,\n        }\n\n    @override\n    def load_from_dict(self, state_dict: Dict[str, Any]) -&gt; None:\n        self.node_type = NodeType(state_dict[\"node_type\"])\n        self.timestep = state_dict[\"timestep\"]\n        self.timestamp = state_dict[\"timestamp\"]\n</code></pre>"},{"location":"reference/bridgic-core/bridgic/core/agentic/recent/#bridgic.core.agentic.recent.BaseEpisodicNode.node_type","title":"node_type  <code>instance-attribute</code>","text":"<pre><code>node_type: NodeType\n</code></pre> <p>The type of the node.</p>"},{"location":"reference/bridgic-core/bridgic/core/agentic/recent/#bridgic.core.agentic.recent.BaseEpisodicNode.timestep","title":"timestep  <code>instance-attribute</code>","text":"<pre><code>timestep: int = timestep\n</code></pre> <p>The timestep of the node.</p>"},{"location":"reference/bridgic-core/bridgic/core/agentic/recent/#bridgic.core.agentic.recent.BaseEpisodicNode.timestamp","title":"timestamp  <code>instance-attribute</code>","text":"<pre><code>timestamp: str = isoformat()\n</code></pre> <p>The timestamp of the node.</p>"},{"location":"reference/bridgic-core/bridgic/core/agentic/recent/#bridgic.core.agentic.recent.GoalEpisodicNode","title":"GoalEpisodicNode","text":"<p>               Bases: <code>BaseEpisodicNode</code></p> Source code in <code>bridgic/core/agentic/recent/_episodic_node.py</code> <pre><code>class GoalEpisodicNode(BaseEpisodicNode):\n\n    goal: str\n    \"\"\"The content of the goal.\"\"\"\n    guidance: str\n    \"\"\"The guidance to achieve the goal.\"\"\"\n\n    previous_goal_node_timestep: int\n    \"\"\"The timestep of the previous goal node (the goal node that was replaced by this one).\"\"\"\n\n    def __init__(\n        self,\n        timestep: int,\n        goal: str,\n        guidance: Optional[str] = None,\n        previous_goal_node_timestep: Optional[int] = None,\n    ):\n        super().__init__(timestep)\n        self.node_type = NodeType.GOAL\n        self.goal = goal\n        self.guidance = guidance if guidance is not None else \"\"\n        self.previous_goal_node_timestep = previous_goal_node_timestep if previous_goal_node_timestep is not None else -1\n\n    @override\n    def dump_to_dict(self) -&gt; Dict[str, Any]:\n        result = super().dump_to_dict()\n        result[\"content\"] = self.goal\n        result[\"previous_goal_node_timestep\"] = self.previous_goal_node_timestep\n        return result\n\n    @override\n    def load_from_dict(self, state_dict: Dict[str, Any]) -&gt; None:\n        super().load_from_dict(state_dict)\n        self.goal = state_dict[\"content\"]\n        self.previous_goal_node_timestep = state_dict.get(\"previous_goal_node_timestep\", -1)\n</code></pre>"},{"location":"reference/bridgic-core/bridgic/core/agentic/recent/#bridgic.core.agentic.recent.GoalEpisodicNode.goal","title":"goal  <code>instance-attribute</code>","text":"<pre><code>goal: str = goal\n</code></pre> <p>The content of the goal.</p>"},{"location":"reference/bridgic-core/bridgic/core/agentic/recent/#bridgic.core.agentic.recent.GoalEpisodicNode.guidance","title":"guidance  <code>instance-attribute</code>","text":"<pre><code>guidance: str = guidance if guidance is not None else ''\n</code></pre> <p>The guidance to achieve the goal.</p>"},{"location":"reference/bridgic-core/bridgic/core/agentic/recent/#bridgic.core.agentic.recent.GoalEpisodicNode.previous_goal_node_timestep","title":"previous_goal_node_timestep  <code>instance-attribute</code>","text":"<pre><code>previous_goal_node_timestep: int = (\n    previous_goal_node_timestep\n    if previous_goal_node_timestep is not None\n    else -1\n)\n</code></pre> <p>The timestep of the previous goal node (the goal node that was replaced by this one).</p>"},{"location":"reference/bridgic-core/bridgic/core/agentic/recent/#bridgic.core.agentic.recent.LeafEpisodicNode","title":"LeafEpisodicNode","text":"<p>               Bases: <code>BaseEpisodicNode</code></p> Source code in <code>bridgic/core/agentic/recent/_episodic_node.py</code> <pre><code>class LeafEpisodicNode(BaseEpisodicNode):\n\n    messages: List[Message]\n    \"\"\"The original messages of the leaf episodic node.\"\"\"\n\n    message_appendable: bool\n    \"\"\"Whether new message could be appended to the leaf episodic node.\"\"\"\n\n    def __init__(self, timestep: int, messages: Optional[List[Message]] = None):\n        super().__init__(timestep)\n        self.node_type = NodeType.LEAF\n        self.messages = messages if messages is not None else []\n        self.message_appendable = True\n\n    @override\n    def dump_to_dict(self) -&gt; Dict[str, Any]:\n        result = super().dump_to_dict()\n        result[\"messages\"] = self.messages\n        result[\"message_appendable\"] = self.message_appendable\n        return result\n\n    @override\n    def load_from_dict(self, state_dict: Dict[str, Any]) -&gt; None:\n        super().load_from_dict(state_dict)\n        self.messages = state_dict.get(\"messages\")\n        self.message_appendable = state_dict.get(\"message_appendable\")\n</code></pre>"},{"location":"reference/bridgic-core/bridgic/core/agentic/recent/#bridgic.core.agentic.recent.LeafEpisodicNode.messages","title":"messages  <code>instance-attribute</code>","text":"<pre><code>messages: List[Message] = (\n    messages if messages is not None else []\n)\n</code></pre> <p>The original messages of the leaf episodic node.</p>"},{"location":"reference/bridgic-core/bridgic/core/agentic/recent/#bridgic.core.agentic.recent.LeafEpisodicNode.message_appendable","title":"message_appendable  <code>instance-attribute</code>","text":"<pre><code>message_appendable: bool = True\n</code></pre> <p>Whether new message could be appended to the leaf episodic node.</p>"},{"location":"reference/bridgic-core/bridgic/core/agentic/recent/#bridgic.core.agentic.recent.CompressionEpisodicNode","title":"CompressionEpisodicNode","text":"<p>               Bases: <code>BaseEpisodicNode</code></p> <p>Compression node that compresses a sequence of episodic nodes.</p> <p>A compression node is created to summarize and compress multiple episodic nodes. The compression  node contains a summary of the compressed nodes and records their timesteps.</p> Source code in <code>bridgic/core/agentic/recent/_episodic_node.py</code> <pre><code>class CompressionEpisodicNode(BaseEpisodicNode):\n    \"\"\"\n    Compression node that compresses a sequence of episodic nodes.\n\n    A compression node is created to summarize and compress multiple episodic nodes. The compression \n    node contains a summary of the compressed nodes and records their timesteps.\n    \"\"\"\n\n    summary: Future[str]\n    \"\"\"The summary of the compression node, which is a concurrent.futures.Future for cross-thread/event-loop dependency handling.\"\"\"\n\n    compressed_node_timesteps: List[int]\n    \"\"\"The timesteps of the compressed nodes (the nodes that were compressed by this compression node).\"\"\"\n\n    def __init__(self, timestep: int, compressed_timesteps: List[int], summary: Optional[str] = None):\n        super().__init__(timestep)\n        self.node_type = NodeType.COMPRESSION\n        self.summary = Future()\n        if summary:\n            self.summary.set_result(summary)\n        self.compressed_node_timesteps = compressed_timesteps if compressed_timesteps is not None else []\n\n    @override\n    def dump_to_dict(self) -&gt; Dict[str, Any]:\n        result = super().dump_to_dict()\n\n        # Future cannot be serialized, so we try to get the result if done, otherwise use empty string\n        if self.summary.done():\n            try:\n                result[\"summary\"] = self.summary.result()\n            except Exception:\n                result[\"summary\"] = \"\"\n                warnings.warn(\"Failed to get the result of the summary of the compression episodic node, thus empty summary will be stored.\")\n        else:\n            result[\"summary\"] = \"\"\n            warnings.warn(\"The summary of the compression episodic node is not ready yet, thus empty summary will be stored.\", FutureWarning)\n\n        # Record the timesteps of the nodes that were compressed.\n        result[\"compressed_node_timesteps\"] = self.compressed_node_timesteps\n        return result\n\n    @override\n    def load_from_dict(self, state_dict: Dict[str, Any]) -&gt; None:\n        super().load_from_dict(state_dict)\n        self.summary = Future()\n        self.summary.set_result(state_dict.get(\"summary\", \"\"))\n        self.compressed_node_timesteps = state_dict.get(\"compressed_node_timesteps\", [])\n</code></pre>"},{"location":"reference/bridgic-core/bridgic/core/agentic/recent/#bridgic.core.agentic.recent.CompressionEpisodicNode.summary","title":"summary  <code>instance-attribute</code>","text":"<pre><code>summary: Future[str] = Future()\n</code></pre> <p>The summary of the compression node, which is a concurrent.futures.Future for cross-thread/event-loop dependency handling.</p>"},{"location":"reference/bridgic-core/bridgic/core/agentic/recent/#bridgic.core.agentic.recent.CompressionEpisodicNode.compressed_node_timesteps","title":"compressed_node_timesteps  <code>instance-attribute</code>","text":"<pre><code>compressed_node_timesteps: List[int] = (\n    compressed_timesteps\n    if compressed_timesteps is not None\n    else []\n)\n</code></pre> <p>The timesteps of the compressed nodes (the nodes that were compressed by this compression node).</p>"},{"location":"reference/bridgic-core/bridgic/core/agentic/tool_specs/","title":"tool_specs","text":""},{"location":"reference/bridgic-core/bridgic/core/agentic/tool_specs/#tool_specs","title":"tool_specs","text":"<p>The Tool Specs module provides definitions and implementations of tool specifications.</p> <p>This module contains various tool specification classes that support transforming  \"tool ingredients\" such as Python functions and Automa workflows into LLM-callable tools,  enabling callable objects to be seamlessly used in agentic systems.</p>"},{"location":"reference/bridgic-core/bridgic/core/agentic/tool_specs/#bridgic.core.agentic.tool_specs.ToolSpec","title":"ToolSpec","text":"<p>               Bases: <code>Serializable</code></p> <p>ToolSpec is an abstract class that represents a tool specification that describes all necessary information about a tool used by the LLM. </p> <p>ToolSpec and its subclasses are responsible for providing four categories of interfaces: 1. Transformations to LLM Tool: <code>to_tool</code>. 2. Worker Creation: <code>create_worker</code>. 3. Serialization and Deserialization. 4. ToolSpec initialization from raw resources: <code>from_raw</code>.</p> Source code in <code>bridgic/core/agentic/tool_specs/_base_tool_spec.py</code> <pre><code>class ToolSpec(Serializable):\n    \"\"\"\n    ToolSpec is an abstract class that represents a tool specification that describes all necessary information about a tool used by the LLM. \n\n    ToolSpec and its subclasses are responsible for providing four categories of interfaces:\n    1. Transformations to LLM Tool: `to_tool`.\n    2. Worker Creation: `create_worker`.\n    3. Serialization and Deserialization.\n    4. ToolSpec initialization from raw resources: `from_raw`.\n    \"\"\"\n    _tool_name: Optional[str]\n    \"\"\"The name of the tool to be called\"\"\"\n    _tool_description: Optional[str]\n    \"\"\"A description of what the tool does, used by the model to choose when and how to call the tool.\"\"\"\n    _tool_parameters: Optional[Dict[str, Any]]\n    \"\"\"The JSON schema of the tool's parameters\"\"\"\n\n    _from_builder: Optional[bool]\n    \"\"\"Whether this ToolSpec is created from a `ToolSetBuilder`.\"\"\"\n\n    def __init__(\n        self,\n        tool_name: Optional[str] = None,\n        tool_description: Optional[str] = None,\n        tool_parameters: Optional[Dict[str, Any]] = None,\n        from_builder: Optional[bool] = False,\n    ):\n        self._tool_name = tool_name\n        self._tool_description = tool_description\n        self._tool_parameters = tool_parameters\n        self._from_builder = from_builder\n\n    @property\n    def tool_name(self) -&gt; Optional[str]:\n        return self._tool_name\n\n    @property\n    def tool_description(self) -&gt; Optional[str]:\n        return self._tool_description\n\n    @property\n    def tool_parameters(self) -&gt; Optional[Dict[str, Any]]:\n        return self._tool_parameters\n\n    def __str__(self) -&gt; str:\n        return f\"{self.__class__.__name__}(tool_name={self._tool_name}, tool_description={self._tool_description}, tool_parameters={self._tool_parameters})\"\n\n    def __repr__(self) -&gt; str:\n        return f\"&lt;{self.__class__.__name__}(tool_name={self._tool_name}, tool_description={self._tool_description}, tool_parameters={self._tool_parameters})&gt;\"\n\n    ###############################################################\n    ######## Part One of interfaces: Transformations to Tool ######\n    ###############################################################\n\n    @abstractmethod\n    def to_tool(self) -&gt; Tool:\n        \"\"\"\n        Transform this ToolSpec to a `Tool` object used by LLM.\n\n        Returns\n        -------\n        Tool\n            A `Tool` object that can be used by LLM.\n        \"\"\"\n        ...\n\n    ###############################################################\n    ######## Part Two of interfaces: Worker Creation ##############\n    ###############################################################\n\n    @abstractmethod\n    def create_worker(self) -&gt; Worker:\n        \"\"\"\n        Create a Worker from the information included in this ToolSpec.\n\n        Returns\n        -------\n        Worker\n            A new `Worker` object that can be added to an Automa to execute the tool.\n        \"\"\"\n        ...\n\n    ###############################################################\n    ######## Part Three of interfaces: \n    ######## Serialization and Deserialization ####################\n    ###############################################################\n\n    @override\n    def dump_to_dict(self) -&gt; Dict[str, Any]:\n        state_dict = {}\n        if self._tool_name:\n            state_dict[\"tool_name\"] = self._tool_name\n        if self._tool_description:\n            state_dict[\"tool_description\"] = self._tool_description\n        if self._tool_parameters:\n            state_dict[\"tool_parameters\"] = self._tool_parameters\n        return state_dict\n\n    @override\n    def load_from_dict(self, state_dict: Dict[str, Any]) -&gt; None:\n        self._tool_name = state_dict.get(\"tool_name\")\n        self._tool_description = state_dict.get(\"tool_description\")\n        self._tool_parameters = state_dict.get(\"tool_parameters\")\n</code></pre>"},{"location":"reference/bridgic-core/bridgic/core/agentic/tool_specs/#bridgic.core.agentic.tool_specs.ToolSpec.to_tool","title":"to_tool","text":"<code>abstractmethod</code> <pre><code>to_tool() -&gt; Tool\n</code></pre> <p>Transform this ToolSpec to a <code>Tool</code> object used by LLM.</p> <p>Returns:</p> Type Description <code>Tool</code> <p>A <code>Tool</code> object that can be used by LLM.</p> Source code in <code>bridgic/core/agentic/tool_specs/_base_tool_spec.py</code> <pre><code>@abstractmethod\ndef to_tool(self) -&gt; Tool:\n    \"\"\"\n    Transform this ToolSpec to a `Tool` object used by LLM.\n\n    Returns\n    -------\n    Tool\n        A `Tool` object that can be used by LLM.\n    \"\"\"\n    ...\n</code></pre>"},{"location":"reference/bridgic-core/bridgic/core/agentic/tool_specs/#bridgic.core.agentic.tool_specs.ToolSpec.create_worker","title":"create_worker","text":"<code>abstractmethod</code> <pre><code>create_worker() -&gt; Worker\n</code></pre> <p>Create a Worker from the information included in this ToolSpec.</p> <p>Returns:</p> Type Description <code>Worker</code> <p>A new <code>Worker</code> object that can be added to an Automa to execute the tool.</p> Source code in <code>bridgic/core/agentic/tool_specs/_base_tool_spec.py</code> <pre><code>@abstractmethod\ndef create_worker(self) -&gt; Worker:\n    \"\"\"\n    Create a Worker from the information included in this ToolSpec.\n\n    Returns\n    -------\n    Worker\n        A new `Worker` object that can be added to an Automa to execute the tool.\n    \"\"\"\n    ...\n</code></pre>"},{"location":"reference/bridgic-core/bridgic/core/agentic/tool_specs/#bridgic.core.agentic.tool_specs.ToolSetResponse","title":"ToolSetResponse","text":"<p>               Bases: <code>TypedDict</code></p> <p>Response from a ToolSetBuilder containing the built tools and optional extras.</p> <p>Attributes:</p> Name Type Description <code>tool_specs</code> <code>List[ToolSpec]</code> <p>List of ToolSpec instances created by the builder.</p> <code>extras</code> <code>Dict[str, Any]</code> <p>Optional additional data returned by the builder.</p> Source code in <code>bridgic/core/agentic/tool_specs/_base_tool_spec.py</code> <pre><code>class ToolSetResponse(TypedDict):\n    \"\"\"\n    Response from a ToolSetBuilder containing the built tools and optional extras.\n\n    Attributes\n    ----------\n    tool_specs : List[ToolSpec]\n        List of ToolSpec instances created by the builder.\n    extras : Dict[str, Any]\n        Optional additional data returned by the builder.\n    \"\"\"\n    tool_specs: List[ToolSpec]\n    \"\"\"List of ToolSpec instances created by the builder.\"\"\"\n    extras: Dict[str, Any]\n    \"\"\"Optional additional data returned by the builder.\"\"\"\n</code></pre>"},{"location":"reference/bridgic-core/bridgic/core/agentic/tool_specs/#bridgic.core.agentic.tool_specs.ToolSetResponse.tool_specs","title":"tool_specs  <code>instance-attribute</code>","text":"<pre><code>tool_specs: List[ToolSpec]\n</code></pre> <p>List of ToolSpec instances created by the builder.</p>"},{"location":"reference/bridgic-core/bridgic/core/agentic/tool_specs/#bridgic.core.agentic.tool_specs.ToolSetResponse.extras","title":"extras  <code>instance-attribute</code>","text":"<pre><code>extras: Dict[str, Any]\n</code></pre> <p>Optional additional data returned by the builder.</p>"},{"location":"reference/bridgic-core/bridgic/core/agentic/tool_specs/#bridgic.core.agentic.tool_specs.ToolSetBuilder","title":"ToolSetBuilder","text":"<p>               Bases: <code>ABC</code>, <code>Serializable</code></p> <p>Abstract base class for building <code>ToolSpec</code> instances dynamically.</p> <p>This pattern is useful for tools that need to be dynamically created based on runtime  conditions or external resources that may not be serializable. Subclasses must implement  the <code>build</code> method to return a <code>ToolSetResponse</code>.</p> Source code in <code>bridgic/core/agentic/tool_specs/_base_tool_spec.py</code> <pre><code>class ToolSetBuilder(ABC, Serializable):\n    \"\"\"\n    Abstract base class for building `ToolSpec` instances dynamically.\n\n    This pattern is useful for tools that need to be dynamically created based on runtime \n    conditions or external resources that may not be serializable. Subclasses must implement \n    the `build` method to return a `ToolSetResponse`.\n    \"\"\"\n\n    @abstractmethod\n    def build(self) -&gt; ToolSetResponse:\n        \"\"\"\n        Generate and return a collection of `ToolSpec` instances. Each element in `tool_specs` \n        must have `_from_builder=True` set.\n\n        Returns\n        -------\n        ToolSetResponse\n            A response containing the list of `ToolSpec` instances along with optional extras.\n        \"\"\"\n        ...\n\n    @override\n    def dump_to_dict(self) -&gt; Dict[str, Any]:\n        return {}\n\n    @override\n    def load_from_dict(self, state_dict: Dict[str, Any]) -&gt; None:\n        pass\n</code></pre>"},{"location":"reference/bridgic-core/bridgic/core/agentic/tool_specs/#bridgic.core.agentic.tool_specs.ToolSetBuilder.build","title":"build","text":"<code>abstractmethod</code> <pre><code>build() -&gt; ToolSetResponse\n</code></pre> <p>Generate and return a collection of <code>ToolSpec</code> instances. Each element in <code>tool_specs</code>  must have <code>_from_builder=True</code> set.</p> <p>Returns:</p> Type Description <code>ToolSetResponse</code> <p>A response containing the list of <code>ToolSpec</code> instances along with optional extras.</p> Source code in <code>bridgic/core/agentic/tool_specs/_base_tool_spec.py</code> <pre><code>@abstractmethod\ndef build(self) -&gt; ToolSetResponse:\n    \"\"\"\n    Generate and return a collection of `ToolSpec` instances. Each element in `tool_specs` \n    must have `_from_builder=True` set.\n\n    Returns\n    -------\n    ToolSetResponse\n        A response containing the list of `ToolSpec` instances along with optional extras.\n    \"\"\"\n    ...\n</code></pre>"},{"location":"reference/bridgic-core/bridgic/core/agentic/tool_specs/#bridgic.core.agentic.tool_specs.FunctionToolSpec","title":"FunctionToolSpec","text":"<p>               Bases: <code>ToolSpec</code></p> Source code in <code>bridgic/core/agentic/tool_specs/_function_tool_spec.py</code> <pre><code>class FunctionToolSpec(ToolSpec):\n    _func: Callable\n    \"\"\"The python function to be used as a tool\"\"\"\n\n    def __init__(\n        self,\n        func: Callable,\n        tool_name: Optional[str] = None,\n        tool_description: Optional[str] = None,\n        tool_parameters: Optional[Dict[str, Any]] = None,\n        from_builder: Optional[bool] = False,\n    ):\n        super().__init__(\n            tool_name=tool_name,\n            tool_description=tool_description,\n            tool_parameters=tool_parameters,\n            from_builder=from_builder,\n        )\n        self._func = func\n\n    @classmethod\n    def from_raw(\n        cls,\n        func: Callable,\n        tool_name: Optional[str] = None,\n        tool_description: Optional[str] = None,\n        tool_parameters: Optional[Dict[str, Any]] = None,\n    ) -&gt; \"FunctionToolSpec\":\n        \"\"\"\n        Create a FunctionToolSpec from a python function. By default, the tool name, description and parameters' json schema will be extracted from the function's docstring and the parameters' type and description. However, these values can be customized by passing in the corresponding arguments.\n\n        Parameters\n        ----------\n        func : Callable\n            The python function to create a FunctionToolSpec from.\n        tool_name : Optional[str]\n            The name of the tool. If not provided, the function name will be used.\n        tool_description : Optional[str]\n            The description of the tool. If not provided, the function docstring will be used.\n        tool_parameters : Optional[Dict[str, Any]]\n            The JSON schema of the tool's parameters. If not provided, the JSON schema will be constructed properly from the parameters' annotations, the function's signature and/or docstring.\n\n        Returns\n        -------\n        FunctionToolSpec\n            A new `FunctionToolSpec` object.\n        \"\"\"\n        if isinstance(func, MethodType):\n            raise ValueError(f\"`func` is not allowed to be a bound method: {func}.\")\n\n        if not tool_name:\n            tool_name = func.__name__\n\n        if not tool_description:\n            tool_description = get_tool_description_from(func, tool_name)\n\n        if not tool_parameters:\n            tool_parameters = create_func_params_json_schema(func)\n            # TODO: whether to remove the `title` field of the params_schema?\n\n        return cls(\n            func=func,\n            tool_name=tool_name,\n            tool_description=tool_description,\n            tool_parameters=tool_parameters\n        )\n\n    @override\n    def to_tool(self) -&gt; Tool:\n        \"\"\"\n        Transform this FunctionToolSpec to a `Tool` object used by LLM.\n\n        Returns\n        -------\n        Tool\n            A `Tool` object that can be used by LLM.\n        \"\"\"\n        return Tool(\n            name=self._tool_name,\n            description=self._tool_description,\n            parameters=self._tool_parameters\n        )\n\n    @override\n    def create_worker(self) -&gt; Worker:\n        \"\"\"\n        Create a Worker from the information included in this FunctionToolSpec.\n\n        Returns\n        -------\n        Worker\n            A new `Worker` object that can be added to an Automa to execute the tool.\n        \"\"\"\n        # TODO: some initialization arguments may be needed in future, e.g., `bound_needed`.\n        return CallableWorker(self._func)\n\n    @override\n    def dump_to_dict(self) -&gt; Dict[str, Any]:\n        state_dict = super().dump_to_dict()\n        state_dict[\"func\"] = self._func.__module__ + \".\" + self._func.__qualname__\n        return state_dict\n\n    @override\n    def load_from_dict(self, state_dict: Dict[str, Any]) -&gt; None:\n        super().load_from_dict(state_dict)\n        self._func = load_qualified_class_or_func(state_dict[\"func\"])\n</code></pre>"},{"location":"reference/bridgic-core/bridgic/core/agentic/tool_specs/#bridgic.core.agentic.tool_specs.FunctionToolSpec.from_raw","title":"from_raw","text":"<code>classmethod</code> <pre><code>from_raw(\n    func: Callable,\n    tool_name: Optional[str] = None,\n    tool_description: Optional[str] = None,\n    tool_parameters: Optional[Dict[str, Any]] = None,\n) -&gt; FunctionToolSpec\n</code></pre> <p>Create a FunctionToolSpec from a python function. By default, the tool name, description and parameters' json schema will be extracted from the function's docstring and the parameters' type and description. However, these values can be customized by passing in the corresponding arguments.</p> <p>Parameters:</p> Name Type Description Default <code>func</code> <code>Callable</code> <p>The python function to create a FunctionToolSpec from.</p> required <code>tool_name</code> <code>Optional[str]</code> <p>The name of the tool. If not provided, the function name will be used.</p> <code>None</code> <code>tool_description</code> <code>Optional[str]</code> <p>The description of the tool. If not provided, the function docstring will be used.</p> <code>None</code> <code>tool_parameters</code> <code>Optional[Dict[str, Any]]</code> <p>The JSON schema of the tool's parameters. If not provided, the JSON schema will be constructed properly from the parameters' annotations, the function's signature and/or docstring.</p> <code>None</code> <p>Returns:</p> Type Description <code>FunctionToolSpec</code> <p>A new <code>FunctionToolSpec</code> object.</p> Source code in <code>bridgic/core/agentic/tool_specs/_function_tool_spec.py</code> <pre><code>@classmethod\ndef from_raw(\n    cls,\n    func: Callable,\n    tool_name: Optional[str] = None,\n    tool_description: Optional[str] = None,\n    tool_parameters: Optional[Dict[str, Any]] = None,\n) -&gt; \"FunctionToolSpec\":\n    \"\"\"\n    Create a FunctionToolSpec from a python function. By default, the tool name, description and parameters' json schema will be extracted from the function's docstring and the parameters' type and description. However, these values can be customized by passing in the corresponding arguments.\n\n    Parameters\n    ----------\n    func : Callable\n        The python function to create a FunctionToolSpec from.\n    tool_name : Optional[str]\n        The name of the tool. If not provided, the function name will be used.\n    tool_description : Optional[str]\n        The description of the tool. If not provided, the function docstring will be used.\n    tool_parameters : Optional[Dict[str, Any]]\n        The JSON schema of the tool's parameters. If not provided, the JSON schema will be constructed properly from the parameters' annotations, the function's signature and/or docstring.\n\n    Returns\n    -------\n    FunctionToolSpec\n        A new `FunctionToolSpec` object.\n    \"\"\"\n    if isinstance(func, MethodType):\n        raise ValueError(f\"`func` is not allowed to be a bound method: {func}.\")\n\n    if not tool_name:\n        tool_name = func.__name__\n\n    if not tool_description:\n        tool_description = get_tool_description_from(func, tool_name)\n\n    if not tool_parameters:\n        tool_parameters = create_func_params_json_schema(func)\n        # TODO: whether to remove the `title` field of the params_schema?\n\n    return cls(\n        func=func,\n        tool_name=tool_name,\n        tool_description=tool_description,\n        tool_parameters=tool_parameters\n    )\n</code></pre>"},{"location":"reference/bridgic-core/bridgic/core/agentic/tool_specs/#bridgic.core.agentic.tool_specs.FunctionToolSpec.to_tool","title":"to_tool","text":"<pre><code>to_tool() -&gt; Tool\n</code></pre> <p>Transform this FunctionToolSpec to a <code>Tool</code> object used by LLM.</p> <p>Returns:</p> Type Description <code>Tool</code> <p>A <code>Tool</code> object that can be used by LLM.</p> Source code in <code>bridgic/core/agentic/tool_specs/_function_tool_spec.py</code> <pre><code>@override\ndef to_tool(self) -&gt; Tool:\n    \"\"\"\n    Transform this FunctionToolSpec to a `Tool` object used by LLM.\n\n    Returns\n    -------\n    Tool\n        A `Tool` object that can be used by LLM.\n    \"\"\"\n    return Tool(\n        name=self._tool_name,\n        description=self._tool_description,\n        parameters=self._tool_parameters\n    )\n</code></pre>"},{"location":"reference/bridgic-core/bridgic/core/agentic/tool_specs/#bridgic.core.agentic.tool_specs.FunctionToolSpec.create_worker","title":"create_worker","text":"<pre><code>create_worker() -&gt; Worker\n</code></pre> <p>Create a Worker from the information included in this FunctionToolSpec.</p> <p>Returns:</p> Type Description <code>Worker</code> <p>A new <code>Worker</code> object that can be added to an Automa to execute the tool.</p> Source code in <code>bridgic/core/agentic/tool_specs/_function_tool_spec.py</code> <pre><code>@override\ndef create_worker(self) -&gt; Worker:\n    \"\"\"\n    Create a Worker from the information included in this FunctionToolSpec.\n\n    Returns\n    -------\n    Worker\n        A new `Worker` object that can be added to an Automa to execute the tool.\n    \"\"\"\n    # TODO: some initialization arguments may be needed in future, e.g., `bound_needed`.\n    return CallableWorker(self._func)\n</code></pre>"},{"location":"reference/bridgic-core/bridgic/core/agentic/tool_specs/#bridgic.core.agentic.tool_specs.AutomaToolSpec","title":"AutomaToolSpec","text":"<p>               Bases: <code>ToolSpec</code></p> Source code in <code>bridgic/core/agentic/tool_specs/_automa_tool_spec.py</code> <pre><code>class AutomaToolSpec(ToolSpec):\n    _automa_cls: Type[Automa]\n    \"\"\"The Automa class to be used as a tool\"\"\"\n    _automa_init_kwargs: Dict[str, Any]\n    \"\"\"The initialization arguments for the Automa\"\"\"\n\n    def __init__(\n        self,\n        automa_cls: Type[Automa],\n        tool_name: Optional[str] = None,\n        tool_description: Optional[str] = None,\n        tool_parameters: Optional[Dict[str, Any]] = None,\n        from_builder: Optional[bool] = False,\n        **automa_init_kwargs: Dict[str, Any],\n    ):\n        super().__init__(\n            tool_name=tool_name,\n            tool_description=tool_description,\n            tool_parameters=tool_parameters,\n            from_builder=from_builder,\n        )\n        self._automa_cls = automa_cls\n        self._automa_init_kwargs = automa_init_kwargs\n\n    @classmethod\n    def from_raw(\n        cls,\n        automa_cls: Type[Automa],\n        tool_name: Optional[str] = None,\n        tool_description: Optional[str] = None,\n        tool_parameters: Optional[Dict[str, Any]] = None,\n        **automa_init_kwargs: Dict[str, Any],\n    ) -&gt; \"AutomaToolSpec\":\n        \"\"\"\n        Create an AutomaToolSpec from an Automa class.\n        \"\"\"\n\n        def check_spec_func(automa_cls):\n            if hasattr(automa_cls, \"spec_func\") and isinstance(automa_cls.spec_func, Callable):\n                return\n            raise ValueError(f\"The Automa class {automa_cls} must be decorated with `@as_tool` in order to be used as a tool.\")\n\n        if (not tool_name) or (not tool_description) or (not tool_parameters):\n            check_spec_func(automa_cls)\n\n        if not tool_name:\n            tool_name = automa_cls.spec_func.__name__\n\n        if not tool_description:\n            tool_description = get_tool_description_from(automa_cls.spec_func, tool_name)\n\n        if not tool_parameters:\n            tool_parameters = create_func_params_json_schema(automa_cls.spec_func)\n            # TODO: whether to remove the `title` field of the params_schema?\n\n        return cls(\n            automa_cls=automa_cls,\n            tool_name=tool_name,\n            tool_description=tool_description,\n            tool_parameters=tool_parameters,\n            **automa_init_kwargs\n        )\n\n    @override\n    def to_tool(self) -&gt; Tool:\n        \"\"\"\n        Transform this AutomaToolSpec to a `Tool` object used by LLM.\n\n        Returns\n        -------\n        Tool\n            A `Tool` object that can be used by LLM.\n        \"\"\"\n        return Tool(\n            name=self._tool_name,\n            description=self._tool_description,\n            parameters=self._tool_parameters\n        )\n\n    @override\n    def create_worker(self) -&gt; Worker:\n        \"\"\"\n        Create a Worker from the information included in this AutomaToolSpec.\n\n        Returns\n        -------\n        Worker\n            A new `Worker` object that can be added to an Automa to execute the tool.\n        \"\"\"\n        return self._automa_cls(**self._automa_init_kwargs)\n\n    @override\n    def dump_to_dict(self) -&gt; Dict[str, Any]:\n        state_dict = super().dump_to_dict()\n        state_dict[\"automa_cls\"] = self._automa_cls.__module__ + \".\" + self._automa_cls.__qualname__\n        if self._automa_init_kwargs:\n            state_dict[\"automa_init_kwargs\"] = self._automa_init_kwargs\n        return state_dict\n\n    @override\n    def load_from_dict(self, state_dict: Dict[str, Any]) -&gt; None:\n        super().load_from_dict(state_dict)\n        self._automa_cls = load_qualified_class_or_func(state_dict[\"automa_cls\"])\n        self._automa_init_kwargs = state_dict.get(\"automa_init_kwargs\") or {}\n</code></pre>"},{"location":"reference/bridgic-core/bridgic/core/agentic/tool_specs/#bridgic.core.agentic.tool_specs.AutomaToolSpec.from_raw","title":"from_raw","text":"<code>classmethod</code> <pre><code>from_raw(\n    automa_cls: Type[Automa],\n    tool_name: Optional[str] = None,\n    tool_description: Optional[str] = None,\n    tool_parameters: Optional[Dict[str, Any]] = None,\n    **automa_init_kwargs: Dict[str, Any]\n) -&gt; AutomaToolSpec\n</code></pre> <p>Create an AutomaToolSpec from an Automa class.</p> Source code in <code>bridgic/core/agentic/tool_specs/_automa_tool_spec.py</code> <pre><code>@classmethod\ndef from_raw(\n    cls,\n    automa_cls: Type[Automa],\n    tool_name: Optional[str] = None,\n    tool_description: Optional[str] = None,\n    tool_parameters: Optional[Dict[str, Any]] = None,\n    **automa_init_kwargs: Dict[str, Any],\n) -&gt; \"AutomaToolSpec\":\n    \"\"\"\n    Create an AutomaToolSpec from an Automa class.\n    \"\"\"\n\n    def check_spec_func(automa_cls):\n        if hasattr(automa_cls, \"spec_func\") and isinstance(automa_cls.spec_func, Callable):\n            return\n        raise ValueError(f\"The Automa class {automa_cls} must be decorated with `@as_tool` in order to be used as a tool.\")\n\n    if (not tool_name) or (not tool_description) or (not tool_parameters):\n        check_spec_func(automa_cls)\n\n    if not tool_name:\n        tool_name = automa_cls.spec_func.__name__\n\n    if not tool_description:\n        tool_description = get_tool_description_from(automa_cls.spec_func, tool_name)\n\n    if not tool_parameters:\n        tool_parameters = create_func_params_json_schema(automa_cls.spec_func)\n        # TODO: whether to remove the `title` field of the params_schema?\n\n    return cls(\n        automa_cls=automa_cls,\n        tool_name=tool_name,\n        tool_description=tool_description,\n        tool_parameters=tool_parameters,\n        **automa_init_kwargs\n    )\n</code></pre>"},{"location":"reference/bridgic-core/bridgic/core/agentic/tool_specs/#bridgic.core.agentic.tool_specs.AutomaToolSpec.to_tool","title":"to_tool","text":"<pre><code>to_tool() -&gt; Tool\n</code></pre> <p>Transform this AutomaToolSpec to a <code>Tool</code> object used by LLM.</p> <p>Returns:</p> Type Description <code>Tool</code> <p>A <code>Tool</code> object that can be used by LLM.</p> Source code in <code>bridgic/core/agentic/tool_specs/_automa_tool_spec.py</code> <pre><code>@override\ndef to_tool(self) -&gt; Tool:\n    \"\"\"\n    Transform this AutomaToolSpec to a `Tool` object used by LLM.\n\n    Returns\n    -------\n    Tool\n        A `Tool` object that can be used by LLM.\n    \"\"\"\n    return Tool(\n        name=self._tool_name,\n        description=self._tool_description,\n        parameters=self._tool_parameters\n    )\n</code></pre>"},{"location":"reference/bridgic-core/bridgic/core/agentic/tool_specs/#bridgic.core.agentic.tool_specs.AutomaToolSpec.create_worker","title":"create_worker","text":"<pre><code>create_worker() -&gt; Worker\n</code></pre> <p>Create a Worker from the information included in this AutomaToolSpec.</p> <p>Returns:</p> Type Description <code>Worker</code> <p>A new <code>Worker</code> object that can be added to an Automa to execute the tool.</p> Source code in <code>bridgic/core/agentic/tool_specs/_automa_tool_spec.py</code> <pre><code>@override\ndef create_worker(self) -&gt; Worker:\n    \"\"\"\n    Create a Worker from the information included in this AutomaToolSpec.\n\n    Returns\n    -------\n    Worker\n        A new `Worker` object that can be added to an Automa to execute the tool.\n    \"\"\"\n    return self._automa_cls(**self._automa_init_kwargs)\n</code></pre>"},{"location":"reference/bridgic-core/bridgic/core/agentic/tool_specs/#bridgic.core.agentic.tool_specs.as_tool","title":"as_tool","text":"<pre><code>as_tool(spec_func: Callable) -&gt; Callable\n</code></pre> <p>A decorator that transforms a class to a tool that may be used by LLM.</p> <p>Parameters:</p> Name Type Description Default <code>spec_func</code> <code>Callable</code> <p>The function used to declare the tool spec. Note that this function is not intended to be called directly.</p> required Source code in <code>bridgic/core/agentic/tool_specs/_automa_tool_spec.py</code> <pre><code>def as_tool(spec_func: Callable) -&gt; Callable:\n    \"\"\"\n    A decorator that transforms a class to a tool that may be used by LLM.\n\n    Parameters\n    ----------\n    spec_func : Callable\n        The function used to declare the tool spec. Note that this function is not intended to be called directly.\n    \"\"\"\n    def decorator(cls):\n        if not isinstance(spec_func, Callable):\n            raise ValueError(f\"A function argument is expected, but got {type(spec_func)}.\")\n        if isinstance(spec_func, MethodType):\n            raise ValueError(f\"`spec_func` is not allowed to be a bound method: {spec_func}.\")\n        cls.spec_func = spec_func\n        return cls\n    return decorator\n</code></pre>"},{"location":"reference/bridgic-core/bridgic/core/agentic/types/","title":"types","text":""},{"location":"reference/bridgic-core/bridgic/core/agentic/types/#types","title":"types","text":"<p>The Agentic Types module defines foundational data structures for agentic systems.</p> <p>This module defines several important type definitions, such as <code>ToolSpec</code> and  <code>ChatMessage</code>, which are designed to be \"model-neutral\" as much as possible,  allowing developers to build agentic systems using different models.</p>"},{"location":"reference/bridgic-core/bridgic/core/agentic/types/#bridgic.core.agentic.types.Function","title":"Function","text":"<p>               Bases: <code>TypedDict</code></p> <p>The function that the model called.</p> Source code in <code>bridgic/core/agentic/types/_chat_message.py</code> <pre><code>class Function(TypedDict, total=True):\n    \"\"\"The function that the model called.\"\"\"\n\n    arguments: Required[str]\n    \"\"\"\n    The arguments to call the function with, as generated by the model in JSON\n    format. Note that the model does not always generate valid JSON, and may\n    hallucinate parameters not defined by your function schema. Validate the\n    arguments in your code before calling your function.\n    \"\"\"\n    name: Required[str]\n    \"\"\"The name of the function to call.\"\"\"\n</code></pre>"},{"location":"reference/bridgic-core/bridgic/core/agentic/types/#bridgic.core.agentic.types.Function.arguments","title":"arguments  <code>instance-attribute</code>","text":"<pre><code>arguments: Required[str]\n</code></pre> <p>The arguments to call the function with, as generated by the model in JSON format. Note that the model does not always generate valid JSON, and may hallucinate parameters not defined by your function schema. Validate the arguments in your code before calling your function.</p>"},{"location":"reference/bridgic-core/bridgic/core/agentic/types/#bridgic.core.agentic.types.Function.name","title":"name  <code>instance-attribute</code>","text":"<pre><code>name: Required[str]\n</code></pre> <p>The name of the function to call.</p>"},{"location":"reference/bridgic-core/bridgic/core/agentic/types/#bridgic.core.agentic.types.FunctionToolCall","title":"FunctionToolCall","text":"<p>               Bases: <code>TypedDict</code></p> <p>A call to a function tool created by the model.</p> Source code in <code>bridgic/core/agentic/types/_chat_message.py</code> <pre><code>class FunctionToolCall(TypedDict, total=True):\n    \"\"\"A call to a function tool created by the model.\"\"\"\n\n    id: Required[str]\n    \"\"\"The ID of the tool call.\"\"\"\n    function: Required[Function]\n    \"\"\"The function that the model called.\"\"\"\n    type: Required[Literal[\"function\"]]\n    \"\"\"The type of the tool. Currently, only `function` is supported.\"\"\"\n</code></pre>"},{"location":"reference/bridgic-core/bridgic/core/agentic/types/#bridgic.core.agentic.types.FunctionToolCall.id","title":"id  <code>instance-attribute</code>","text":"<pre><code>id: Required[str]\n</code></pre> <p>The ID of the tool call.</p>"},{"location":"reference/bridgic-core/bridgic/core/agentic/types/#bridgic.core.agentic.types.FunctionToolCall.function","title":"function  <code>instance-attribute</code>","text":"<pre><code>function: Required[Function]\n</code></pre> <p>The function that the model called.</p>"},{"location":"reference/bridgic-core/bridgic/core/agentic/types/#bridgic.core.agentic.types.FunctionToolCall.type","title":"type  <code>instance-attribute</code>","text":"<pre><code>type: Required[Literal['function']]\n</code></pre> <p>The type of the tool. Currently, only <code>function</code> is supported.</p>"},{"location":"reference/bridgic-core/bridgic/core/agentic/types/#bridgic.core.agentic.types.SystemMessage","title":"SystemMessage","text":"<p>               Bases: <code>TypedDict</code></p> <p>Developer-provided instructions that the model should follow, regardless of messages sent by the user.</p> Source code in <code>bridgic/core/agentic/types/_chat_message.py</code> <pre><code>class SystemMessage(TypedDict, total=False):\n    \"\"\"Developer-provided instructions that the model should follow, regardless of messages sent by the user.\"\"\"\n\n    role: Required[Literal[\"system\"]]\n    \"\"\"The role of the messages author, in this case `system`.\"\"\"\n    content: Required[str]\n    \"\"\"The contents of the system message, which is a text.\"\"\"\n</code></pre>"},{"location":"reference/bridgic-core/bridgic/core/agentic/types/#bridgic.core.agentic.types.SystemMessage.role","title":"role  <code>instance-attribute</code>","text":"<pre><code>role: Required[Literal['system']]\n</code></pre> <p>The role of the messages author, in this case <code>system</code>.</p>"},{"location":"reference/bridgic-core/bridgic/core/agentic/types/#bridgic.core.agentic.types.SystemMessage.content","title":"content  <code>instance-attribute</code>","text":"<pre><code>content: Required[str]\n</code></pre> <p>The contents of the system message, which is a text.</p>"},{"location":"reference/bridgic-core/bridgic/core/agentic/types/#bridgic.core.agentic.types.UserTextMessage","title":"UserTextMessage","text":"<p>               Bases: <code>TypedDict</code></p> <p>Messages sent by an end user, containing prompts.</p> Source code in <code>bridgic/core/agentic/types/_chat_message.py</code> <pre><code>class UserTextMessage(TypedDict, total=False):\n    \"\"\"Messages sent by an end user, containing prompts.\"\"\"\n\n    role: Required[Literal[\"user\"]]\n    \"\"\"The role of the messages author, in this case `user`.\"\"\"\n    content: Required[str]\n    \"\"\"The content of the user message, which is a text.\"\"\"\n</code></pre>"},{"location":"reference/bridgic-core/bridgic/core/agentic/types/#bridgic.core.agentic.types.UserTextMessage.role","title":"role  <code>instance-attribute</code>","text":"<pre><code>role: Required[Literal['user']]\n</code></pre> <p>The role of the messages author, in this case <code>user</code>.</p>"},{"location":"reference/bridgic-core/bridgic/core/agentic/types/#bridgic.core.agentic.types.UserTextMessage.content","title":"content  <code>instance-attribute</code>","text":"<pre><code>content: Required[str]\n</code></pre> <p>The content of the user message, which is a text.</p>"},{"location":"reference/bridgic-core/bridgic/core/agentic/types/#bridgic.core.agentic.types.AssistantTextMessage","title":"AssistantTextMessage","text":"<p>               Bases: <code>TypedDict</code></p> <p>Messages sent by the model in response to user messages.</p> Source code in <code>bridgic/core/agentic/types/_chat_message.py</code> <pre><code>class AssistantTextMessage(TypedDict, total=False):\n    \"\"\"Messages sent by the model in response to user messages.\"\"\"\n\n    role: Required[Literal[\"assistant\"]]\n    \"\"\"The role of the messages author, in this case `assistant`.\"\"\"\n    content: Optional[str]\n    \"\"\"The content of the assistant message, which is a text. Required unless `tool_calls` is specified.\"\"\"\n    tool_calls: Optional[Iterable[FunctionToolCall]]\n    \"\"\"The tool calls generated by the model, such as function calls.\"\"\"\n</code></pre>"},{"location":"reference/bridgic-core/bridgic/core/agentic/types/#bridgic.core.agentic.types.AssistantTextMessage.role","title":"role  <code>instance-attribute</code>","text":"<pre><code>role: Required[Literal['assistant']]\n</code></pre> <p>The role of the messages author, in this case <code>assistant</code>.</p>"},{"location":"reference/bridgic-core/bridgic/core/agentic/types/#bridgic.core.agentic.types.AssistantTextMessage.content","title":"content  <code>instance-attribute</code>","text":"<pre><code>content: Optional[str]\n</code></pre> <p>The content of the assistant message, which is a text. Required unless <code>tool_calls</code> is specified.</p>"},{"location":"reference/bridgic-core/bridgic/core/agentic/types/#bridgic.core.agentic.types.AssistantTextMessage.tool_calls","title":"tool_calls  <code>instance-attribute</code>","text":"<pre><code>tool_calls: Optional[Iterable[FunctionToolCall]]\n</code></pre> <p>The tool calls generated by the model, such as function calls.</p>"},{"location":"reference/bridgic-core/bridgic/core/agentic/types/#bridgic.core.agentic.types.ToolMessage","title":"ToolMessage","text":"<p>               Bases: <code>TypedDict</code></p> <p>Messages generated by tools.</p> Source code in <code>bridgic/core/agentic/types/_chat_message.py</code> <pre><code>class ToolMessage(TypedDict, total=False):\n    \"\"\"Messages generated by tools.\"\"\"\n\n    role: Required[Literal[\"tool\"]]\n    \"\"\"The role of the messages author, in this case `tool`.\"\"\"\n    content: Required[str]\n    \"\"\"The contents of the tool message.\"\"\"\n    tool_call_id: Required[str]\n    \"\"\"Tool call that this message is responding to.\"\"\"\n</code></pre>"},{"location":"reference/bridgic-core/bridgic/core/agentic/types/#bridgic.core.agentic.types.ToolMessage.role","title":"role  <code>instance-attribute</code>","text":"<pre><code>role: Required[Literal['tool']]\n</code></pre> <p>The role of the messages author, in this case <code>tool</code>.</p>"},{"location":"reference/bridgic-core/bridgic/core/agentic/types/#bridgic.core.agentic.types.ToolMessage.content","title":"content  <code>instance-attribute</code>","text":"<pre><code>content: Required[str]\n</code></pre> <p>The contents of the tool message.</p>"},{"location":"reference/bridgic-core/bridgic/core/agentic/types/#bridgic.core.agentic.types.ToolMessage.tool_call_id","title":"tool_call_id  <code>instance-attribute</code>","text":"<pre><code>tool_call_id: Required[str]\n</code></pre> <p>Tool call that this message is responding to.</p>"},{"location":"reference/bridgic-core/bridgic/core/agentic/types/#bridgic.core.agentic.types.LlmTaskConfig","title":"LlmTaskConfig","text":"<p>               Bases: <code>Serializable</code></p> <p>Configuration for a single LLM task in an agentic system.</p> <p>This class provides a generic abstraction for configuring LLM tasks with: - A dedicated LLM instance for the task - Optional system prompt template - Optional instruction prompt template</p> <p>This class serves as a configuration holder and the actual behavior of the system are  determined by the concrete implementations utilizing this configuration.</p> <p>Attributes:</p> Name Type Description <code>llm</code> <code>BaseLlm</code> <p>The LLM instance to use for this task.</p> <code>system_template</code> <code>Optional[EjinjaPromptTemplate]</code> <p>Optional system prompt template. If None, no system message will be added.</p> <code>instruction_template</code> <code>Optional[EjinjaPromptTemplate]</code> <p>Optional instruction prompt template. If None, no instruction message will be added.</p> Source code in <code>bridgic/core/agentic/types/_llm_task_config.py</code> <pre><code>class LlmTaskConfig(Serializable):\n    \"\"\"\n    Configuration for a single LLM task in an agentic system.\n\n    This class provides a generic abstraction for configuring LLM tasks with:\n    - A dedicated LLM instance for the task\n    - Optional system prompt template\n    - Optional instruction prompt template\n\n    This class serves as a configuration holder and the actual behavior of the system are \n    determined by the concrete implementations utilizing this configuration.\n\n    Attributes\n    ----------\n    llm : BaseLlm\n        The LLM instance to use for this task.\n    system_template : Optional[EjinjaPromptTemplate]\n        Optional system prompt template. If None, no system message will be added.\n    instruction_template : Optional[EjinjaPromptTemplate]\n        Optional instruction prompt template. If None, no instruction message will be added.\n    \"\"\"\n\n    llm: BaseLlm\n    \"\"\"The LLM instance to use for this task.\"\"\"\n\n    system_template: Optional[EjinjaPromptTemplate]\n    \"\"\"Optional system prompt template for this task.\"\"\"\n\n    instruction_template: Optional[EjinjaPromptTemplate]\n    \"\"\"Optional instruction prompt template for this task.\"\"\"\n\n    def __init__(\n        self,\n        llm: BaseLlm,\n        system_template: Optional[Union[str, EjinjaPromptTemplate]] = None,\n        instruction_template: Optional[Union[str, EjinjaPromptTemplate]] = None,\n    ):\n        \"\"\"\n        Initialize LLM task configuration.\n\n        Parameters\n        ----------\n        llm : BaseLlm\n            The LLM instance to use for this task.\n        system_template : Optional[Union[str, EjinjaPromptTemplate]]\n            System prompt template. Can be a string (will be converted to EjinjaPromptTemplate)\n            or an EjinjaPromptTemplate instance. If None, no system message will be added.\n        instruction_template : Optional[Union[str, EjinjaPromptTemplate]]\n            Instruction prompt template. Can be a string (will be converted to EjinjaPromptTemplate)\n            or an EjinjaPromptTemplate instance. If None, no instruction message will be added.\n        \"\"\"\n        self.llm = llm\n\n        if system_template is None:\n            self.system_template = None\n        elif isinstance(system_template, str):\n            self.system_template = EjinjaPromptTemplate(system_template)\n        elif isinstance(system_template, EjinjaPromptTemplate):\n            self.system_template = system_template\n        else:\n            raise TypeError(\n                f\"system_template must be str, EjinjaPromptTemplate, or None, \"\n                f\"got {type(system_template)}\"\n            )\n\n        if instruction_template is None:\n            self.instruction_template = None\n        elif isinstance(instruction_template, str):\n            self.instruction_template = EjinjaPromptTemplate(instruction_template)\n        elif isinstance(instruction_template, EjinjaPromptTemplate):\n            self.instruction_template = instruction_template\n        else:\n            raise TypeError(\n                f\"instruction_template must be str, EjinjaPromptTemplate, or None, \"\n                f\"got {type(instruction_template)}\"\n            )\n\n    @override\n    def dump_to_dict(self) -&gt; Dict[str, Any]:\n        state_dict = {}\n        state_dict[\"llm\"] = self.llm\n        state_dict[\"system_template\"] = self.system_template\n        state_dict[\"instruction_template\"] = self.instruction_template\n        return state_dict\n\n    @override\n    def load_from_dict(self, state_dict: Dict[str, Any]) -&gt; None:\n        self.llm = state_dict[\"llm\"]\n        self.system_template = state_dict[\"system_template\"]\n        self.instruction_template = state_dict[\"instruction_template\"]\n</code></pre>"},{"location":"reference/bridgic-core/bridgic/core/agentic/types/#bridgic.core.agentic.types.LlmTaskConfig.llm","title":"llm  <code>instance-attribute</code>","text":"<pre><code>llm: BaseLlm = llm\n</code></pre> <p>The LLM instance to use for this task.</p>"},{"location":"reference/bridgic-core/bridgic/core/agentic/types/#bridgic.core.agentic.types.LlmTaskConfig.system_template","title":"system_template  <code>instance-attribute</code>","text":"<pre><code>system_template: Optional[EjinjaPromptTemplate]\n</code></pre> <p>Optional system prompt template for this task.</p>"},{"location":"reference/bridgic-core/bridgic/core/agentic/types/#bridgic.core.agentic.types.LlmTaskConfig.instruction_template","title":"instruction_template  <code>instance-attribute</code>","text":"<pre><code>instruction_template: Optional[EjinjaPromptTemplate]\n</code></pre> <p>Optional instruction prompt template for this task.</p>"},{"location":"reference/bridgic-core/bridgic/core/agentic/workers/","title":"workers","text":""},{"location":"reference/bridgic-core/bridgic/core/agentic/workers/#workers","title":"workers","text":"<p>The Agentic Workers module provides specialized implementation of Worker for agentic systems.</p> <p>This module provides specialized Worker implementations for specific functions to support  building Agentic systems with complex capabilities.</p>"},{"location":"reference/bridgic-core/bridgic/core/agentic/workers/#bridgic.core.agentic.workers.ToolSelectionWorker","title":"ToolSelectionWorker","text":"<p>               Bases: <code>Worker</code></p> <p>A worker that calls an LLM to select tools and/or generate a response.</p> Source code in <code>bridgic/core/agentic/workers/_tool_selection_worker.py</code> <pre><code>class ToolSelectionWorker(Worker):\n    \"\"\"\n    A worker that calls an LLM to select tools and/or generate a response.\n    \"\"\"\n\n    # Note: the ToolSelection LLM instance need support serialization and deserialization.\n    _tool_selection_llm: ToolSelection\n    \"\"\"The LLM to be used for tool selection.\"\"\"\n\n    def __init__(self, tool_selection_llm: ToolSelection):\n        \"\"\"\n        Parameters\n        ----------\n        tool_selection_llm: ToolSelect\n            The LLM to be used for tool selection.\n        \"\"\"\n        super().__init__()\n        self._tool_selection_llm = tool_selection_llm\n\n    async def arun(\n        self,\n        messages: List[ChatMessage],\n        tools: List[Tool],\n    ) -&gt; Tuple[List[ToolCall], Optional[str]]:\n        \"\"\"\n        Run the worker.\n\n        Parameters\n        ----------\n        messages: List[ChatMessage]\n            The messages to send to the LLM.\n        tools: List[Tool]\n            The tool list for the LLM to select from.\n\n        Returns\n        -------\n        Tuple[List[ToolCall], Optional[str]]\n            * The first element is a list of `ToolCall` that the LLM selected.\n            * The second element is the text response from the LLM.\n        \"\"\"\n        # Validate and transform the input messages and tools to the format expected by the LLM.\n        llm_messages: List[Message] = []\n        for message in messages:\n            llm_messages.append(transform_chat_message_to_llm_message(message))\n\n        tool_calls, llm_response = await self._tool_selection_llm.aselect_tool(\n            messages=llm_messages, \n            tools=tools, \n        )\n\n        from bridgic.core.automa import Automa\n        if isinstance(self.parent, Automa) and self.parent._running_options.debug:\n            for tool_call in tool_calls:\n                printer.print(f\"tool_call: {tool_call}\", color=\"yellow\")\n\n        return tool_calls, llm_response\n\n    @override\n    def dump_to_dict(self) -&gt; Dict[str, Any]:\n        state_dict = super().dump_to_dict()\n        state_dict[\"tool_selection_llm\"] = self._tool_selection_llm\n        return state_dict\n\n    @override\n    def load_from_dict(self, state_dict: Dict[str, Any]) -&gt; None:\n        super().load_from_dict(state_dict)\n        self._tool_selection_llm = state_dict[\"tool_selection_llm\"]\n</code></pre>"},{"location":"reference/bridgic-core/bridgic/core/agentic/workers/#bridgic.core.agentic.workers.ToolSelectionWorker.arun","title":"arun","text":"<code>async</code> <pre><code>arun(\n    messages: List[ChatMessage], tools: List[Tool]\n) -&gt; Tuple[List[ToolCall], Optional[str]]\n</code></pre> <p>Run the worker.</p> <p>Parameters:</p> Name Type Description Default <code>messages</code> <code>List[ChatMessage]</code> <p>The messages to send to the LLM.</p> required <code>tools</code> <code>List[Tool]</code> <p>The tool list for the LLM to select from.</p> required <p>Returns:</p> Type Description <code>Tuple[List[ToolCall], Optional[str]]</code> <ul> <li>The first element is a list of <code>ToolCall</code> that the LLM selected.</li> <li>The second element is the text response from the LLM.</li> </ul> Source code in <code>bridgic/core/agentic/workers/_tool_selection_worker.py</code> <pre><code>async def arun(\n    self,\n    messages: List[ChatMessage],\n    tools: List[Tool],\n) -&gt; Tuple[List[ToolCall], Optional[str]]:\n    \"\"\"\n    Run the worker.\n\n    Parameters\n    ----------\n    messages: List[ChatMessage]\n        The messages to send to the LLM.\n    tools: List[Tool]\n        The tool list for the LLM to select from.\n\n    Returns\n    -------\n    Tuple[List[ToolCall], Optional[str]]\n        * The first element is a list of `ToolCall` that the LLM selected.\n        * The second element is the text response from the LLM.\n    \"\"\"\n    # Validate and transform the input messages and tools to the format expected by the LLM.\n    llm_messages: List[Message] = []\n    for message in messages:\n        llm_messages.append(transform_chat_message_to_llm_message(message))\n\n    tool_calls, llm_response = await self._tool_selection_llm.aselect_tool(\n        messages=llm_messages, \n        tools=tools, \n    )\n\n    from bridgic.core.automa import Automa\n    if isinstance(self.parent, Automa) and self.parent._running_options.debug:\n        for tool_call in tool_calls:\n            printer.print(f\"tool_call: {tool_call}\", color=\"yellow\")\n\n    return tool_calls, llm_response\n</code></pre>"},{"location":"reference/bridgic-core/bridgic/core/automa/","title":"automa","text":""},{"location":"reference/bridgic-core/bridgic/core/automa/#automa","title":"automa","text":"<p>This module contains the core Automa classes and functions.</p>"},{"location":"reference/bridgic-core/bridgic/core/automa/#bridgic.core.automa.Automa","title":"Automa","text":"<p>               Bases: <code>Worker</code></p> <p>Base class for an Automa.</p> <p>In Bridgic, an Automa is an entity that manages and orchestrates a group of workers. An Automa itself is also a Worker, which enables the nesting of Automa instances within each other.</p> Source code in <code>bridgic/core/automa/_automa.py</code> <pre><code>class Automa(Worker):\n    \"\"\"\n    Base class for an Automa.\n\n    In Bridgic, an Automa is an entity that manages and orchestrates a group of workers. An Automa itself is also a Worker, which enables the nesting of Automa instances within each other.\n    \"\"\"\n    _running_options: RunningOptions\n\n    # For event handling.\n    _event_handlers: Dict[str, EventHandlerType]\n    _default_event_handler: EventHandlerType\n\n    # For human interaction.\n    _worker_interaction_indices: Dict[str, int]\n\n    # Ongoing human interactions triggered by the `interact_with_human()` call from workers of the current Automa.\n    # worker_key -&gt; list of interactions.\n    _ongoing_interactions: Dict[str, List[_InteractionAndFeedback]]\n\n    _thread_pool: ThreadPoolExecutor\n    _main_thread_id: int\n    _main_loop: asyncio.AbstractEventLoop\n\n    # Cached callbacks for top-level automa execution, which are built once and reused across multiple arun() calls.\n    _cached_callbacks: Optional[List[WorkerCallback]] = None\n\n    def __init__(\n        self,\n        name: str = None,\n        thread_pool: Optional[ThreadPoolExecutor] = None,\n        running_options: Optional[RunningOptions] = None,\n    ):\n        super().__init__()\n\n        # Set parent to self for top-level Automa\n        self.parent = self\n\n        # Set the name of the Automa instance.\n        self.name = name or f\"{self.__class__.__name__}-{uuid.uuid4().hex[:8]}\"\n\n        # Initialize the shared running options.\n        self._running_options = running_options or RunningOptions()\n\n        # Initialize data structures for event handling and human interactions\n        self._event_handlers = {}\n        self._default_event_handler = None\n        self._worker_interaction_indices = {}\n        self._ongoing_interactions = {}\n\n        self._thread_pool = thread_pool\n\n    @override\n    def dump_to_dict(self) -&gt; Dict[str, Any]:\n        state_dict = super().dump_to_dict()\n        state_dict[\"name\"] = self.name\n        state_dict[\"running_options\"] = self._running_options\n        state_dict[\"ongoing_interactions\"] = self._ongoing_interactions\n        return state_dict\n\n    @override\n    def load_from_dict(self, state_dict: Dict[str, Any]) -&gt; None:\n        super().load_from_dict(state_dict)\n        self.name = state_dict[\"name\"]\n        self._running_options = state_dict[\"running_options\"]\n\n        self._event_handlers = {}\n        self._default_event_handler = None\n        self._worker_interaction_indices = {}\n        self._ongoing_interactions = state_dict[\"ongoing_interactions\"]\n        self._thread_pool = None\n\n    @classmethod\n    def load_from_snapshot(\n        cls, \n        snapshot: Snapshot,\n        thread_pool: Optional[ThreadPoolExecutor] = None,\n    ) -&gt; \"Automa\":\n        \"\"\"\n        Load an Automa instance from a snapshot.\n\n        Parameters\n        ----------\n        snapshot: Snapshot\n            The snapshot to load the Automa instance from.\n        thread_pool: Optional[ThreadPoolExecutor]\n            The thread pool for parallel running of I/O-bound tasks. If not provided, a default thread pool will be used.\n\n        Returns\n        -------\n        Automa\n            The loaded Automa instance.\n        \"\"\"\n        # Here you can compare snapshot.serialization_version with SERIALIZATION_VERSION, and handle any necessary version compatibility issues if needed.\n        automa = load_bytes(snapshot.serialized_bytes)\n        if thread_pool:\n            automa.thread_pool = thread_pool\n        return automa\n\n    @property\n    def thread_pool(self) -&gt; Optional[ThreadPoolExecutor]:\n        \"\"\"\n        Get/Set the thread pool for parallel running of I/O-bound tasks used by the current Automa instance and its nested Automa instances.\n\n        Note: If an Automa is nested within another Automa, the thread pool of the top-level Automa will be used, rather than the thread pool of the nested Automa.\n\n        Returns\n        -------\n        Optional[ThreadPoolExecutor]\n            The thread pool.\n        \"\"\"\n        return self._thread_pool\n\n    @thread_pool.setter\n    def thread_pool(self, executor: ThreadPoolExecutor) -&gt; None:\n        \"\"\"\n        Set the thread pool for parallel running of I/O-bound tasks.\n\n        Note: If an Automa is nested within another Automa, the thread pool of the top-level Automa will be used, rather than the thread pool of the nested Automa.\n        \"\"\"\n        self._thread_pool = executor\n\n    @abstractmethod\n    def _locate_interacting_worker(self) -&gt; Optional[str]:\n        \"\"\"\n        Locate the worker that is currently interacting with human.\n\n        Returns\n        -------\n        Optional[str]\n            The necessary identifier of the worker that is currently interacting with human.\n        \"\"\"\n        ...\n\n    @abstractmethod\n    def _get_worker_key(self, worker: Worker) -&gt; Optional[str]:\n        \"\"\"\n        Identify the worker key by the worker instance.\n        \"\"\"\n        ...\n\n    @abstractmethod\n    def _get_worker_instance(self, worker_key: str) -&gt; Worker:\n        \"\"\"\n        Get the worker instance by the worker key.\n        \"\"\"\n        ...\n\n    def set_running_options(\n        self,\n        debug: Optional[bool] = None,\n        verbose: Optional[bool] = None,\n    ):\n        \"\"\"\n        Set runtime-configurable running options for this Automa instance.\n\n        This method only supports fields that can be changed at runtime. Fields that must be set \n        during initialization (such as `callback_builders`) cannot be changed here and must be \n        provided via the `running_options` parameter in the Automa constructor.\n\n        For fields that can be delayed to be set, some settings (like `debug`) from the outermost \n        (top-level) Automa will override the settings of all inner (nested) Automa instances.\n        For example, if the top-level Automa instance sets `debug = True` and the nested instances \n        sets `debug = False`, then the nested Automa instance will run in debug mode when the \n        top-level Automa instance is executed. We call it the Setting Penetration Mechanism.\n\n        Parameters\n        ----------\n        debug : bool, optional\n            Whether to enable debug mode. If None, the current value is not changed.\n            This field is subject to the Setting Penetration Mechanism.\n        verbose : bool, optional\n            Whether to print more verbose runtime debug information. Only takes effect when `debug=True`.\n            This field is subject to the Setting Penetration Mechanism.\n        \"\"\"\n        if debug is not None:\n            self._running_options.debug = debug\n        if verbose is not None:\n            self._running_options.verbose = verbose\n\n    def _get_top_running_options(self) -&gt; RunningOptions:\n        if self.is_top_level():\n            # Here we are at the top-level automa.\n            return self._running_options\n        return self.parent._get_top_running_options()\n\n    def _collect_ancestor_callback_builders(self) -&gt; List[WorkerCallbackBuilder]:\n        \"\"\"\n        Collect callback builders from all ancestor automas in the ancestor chain.\n\n        This method traverses up the automa ancestor chain (from current to top-level)\n        to collect all callback builders from ancestor automas' RunningOptions, ensuring\n        that callbacks from all levels are propagated to nested workers. The current\n        automa's callbacks are included as the last in the chain.\n\n        The ancestor chain is the path from the current automa up to the top-level automa\n        through the parent relationship. This method collects callbacks from all automas\n        in this chain, ordered from top-level (first) to current level (last).\n\n        Returns\n        -------\n        List[WorkerCallbackBuilder]\n            A list of callback builders collected from all ancestor automas in the ancestor\n            chain and the current automa, ordered from top-level (first) to current level (last).\n        \"\"\"\n        # First, collect all callback builders by traversing up the ancestor chain\n        # (from current to top-level, stored in reverse order)\n        ancestor_callback_builders_reverse = []\n        current: Optional[Automa] = self\n\n        # Traverse up the ancestor chain to collect callback builders\n        while True:\n            ancestor_callback_builders_reverse.append(current._running_options.callback_builders)\n            if current.is_top_level():\n                break\n            else:\n                current = current.parent\n\n        # Reverse to get order from top-level (first) to current (last)\n        callback_builders = []\n        for builders in reversed(ancestor_callback_builders_reverse):\n            callback_builders.extend(builders)\n\n        return callback_builders\n\n    def _get_automa_callbacks(self) -&gt; List[WorkerCallback]:\n        \"\"\"\n        Get or build cached callback instances for top-level automa execution.\n\n        This method ensures that callback instances are built once and reused across\n        multiple arun() calls, respecting the is_shared setting of each builder.\n\n        Returns\n        -------\n        List[WorkerCallback]\n            List of callback instances for top-level automa execution.\n        \"\"\"\n        if self._cached_callbacks is None:\n            effective_builders = []\n            effective_builders.extend(GlobalSetting.read().callback_builders)\n            effective_builders.extend(self._running_options.callback_builders)\n            self._cached_callbacks = [cb.build() for cb in effective_builders]\n        return self._cached_callbacks\n\n    ###############################################################\n    ########## [Bridgic Event Handling Mechanism] starts ##########\n    ###############################################################\n\n    def register_event_handler(self, event_type: Optional[str], event_handler: EventHandlerType) -&gt; None:\n        \"\"\"\n        Register an event handler for the specified event type. If `event_type` is set to None, the event handler will be registered as the default handler that will handle all event types.\n\n        Note: Only event handlers registered on the top-level Automa will be invoked to handle events.\n\n        Parameters\n        ----------\n        event_type: Optional[str]\n            The type of event to be handled. If set to None, the event handler will be registered as the default handler and will be used to handle all event types.\n        event_handler: EventHandlerType\n            The event handler to be registered.\n        \"\"\"\n        if event_type is None:\n            self._default_event_handler = event_handler\n        else:\n            self._event_handlers[event_type] = event_handler\n\n    def unregister_event_handler(self, event_type: Optional[str]) -&gt; None:\n        \"\"\"\n        Unregister an event handler for the specified event type.\n\n        Parameters\n        ----------\n        event_type: Optional[str]\n            The type of event to be unregistered. If set to None, the default event handler will be unregistered.\n        \"\"\"\n        if event_type in self._event_handlers:\n            del self._event_handlers[event_type]\n        if event_type is None:\n            self._default_event_handler = None\n\n    def unregister_all_event_handlers(self) -&gt; None:\n        \"\"\"\n        Unregister all event handlers.\n        \"\"\"\n        self._event_handlers.clear()\n        self._default_event_handler = None\n\n    class _FeedbackSender(FeedbackSender):\n        def __init__(\n                self, \n                future: asyncio.Future[Feedback],\n                post_loop: asyncio.AbstractEventLoop,\n                ):\n            self._future = future\n            self._post_loop = post_loop\n\n        def send(self, feedback: Feedback) -&gt; None:\n            try:\n                current_loop = asyncio.get_running_loop()\n            except Exception:\n                current_loop = None\n            try:\n                if current_loop is self._post_loop:\n                    self._future.set_result(feedback)\n                else:\n                    self._post_loop.call_soon_threadsafe(self._future.set_result, feedback)\n            except asyncio.InvalidStateError:\n                # Suppress the InvalidStateError to be raised, maybe due to timeout.\n                import warnings\n                warnings.warn(f\"Feedback future already set. feedback: {feedback}\", FutureWarning)\n\n    @override\n    def post_event(self, event: Event) -&gt; None:\n        \"\"\"\n        Post an event to the application layer outside the Automa.\n\n        The event handler implemented by the application layer will be called in the same thread as the worker (maybe the main thread or a new thread from the thread pool).\n\n        Note that `post_event` can be called either in a non-async method or in an async method.\n\n        The event will be bubbled up to the top-level Automa, where it will be processed by the event handler registered with the event type.\n\n        Parameters\n        ----------\n        event: Event\n            The event to be posted.\n        \"\"\"\n        def _handler_need_feedback_sender(handler: EventHandlerType):\n            positional_param_names = get_param_names_by_kind(handler, Parameter.POSITIONAL_ONLY) + get_param_names_by_kind(handler, Parameter.POSITIONAL_OR_KEYWORD)\n            var_positional_param_names = get_param_names_by_kind(handler, Parameter.VAR_POSITIONAL)\n            return len(var_positional_param_names) &gt; 0 or len(positional_param_names) &gt; 1\n\n        if not self.is_top_level():\n            # Bubble up the event to the top-level Automa.\n            return self.parent.post_event(event)\n\n        # Here is the top-level Automa.\n        # Call event handlers\n        if event.event_type in self._event_handlers:\n            if _handler_need_feedback_sender(self._event_handlers[event.event_type]):\n                self._event_handlers[event.event_type](event, feedback_sender=None)\n            else:\n                self._event_handlers[event.event_type](event)\n        if self._default_event_handler is not None:\n            if _handler_need_feedback_sender(self._default_event_handler):\n                self._default_event_handler(event, feedback_sender=None)\n            else:\n                self._default_event_handler(event)\n\n    def request_feedback(\n        self, \n        event: Event,\n        timeout: Optional[float] = None\n    ) -&gt; Feedback:\n        \"\"\"\n        Request feedback for the specified event from the application layer outside the Automa. This method blocks the caller until the feedback is received.\n\n        Note that `request_feedback` should only be called from within a non-async method running in a new thread of the Automa thread pool.\n\n        Parameters\n        ----------\n        event: Event\n            The event to be posted to the event handler implemented by the application layer.\n        timeout: Optional[float]\n            A float or int number of seconds to wait for if the feedback is not received. If None, then there is no limit on the wait time.\n\n        Returns\n        -------\n        Feedback\n            The feedback received from the application layer.\n\n        Raises\n        ------\n        TimeoutError\n            If the feedback is not received before the timeout. Note that the raised exception is the built-in `TimeoutError` exception, instead of asyncio.TimeoutError or concurrent.futures.TimeoutError!\n        \"\"\"\n        if threading.get_ident() == self._main_thread_id:\n            raise AutomaRuntimeError(\n                f\"`request_feedback` should only be called in a different thread from the main thread of the {self.name}. \"\n            )\n        return asyncio.run_coroutine_threadsafe(\n            self.request_feedback_async(event, timeout),\n            self._main_loop\n        ).result()\n\n    async def request_feedback_async(\n        self, \n        event: Event,\n        timeout: Optional[float] = None\n    ) -&gt; Feedback:\n        \"\"\"\n        Request feedback for the specified event from the application layer outside the Automa. This method blocks the caller until the feedback is received.\n\n        The event handler implemented by the application layer will be called in the next event loop iteration, in the main thread.\n\n        Parameters\n        ----------\n        event: Event\n            The event to be posted to the event handler implemented by the application layer.\n        timeout: Optional[float]\n            A float or int number of seconds to wait for if the feedback is not received. If None, then there is no limit on the wait time.\n\n        Returns\n        -------\n        Feedback\n            The feedback received from the application layer.\n\n        Raises\n        ------\n        TimeoutError\n            If the feedback is not received before the timeout. Note that the raised exception is the built-in `TimeoutError` exception, instead of asyncio.TimeoutError!\n        \"\"\"\n        if not self.is_top_level():\n            # Bubble up the event to the top-level Automa.\n            return await self.parent.request_feedback_async(event, timeout)\n\n        # Here is the top-level Automa.\n        event_loop = asyncio.get_running_loop()\n        future = event_loop.create_future()\n        feedback_sender = self._FeedbackSender(future, event_loop)\n        # Call event handlers\n        if event.event_type in self._event_handlers:\n            self._event_handlers[event.event_type](event, feedback_sender)\n        if self._default_event_handler is not None:\n            self._default_event_handler(event, feedback_sender)\n\n        try:\n            return await asyncio.wait_for(future, timeout)\n        except TimeoutError as e:\n            # When python &gt;= 3.11 here.\n            raise TimeoutError(f\"No feedback is received before timeout in Automa[{self.name}]\") from e\n        except asyncio.TimeoutError as e:\n            # Version compatibility resolution: asyncio.wait_for raises asyncio.TimeoutError before python 3.11.\n            # https://docs.python.org/3/library/asyncio-task.html#asyncio.wait_for\n            raise TimeoutError(f\"No feedback is received before timeout in Automa[{self.name}]\") from e\n\n    ###############################################################\n    ########### [Bridgic Event Handling Mechanism] ends ###########\n    ###############################################################\n\n    ###############################################################\n    ######## [Bridgic Human Interaction Mechanism] starts #########\n    ###############################################################\n\n    def interact_with_human(\n        self,\n        event: Event,\n        interacting_worker: Optional[Worker] = None,\n    ) -&gt; InteractionFeedback:\n        \"\"\"\n        Trigger an interruption in the \"human-in-the-loop interaction\" during the execution of the Automa.\n\n        Parameters\n        ----------\n        event: Event\n            The event that triggered the interaction.\n        interacting_worker: Optional[Worker]\n            The worker instance that is currently interacting with human. If not provided, the worker will be located automatically.\n\n        Returns\n        -------\n        InteractionFeedback\n            The feedback received from the application layer.\n        \"\"\"\n        if not interacting_worker:\n            kickoff_worker_key: str = self._locate_interacting_worker()\n        else:\n            kickoff_worker_key = self._get_worker_key(interacting_worker)\n\n        if kickoff_worker_key:\n            return self.interact_with_human_from_worker_key(event, kickoff_worker_key)\n        raise AutomaRuntimeError(\n            f\"Get kickoff worker failed in Automa[{self.name}] \"\n            f\"when trying to interact with human with event: {event}\"\n        )\n\n    def interact_with_human_from_worker_key(\n        self,\n        event: Event,\n        worker_key: str\n    ) -&gt; InteractionFeedback:\n        # Match the interaction and feedback to see if it matches\n        matched_feedback: _InteractionAndFeedback = None\n        cur_interact_index = self._get_and_increment_interaction_index(worker_key)\n        if worker_key in self._ongoing_interactions:\n            interaction_and_feedbacks = self._ongoing_interactions[worker_key]\n            if cur_interact_index &lt; len(interaction_and_feedbacks):\n                matched_feedback = interaction_and_feedbacks[cur_interact_index]\n                # Check the event type\n                if event.event_type != matched_feedback.interaction.event.event_type:\n                    raise AutomaRuntimeError(\n                        f\"Event type mismatch! Automa[{self.name}-worker[{worker_key}]]. \"\n                        f\"interact_with_human passed-in event: {event}\\n\"\n                        f\"ongoing interaction &amp;&amp; feedback: {matched_feedback}\\n\"\n                    )\n        if matched_feedback is None or matched_feedback.feedback is None:\n            # Important: The interaction_id should be unique for each human interaction.\n            interaction_id = uuid.uuid4().hex if matched_feedback is None else matched_feedback.interaction.interaction_id\n            # Match failed, raise an exception to go into the human interactioin process.\n            raise _InteractionEventException(Interaction(\n                interaction_id=interaction_id,\n                event=event,\n            ))\n        else:\n            # Match the interaction and feedback succeeded, return it.\n            return matched_feedback.feedback\n\n    def _get_and_increment_interaction_index(self, worker_key: str) -&gt; int:\n        cur_index = self._worker_interaction_indices.setdefault(worker_key, 0)\n        self._worker_interaction_indices[worker_key] += 1\n        return cur_index\n\n    ###############################################################\n    ######### [Bridgic Human Interaction Mechanism] ends ##########\n    ###############################################################\n\n    def get_local_space(self, runtime_context: RuntimeContext) -&gt; Dict[str, Any]:\n        \"\"\"\n        Retrieve the local execution context (local space) associated with the current worker. \n        If you require the local space to be cleared after the completion of `automa.arun()`, \n        you may customize this behavior by overriding the `should_reset_local_space()` method.\n\n        Parameters\n        ----------\n        runtime_context : RuntimeContext\n            The runtime context.\n\n        Returns\n        -------\n        Dict[str, Any]\n            The local space.\n        \"\"\"\n        worker_key = runtime_context.worker_key\n        worker_obj = self._get_worker_instance(worker_key)\n        return worker_obj.local_space\n\n    def should_reset_local_space(self) -&gt; bool:\n        \"\"\"\n        This method indicates whether to reset the local space at the end of the arun method of Automa. \n        By default, it returns True, standing for resetting. Otherwise, it means doing nothing.\n\n        Examples:\n        --------\n        ```python\n        class MyAutoma(Automa):\n            def should_reset_local_space(self) -&gt; bool:\n                return False\n        ```\n        \"\"\"\n        return True\n</code></pre>"},{"location":"reference/bridgic-core/bridgic/core/automa/#bridgic.core.automa.Automa.thread_pool","title":"thread_pool  <code>property</code> <code>writable</code>","text":"<pre><code>thread_pool: Optional[ThreadPoolExecutor]\n</code></pre> <p>Get/Set the thread pool for parallel running of I/O-bound tasks used by the current Automa instance and its nested Automa instances.</p> <p>Note: If an Automa is nested within another Automa, the thread pool of the top-level Automa will be used, rather than the thread pool of the nested Automa.</p> <p>Returns:</p> Type Description <code>Optional[ThreadPoolExecutor]</code> <p>The thread pool.</p>"},{"location":"reference/bridgic-core/bridgic/core/automa/#bridgic.core.automa.Automa.load_from_snapshot","title":"load_from_snapshot","text":"<code>classmethod</code> <pre><code>load_from_snapshot(\n    snapshot: Snapshot,\n    thread_pool: Optional[ThreadPoolExecutor] = None,\n) -&gt; Automa\n</code></pre> <p>Load an Automa instance from a snapshot.</p> <p>Parameters:</p> Name Type Description Default <code>snapshot</code> <code>Snapshot</code> <p>The snapshot to load the Automa instance from.</p> required <code>thread_pool</code> <code>Optional[ThreadPoolExecutor]</code> <p>The thread pool for parallel running of I/O-bound tasks. If not provided, a default thread pool will be used.</p> <code>None</code> <p>Returns:</p> Type Description <code>Automa</code> <p>The loaded Automa instance.</p> Source code in <code>bridgic/core/automa/_automa.py</code> <pre><code>@classmethod\ndef load_from_snapshot(\n    cls, \n    snapshot: Snapshot,\n    thread_pool: Optional[ThreadPoolExecutor] = None,\n) -&gt; \"Automa\":\n    \"\"\"\n    Load an Automa instance from a snapshot.\n\n    Parameters\n    ----------\n    snapshot: Snapshot\n        The snapshot to load the Automa instance from.\n    thread_pool: Optional[ThreadPoolExecutor]\n        The thread pool for parallel running of I/O-bound tasks. If not provided, a default thread pool will be used.\n\n    Returns\n    -------\n    Automa\n        The loaded Automa instance.\n    \"\"\"\n    # Here you can compare snapshot.serialization_version with SERIALIZATION_VERSION, and handle any necessary version compatibility issues if needed.\n    automa = load_bytes(snapshot.serialized_bytes)\n    if thread_pool:\n        automa.thread_pool = thread_pool\n    return automa\n</code></pre>"},{"location":"reference/bridgic-core/bridgic/core/automa/#bridgic.core.automa.Automa.set_running_options","title":"set_running_options","text":"<pre><code>set_running_options(\n    debug: Optional[bool] = None,\n    verbose: Optional[bool] = None,\n)\n</code></pre> <p>Set runtime-configurable running options for this Automa instance.</p> <p>This method only supports fields that can be changed at runtime. Fields that must be set  during initialization (such as <code>callback_builders</code>) cannot be changed here and must be  provided via the <code>running_options</code> parameter in the Automa constructor.</p> <p>For fields that can be delayed to be set, some settings (like <code>debug</code>) from the outermost  (top-level) Automa will override the settings of all inner (nested) Automa instances. For example, if the top-level Automa instance sets <code>debug = True</code> and the nested instances  sets <code>debug = False</code>, then the nested Automa instance will run in debug mode when the  top-level Automa instance is executed. We call it the Setting Penetration Mechanism.</p> <p>Parameters:</p> Name Type Description Default <code>debug</code> <code>bool</code> <p>Whether to enable debug mode. If None, the current value is not changed. This field is subject to the Setting Penetration Mechanism.</p> <code>None</code> <code>verbose</code> <code>bool</code> <p>Whether to print more verbose runtime debug information. Only takes effect when <code>debug=True</code>. This field is subject to the Setting Penetration Mechanism.</p> <code>None</code> Source code in <code>bridgic/core/automa/_automa.py</code> <pre><code>def set_running_options(\n    self,\n    debug: Optional[bool] = None,\n    verbose: Optional[bool] = None,\n):\n    \"\"\"\n    Set runtime-configurable running options for this Automa instance.\n\n    This method only supports fields that can be changed at runtime. Fields that must be set \n    during initialization (such as `callback_builders`) cannot be changed here and must be \n    provided via the `running_options` parameter in the Automa constructor.\n\n    For fields that can be delayed to be set, some settings (like `debug`) from the outermost \n    (top-level) Automa will override the settings of all inner (nested) Automa instances.\n    For example, if the top-level Automa instance sets `debug = True` and the nested instances \n    sets `debug = False`, then the nested Automa instance will run in debug mode when the \n    top-level Automa instance is executed. We call it the Setting Penetration Mechanism.\n\n    Parameters\n    ----------\n    debug : bool, optional\n        Whether to enable debug mode. If None, the current value is not changed.\n        This field is subject to the Setting Penetration Mechanism.\n    verbose : bool, optional\n        Whether to print more verbose runtime debug information. Only takes effect when `debug=True`.\n        This field is subject to the Setting Penetration Mechanism.\n    \"\"\"\n    if debug is not None:\n        self._running_options.debug = debug\n    if verbose is not None:\n        self._running_options.verbose = verbose\n</code></pre>"},{"location":"reference/bridgic-core/bridgic/core/automa/#bridgic.core.automa.Automa.register_event_handler","title":"register_event_handler","text":"<pre><code>register_event_handler(\n    event_type: Optional[str],\n    event_handler: EventHandlerType,\n) -&gt; None\n</code></pre> <p>Register an event handler for the specified event type. If <code>event_type</code> is set to None, the event handler will be registered as the default handler that will handle all event types.</p> <p>Note: Only event handlers registered on the top-level Automa will be invoked to handle events.</p> <p>Parameters:</p> Name Type Description Default <code>event_type</code> <code>Optional[str]</code> <p>The type of event to be handled. If set to None, the event handler will be registered as the default handler and will be used to handle all event types.</p> required <code>event_handler</code> <code>EventHandlerType</code> <p>The event handler to be registered.</p> required Source code in <code>bridgic/core/automa/_automa.py</code> <pre><code>def register_event_handler(self, event_type: Optional[str], event_handler: EventHandlerType) -&gt; None:\n    \"\"\"\n    Register an event handler for the specified event type. If `event_type` is set to None, the event handler will be registered as the default handler that will handle all event types.\n\n    Note: Only event handlers registered on the top-level Automa will be invoked to handle events.\n\n    Parameters\n    ----------\n    event_type: Optional[str]\n        The type of event to be handled. If set to None, the event handler will be registered as the default handler and will be used to handle all event types.\n    event_handler: EventHandlerType\n        The event handler to be registered.\n    \"\"\"\n    if event_type is None:\n        self._default_event_handler = event_handler\n    else:\n        self._event_handlers[event_type] = event_handler\n</code></pre>"},{"location":"reference/bridgic-core/bridgic/core/automa/#bridgic.core.automa.Automa.unregister_event_handler","title":"unregister_event_handler","text":"<pre><code>unregister_event_handler(event_type: Optional[str]) -&gt; None\n</code></pre> <p>Unregister an event handler for the specified event type.</p> <p>Parameters:</p> Name Type Description Default <code>event_type</code> <code>Optional[str]</code> <p>The type of event to be unregistered. If set to None, the default event handler will be unregistered.</p> required Source code in <code>bridgic/core/automa/_automa.py</code> <pre><code>def unregister_event_handler(self, event_type: Optional[str]) -&gt; None:\n    \"\"\"\n    Unregister an event handler for the specified event type.\n\n    Parameters\n    ----------\n    event_type: Optional[str]\n        The type of event to be unregistered. If set to None, the default event handler will be unregistered.\n    \"\"\"\n    if event_type in self._event_handlers:\n        del self._event_handlers[event_type]\n    if event_type is None:\n        self._default_event_handler = None\n</code></pre>"},{"location":"reference/bridgic-core/bridgic/core/automa/#bridgic.core.automa.Automa.unregister_all_event_handlers","title":"unregister_all_event_handlers","text":"<pre><code>unregister_all_event_handlers() -&gt; None\n</code></pre> <p>Unregister all event handlers.</p> Source code in <code>bridgic/core/automa/_automa.py</code> <pre><code>def unregister_all_event_handlers(self) -&gt; None:\n    \"\"\"\n    Unregister all event handlers.\n    \"\"\"\n    self._event_handlers.clear()\n    self._default_event_handler = None\n</code></pre>"},{"location":"reference/bridgic-core/bridgic/core/automa/#bridgic.core.automa.Automa.post_event","title":"post_event","text":"<pre><code>post_event(event: Event) -&gt; None\n</code></pre> <p>Post an event to the application layer outside the Automa.</p> <p>The event handler implemented by the application layer will be called in the same thread as the worker (maybe the main thread or a new thread from the thread pool).</p> <p>Note that <code>post_event</code> can be called either in a non-async method or in an async method.</p> <p>The event will be bubbled up to the top-level Automa, where it will be processed by the event handler registered with the event type.</p> <p>Parameters:</p> Name Type Description Default <code>event</code> <code>Event</code> <p>The event to be posted.</p> required Source code in <code>bridgic/core/automa/_automa.py</code> <pre><code>@override\ndef post_event(self, event: Event) -&gt; None:\n    \"\"\"\n    Post an event to the application layer outside the Automa.\n\n    The event handler implemented by the application layer will be called in the same thread as the worker (maybe the main thread or a new thread from the thread pool).\n\n    Note that `post_event` can be called either in a non-async method or in an async method.\n\n    The event will be bubbled up to the top-level Automa, where it will be processed by the event handler registered with the event type.\n\n    Parameters\n    ----------\n    event: Event\n        The event to be posted.\n    \"\"\"\n    def _handler_need_feedback_sender(handler: EventHandlerType):\n        positional_param_names = get_param_names_by_kind(handler, Parameter.POSITIONAL_ONLY) + get_param_names_by_kind(handler, Parameter.POSITIONAL_OR_KEYWORD)\n        var_positional_param_names = get_param_names_by_kind(handler, Parameter.VAR_POSITIONAL)\n        return len(var_positional_param_names) &gt; 0 or len(positional_param_names) &gt; 1\n\n    if not self.is_top_level():\n        # Bubble up the event to the top-level Automa.\n        return self.parent.post_event(event)\n\n    # Here is the top-level Automa.\n    # Call event handlers\n    if event.event_type in self._event_handlers:\n        if _handler_need_feedback_sender(self._event_handlers[event.event_type]):\n            self._event_handlers[event.event_type](event, feedback_sender=None)\n        else:\n            self._event_handlers[event.event_type](event)\n    if self._default_event_handler is not None:\n        if _handler_need_feedback_sender(self._default_event_handler):\n            self._default_event_handler(event, feedback_sender=None)\n        else:\n            self._default_event_handler(event)\n</code></pre>"},{"location":"reference/bridgic-core/bridgic/core/automa/#bridgic.core.automa.Automa.request_feedback","title":"request_feedback","text":"<pre><code>request_feedback(\n    event: Event, timeout: Optional[float] = None\n) -&gt; Feedback\n</code></pre> <p>Request feedback for the specified event from the application layer outside the Automa. This method blocks the caller until the feedback is received.</p> <p>Note that <code>request_feedback</code> should only be called from within a non-async method running in a new thread of the Automa thread pool.</p> <p>Parameters:</p> Name Type Description Default <code>event</code> <code>Event</code> <p>The event to be posted to the event handler implemented by the application layer.</p> required <code>timeout</code> <code>Optional[float]</code> <p>A float or int number of seconds to wait for if the feedback is not received. If None, then there is no limit on the wait time.</p> <code>None</code> <p>Returns:</p> Type Description <code>Feedback</code> <p>The feedback received from the application layer.</p> <p>Raises:</p> Type Description <code>TimeoutError</code> <p>If the feedback is not received before the timeout. Note that the raised exception is the built-in <code>TimeoutError</code> exception, instead of asyncio.TimeoutError or concurrent.futures.TimeoutError!</p> Source code in <code>bridgic/core/automa/_automa.py</code> <pre><code>def request_feedback(\n    self, \n    event: Event,\n    timeout: Optional[float] = None\n) -&gt; Feedback:\n    \"\"\"\n    Request feedback for the specified event from the application layer outside the Automa. This method blocks the caller until the feedback is received.\n\n    Note that `request_feedback` should only be called from within a non-async method running in a new thread of the Automa thread pool.\n\n    Parameters\n    ----------\n    event: Event\n        The event to be posted to the event handler implemented by the application layer.\n    timeout: Optional[float]\n        A float or int number of seconds to wait for if the feedback is not received. If None, then there is no limit on the wait time.\n\n    Returns\n    -------\n    Feedback\n        The feedback received from the application layer.\n\n    Raises\n    ------\n    TimeoutError\n        If the feedback is not received before the timeout. Note that the raised exception is the built-in `TimeoutError` exception, instead of asyncio.TimeoutError or concurrent.futures.TimeoutError!\n    \"\"\"\n    if threading.get_ident() == self._main_thread_id:\n        raise AutomaRuntimeError(\n            f\"`request_feedback` should only be called in a different thread from the main thread of the {self.name}. \"\n        )\n    return asyncio.run_coroutine_threadsafe(\n        self.request_feedback_async(event, timeout),\n        self._main_loop\n    ).result()\n</code></pre>"},{"location":"reference/bridgic-core/bridgic/core/automa/#bridgic.core.automa.Automa.request_feedback_async","title":"request_feedback_async","text":"<code>async</code> <pre><code>request_feedback_async(\n    event: Event, timeout: Optional[float] = None\n) -&gt; Feedback\n</code></pre> <p>Request feedback for the specified event from the application layer outside the Automa. This method blocks the caller until the feedback is received.</p> <p>The event handler implemented by the application layer will be called in the next event loop iteration, in the main thread.</p> <p>Parameters:</p> Name Type Description Default <code>event</code> <code>Event</code> <p>The event to be posted to the event handler implemented by the application layer.</p> required <code>timeout</code> <code>Optional[float]</code> <p>A float or int number of seconds to wait for if the feedback is not received. If None, then there is no limit on the wait time.</p> <code>None</code> <p>Returns:</p> Type Description <code>Feedback</code> <p>The feedback received from the application layer.</p> <p>Raises:</p> Type Description <code>TimeoutError</code> <p>If the feedback is not received before the timeout. Note that the raised exception is the built-in <code>TimeoutError</code> exception, instead of asyncio.TimeoutError!</p> Source code in <code>bridgic/core/automa/_automa.py</code> <pre><code>async def request_feedback_async(\n    self, \n    event: Event,\n    timeout: Optional[float] = None\n) -&gt; Feedback:\n    \"\"\"\n    Request feedback for the specified event from the application layer outside the Automa. This method blocks the caller until the feedback is received.\n\n    The event handler implemented by the application layer will be called in the next event loop iteration, in the main thread.\n\n    Parameters\n    ----------\n    event: Event\n        The event to be posted to the event handler implemented by the application layer.\n    timeout: Optional[float]\n        A float or int number of seconds to wait for if the feedback is not received. If None, then there is no limit on the wait time.\n\n    Returns\n    -------\n    Feedback\n        The feedback received from the application layer.\n\n    Raises\n    ------\n    TimeoutError\n        If the feedback is not received before the timeout. Note that the raised exception is the built-in `TimeoutError` exception, instead of asyncio.TimeoutError!\n    \"\"\"\n    if not self.is_top_level():\n        # Bubble up the event to the top-level Automa.\n        return await self.parent.request_feedback_async(event, timeout)\n\n    # Here is the top-level Automa.\n    event_loop = asyncio.get_running_loop()\n    future = event_loop.create_future()\n    feedback_sender = self._FeedbackSender(future, event_loop)\n    # Call event handlers\n    if event.event_type in self._event_handlers:\n        self._event_handlers[event.event_type](event, feedback_sender)\n    if self._default_event_handler is not None:\n        self._default_event_handler(event, feedback_sender)\n\n    try:\n        return await asyncio.wait_for(future, timeout)\n    except TimeoutError as e:\n        # When python &gt;= 3.11 here.\n        raise TimeoutError(f\"No feedback is received before timeout in Automa[{self.name}]\") from e\n    except asyncio.TimeoutError as e:\n        # Version compatibility resolution: asyncio.wait_for raises asyncio.TimeoutError before python 3.11.\n        # https://docs.python.org/3/library/asyncio-task.html#asyncio.wait_for\n        raise TimeoutError(f\"No feedback is received before timeout in Automa[{self.name}]\") from e\n</code></pre>"},{"location":"reference/bridgic-core/bridgic/core/automa/#bridgic.core.automa.Automa.interact_with_human","title":"interact_with_human","text":"<pre><code>interact_with_human(\n    event: Event,\n    interacting_worker: Optional[Worker] = None,\n) -&gt; InteractionFeedback\n</code></pre> <p>Trigger an interruption in the \"human-in-the-loop interaction\" during the execution of the Automa.</p> <p>Parameters:</p> Name Type Description Default <code>event</code> <code>Event</code> <p>The event that triggered the interaction.</p> required <code>interacting_worker</code> <code>Optional[Worker]</code> <p>The worker instance that is currently interacting with human. If not provided, the worker will be located automatically.</p> <code>None</code> <p>Returns:</p> Type Description <code>InteractionFeedback</code> <p>The feedback received from the application layer.</p> Source code in <code>bridgic/core/automa/_automa.py</code> <pre><code>def interact_with_human(\n    self,\n    event: Event,\n    interacting_worker: Optional[Worker] = None,\n) -&gt; InteractionFeedback:\n    \"\"\"\n    Trigger an interruption in the \"human-in-the-loop interaction\" during the execution of the Automa.\n\n    Parameters\n    ----------\n    event: Event\n        The event that triggered the interaction.\n    interacting_worker: Optional[Worker]\n        The worker instance that is currently interacting with human. If not provided, the worker will be located automatically.\n\n    Returns\n    -------\n    InteractionFeedback\n        The feedback received from the application layer.\n    \"\"\"\n    if not interacting_worker:\n        kickoff_worker_key: str = self._locate_interacting_worker()\n    else:\n        kickoff_worker_key = self._get_worker_key(interacting_worker)\n\n    if kickoff_worker_key:\n        return self.interact_with_human_from_worker_key(event, kickoff_worker_key)\n    raise AutomaRuntimeError(\n        f\"Get kickoff worker failed in Automa[{self.name}] \"\n        f\"when trying to interact with human with event: {event}\"\n    )\n</code></pre>"},{"location":"reference/bridgic-core/bridgic/core/automa/#bridgic.core.automa.Automa.get_local_space","title":"get_local_space","text":"<pre><code>get_local_space(\n    runtime_context: RuntimeContext,\n) -&gt; Dict[str, Any]\n</code></pre> <p>Retrieve the local execution context (local space) associated with the current worker.  If you require the local space to be cleared after the completion of <code>automa.arun()</code>,  you may customize this behavior by overriding the <code>should_reset_local_space()</code> method.</p> <p>Parameters:</p> Name Type Description Default <code>runtime_context</code> <code>RuntimeContext</code> <p>The runtime context.</p> required <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>The local space.</p> Source code in <code>bridgic/core/automa/_automa.py</code> <pre><code>def get_local_space(self, runtime_context: RuntimeContext) -&gt; Dict[str, Any]:\n    \"\"\"\n    Retrieve the local execution context (local space) associated with the current worker. \n    If you require the local space to be cleared after the completion of `automa.arun()`, \n    you may customize this behavior by overriding the `should_reset_local_space()` method.\n\n    Parameters\n    ----------\n    runtime_context : RuntimeContext\n        The runtime context.\n\n    Returns\n    -------\n    Dict[str, Any]\n        The local space.\n    \"\"\"\n    worker_key = runtime_context.worker_key\n    worker_obj = self._get_worker_instance(worker_key)\n    return worker_obj.local_space\n</code></pre>"},{"location":"reference/bridgic-core/bridgic/core/automa/#bridgic.core.automa.Automa.should_reset_local_space","title":"should_reset_local_space","text":"<pre><code>should_reset_local_space() -&gt; bool\n</code></pre> <p>This method indicates whether to reset the local space at the end of the arun method of Automa.  By default, it returns True, standing for resetting. Otherwise, it means doing nothing.</p> Examples: <pre><code>class MyAutoma(Automa):\n    def should_reset_local_space(self) -&gt; bool:\n        return False\n</code></pre> Source code in <code>bridgic/core/automa/_automa.py</code> <pre><code>def should_reset_local_space(self) -&gt; bool:\n    \"\"\"\n    This method indicates whether to reset the local space at the end of the arun method of Automa. \n    By default, it returns True, standing for resetting. Otherwise, it means doing nothing.\n\n    Examples:\n    --------\n    ```python\n    class MyAutoma(Automa):\n        def should_reset_local_space(self) -&gt; bool:\n            return False\n    ```\n    \"\"\"\n    return True\n</code></pre>"},{"location":"reference/bridgic-core/bridgic/core/automa/#bridgic.core.automa.Snapshot","title":"Snapshot","text":"<p>               Bases: <code>BaseModel</code></p> <p>A snapshot that represents the current state of an Automa. It is used when an Automa resumes after a human interaction.</p> Source code in <code>bridgic/core/automa/_automa.py</code> <pre><code>class Snapshot(BaseModel):\n    \"\"\"\n    A snapshot that represents the current state of an Automa. It is used when an Automa resumes after a human interaction.\n    \"\"\"\n    serialized_bytes: bytes\n    \"\"\"\n    The serialized bytes that represents the snapshot of the Automa.\n    \"\"\"\n    serialization_version: str\n    \"\"\"\n    The serialization version.\n    \"\"\"\n</code></pre>"},{"location":"reference/bridgic-core/bridgic/core/automa/#bridgic.core.automa.Snapshot.serialized_bytes","title":"serialized_bytes  <code>instance-attribute</code>","text":"<pre><code>serialized_bytes: bytes\n</code></pre> <p>The serialized bytes that represents the snapshot of the Automa.</p>"},{"location":"reference/bridgic-core/bridgic/core/automa/#bridgic.core.automa.Snapshot.serialization_version","title":"serialization_version  <code>instance-attribute</code>","text":"<pre><code>serialization_version: str\n</code></pre> <p>The serialization version.</p>"},{"location":"reference/bridgic-core/bridgic/core/automa/#bridgic.core.automa.RunningOptions","title":"RunningOptions","text":"<p>               Bases: <code>BaseModel</code></p> <p>Running options for an Automa instance.</p> <p>This class contains two types of fields:</p> <ol> <li>Runtime-configurable fields: Can be set at any time via <code>set_running_options()</code>.</li> <li><code>debug</code>: Whether to enable debug mode.</li> <li> <p><code>verbose</code>: Whether to print more verbose runtime debug information. Only takes effect when <code>debug=True</code>.</p> </li> <li> <p>Initialization-only fields: Must be set during Automa instantiation via the <code>running_options</code> parameter.</p> </li> <li><code>callback_builders</code>: Callback builders at the Automa instance level. These will be merged with       global callback builders when workers are created during Automa initialization.</li> </ol> Source code in <code>bridgic/core/automa/_automa.py</code> <pre><code>class RunningOptions(BaseModel):\n    \"\"\"\n    Running options for an Automa instance.\n\n    This class contains two types of fields:\n\n    1. **Runtime-configurable fields**: Can be set at any time via `set_running_options()`.\n       - `debug`: Whether to enable debug mode.\n       - `verbose`: Whether to print more verbose runtime debug information. Only takes effect when `debug=True`.\n\n    2. **Initialization-only fields**: Must be set during Automa instantiation via the `running_options` parameter.\n       - `callback_builders`: Callback builders at the Automa instance level. These will be merged with \n         global callback builders when workers are created during Automa initialization.\n    \"\"\"\n    debug: bool = False\n    \"\"\"Whether to enable debug mode. Can be set at runtime via set_running_options().\"\"\"\n\n    verbose: bool = False\n    \"\"\"Whether to print more verbose runtime debug information. Only takes effect when debug=True. Can be set at runtime via set_running_options().\"\"\"\n\n    callback_builders: List[WorkerCallbackBuilder] = []\n    \"\"\"A list of callback builders specific to this Automa instance.\"\"\"\n\n    model_config = {\"arbitrary_types_allowed\": True}\n</code></pre>"},{"location":"reference/bridgic-core/bridgic/core/automa/#bridgic.core.automa.RunningOptions.debug","title":"debug  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>debug: bool = False\n</code></pre> <p>Whether to enable debug mode. Can be set at runtime via set_running_options().</p>"},{"location":"reference/bridgic-core/bridgic/core/automa/#bridgic.core.automa.RunningOptions.verbose","title":"verbose  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>verbose: bool = False\n</code></pre> <p>Whether to print more verbose runtime debug information. Only takes effect when debug=True. Can be set at runtime via set_running_options().</p>"},{"location":"reference/bridgic-core/bridgic/core/automa/#bridgic.core.automa.RunningOptions.callback_builders","title":"callback_builders  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>callback_builders: List[WorkerCallbackBuilder] = []\n</code></pre> <p>A list of callback builders specific to this Automa instance.</p>"},{"location":"reference/bridgic-core/bridgic/core/automa/#bridgic.core.automa.GraphAutoma","title":"GraphAutoma","text":"<p>               Bases: <code>Automa</code></p> <p>Dynamic Directed Graph (abbreviated as DDG) implementation of Automa. <code>GraphAutoma</code> manages  the running control flow between workers automatically, via <code>dependencies</code> and <code>ferry_to</code>. Outputs of workers can be mapped and passed to their successor workers in the runtime,  following <code>args_mapping_rule</code> and <code>result_dispatching_rule</code>.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>Optional[str]</code> <p>The name of the automa.</p> <code>None</code> <code>thread_pool</code> <code>Optional[ThreadPoolExecutor]</code> <p>The thread pool for parallel running of I/O-bound or CPU-bound tasks.</p> <ul> <li> <p>If not provided, a default thread pool will be used. The maximum number of threads in the default thread pool dependends on the number of CPU cores. Please refer to  the ThreadPoolExecutor for detail.</p> </li> <li> <p>If provided, all workers (including all nested Automa instances) will be run in it. In this case, the  application layer code is responsible to create it and shut it down.</p> </li> </ul> <code>None</code> <code>running_options</code> <code>Optional[RunningOptions]</code> <p>The running options for an automa instance (if needed).</p> <code>None</code> <p>Examples:</p> <p>The following example shows how to use <code>GraphAutoma</code> to create a simple graph automa that prints \"Hello, Bridgic\".</p> <pre><code>import asyncio\nfrom bridgic.core.automa import GraphAutoma, worker, ArgsMappingRule\n\nclass MyGraphAutoma(GraphAutoma):\n    @worker(is_start=True)\n    async def greet(self) -&gt; list[str]:\n        return [\"Hello\", \"Bridgic\"]\n\n    @worker(dependencies=[\"greet\"], args_mapping_rule=ArgsMappingRule.AS_IS, result_dispatching_rule=ResultDispatchingRule.AS_IS, is_output=True)\n    async def output(self, message: list[str]):\n        print(\"Echo: \" + \" \".join(message))\n\nasync def main():\n    automa_obj = MyGraphAutoma(name=\"my_graph_automa\")\n    await automa_obj.arun()\n\nasyncio.run(main())\n</code></pre> Source code in <code>bridgic/core/automa/_graph_automa.py</code> <pre><code>class GraphAutoma(Automa, metaclass=GraphMeta):\n    \"\"\"\n    Dynamic Directed Graph (abbreviated as DDG) implementation of Automa. `GraphAutoma` manages \n    the running control flow between workers automatically, via `dependencies` and `ferry_to`.\n    Outputs of workers can be mapped and passed to their successor workers in the runtime, \n    following `args_mapping_rule` and `result_dispatching_rule`.\n\n    Parameters\n    ----------\n    name : Optional[str]\n        The name of the automa.\n\n    thread_pool : Optional[ThreadPoolExecutor]\n        The thread pool for parallel running of I/O-bound or CPU-bound tasks.\n\n        - If not provided, a default thread pool will be used.\n        The maximum number of threads in the default thread pool dependends on the number of CPU cores. Please refer to \n        the [ThreadPoolExecutor](https://docs.python.org/3/library/concurrent.futures.html#threadpoolexecutor) for detail.\n\n        - If provided, all workers (including all nested Automa instances) will be run in it. In this case, the \n        application layer code is responsible to create it and shut it down.\n\n    running_options : Optional[RunningOptions]\n        The running options for an automa instance (if needed).\n\n    Examples\n    --------\n\n    The following example shows how to use `GraphAutoma` to create a simple graph automa that prints \"Hello, Bridgic\".\n\n    ```python\n    import asyncio\n    from bridgic.core.automa import GraphAutoma, worker, ArgsMappingRule\n\n    class MyGraphAutoma(GraphAutoma):\n        @worker(is_start=True)\n        async def greet(self) -&gt; list[str]:\n            return [\"Hello\", \"Bridgic\"]\n\n        @worker(dependencies=[\"greet\"], args_mapping_rule=ArgsMappingRule.AS_IS, result_dispatching_rule=ResultDispatchingRule.AS_IS, is_output=True)\n        async def output(self, message: list[str]):\n            print(\"Echo: \" + \" \".join(message))\n\n    async def main():\n        automa_obj = MyGraphAutoma(name=\"my_graph_automa\")\n        await automa_obj.arun()\n\n    asyncio.run(main())\n    ```\n    \"\"\"\n\n    # Automa type.\n    AUTOMA_TYPE: ClassVar[AutomaType] = AutomaType.Graph\n\n    # The initial topology defined by @worker functions.\n    _registered_worker_funcs: ClassVar[Dict[str, Callable]] = {}\n\n    # IMPORTANT: The entire states of a GraphAutoma instance include 2 part:\n    # \n    # Part-1 (for the states of topology structure):\n    #   1. Inner worker instances: self._workers\n    #   2. Relations between worker: self._worker_forwards\n    #   3. Dynamic states that serve as trigger of execution of workers: self._workers_dynamic_states\n    #   4. Execution result of inner workers: self._worker_output\n    #   5. Configurations of this automa instance: self._output_worker_key\n    # \n    # Part-2 (for the states of running states):\n    #   1. Records of Workers that are going to be kicked off: self._current_kickoff_workers\n    #   2. Records of running or deferred tasks:\n    #      - self._running_tasks\n    #      - self._topology_deferred_tasks\n    #      - self._ferry_deferred_tasks\n    #      - self._set_output_worker_deferred_task\n    #   3. Buffers of automa inputs: self._input_buffer\n    #   4. Ongoing human interactions: self._ongoing_interactions\n    #   ...\n\n    _workers: Dict[str, _GraphAdaptedWorker]\n    _worker_output: Dict[str, Any]\n    _worker_forwards: Dict[str, List[str]]\n\n    _current_kickoff_workers: List[_KickoffInfo]\n    _input_buffer: _AutomaInputBuffer\n    _workers_dynamic_states: Dict[str, _WorkerDynamicState]\n\n    # The whole running process of the DDG is divided into two main phases:\n    # 1. [Initialization Phase] The first phase (when _automa_running is False): the initial topology of DDG was constructed.\n    # 2. [Running Phase] The second phase (when _automa_running is True): the DDG is running, and the workers are executed in a dynamic step-by-step manner (DS loop).\n    _automa_running: bool\n\n    #########################################################\n    #### The following fields need not to be serialized. ####\n    #########################################################\n    _running_tasks: List[_RunnningTask]\n\n    # TODO: The following deferred task structures need to be thread-safe.\n    # TODO: Need to be refactored when parallelization features are added.\n    _topology_deferred_tasks: List[Union[_AddWorkerDeferredTask, _RemoveWorkerDeferredTask]]\n    _ferry_deferred_tasks: List[_FerryDeferredTask]\n\n    def __init__(\n        self,\n        name: Optional[str] = None,\n        thread_pool: Optional[ThreadPoolExecutor] = None,\n        running_options: Optional[RunningOptions] = None,\n    ):\n        \"\"\"\n        Parameters\n        ----------\n        name : Optional[str]\n            The name of the automa.\n\n        thread_pool : Optional[ThreadPoolExecutor]\n            The thread pool for parallel running of I/O-bound tasks.\n\n            - If not provided, a default thread pool will be used.\n            The maximum number of threads in the default thread pool dependends on the number of CPU cores. Please refer to \n            the [ThreadPoolExecutor](https://docs.python.org/3/library/concurrent.futures.html#threadpoolexecutor) for detail.\n\n            - If provided, all workers (including all nested Automa instances) will be run in it. In this case, the \n            application layer code is responsible to create it and shut it down.\n\n        running_options : Optional[RunningOptions]\n            Running options for this Automa instance, including callback_builders.\n            If None, uses default RunningOptions.\n\n        state_dict : Optional[Dict[str, Any]]\n            A dictionary for initializing the automa's runtime states. This parameter is designed for framework use only.\n        \"\"\"\n        super().__init__(name=name, thread_pool=thread_pool, running_options=running_options)\n\n        self._workers = {}\n        self._worker_outputs = {}\n        self._automa_running = False\n\n        # Initialize the states that need to be serialized.\n        self._normal_init()\n\n        # The list of the tasks that are currently being executed.\n        self._running_tasks = []\n        # deferred tasks\n        self._topology_deferred_tasks = []\n        self._ferry_deferred_tasks = []\n\n    def _normal_init(self):\n        ###############################################################################\n        # Initialization of [Part One: Topology-Related Runtime States] #### Strat ####\n        ###############################################################################\n\n        cls = type(self)\n\n        # _workers, _worker_forwards and _workers_dynamic_states will be initialized incrementally by add_worker()...\n        self._worker_forwards = {}\n        self._worker_output = {}\n        self._workers_dynamic_states = {}\n\n        if cls.AUTOMA_TYPE == AutomaType.Graph:\n            # The _registered_worker_funcs data are from @worker decorators.\n            for worker_key, worker_func in cls._registered_worker_funcs.items():\n                # The decorator based mechanism (i.e. @worker) is based on the add_worker() interface.\n                # Parameters check and other implementation details can be unified.\n                self._add_func_as_worker_internal(\n                    key=worker_key,\n                    func=worker_func,\n                    dependencies=worker_func.__dependencies__,\n                    is_start=worker_func.__is_start__,\n                    is_output=worker_func.__is_output__,\n                    args_mapping_rule=worker_func.__args_mapping_rule__,\n                    result_dispatching_rule=worker_func.__result_dispatching_rule__,\n                    callback_builders=worker_func.__callback_builders__,\n                )\n\n        ###############################################################################\n        # Initialization of [Part One: Topology-Related Runtime States] ##### End #####\n        ###############################################################################\n\n        ###############################################################################\n        # Initialization of [Part Two: Task-Related Runtime States] ###### Strat ######\n        ###############################################################################\n\n        # -- Current kickoff workers list.\n        # The key list of the workers that are ready to be immediately executed in the next DS (Dynamic Step). It will be lazily initialized in _compile_graph_and_detect_risks().\n        self._current_kickoff_workers = []\n        # -- Automa input buffer.\n        self._input_buffer = _AutomaInputBuffer()\n\n        ###############################################################################\n        # Initialization of [Part Two: Task-Related Runtime States] ####### End #######\n        ###############################################################################\n\n    ###############################################################\n    ########## [Bridgic Serialization Mechanism] starts ###########\n    ###############################################################\n\n    # The version of the serialization format.\n    SERIALIZATION_VERSION: str = \"1.0\"\n\n    @override\n    def dump_to_dict(self) -&gt; Dict[str, Any]:\n        state_dict = super().dump_to_dict()\n\n        state_dict[\"name\"] = self.name\n        state_dict[\"automa_running\"] = self._automa_running\n\n        # States related to workers.\n        state_dict[\"workers\"] = self._workers\n        state_dict[\"worker_forwards\"] = self._worker_forwards\n        state_dict[\"workers_dynamic_states\"] = self._workers_dynamic_states\n        state_dict[\"worker_output\"] = self._worker_output\n\n        # States related to interruption recovery.\n        state_dict[\"current_kickoff_workers\"] = self._current_kickoff_workers\n        state_dict[\"input_buffer\"] = self._input_buffer\n\n        return state_dict\n\n    @override\n    def load_from_dict(self, state_dict: Dict[str, Any]) -&gt; None:\n        super().load_from_dict(state_dict)\n\n        self.name = state_dict[\"name\"]\n        self._automa_running = state_dict[\"automa_running\"]\n\n        # States related to workers.\n        self._workers = state_dict[\"workers\"]\n        for worker in self._workers.values():\n            worker.parent = self\n        self._worker_forwards = state_dict[\"worker_forwards\"]\n        self._workers_dynamic_states = state_dict[\"workers_dynamic_states\"]\n        self._worker_output = state_dict[\"worker_output\"]\n\n        # States related to interruption recovery.\n        self._current_kickoff_workers = state_dict[\"current_kickoff_workers\"]\n        self._input_buffer = state_dict[\"input_buffer\"]\n\n        # The list of the tasks that are currently being executed.\n        self._running_tasks = []\n        # Deferred tasks\n        self._topology_deferred_tasks = []\n        self._set_output_worker_deferred_task = None\n        self._ferry_deferred_tasks = []\n\n    ###############################################################\n    ########### [Bridgic Serialization Mechanism] ends ############\n    ###############################################################\n\n    def _add_worker_incrementally(\n        self,\n        key: str,\n        worker: Worker,\n        *,\n        dependencies: List[str] = [],\n        is_start: bool = False,\n        is_output: bool = False,\n        args_mapping_rule: ArgsMappingRule = ArgsMappingRule.AS_IS,\n        result_dispatching_rule: ResultDispatchingRule = ResultDispatchingRule.AS_IS,\n        callback_builders: List[WorkerCallbackBuilder] = [],\n    ) -&gt; None:\n        \"\"\"\n        Incrementally add a worker into the automa. For internal use only.\n        This method is one of the very basic primitives of DDG for dynamic topology changes. \n        \"\"\"\n        if key in self._workers:\n            raise AutomaRuntimeError(\n                f\"duplicate workers with the same key '{key}' are not allowed to be added!\"\n            )\n\n        # Merge callback builders: Global -&gt; Ancestor Automa(s) -&gt; Current Automa -&gt; Nested Automa (if worker is automa) -&gt; Worker\n        effective_callback_builders = []\n        effective_callback_builders.extend(GlobalSetting.read().callback_builders)\n        # Collect callback builders from all ancestor automas in the ancestor chain (from top-level to current)\n        effective_callback_builders.extend(self._collect_ancestor_callback_builders())\n        # If the worker itself is an automa, include its own RunningOptions callback builders\n        if isinstance(worker, Automa):\n            effective_callback_builders.extend(worker._running_options.callback_builders)\n        # Include the callback builders from the worker itself.\n        effective_callback_builders.extend(callback_builders)\n\n        # Note: the dependencies argument must be a new copy of the list, created with list(dependencies).\n        # Refer to the Python documentation for more details:\n        # 1. https://docs.python.org/3/reference/compound_stmts.html#function-definitions\n        # \"Default parameter values are evaluated from left to right when the function definition is executed\"\n        # 2. https://docs.python.org/3/tutorial/controlflow.html#default-argument-values\n        # \"The default values are evaluated at the point of function definition in the defining scope\"\n        # \"Important warning: The default value is evaluated only once.\"\n        new_worker_obj = _GraphAdaptedWorker(\n            key=key,\n            worker=worker,\n            dependencies=list(dependencies),\n            is_start=is_start,\n            is_output=is_output,\n            args_mapping_rule=args_mapping_rule,\n            result_dispatching_rule=result_dispatching_rule,\n            callback_builders=effective_callback_builders,\n        )\n\n        # Register the worker_obj.\n        new_worker_obj.parent = self\n        self._workers[new_worker_obj.key] = new_worker_obj\n\n        # Incrementally update the dynamic states of added workers.\n        self._workers_dynamic_states[key] = _WorkerDynamicState(\n            dependency_triggers=set(dependencies)\n        )\n\n        # Incrementally update the forwards table.\n        for trigger in dependencies:\n            if trigger not in self._worker_forwards:\n                self._worker_forwards[trigger] = []\n            self._worker_forwards[trigger].append(key)\n\n        # If the added worker is an automa, recursively propagate callbacks to inner workers.\n        if new_worker_obj.is_automa():\n            nested_automa = new_worker_obj.get_decorated_worker()\n            if isinstance(nested_automa, GraphAutoma):\n                # Collect callback builders from all ancestor automas in the ancestor chain (from top-level to current)\n                ancestor_callback_builders = self._collect_ancestor_callback_builders()\n                # Append ancestor callbacks to the _cached_callbacks of the nested automa instance.\n                nested_automa._cached_callbacks = nested_automa._get_automa_callbacks() + [cb.build() for cb in ancestor_callback_builders]\n                # Recursively propagate ancestor callbacks to inner workers.\n                self._propagate_callbacks_to_nested_automa(\n                    nested_automa=nested_automa,\n                    callback_builders=ancestor_callback_builders,\n                )\n\n    def _propagate_callbacks_to_nested_automa(\n        self,\n        nested_automa: \"GraphAutoma\",\n        callback_builders: List[WorkerCallbackBuilder],\n    ) -&gt; None:\n        \"\"\"\n        Recursively propagate callback builders to all workers in a nested automa.\n\n        This method ensures that callbacks from all ancestor automas in the ancestor chain\n        are applied to all workers in nested automa instances, including deeply nested ones.\n\n        Parameters\n        ----------\n        nested_automa : GraphAutoma\n            The nested automa instance to propagate callbacks to.\n        callback_builders : List[WorkerCallbackBuilder]\n            The callback builders from all ancestor automas in the ancestor chain \n            (from top-level to current) to propagate.\n        \"\"\"\n        for worker_key in nested_automa.all_workers():\n            nested_worker = nested_automa._workers[worker_key]\n\n            # Add callback instances built from all ancestor automas' callback builders in the ancestor chain.\n            new_callbacks = [cb.build() for cb in callback_builders]\n            nested_worker._worker_callbacks += new_callbacks\n\n            # Check if the nested worker is also an automa, and recursively propagate.\n            if nested_worker.is_automa():\n                deeper_nested_automa = nested_worker.get_decorated_worker()\n                if isinstance(deeper_nested_automa, GraphAutoma):\n                    # Recursively propagate to deeper nested automas.\n                    # Include current nested automa's callbacks in the propagation chain,\n                    # so that deeper nested workers get callbacks from all ancestor automas.\n                    self._propagate_callbacks_to_nested_automa(\n                        nested_automa=deeper_nested_automa,\n                        callback_builders=callback_builders,\n                    )\n\n    def _remove_worker_incrementally(\n        self,\n        key: str\n    ) -&gt; None:\n        \"\"\"\n        Incrementally remove a worker from the automa. For internal use only.\n        This method is one of the very basic primitives of DDG for dynamic topology changes.\n        \"\"\"\n        if key not in self._workers:\n            raise AutomaRuntimeError(\n                f\"fail to remove worker '{key}' that does not exist!\"\n            )\n\n        worker_to_remove = self._workers[key]\n\n        # Remove the worker.\n        del self._workers[key]\n        # Incrementally update the dynamic states of removed workers.\n        del self._workers_dynamic_states[key]\n\n        if key in self._worker_forwards:\n            # Update the dependencies of the successor workers, if needed.\n            for successor in self._worker_forwards[key]:\n                self._workers[successor].dependencies.remove(key)\n                # Note this detail here: use discard() instead of remove() to avoid KeyError.\n                # This case occurs when a worker call remove_worker() to remove its predecessor worker.\n                self._workers_dynamic_states[successor].dependency_triggers.discard(key)\n            # Incrementally update the forwards table.\n            del self._worker_forwards[key]\n\n        # Remove from the forwards list of all dependencies worker.\n        for trigger in worker_to_remove.dependencies:\n            self._worker_forwards[trigger].remove(key)\n        if key in self._worker_interaction_indices:\n            del self._worker_interaction_indices[key]\n        if key in self._ongoing_interactions:\n            del self._ongoing_interactions[key]\n\n    def _add_dependency_incrementally(\n        self,\n        key: str,\n        dependency: str,\n    ) -&gt; None:\n        \"\"\"\n        Incrementally add a dependency from `key` to `depends`. For internal use only.\n        This method is one of the very basic primitives of DDG for dynamic topology changes.\n        \"\"\"\n        if key not in self._workers:\n            raise AutomaRuntimeError(\n                f\"fail to add dependency from a worker that does not exist: `{key}`!\"\n            )\n        if dependency not in self._workers:\n            raise AutomaRuntimeError(\n                f\"fail to add dependency to a worker that does not exist: `{dependency}`!\"\n            )\n        if dependency in self._workers[key].dependencies:\n            raise AutomaRuntimeError(\n                f\"dependency from '{key}' to '{dependency}' already exists!\"\n            )\n\n        self._workers[key].dependencies.append(dependency)\n        # Note this detail here for dynamic states change:\n        # The new dependency added here may be removed right away if the dependency is just the next kickoff worker. This is a valid behavior.\n        self._workers_dynamic_states[key].dependency_triggers.add(dependency)\n\n        if dependency not in self._worker_forwards:\n            self._worker_forwards[dependency] = []\n        self._worker_forwards[dependency].append(key)\n\n    def _add_worker_internal(\n        self,\n        key: str,\n        worker: Worker,\n        *,\n        dependencies: List[str] = [],\n        is_start: bool = False,\n        is_output: bool = False,\n        args_mapping_rule: ArgsMappingRule = ArgsMappingRule.AS_IS,\n        result_dispatching_rule: ResultDispatchingRule = ResultDispatchingRule.AS_IS,\n        callback_builders: List[WorkerCallbackBuilder] = [],\n    ) -&gt; None:\n        \"\"\"\n        The private version of the method `add_worker()`.\n        \"\"\"\n\n        def _basic_worker_params_check(key: str, worker_obj: Worker):\n            if not isinstance(worker_obj, Worker):\n                raise TypeError(\n                    f\"worker_obj to be registered must be a Worker, \"\n                    f\"but got {type(worker_obj)} for worker '{key}'\"\n                )\n\n            if not (asyncio.iscoroutinefunction(worker_obj.arun) or inspect.isasyncgenfunction(worker_obj.arun)):\n                raise WorkerSignatureError(\n                    f\"arun of Worker must be an async method or async generator, \"\n                    f\"but got {type(worker_obj.arun)} for worker '{key}'\"\n                )\n\n            if not isinstance(dependencies, list):\n                raise TypeError(\n                    f\"dependencies must be a list, \"\n                    f\"but got {type(dependencies)} for worker '{key}'\"\n                )\n            if not all([isinstance(d, str) for d in dependencies]):\n                raise ValueError(\n                    f\"dependencies must be a List of str, \"\n                    f\"but got {dependencies} for worker {key}\"\n                )\n\n            if args_mapping_rule not in ArgsMappingRule:\n                raise ValueError(\n                    f\"args_mapping_rule must be one of the following: {[e for e in ArgsMappingRule]}, \"\n                    f\"but got {args_mapping_rule} for worker {key}\"\n                )\n\n            if result_dispatching_rule not in ResultDispatchingRule:\n                raise ValueError(\n                    f\"result_dispatching_rule must be one of the following: {[e for e in ResultDispatchingRule]}, \"\n                    f\"but got {result_dispatching_rule} for worker {key}\"\n                )\n\n        # Ensure the parameters are valid.\n        _basic_worker_params_check(key, worker)\n\n        if not self._automa_running:\n            # Add worker during the [Initialization Phase].\n            self._add_worker_incrementally(\n                key=key,\n                worker=worker,\n                dependencies=dependencies,\n                is_start=is_start,\n                is_output=is_output,\n                args_mapping_rule=args_mapping_rule,\n                result_dispatching_rule=result_dispatching_rule,\n                callback_builders=callback_builders,\n            )\n        else:\n            # Add worker during the [Running Phase].\n            deferred_task = _AddWorkerDeferredTask(\n                worker_key=key,\n                worker_obj=worker,\n                dependencies=dependencies,\n                is_start=is_start,\n                is_output=is_output,\n                args_mapping_rule=args_mapping_rule,\n                result_dispatching_rule=result_dispatching_rule,\n                callback_builders=callback_builders,\n            )\n            # The order of topology deferred tasks is determined by the order of the calls of topology changing methods.\n            self._topology_deferred_tasks.append(deferred_task)\n\n    def _add_func_as_worker_internal(\n        self,\n        key: str,\n        func: Callable,\n        *,\n        dependencies: List[str] = [],\n        is_start: bool = False,\n        is_output: bool = False,\n        args_mapping_rule: ArgsMappingRule = ArgsMappingRule.AS_IS,\n        result_dispatching_rule: ResultDispatchingRule = ResultDispatchingRule.AS_IS,\n        callback_builders: List[WorkerCallbackBuilder] = [],\n    ) -&gt; None:\n        \"\"\"\n        The private version of the method `add_func_as_worker()`.\n        \"\"\"\n        if not isinstance(func, MethodType) and key in self._registered_worker_funcs:\n            func = MethodType(func, self)\n\n        # Validate: if func is a method, its bounded __self__ must be self when add_func_as_worker() is called.\n        if hasattr(func, \"__self__\") and func.__self__ is not self:\n            raise AutomaRuntimeError(\n                f\"the bounded instance of `func` must be the same as the instance of the GraphAutoma, \"\n                f\"but got {func.__self__}\"\n            )\n\n        # Register func as an instance of CallableWorker.\n        func_worker = CallableWorker(func)\n\n        self._add_worker_internal(\n            key=key,\n            worker=func_worker,\n            dependencies=dependencies,\n            is_start=is_start,\n            is_output=is_output,\n            args_mapping_rule=args_mapping_rule,\n            result_dispatching_rule=result_dispatching_rule,\n            callback_builders=callback_builders,\n        )\n\n    def all_workers(self) -&gt; List[str]:\n        \"\"\"\n        Gets a list containing the keys of all workers registered in this Automa.\n\n        Returns\n        -------\n        List[str]\n            A list of worker keys.\n        \"\"\"\n        return list(self._workers.keys())\n\n    def add_worker(\n        self,\n        key: str,\n        worker: Worker,\n        *,\n        dependencies: List[str] = [],\n        is_start: bool = False,\n        is_output: bool = False,\n        args_mapping_rule: ArgsMappingRule = ArgsMappingRule.AS_IS,\n        result_dispatching_rule: ResultDispatchingRule = ResultDispatchingRule.AS_IS,\n        callback_builders: List[WorkerCallbackBuilder] = [],\n    ) -&gt; None:\n        \"\"\"\n        This method is used to add a worker dynamically into the automa.\n\n        If this method is called during the [Initialization Phase], the worker will be added immediately. If this method is called during the [Running Phase], the worker will be added as a deferred task which will be executed in the next DS.\n\n        The dependencies can be added together with a worker. However, you can add a worker without any dependencies.\n\n        Note: args_mapping_rule and result_dispatching_rule could only be set when using worker-adding API. Even if the worker has no any dependencies.\n\n        Parameters\n        ----------\n        key : str\n            The key of the worker.\n        worker : Worker\n            The worker instance to be registered.\n        dependencies : List[str]\n            A list of worker keys that the worker depends on.\n        is_start : bool\n            Whether the worker is a start worker.\n        is_output : bool\n            Whether the worker is an output worker.\n        args_mapping_rule : ArgsMappingRule\n            The rule of arguments mapping.\n        result_dispatching_rule : ResultDispatchingRule\n            The rule of result dispatch.\n        callback_builders : List[WorkerCallbackBuilder]\n            A list of worker callback builders to be registered.\n            Callback instances will be created from builders when the worker is instantiated.\n        \"\"\"\n        self._add_worker_internal(\n            key=key,\n            worker=worker,\n            dependencies=dependencies,\n            is_start=is_start,\n            is_output=is_output,\n            args_mapping_rule=args_mapping_rule,\n            result_dispatching_rule=result_dispatching_rule,\n            callback_builders=callback_builders,\n        )\n\n    def add_func_as_worker(\n        self,\n        key: str,\n        func: Callable,\n        *,\n        dependencies: List[str] = [],\n        is_start: bool = False,\n        is_output: bool = False,\n        args_mapping_rule: ArgsMappingRule = ArgsMappingRule.AS_IS,\n        result_dispatching_rule: ResultDispatchingRule = ResultDispatchingRule.AS_IS,\n        callback_builders: List[WorkerCallbackBuilder] = [],\n    ) -&gt; None:\n        \"\"\"\n        This method is used to add a function as a worker into the automa.\n\n        The format of the parameters will follow that of the decorator @worker(...), so that the \n        behavior of the decorated function is consistent with that of normal CallableLandableWorker objects.\n\n        Parameters\n        ----------\n        key : str\n            The key of the function worker.\n        func : Callable\n            The function to be added as a worker to the automa.\n        dependencies : List[str]\n            A list of worker names that the decorated callable depends on.\n        is_start : bool\n            Whether the decorated callable is a start worker. True means it is, while False means it is not.\n        is_output : bool\n            Whether the decorated callable is an output worker. True means it is, while False means it is not.\n        args_mapping_rule : ArgsMappingRule\n            The rule of arguments mapping.\n        result_dispatching_rule : ResultDispatchingRule\n            The rule of result dispatch.\n        callback_builders : List[WorkerCallbackBuilder]\n            A list of worker callback builders to be registered.\n            Callback instances will be created from builders when the worker is instantiated.\n        \"\"\"\n        self._add_func_as_worker_internal(\n            key=key,\n            func=func,\n            dependencies=dependencies,\n            is_start=is_start,\n            is_output=is_output,\n            args_mapping_rule=args_mapping_rule,\n            result_dispatching_rule=result_dispatching_rule,\n            callback_builders=callback_builders,\n        )\n\n    def worker(\n        self,\n        *,\n        key: Optional[str] = None,\n        dependencies: List[str] = [],\n        is_start: bool = False,\n        is_output: bool = False,\n        args_mapping_rule: ArgsMappingRule = ArgsMappingRule.AS_IS,\n        result_dispatching_rule: ResultDispatchingRule = ResultDispatchingRule.AS_IS,\n        callback_builders: List[WorkerCallbackBuilder] = [],\n    ) -&gt; Callable:\n        \"\"\"\n        This is a decorator used to mark a function as an GraphAutoma detectable Worker. Dislike the \n        global decorator @worker(...), it is usally used after an GraphAutoma instance is initialized.\n\n        The format of the parameters will follow that of the decorator @worker(...), so that the \n        behavior of the decorated function is consistent with that of normal CallableLandableWorker objects.\n\n        Parameters\n        ----------\n        key : str\n            The key of the worker. If not provided, the name of the decorated callable will be used.\n        dependencies : List[str]\n            A list of worker names that the decorated callable depends on.\n        is_start : bool\n            Whether the decorated callable is a start worker. True means it is, while False means it is not.\n        is_output : bool\n            Whether the decorated callable is an output worker. True means it is, while False means it is not.\n        args_mapping_rule : str\n            The rule of arguments mapping. The options are: \"auto\", \"as_list\", \"as_dict\", \"suppressed\".\n        result_dispatching_rule : ResultDispatchingRule\n            The rule of result dispatch.\n        callback_builders : List[WorkerCallbackBuilder]\n            A list of worker callback builders to be registered.\n            Callback instances will be created from builders when the worker is instantiated.\n        \"\"\"\n        def wrapper(func: Callable):\n            self._add_func_as_worker_internal(\n                key=(key or func.__name__),\n                func=func,\n                dependencies=dependencies,\n                is_start=is_start,\n                is_output=is_output,\n                args_mapping_rule=args_mapping_rule,\n                result_dispatching_rule=result_dispatching_rule,\n                callback_builders=callback_builders,\n            )\n\n        return wrapper\n\n    def remove_worker(self, key: str) -&gt; None:\n        \"\"\"\n        Remove a worker from the Automa. This method can be called at any time to remove a worker from the Automa.\n\n        When a worker is removed, all dependencies related to this worker, including all the dependencies of the worker itself and the dependencies between the worker and its successor workers, will be also removed.\n\n        Parameters\n        ----------\n        key : str\n            The key of the worker to be removed.\n\n        Returns\n        -------\n        None\n\n        Raises\n        ------\n        AutomaDeclarationError\n            If the worker specified by key does not exist in the Automa, this exception will be raised.\n        \"\"\"\n        if not self._automa_running:\n            # remove immediately\n            self._remove_worker_incrementally(key)\n        else:\n            deferred_task = _RemoveWorkerDeferredTask(\n                worker_key=key,\n            )\n            # Note: the execution order of topology change deferred tasks is important and is determined by the order of the calls of add_worker(), remove_worker() and add_dependency() in one DS.\n            self._topology_deferred_tasks.append(deferred_task)\n\n    def add_dependency(\n        self,\n        key: str,\n        dependency: str,\n    ) -&gt; None:\n        \"\"\"\n        This method is used to dynamically add a dependency from `key` to `dependency`.\n\n        Note: args_mapping_rule and result_dispatching_rule is not allowed to be set by this method, \n        instead they should be set together with add_worker() or add_func_as_worker() when adding the worker.\n\n        Parameters\n        ----------\n        key : str\n            The key of the worker that will depend on the worker with key `dependency`.\n        dependency : str\n            The key of the worker on which the worker with key `key` will depend.\n        \"\"\"\n        ...\n        if not self._automa_running:\n            # add the dependency immediately\n            self._add_dependency_incrementally(key, dependency)\n        else:\n            deferred_task = _AddDependencyDeferredTask(\n                worker_key=key,\n                dependency=dependency,\n            )\n            # Note: the execution order of topology change deferred tasks is important and is determined by the order of the calls of add_worker(), remove_worker() and add_dependency() in one DS.\n            self._topology_deferred_tasks.append(deferred_task)\n\n    def _validate_canonical_graph(self):\n        \"\"\"\n        This method is used to validate that DDG graph is canonical.\n        \"\"\"\n        for worker_key, worker_obj in self._workers.items():\n            for dependency_key in worker_obj.dependencies:\n                if dependency_key not in self._workers:\n                    raise AutomaCompilationError(\n                        f\"the dependency `{dependency_key}` of worker `{worker_key}` does not exist\"\n                    )\n        assert set(self._workers.keys()) == set(self._workers_dynamic_states.keys())\n        for worker_key, worker_dynamic_state in self._workers_dynamic_states.items():\n            for dependency_key in worker_dynamic_state.dependency_triggers:\n                assert dependency_key in self._workers[worker_key].dependencies\n\n        for worker_key, worker_obj in self._workers.items():\n            for dependency_key in worker_obj.dependencies:\n                assert worker_key in self._worker_forwards[dependency_key]\n        for worker_key, successor_keys in self._worker_forwards.items():\n            for successor_key in successor_keys:\n                assert worker_key in self._workers[successor_key].dependencies\n\n    def _compile_graph_and_detect_risks(self):\n        \"\"\"\n        This method should be called at the very beginning of self.run() to ensure that:\n        1. The whole graph is built out of all of the following worker sources:\n            - Pre-defined workers, such as:\n                - Methods decorated with @worker(...)\n            - Post-added workers, such as:\n                - Functions decorated with @automa_obj.worker(...)\n                - Workers added via automa_obj.add_func_as_worker(...)\n                - Workers added via automa_obj.add_worker(...)\n        2. The dependencies of each worker are confirmed to satisfy the DAG constraints.\n        \"\"\"\n\n        # Validate the canonical graph.\n        self._validate_canonical_graph()\n        # Validate the DAG constraints.\n        GraphMeta.validate_dag_constraints(self._worker_forwards)\n        # TODO: More validations can be added here...\n\n        # Find all connected components of the whole automa graph.\n        self._find_connected_components()\n\n    def ferry_to(self, key: str, /, *args, **kwargs):\n        \"\"\"\n        Defer the invocation to the specified worker, passing any provided arguments. This creates a \n        delayed call, ensuring the worker will be scheduled to run asynchronously in the next event loop, \n        independent of its dependencies.\n\n        This primitive is commonly used for:\n\n        1. Implementing dynamic branching based on runtime conditions.\n        2. Creating logic that forms cyclic graphs.\n\n        Parameters\n        ----------\n        key : str\n            The key of the worker to run.\n        args : optional\n            Positional arguments to be passed.\n        kwargs : optional\n            Keyword arguments to be passed.\n\n        Examples\n        --------\n        ```python\n        class MyGraphAutoma(GraphAutoma):\n            @worker(is_start=True)\n            def start_worker(self):\n                number = random.randint(0, 1)\n                if number == 0:\n                    self.ferry_to(\"cond_1_worker\", number=number)\n                else:\n                    self.ferry_to(\"cond_2_worker\")\n\n            @worker()\n            def cond_1_worker(self, number: int):\n                print(f'Got {{number}}!')\n\n            @worker()\n            def cond_2_worker(self):\n                self.ferry_to(\"start_worker\")\n\n        automa = MyGraphAutoma()\n        await automa.arun()\n\n        # Output: Got 0!\n        ```\n        \"\"\"\n        # TODO: check worker_key is valid, maybe deferred check...\n        running_options = self._get_top_running_options()\n        # if debug is enabled, trace back the kickoff worker key from stacktrace.\n        kickoff_worker_key: str = self._trace_back_kickoff_worker_key_from_stack() if running_options.debug else None\n        deferred_task = _FerryDeferredTask(\n            ferry_to_worker_key=key,\n            kickoff_worker_key=kickoff_worker_key,\n            args=args,\n            kwargs=kwargs,\n        )\n        # Note: ferry_to() may be called in a new thread.\n        # But _ferry_deferred_tasks is not necessary to be thread-safe due to Visibility Guarantees of the Bridgic Concurrency Model.\n        self._ferry_deferred_tasks.append(deferred_task)\n\n    def _clean_all_worker_local_space(self):\n        \"\"\"\n        Clean the local space of all workers.\n        \"\"\"\n        for worker_obj in self._workers.values():\n            worker_obj.local_space = {}\n\n    def _clear_task_level_state(self) -&gt; None:\n        \"\"\"\n        Clear task-level state which is built during each dynamic step (DS) of the execution.\n        Used when:\n\n        - (1) ending a DS and proceeding to the next DS\n        - (2) going to raise due to interaction exceptions, to leave the automa in a consistent and pausable state\n        \"\"\"\n        self._running_tasks.clear()\n        self._topology_deferred_tasks.clear()\n        self._ferry_deferred_tasks.clear()\n\n    def _clear_interaction_indices(self) -&gt; None:\n        \"\"\"\n        Clear _worker_interaction_indices so the next interact_with_human in a worker uses index 0.\n        \"\"\"\n        self._worker_interaction_indices.clear()\n\n    def _clear_run_level_state(self) -&gt; None:\n        \"\"\"\n        Clear run-level state after a complete run. Enables the automa instance to be run again.\n        \"\"\"\n        self._input_buffer = _AutomaInputBuffer()\n        if self.should_reset_local_space():\n            self._clean_all_worker_local_space()\n        self._clear_interaction_indices()\n        self._ongoing_interactions.clear()\n        self._automa_running = False\n\n    async def arun(\n        self,\n        *args: Tuple[Any, ...],\n        feedback_data: Optional[Union[InteractionFeedback, List[InteractionFeedback]]] = None,\n        **kwargs: Dict[str, Any]\n    ) -&gt; Any:\n        \"\"\"\n        The entry point for running the constructed `GraphAutoma` instance.\n\n        This method serves as the entry point for both initial execution and resumption after \n        interruption of an automa instance. It automatically drives the execution of workers \n        based on their `dependencies` and explicit `ferry_to()` calls. Each execution will be \n        wrapped in an `asyncio.Task` to ensure context isolation.\n\n        **Automatic Scheduling**\n\n        The scheduling behavior in `GraphAutoma` is automatically driven by:\n\n        - Worker dependencies: Workers are scheduled to run only after all their necessary \n          dependencies are satisfied. The dependencies automatically drive the execution order.\n\n        - Calling ferry_to: During execution, a worker can explicitly trigger another worker \n          by calling `ferry_to()`, which enables dynamic flow control and conditional branching.\n\n        - Dynamic topology changes: When the graph topology is modified at runtime (such as \n          adding or removing workers or dependencies), the scheduling system seamlessly updates \n          to reflect the latest structure, ensuring that worker execution always follows the \n          current graph.\n\n        **Human Interaction Mechanism**\n\n        Workers can request human input by calling `interact_with_human()` during execution. \n        When this occurs:\n\n        - The execution will be paused after the running workers finish their execution.\n        - The Automa's state will be serialized into a `Snapshot` object.\n        - An `InteractionException` will be raised to the application layer. It contains both the \n          list of pending `Interaction` objects and the `Snapshot` object.\n        - The application layer may persist the `Snapshot` properly to resume the execution later.\n        - To resume execution, the application layer should reload the Automa state using \n          `load_from_snapshot()` with the saved `Snapshot` object and call `arun()` again with \n          `feedback_data` containing the user's feedback(s) to finish a complete interaction.\n\n        Parameters\n        ----------\n        args : optional\n            Positional arguments to be passed.\n        feedback_data : Optional[Union[InteractionFeedback, List[InteractionFeedback]]]\n            Feedbacks that are received from one or multiple human interactions occurred before the\n            Automa was paused. This argument may be of type `InteractionFeedback` or \n            `List[InteractionFeedback]`. If only one interaction occurred, `feedback_data` should be\n            of type `InteractionFeedback`. If multiple interactions occurred simultaneously, \n            `feedback_data` should be of type `List[InteractionFeedback]`.\n        kwargs : optional\n            Keyword arguments which may be further propagated to contained workers.\n\n        Returns\n        -------\n        Any\n            The execution result of the output-worker that has the setting `is_output=True`,\n            otherwise None.\n\n        Raises\n        ------\n        InteractionException\n            If the Automa is the top-level Automa and the `interact_with_human()` method is called\n            by one or more workers within the lastest event loop iteration, this exception will be\n            raised to the application layer.\n        \"\"\"\n        if self.is_top_level():\n            # For top-level automa, wrap in a task to ensure context isolation\n            task = asyncio.create_task(\n                self._arun_internal(*args, feedback_data=feedback_data, **kwargs),\n                name=f\"GraphAutoma-{self.name}-arun\"\n            )\n            return await task\n        else:\n            # For nested automa, directly call _arun_internal to avoid redundant task creation\n            return await self._arun_internal(*args, feedback_data=feedback_data, **kwargs)\n\n    async def _arun_internal(\n        self,\n        *args: Tuple[Any, ...],\n        feedback_data: Optional[Union[InteractionFeedback, List[InteractionFeedback]]] = None,\n        **kwargs: Dict[str, Any]\n    ) -&gt; Any:\n        \"\"\"\n        Internal implementation of `arun()` for `GraphAutoma`.\n\n        The scheduling behavior in `GraphAutoma` is automatically driven by:\n\n        1. **Worker dependencies**: Workers are scheduled to run only after all their necessary \n        dependencies are satisfied. The dependencies automatically drives the execution order.\n\n        2. **Calling ferry_to**: During execution, a worker can explicitly trigger another worker \n        with calling `ferry_to()`, which enables dynamic flow control and conditional branching.\n\n        3. **Dynamic topology changes**: When the graph topology is modified at runtime (such as adding \n        or removing workers or dependencies), the scheduling system seamlessly updates to reflect \n        the latest structure, ensuring that worker execution always follows the current graph.\n        \"\"\"\n\n        def _init_input_buffer_if_needed(args: Tuple[Any, ...], kwargs: Dict[str, Any]):\n            if not self._current_kickoff_workers:\n                self._input_buffer.args = args\n                self._input_buffer.kwargs = kwargs\n\n        def _init_start_kickoff_workers_if_needed():\n            # Note: After deserialization, the _current_kickoff_workers must not be empty!\n            # Therefore, _current_kickoff_workers will only be reinitialized when the Automa is run for the first time or rerun.\n            # It is guaranteed that _current_kickoff_workers will not be reinitialized when the Automa is resumed after deserialization.\n            if not self._current_kickoff_workers:\n                self._current_kickoff_workers = [\n                    _KickoffInfo(\n                        worker_key=worker_key,\n                        last_kickoff=\"__automa__\"\n                    ) for worker_key, worker_obj in self._workers.items()\n                    if getattr(worker_obj, \"is_start\", False)\n                ]\n\n        def _execute_topology_change_deferred_tasks(tc_tasks: List[Union[_AddWorkerDeferredTask, _RemoveWorkerDeferredTask, _AddDependencyDeferredTask]]):\n            # update the control flow topology\n            for topology_task in tc_tasks:\n                if topology_task.task_type == \"add_worker\":\n                    self._add_worker_incrementally(\n                        key=topology_task.worker_key,\n                        worker=topology_task.worker_obj,\n                        dependencies=topology_task.dependencies,\n                        is_start=topology_task.is_start,\n                        is_output=topology_task.is_output,\n                        args_mapping_rule=topology_task.args_mapping_rule,\n                        result_dispatching_rule=topology_task.result_dispatching_rule,\n                        callback_builders=topology_task.callback_builders,\n                    )\n                elif topology_task.task_type == \"remove_worker\":\n                    self._remove_worker_incrementally(topology_task.worker_key)\n                elif topology_task.task_type == \"add_dependency\":\n                    self._add_dependency_incrementally(topology_task.worker_key, topology_task.dependency)\n\n            # update the data flow topology\n            args_manager.update_data_flow_topology(dynamic_tasks=tc_tasks)\n\n        def _set_worker_run_finished(worker_key: str):\n            for kickoff_info in self._current_kickoff_workers:\n                if kickoff_info.worker_key == worker_key:\n                    kickoff_info.run_finished = True\n                    break\n\n        def _check_and_normalize_interaction_params(\n            feedback_data: Optional[Union[InteractionFeedback, List[InteractionFeedback]]] = None,\n            interaction_feedback: Optional[InteractionFeedback] = None,\n            interaction_feedbacks: Optional[List[InteractionFeedback]] = None,\n        ) -&gt; List[InteractionFeedback]:\n            if feedback_data:\n                if isinstance(feedback_data, list):\n                    rx_feedbacks = feedback_data\n                else:\n                    rx_feedbacks = [feedback_data]\n                return rx_feedbacks\n            # For backward compatibility with old parameter names. To be removed in the future.\n            if interaction_feedback and interaction_feedbacks:\n                raise AutomaRuntimeError(\n                    f\"Only one of interaction_feedback or interaction_feedbacks can be used. \"\n                    f\"But received interaction_feedback={interaction_feedback} and \\n\"\n                    f\"interaction_feedbacks={interaction_feedbacks}\"\n                )\n            if interaction_feedback:\n                rx_feedbacks = [interaction_feedback]\n            else:\n                rx_feedbacks = interaction_feedbacks\n            return rx_feedbacks\n\n        def _match_ongoing_interaction_and_feedbacks(rx_feedbacks:List[InteractionFeedback]):\n            match_left_feedbacks = []\n            for feedback in rx_feedbacks:\n                matched = False\n                for interaction_and_feedbacks in self._ongoing_interactions.values():\n                    for interaction_and_feedback in interaction_and_feedbacks:\n                        if interaction_and_feedback.interaction.interaction_id == feedback.interaction_id:\n                            matched = True\n                            # Set the feedback to its corresponding interaction which is maintained in self._ongoing_interactions\n                            # Note: Only one feedback is allowed for each interaction.\n                            if interaction_and_feedback.feedback is None:\n                                interaction_and_feedback.feedback = feedback\n                            break\n                    if matched:\n                        break\n                if not matched:\n                    match_left_feedbacks.append(feedback)\n            return match_left_feedbacks\n\n        _init_input_buffer_if_needed(args, kwargs)\n\n        self._main_loop = asyncio.get_running_loop()\n        self._main_thread_id = threading.get_ident()\n        if self.thread_pool is None:\n            self.thread_pool = ThreadPoolExecutor(thread_name_prefix=\"bridgic-thread\")\n\n        is_top_level = self.is_top_level()\n        running_options = self._get_top_running_options()\n\n        # If this is the top-level automa, execute its callbacks separately.\n        if is_top_level:\n            automa_callbacks = self._get_automa_callbacks()\n\n            for callback in automa_callbacks:\n                await callback.on_worker_start(\n                    key=self.name,\n                    is_top_level=True,\n                    parent=self.parent,\n                    arguments={\n                        \"args\": self._input_buffer.args,\n                        \"kwargs\": self._input_buffer.kwargs,\n                        \"feedback_data\": feedback_data,\n                    },\n                )\n\n        if not self._automa_running:\n            # Here is the last chance to compile and check the DDG in the end of the [Initialization Phase] (phase 1 just before the first DS).\n            self._compile_graph_and_detect_risks()\n            self._automa_running = True\n\n        # An Automa needs to be re-run with _current_kickoff_workers reinitialized.\n        _init_start_kickoff_workers_if_needed()\n\n        # For backward compatibility with old parameter names. To be removed in the future.\n        interaction_feedback = kwargs.get(\"interaction_feedback\")\n        interaction_feedbacks = kwargs.get(\"interaction_feedbacks\")\n        rx_feedbacks = _check_and_normalize_interaction_params(feedback_data, interaction_feedback, interaction_feedbacks)\n        if rx_feedbacks:\n            rx_feedbacks = _match_ongoing_interaction_and_feedbacks(rx_feedbacks)\n\n        if running_options.debug:\n            printer.print(f\"[{type(self).__name__}]-[{self.name}] is started.\", color=\"green\")\n\n        # Task loop divided into many dynamic steps (DS).\n        args_manager = ArgsManager(\n            input_args=self._input_buffer.args,\n            input_kwargs=self._input_buffer.kwargs,\n            worker_outputs=self._worker_output,\n            worker_forwards=self._worker_forwards,\n            worker_dict=self._workers\n        )\n        is_output_worker_keys = set()\n\n        # For each worker scheduled for execution, initiate a corresponding task (within the current event loop).\n        while self._current_kickoff_workers:\n            # A new Dynamic Step is started now.\n            if running_options.debug:\n                kickoff_worker_keys = [kickoff_info.worker_key for kickoff_info in self._current_kickoff_workers]\n                printer.print(f\"[{type(self).__name__}]-[{self.name}] [__dynamic_step__] driving [{', '.join(kickoff_worker_keys)}]\", color=\"purple\")\n\n            for kickoff_info in self._current_kickoff_workers:\n                if kickoff_info.run_finished:\n                    # Skip finished workers. Here is the case that the Automa is resumed after a human interaction.\n                    if running_options.debug:\n                        printer.print(f\"[{type(self).__name__}]-[{self.name}] [{kickoff_info.worker_key}] will be skipped - run finished\", color=\"cyan\")\n                    continue\n\n                if running_options.debug:\n                    trigger_name = kickoff_info.last_kickoff\n                    if trigger_name == \"__automa__\":\n                        trigger_name = \"__automa__\"\n                    printer.print(f\"[{type(self).__name__}]-[{self.name}] [{trigger_name}] triggers [{kickoff_info.worker_key}]\", color=\"cyan\")\n\n                # Arguments Mapping:\n                binding_args, binding_kwargs = args_manager.args_binding(\n                    last_worker_key=kickoff_info.last_kickoff,\n                    current_worker_key=kickoff_info.worker_key\n                ) if not kickoff_info.from_ferry else ((), {})\n                # Inputs Propagation\n                _, propagation_kwargs = args_manager.inputs_propagation(current_worker_key=kickoff_info.worker_key)\n                # Data injection.\n                _, injection_kwargs = args_manager.args_injection(\n                    current_worker_key=kickoff_info.worker_key, \n                    current_automa=self\n                )\n                # Ferry arguments.\n                ferry_args, ferry_kwargs = kickoff_info.args, kickoff_info.kwargs\n                # combine the arguments from the three steps.\n                # kwargs will cover priority follows: propagation_kwargs &lt; binding_kwargs &lt; injection_kwargs &lt; ferry_kwargs\n                next_args, next_kwargs = safely_map_args(\n                    (*binding_args, *ferry_args), \n                    {**propagation_kwargs, **binding_kwargs, **injection_kwargs, **ferry_kwargs}, \n                    self._workers[kickoff_info.worker_key].get_input_param_names(),\n                )\n\n                # Collect the output worker keys.\n                if self._workers[kickoff_info.worker_key].is_output:\n                    is_output_worker_keys.add(kickoff_info.worker_key)\n                    if len(is_output_worker_keys) &gt; 1:\n                        raise AutomaRuntimeError(\n                            f\"It is not allowed to have more than one worker with `is_output=True` and \"\n                            f\"they are all considered as output-worker when the automa terminates and returns.\"\n                            f\"The current output-worker keys are: {is_output_worker_keys}.\"\n                            f\"If you want to collect the results of multiple workers simultaneously, \"\n                            f\"it is recommended that you add one worker to gather them.\"\n                        )\n\n                # Schedule task for each kickoff worker.\n                worker_obj = self._workers[kickoff_info.worker_key]\n                if worker_obj.is_automa():\n                    coro = worker_obj.arun(\n                        *next_args,\n                        feedback_data=rx_feedbacks,\n                        **next_kwargs,\n                    )\n                else:\n                    # The result of `worker_obj.arun()` may be a coroutine or an async generator.\n                    arun_result = worker_obj.arun(*next_args, **next_kwargs)\n\n                    if inspect.isasyncgen(arun_result):\n                        async def _wrap_async_gen():\n                            return arun_result\n                        coro = _wrap_async_gen()\n                    else:\n                        coro = arun_result\n\n                # Create a task for the current worker and record it.\n                task = asyncio.create_task(\n                    # TODO1: arun() may need to be wrapped to support better interrupt...\n                    coro,\n                    name=f\"Task-{kickoff_info.worker_key}\"\n                )\n                self._running_tasks.append(_RunnningTask(\n                    worker_key=kickoff_info.worker_key,\n                    task=task,\n                ))\n\n            # Block until all of the running tasks are finished.\n            while True:\n                undone_tasks = [t.task for t in self._running_tasks if not t.task.done()]\n                if not undone_tasks:\n                    break\n                try:\n                    await undone_tasks[0]\n                except Exception as e:\n                    ...\n                    # The same exception will be raised again in the following task.result().\n                    # Note: A Task is done when the wrapped coroutine either returned a value, raised an exception, or the Task was cancelled.\n                    # Refer to: https://docs.python.org/3/library/asyncio-task.html#task-object\n\n            # Process graph topology change deferred tasks triggered by add_worker() and remove_worker().\n            _execute_topology_change_deferred_tasks(self._topology_deferred_tasks)\n\n            # Handle exceptions raised by all running tasks.\n            interaction_exceptions: List[_InteractionEventException] = []\n            non_interaction_exceptions: List[Exception] = []\n\n            for task in self._running_tasks:\n                try:\n                    # It will raise an exception if task failed.\n                    task_result = task.task.result()\n                    _set_worker_run_finished(task.worker_key)\n\n                    if task.worker_key in self._workers:\n                        # The current running worker may be removed.\n                        worker_obj = self._workers[task.worker_key]\n                        # Collect results of the finished tasks.\n                        self._worker_output[task.worker_key] = task_result\n                        # reset dynamic states of finished workers.\n                        self._workers_dynamic_states[task.worker_key].dependency_triggers = set(getattr(worker_obj, \"dependencies\", []))\n                        # Update the dynamic states of successor workers.\n                        for successor_key in self._worker_forwards.get(task.worker_key, []):\n                            self._workers_dynamic_states[successor_key].dependency_triggers.remove(task.worker_key)\n                        # Each time a worker is finished running, the ongoing interaction states should be cleared. Once it is re-run, the human interactions in the worker can be triggered again.\n                        if task.worker_key in self._worker_interaction_indices:\n                            del self._worker_interaction_indices[task.worker_key]\n                        if task.worker_key in self._ongoing_interactions:\n                            del self._ongoing_interactions[task.worker_key]\n                except Exception as e:\n                    if isinstance(e, _InteractionEventException):\n                        interaction_exceptions.append(e)\n                        if (\n                            task.worker_key in self._workers \n                            and not self._workers[task.worker_key].is_automa()\n                        ):\n                            interactions = self._ongoing_interactions.setdefault(task.worker_key, [])\n                            current_interaction = e.args[0]\n                            # Ensure unique interaction_id for each human interaction.\n                            if all(iaf.interaction.interaction_id != current_interaction.interaction_id for iaf in interactions):\n                                interactions.append(_InteractionAndFeedback(interaction=current_interaction))\n                    else:\n                        non_interaction_exceptions.append(e)\n\n            if len(self._topology_deferred_tasks) &gt; 0:\n                # Graph topology validation and risk detection. Only needed when topology changes.\n                # Guarantee the graph topology is valid and consistent after each DS.\n                # 1. Validate the canonical graph.\n                self._validate_canonical_graph()\n                # 2. Validate the DAG constraints.\n                GraphMeta.validate_dag_constraints(self._worker_forwards)\n                # TODO: more validations can be added here...\n\n            # TODO: Ferry-related risk detection may be added here...\n\n            # Handle exceptions with callbacks at the top-level automa before re-raising them.\n            if is_top_level:\n                # Get cached callbacks for top-level automa\n                automa_callbacks = self._get_automa_callbacks()\n\n                # Process interaction exceptions with callbacks (they cannot be suppressed, but callbacks can observe them)\n                for e in interaction_exceptions + non_interaction_exceptions:\n                    await try_handle_error_with_callbacks(\n                        callbacks=automa_callbacks,\n                        key=self.name,\n                        is_top_level=True,\n                        parent=self.parent,\n                        arguments={\n                            \"args\": self._input_buffer.args,\n                            \"kwargs\": self._input_buffer.kwargs,\n                            \"feedback_data\": feedback_data,\n                        },\n                        error=e,\n                    )\n\n            # For inner interaction exceptions, collect them and throw an InteractionException as a whole.\n            if len(interaction_exceptions) &gt; 0:\n                # Ensure the automa's task and interaction states are clean when resuming.\n                self._clear_task_level_state()\n                self._clear_interaction_indices()\n\n                all_interactions: List[Interaction] = [interaction for e in interaction_exceptions for interaction in e.args]\n                if self.is_top_level():\n                    # This is the top-level Automa. Serialize the Automa and raise InteractionException to the application layer.\n                    serialized_automa = dump_bytes(self)\n                    snapshot = Snapshot(\n                        serialized_bytes=serialized_automa,\n                        serialization_version=GraphAutoma.SERIALIZATION_VERSION,\n                    )\n                    raise InteractionException(\n                        interactions=all_interactions,\n                        snapshot=snapshot,\n                    )\n                else:\n                    # Continue raise exception to the upper level Automa.\n                    raise _InteractionEventException(*all_interactions)\n\n            # For non-interaction exceptions, immediately raise the first one directly, since none of them are meant to be suppressed.\n            if len(non_interaction_exceptions) &gt; 0:\n                self._clear_run_level_state()\n                raise non_interaction_exceptions[0]\n\n            # Find next kickoff workers and rebuild _current_kickoff_workers\n            run_finished_worker_keys: List[str] = [kickoff_info.worker_key for kickoff_info in self._current_kickoff_workers if kickoff_info.run_finished]\n            assert len(run_finished_worker_keys) == len(self._current_kickoff_workers)\n            self._current_kickoff_workers = []\n            # New kickoff workers can be triggered by two ways:\n            # 1. The ferry_to() operation is called during current worker execution.\n            # 2. The dependencies are eliminated after all predecessor workers are finished.\n            # So,\n            # First add kickoff workers triggered by ferry_to();\n            for ferry_task in self._ferry_deferred_tasks:\n                self._current_kickoff_workers.append(_KickoffInfo(\n                    worker_key=ferry_task.ferry_to_worker_key,\n                    last_kickoff=ferry_task.kickoff_worker_key,\n                    from_ferry=True,\n                    args=ferry_task.args,\n                    kwargs=ferry_task.kwargs,\n                ))\n            # Then add kickoff workers triggered by dependencies elimination.\n            # Merge successor keys of all finished tasks.\n            successor_keys = set()\n            for worker_key in run_finished_worker_keys:\n                # Note: The `worker_key` worker may have been removed from the Automa.\n                for successor_key in self._worker_forwards.get(worker_key, []):\n                    if successor_key not in successor_keys:\n                        dependency_triggers = self._workers_dynamic_states[successor_key].dependency_triggers\n                        if not dependency_triggers:\n                            self._current_kickoff_workers.append(_KickoffInfo(\n                                worker_key=successor_key,\n                                last_kickoff=worker_key,\n                            ))\n                        successor_keys.add(successor_key)\n\n            self._clear_task_level_state()\n\n        if running_options.debug:\n            printer.print(f\"[{type(self).__name__}]-[{self.name}] is finished.\", color=\"green\")\n\n        self._clear_run_level_state()\n\n        # Get result before calling callbacks\n        if is_output_worker_keys:\n            result = self._worker_output.get(list(is_output_worker_keys)[0], None)\n        else:\n            result = None\n\n        # If this is the top-level automa, execute its callbacks separately.\n        if is_top_level:\n            automa_callbacks = self._get_automa_callbacks()\n            for callback in automa_callbacks:\n                await callback.on_worker_end(\n                    key=self.name,\n                    is_top_level=True,\n                    parent=self.parent,\n                    arguments={\n                        \"args\": self._input_buffer.args,\n                        \"kwargs\": self._input_buffer.kwargs,\n                        \"feedback_data\": feedback_data,\n                    },\n                    result=result,\n                )\n\n        return result\n\n    def _get_worker_dependencies(self, worker_key: str) -&gt; List[str]:\n        \"\"\"\n        Get the worker keys of all dependencies of the worker.\n        \"\"\"\n        deps = self._workers[worker_key].dependencies\n        return [] if deps is None else deps\n\n    def _find_connected_components(self):\n        \"\"\"\n        Find all of the connected components in the whole automa graph described by self._workers.\n        \"\"\"\n        visited = set()\n        component_list = []\n        component_idx = {}\n\n        def dfs(worker: str, component: List[str]):\n            visited.add(worker)\n            component.append(worker)\n            for target in self._worker_forwards.get(worker, []):\n                if target not in visited:\n                    dfs(target, component)\n\n        for worker in self._workers.keys():\n            if worker not in visited:\n                component_list.append([])\n                current_idx = len(component_list) - 1\n                current_component = component_list[current_idx]\n\n                dfs(worker, current_component)\n\n                for worker in current_component:\n                    component_idx[worker] = current_idx\n\n        # self._component_list, self._component_idx = component_list, component_idx\n        # TODO: check how to use _component_list and _component_idx...\n\n    @override\n    def _get_worker_key(self, worker: Worker) -&gt; Optional[str]:\n        for worker_key, worker_obj in self._workers.items():\n            if worker_obj == worker:\n                # Note: _GraphAdaptedWorker.__eq__() is overridden to support the '==' operator.\n                return worker_key\n        return None\n\n    @override\n    def _get_worker_instance(self, worker_key: str) -&gt; Worker:\n        return self._workers[worker_key]\n\n    @override\n    def _locate_interacting_worker(self) -&gt; Optional[str]:\n        return self._trace_back_kickoff_worker_key_from_stack()\n\n    def _trace_back_kickoff_worker_key_from_stack(self) -&gt; Optional[str]:\n        worker = self._get_current_running_worker_instance_by_stacktrace()\n        if worker:\n            return self._get_worker_key(worker)\n        return None\n\n    def _get_current_running_worker_instance_by_stacktrace(self) -&gt; Optional[Worker]:\n        for frame_info in inspect.stack():\n            frame = frame_info.frame\n            if 'self' in frame.f_locals:\n                self_obj = frame.f_locals['self']\n                if isinstance(self_obj, Worker) and (not isinstance(self_obj, Automa)) and (frame_info.function == \"arun\" or frame_info.function == \"run\"):\n                    return self_obj\n        return None\n\n    def __repr__(self) -&gt; str:\n        # TODO : It's good to make __repr__() of Automa compatible with eval().\n        # This feature depends on the implementation of __repr__() of workers.\n        class_name = self.__class__.__name__\n        workers_str = self._workers.__repr__()\n        return f\"{class_name}(workers={workers_str})\"\n\n    def __str__(self) -&gt; str:\n        d = {}\n        for k, v in self._workers.items():\n            d[k] = f\"{v} depends on {getattr(v, 'dependencies', [])}, is_start: {getattr(v, 'is_start', False)}, is_output: {getattr(v, 'is_output', False)}\"\n        return json.dumps(d, ensure_ascii=False, indent=4)\n</code></pre>"},{"location":"reference/bridgic-core/bridgic/core/automa/#bridgic.core.automa.GraphAutoma.all_workers","title":"all_workers","text":"<pre><code>all_workers() -&gt; List[str]\n</code></pre> <p>Gets a list containing the keys of all workers registered in this Automa.</p> <p>Returns:</p> Type Description <code>List[str]</code> <p>A list of worker keys.</p> Source code in <code>bridgic/core/automa/_graph_automa.py</code> <pre><code>def all_workers(self) -&gt; List[str]:\n    \"\"\"\n    Gets a list containing the keys of all workers registered in this Automa.\n\n    Returns\n    -------\n    List[str]\n        A list of worker keys.\n    \"\"\"\n    return list(self._workers.keys())\n</code></pre>"},{"location":"reference/bridgic-core/bridgic/core/automa/#bridgic.core.automa.GraphAutoma.add_worker","title":"add_worker","text":"<pre><code>add_worker(\n    key: str,\n    worker: Worker,\n    *,\n    dependencies: List[str] = [],\n    is_start: bool = False,\n    is_output: bool = False,\n    args_mapping_rule: ArgsMappingRule = AS_IS,\n    result_dispatching_rule: ResultDispatchingRule = AS_IS,\n    callback_builders: List[WorkerCallbackBuilder] = []\n) -&gt; None\n</code></pre> <p>This method is used to add a worker dynamically into the automa.</p> <p>If this method is called during the [Initialization Phase], the worker will be added immediately. If this method is called during the [Running Phase], the worker will be added as a deferred task which will be executed in the next DS.</p> <p>The dependencies can be added together with a worker. However, you can add a worker without any dependencies.</p> <p>Note: args_mapping_rule and result_dispatching_rule could only be set when using worker-adding API. Even if the worker has no any dependencies.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>The key of the worker.</p> required <code>worker</code> <code>Worker</code> <p>The worker instance to be registered.</p> required <code>dependencies</code> <code>List[str]</code> <p>A list of worker keys that the worker depends on.</p> <code>[]</code> <code>is_start</code> <code>bool</code> <p>Whether the worker is a start worker.</p> <code>False</code> <code>is_output</code> <code>bool</code> <p>Whether the worker is an output worker.</p> <code>False</code> <code>args_mapping_rule</code> <code>ArgsMappingRule</code> <p>The rule of arguments mapping.</p> <code>AS_IS</code> <code>result_dispatching_rule</code> <code>ResultDispatchingRule</code> <p>The rule of result dispatch.</p> <code>AS_IS</code> <code>callback_builders</code> <code>List[WorkerCallbackBuilder]</code> <p>A list of worker callback builders to be registered. Callback instances will be created from builders when the worker is instantiated.</p> <code>[]</code> Source code in <code>bridgic/core/automa/_graph_automa.py</code> <pre><code>def add_worker(\n    self,\n    key: str,\n    worker: Worker,\n    *,\n    dependencies: List[str] = [],\n    is_start: bool = False,\n    is_output: bool = False,\n    args_mapping_rule: ArgsMappingRule = ArgsMappingRule.AS_IS,\n    result_dispatching_rule: ResultDispatchingRule = ResultDispatchingRule.AS_IS,\n    callback_builders: List[WorkerCallbackBuilder] = [],\n) -&gt; None:\n    \"\"\"\n    This method is used to add a worker dynamically into the automa.\n\n    If this method is called during the [Initialization Phase], the worker will be added immediately. If this method is called during the [Running Phase], the worker will be added as a deferred task which will be executed in the next DS.\n\n    The dependencies can be added together with a worker. However, you can add a worker without any dependencies.\n\n    Note: args_mapping_rule and result_dispatching_rule could only be set when using worker-adding API. Even if the worker has no any dependencies.\n\n    Parameters\n    ----------\n    key : str\n        The key of the worker.\n    worker : Worker\n        The worker instance to be registered.\n    dependencies : List[str]\n        A list of worker keys that the worker depends on.\n    is_start : bool\n        Whether the worker is a start worker.\n    is_output : bool\n        Whether the worker is an output worker.\n    args_mapping_rule : ArgsMappingRule\n        The rule of arguments mapping.\n    result_dispatching_rule : ResultDispatchingRule\n        The rule of result dispatch.\n    callback_builders : List[WorkerCallbackBuilder]\n        A list of worker callback builders to be registered.\n        Callback instances will be created from builders when the worker is instantiated.\n    \"\"\"\n    self._add_worker_internal(\n        key=key,\n        worker=worker,\n        dependencies=dependencies,\n        is_start=is_start,\n        is_output=is_output,\n        args_mapping_rule=args_mapping_rule,\n        result_dispatching_rule=result_dispatching_rule,\n        callback_builders=callback_builders,\n    )\n</code></pre>"},{"location":"reference/bridgic-core/bridgic/core/automa/#bridgic.core.automa.GraphAutoma.add_func_as_worker","title":"add_func_as_worker","text":"<pre><code>add_func_as_worker(\n    key: str,\n    func: Callable,\n    *,\n    dependencies: List[str] = [],\n    is_start: bool = False,\n    is_output: bool = False,\n    args_mapping_rule: ArgsMappingRule = AS_IS,\n    result_dispatching_rule: ResultDispatchingRule = AS_IS,\n    callback_builders: List[WorkerCallbackBuilder] = []\n) -&gt; None\n</code></pre> <p>This method is used to add a function as a worker into the automa.</p> <p>The format of the parameters will follow that of the decorator @worker(...), so that the  behavior of the decorated function is consistent with that of normal CallableLandableWorker objects.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>The key of the function worker.</p> required <code>func</code> <code>Callable</code> <p>The function to be added as a worker to the automa.</p> required <code>dependencies</code> <code>List[str]</code> <p>A list of worker names that the decorated callable depends on.</p> <code>[]</code> <code>is_start</code> <code>bool</code> <p>Whether the decorated callable is a start worker. True means it is, while False means it is not.</p> <code>False</code> <code>is_output</code> <code>bool</code> <p>Whether the decorated callable is an output worker. True means it is, while False means it is not.</p> <code>False</code> <code>args_mapping_rule</code> <code>ArgsMappingRule</code> <p>The rule of arguments mapping.</p> <code>AS_IS</code> <code>result_dispatching_rule</code> <code>ResultDispatchingRule</code> <p>The rule of result dispatch.</p> <code>AS_IS</code> <code>callback_builders</code> <code>List[WorkerCallbackBuilder]</code> <p>A list of worker callback builders to be registered. Callback instances will be created from builders when the worker is instantiated.</p> <code>[]</code> Source code in <code>bridgic/core/automa/_graph_automa.py</code> <pre><code>def add_func_as_worker(\n    self,\n    key: str,\n    func: Callable,\n    *,\n    dependencies: List[str] = [],\n    is_start: bool = False,\n    is_output: bool = False,\n    args_mapping_rule: ArgsMappingRule = ArgsMappingRule.AS_IS,\n    result_dispatching_rule: ResultDispatchingRule = ResultDispatchingRule.AS_IS,\n    callback_builders: List[WorkerCallbackBuilder] = [],\n) -&gt; None:\n    \"\"\"\n    This method is used to add a function as a worker into the automa.\n\n    The format of the parameters will follow that of the decorator @worker(...), so that the \n    behavior of the decorated function is consistent with that of normal CallableLandableWorker objects.\n\n    Parameters\n    ----------\n    key : str\n        The key of the function worker.\n    func : Callable\n        The function to be added as a worker to the automa.\n    dependencies : List[str]\n        A list of worker names that the decorated callable depends on.\n    is_start : bool\n        Whether the decorated callable is a start worker. True means it is, while False means it is not.\n    is_output : bool\n        Whether the decorated callable is an output worker. True means it is, while False means it is not.\n    args_mapping_rule : ArgsMappingRule\n        The rule of arguments mapping.\n    result_dispatching_rule : ResultDispatchingRule\n        The rule of result dispatch.\n    callback_builders : List[WorkerCallbackBuilder]\n        A list of worker callback builders to be registered.\n        Callback instances will be created from builders when the worker is instantiated.\n    \"\"\"\n    self._add_func_as_worker_internal(\n        key=key,\n        func=func,\n        dependencies=dependencies,\n        is_start=is_start,\n        is_output=is_output,\n        args_mapping_rule=args_mapping_rule,\n        result_dispatching_rule=result_dispatching_rule,\n        callback_builders=callback_builders,\n    )\n</code></pre>"},{"location":"reference/bridgic-core/bridgic/core/automa/#bridgic.core.automa.GraphAutoma.worker","title":"worker","text":"<pre><code>worker(\n    *,\n    key: Optional[str] = None,\n    dependencies: List[str] = [],\n    is_start: bool = False,\n    is_output: bool = False,\n    args_mapping_rule: ArgsMappingRule = AS_IS,\n    result_dispatching_rule: ResultDispatchingRule = AS_IS,\n    callback_builders: List[WorkerCallbackBuilder] = []\n) -&gt; Callable\n</code></pre> <p>This is a decorator used to mark a function as an GraphAutoma detectable Worker. Dislike the  global decorator @worker(...), it is usally used after an GraphAutoma instance is initialized.</p> <p>The format of the parameters will follow that of the decorator @worker(...), so that the  behavior of the decorated function is consistent with that of normal CallableLandableWorker objects.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>The key of the worker. If not provided, the name of the decorated callable will be used.</p> <code>None</code> <code>dependencies</code> <code>List[str]</code> <p>A list of worker names that the decorated callable depends on.</p> <code>[]</code> <code>is_start</code> <code>bool</code> <p>Whether the decorated callable is a start worker. True means it is, while False means it is not.</p> <code>False</code> <code>is_output</code> <code>bool</code> <p>Whether the decorated callable is an output worker. True means it is, while False means it is not.</p> <code>False</code> <code>args_mapping_rule</code> <code>str</code> <p>The rule of arguments mapping. The options are: \"auto\", \"as_list\", \"as_dict\", \"suppressed\".</p> <code>AS_IS</code> <code>result_dispatching_rule</code> <code>ResultDispatchingRule</code> <p>The rule of result dispatch.</p> <code>AS_IS</code> <code>callback_builders</code> <code>List[WorkerCallbackBuilder]</code> <p>A list of worker callback builders to be registered. Callback instances will be created from builders when the worker is instantiated.</p> <code>[]</code> Source code in <code>bridgic/core/automa/_graph_automa.py</code> <pre><code>def worker(\n    self,\n    *,\n    key: Optional[str] = None,\n    dependencies: List[str] = [],\n    is_start: bool = False,\n    is_output: bool = False,\n    args_mapping_rule: ArgsMappingRule = ArgsMappingRule.AS_IS,\n    result_dispatching_rule: ResultDispatchingRule = ResultDispatchingRule.AS_IS,\n    callback_builders: List[WorkerCallbackBuilder] = [],\n) -&gt; Callable:\n    \"\"\"\n    This is a decorator used to mark a function as an GraphAutoma detectable Worker. Dislike the \n    global decorator @worker(...), it is usally used after an GraphAutoma instance is initialized.\n\n    The format of the parameters will follow that of the decorator @worker(...), so that the \n    behavior of the decorated function is consistent with that of normal CallableLandableWorker objects.\n\n    Parameters\n    ----------\n    key : str\n        The key of the worker. If not provided, the name of the decorated callable will be used.\n    dependencies : List[str]\n        A list of worker names that the decorated callable depends on.\n    is_start : bool\n        Whether the decorated callable is a start worker. True means it is, while False means it is not.\n    is_output : bool\n        Whether the decorated callable is an output worker. True means it is, while False means it is not.\n    args_mapping_rule : str\n        The rule of arguments mapping. The options are: \"auto\", \"as_list\", \"as_dict\", \"suppressed\".\n    result_dispatching_rule : ResultDispatchingRule\n        The rule of result dispatch.\n    callback_builders : List[WorkerCallbackBuilder]\n        A list of worker callback builders to be registered.\n        Callback instances will be created from builders when the worker is instantiated.\n    \"\"\"\n    def wrapper(func: Callable):\n        self._add_func_as_worker_internal(\n            key=(key or func.__name__),\n            func=func,\n            dependencies=dependencies,\n            is_start=is_start,\n            is_output=is_output,\n            args_mapping_rule=args_mapping_rule,\n            result_dispatching_rule=result_dispatching_rule,\n            callback_builders=callback_builders,\n        )\n\n    return wrapper\n</code></pre>"},{"location":"reference/bridgic-core/bridgic/core/automa/#bridgic.core.automa.GraphAutoma.remove_worker","title":"remove_worker","text":"<pre><code>remove_worker(key: str) -&gt; None\n</code></pre> <p>Remove a worker from the Automa. This method can be called at any time to remove a worker from the Automa.</p> <p>When a worker is removed, all dependencies related to this worker, including all the dependencies of the worker itself and the dependencies between the worker and its successor workers, will be also removed.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>The key of the worker to be removed.</p> required <p>Returns:</p> Type Description <code>None</code> <p>Raises:</p> Type Description <code>AutomaDeclarationError</code> <p>If the worker specified by key does not exist in the Automa, this exception will be raised.</p> Source code in <code>bridgic/core/automa/_graph_automa.py</code> <pre><code>def remove_worker(self, key: str) -&gt; None:\n    \"\"\"\n    Remove a worker from the Automa. This method can be called at any time to remove a worker from the Automa.\n\n    When a worker is removed, all dependencies related to this worker, including all the dependencies of the worker itself and the dependencies between the worker and its successor workers, will be also removed.\n\n    Parameters\n    ----------\n    key : str\n        The key of the worker to be removed.\n\n    Returns\n    -------\n    None\n\n    Raises\n    ------\n    AutomaDeclarationError\n        If the worker specified by key does not exist in the Automa, this exception will be raised.\n    \"\"\"\n    if not self._automa_running:\n        # remove immediately\n        self._remove_worker_incrementally(key)\n    else:\n        deferred_task = _RemoveWorkerDeferredTask(\n            worker_key=key,\n        )\n        # Note: the execution order of topology change deferred tasks is important and is determined by the order of the calls of add_worker(), remove_worker() and add_dependency() in one DS.\n        self._topology_deferred_tasks.append(deferred_task)\n</code></pre>"},{"location":"reference/bridgic-core/bridgic/core/automa/#bridgic.core.automa.GraphAutoma.add_dependency","title":"add_dependency","text":"<pre><code>add_dependency(key: str, dependency: str) -&gt; None\n</code></pre> <p>This method is used to dynamically add a dependency from <code>key</code> to <code>dependency</code>.</p> <p>Note: args_mapping_rule and result_dispatching_rule is not allowed to be set by this method,  instead they should be set together with add_worker() or add_func_as_worker() when adding the worker.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>The key of the worker that will depend on the worker with key <code>dependency</code>.</p> required <code>dependency</code> <code>str</code> <p>The key of the worker on which the worker with key <code>key</code> will depend.</p> required Source code in <code>bridgic/core/automa/_graph_automa.py</code> <pre><code>def add_dependency(\n    self,\n    key: str,\n    dependency: str,\n) -&gt; None:\n    \"\"\"\n    This method is used to dynamically add a dependency from `key` to `dependency`.\n\n    Note: args_mapping_rule and result_dispatching_rule is not allowed to be set by this method, \n    instead they should be set together with add_worker() or add_func_as_worker() when adding the worker.\n\n    Parameters\n    ----------\n    key : str\n        The key of the worker that will depend on the worker with key `dependency`.\n    dependency : str\n        The key of the worker on which the worker with key `key` will depend.\n    \"\"\"\n    ...\n    if not self._automa_running:\n        # add the dependency immediately\n        self._add_dependency_incrementally(key, dependency)\n    else:\n        deferred_task = _AddDependencyDeferredTask(\n            worker_key=key,\n            dependency=dependency,\n        )\n        # Note: the execution order of topology change deferred tasks is important and is determined by the order of the calls of add_worker(), remove_worker() and add_dependency() in one DS.\n        self._topology_deferred_tasks.append(deferred_task)\n</code></pre>"},{"location":"reference/bridgic-core/bridgic/core/automa/#bridgic.core.automa.GraphAutoma.ferry_to","title":"ferry_to","text":"<pre><code>ferry_to(key: str, /, *args, **kwargs)\n</code></pre> <p>Defer the invocation to the specified worker, passing any provided arguments. This creates a  delayed call, ensuring the worker will be scheduled to run asynchronously in the next event loop,  independent of its dependencies.</p> <p>This primitive is commonly used for:</p> <ol> <li>Implementing dynamic branching based on runtime conditions.</li> <li>Creating logic that forms cyclic graphs.</li> </ol> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>The key of the worker to run.</p> required <code>args</code> <code>optional</code> <p>Positional arguments to be passed.</p> <code>()</code> <code>kwargs</code> <code>optional</code> <p>Keyword arguments to be passed.</p> <code>{}</code> <p>Examples:</p> <pre><code>class MyGraphAutoma(GraphAutoma):\n    @worker(is_start=True)\n    def start_worker(self):\n        number = random.randint(0, 1)\n        if number == 0:\n            self.ferry_to(\"cond_1_worker\", number=number)\n        else:\n            self.ferry_to(\"cond_2_worker\")\n\n    @worker()\n    def cond_1_worker(self, number: int):\n        print(f'Got {{number}}!')\n\n    @worker()\n    def cond_2_worker(self):\n        self.ferry_to(\"start_worker\")\n\nautoma = MyGraphAutoma()\nawait automa.arun()\n\n# Output: Got 0!\n</code></pre> Source code in <code>bridgic/core/automa/_graph_automa.py</code> <pre><code>def ferry_to(self, key: str, /, *args, **kwargs):\n    \"\"\"\n    Defer the invocation to the specified worker, passing any provided arguments. This creates a \n    delayed call, ensuring the worker will be scheduled to run asynchronously in the next event loop, \n    independent of its dependencies.\n\n    This primitive is commonly used for:\n\n    1. Implementing dynamic branching based on runtime conditions.\n    2. Creating logic that forms cyclic graphs.\n\n    Parameters\n    ----------\n    key : str\n        The key of the worker to run.\n    args : optional\n        Positional arguments to be passed.\n    kwargs : optional\n        Keyword arguments to be passed.\n\n    Examples\n    --------\n    ```python\n    class MyGraphAutoma(GraphAutoma):\n        @worker(is_start=True)\n        def start_worker(self):\n            number = random.randint(0, 1)\n            if number == 0:\n                self.ferry_to(\"cond_1_worker\", number=number)\n            else:\n                self.ferry_to(\"cond_2_worker\")\n\n        @worker()\n        def cond_1_worker(self, number: int):\n            print(f'Got {{number}}!')\n\n        @worker()\n        def cond_2_worker(self):\n            self.ferry_to(\"start_worker\")\n\n    automa = MyGraphAutoma()\n    await automa.arun()\n\n    # Output: Got 0!\n    ```\n    \"\"\"\n    # TODO: check worker_key is valid, maybe deferred check...\n    running_options = self._get_top_running_options()\n    # if debug is enabled, trace back the kickoff worker key from stacktrace.\n    kickoff_worker_key: str = self._trace_back_kickoff_worker_key_from_stack() if running_options.debug else None\n    deferred_task = _FerryDeferredTask(\n        ferry_to_worker_key=key,\n        kickoff_worker_key=kickoff_worker_key,\n        args=args,\n        kwargs=kwargs,\n    )\n    # Note: ferry_to() may be called in a new thread.\n    # But _ferry_deferred_tasks is not necessary to be thread-safe due to Visibility Guarantees of the Bridgic Concurrency Model.\n    self._ferry_deferred_tasks.append(deferred_task)\n</code></pre>"},{"location":"reference/bridgic-core/bridgic/core/automa/#bridgic.core.automa.GraphAutoma.arun","title":"arun","text":"<code>async</code> <pre><code>arun(\n    *args: Tuple[Any, ...],\n    feedback_data: Optional[\n        Union[\n            InteractionFeedback, List[InteractionFeedback]\n        ]\n    ] = None,\n    **kwargs: Dict[str, Any]\n) -&gt; Any\n</code></pre> <p>The entry point for running the constructed <code>GraphAutoma</code> instance.</p> <p>This method serves as the entry point for both initial execution and resumption after  interruption of an automa instance. It automatically drives the execution of workers  based on their <code>dependencies</code> and explicit <code>ferry_to()</code> calls. Each execution will be  wrapped in an <code>asyncio.Task</code> to ensure context isolation.</p> <p>Automatic Scheduling</p> <p>The scheduling behavior in <code>GraphAutoma</code> is automatically driven by:</p> <ul> <li> <p>Worker dependencies: Workers are scheduled to run only after all their necessary    dependencies are satisfied. The dependencies automatically drive the execution order.</p> </li> <li> <p>Calling ferry_to: During execution, a worker can explicitly trigger another worker    by calling <code>ferry_to()</code>, which enables dynamic flow control and conditional branching.</p> </li> <li> <p>Dynamic topology changes: When the graph topology is modified at runtime (such as    adding or removing workers or dependencies), the scheduling system seamlessly updates    to reflect the latest structure, ensuring that worker execution always follows the    current graph.</p> </li> </ul> <p>Human Interaction Mechanism</p> <p>Workers can request human input by calling <code>interact_with_human()</code> during execution.  When this occurs:</p> <ul> <li>The execution will be paused after the running workers finish their execution.</li> <li>The Automa's state will be serialized into a <code>Snapshot</code> object.</li> <li>An <code>InteractionException</code> will be raised to the application layer. It contains both the    list of pending <code>Interaction</code> objects and the <code>Snapshot</code> object.</li> <li>The application layer may persist the <code>Snapshot</code> properly to resume the execution later.</li> <li>To resume execution, the application layer should reload the Automa state using    <code>load_from_snapshot()</code> with the saved <code>Snapshot</code> object and call <code>arun()</code> again with    <code>feedback_data</code> containing the user's feedback(s) to finish a complete interaction.</li> </ul> <p>Parameters:</p> Name Type Description Default <code>args</code> <code>optional</code> <p>Positional arguments to be passed.</p> <code>()</code> <code>feedback_data</code> <code>Optional[Union[InteractionFeedback, List[InteractionFeedback]]]</code> <p>Feedbacks that are received from one or multiple human interactions occurred before the Automa was paused. This argument may be of type <code>InteractionFeedback</code> or  <code>List[InteractionFeedback]</code>. If only one interaction occurred, <code>feedback_data</code> should be of type <code>InteractionFeedback</code>. If multiple interactions occurred simultaneously,  <code>feedback_data</code> should be of type <code>List[InteractionFeedback]</code>.</p> <code>None</code> <code>kwargs</code> <code>optional</code> <p>Keyword arguments which may be further propagated to contained workers.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Any</code> <p>The execution result of the output-worker that has the setting <code>is_output=True</code>, otherwise None.</p> <p>Raises:</p> Type Description <code>InteractionException</code> <p>If the Automa is the top-level Automa and the <code>interact_with_human()</code> method is called by one or more workers within the lastest event loop iteration, this exception will be raised to the application layer.</p> Source code in <code>bridgic/core/automa/_graph_automa.py</code> <pre><code>async def arun(\n    self,\n    *args: Tuple[Any, ...],\n    feedback_data: Optional[Union[InteractionFeedback, List[InteractionFeedback]]] = None,\n    **kwargs: Dict[str, Any]\n) -&gt; Any:\n    \"\"\"\n    The entry point for running the constructed `GraphAutoma` instance.\n\n    This method serves as the entry point for both initial execution and resumption after \n    interruption of an automa instance. It automatically drives the execution of workers \n    based on their `dependencies` and explicit `ferry_to()` calls. Each execution will be \n    wrapped in an `asyncio.Task` to ensure context isolation.\n\n    **Automatic Scheduling**\n\n    The scheduling behavior in `GraphAutoma` is automatically driven by:\n\n    - Worker dependencies: Workers are scheduled to run only after all their necessary \n      dependencies are satisfied. The dependencies automatically drive the execution order.\n\n    - Calling ferry_to: During execution, a worker can explicitly trigger another worker \n      by calling `ferry_to()`, which enables dynamic flow control and conditional branching.\n\n    - Dynamic topology changes: When the graph topology is modified at runtime (such as \n      adding or removing workers or dependencies), the scheduling system seamlessly updates \n      to reflect the latest structure, ensuring that worker execution always follows the \n      current graph.\n\n    **Human Interaction Mechanism**\n\n    Workers can request human input by calling `interact_with_human()` during execution. \n    When this occurs:\n\n    - The execution will be paused after the running workers finish their execution.\n    - The Automa's state will be serialized into a `Snapshot` object.\n    - An `InteractionException` will be raised to the application layer. It contains both the \n      list of pending `Interaction` objects and the `Snapshot` object.\n    - The application layer may persist the `Snapshot` properly to resume the execution later.\n    - To resume execution, the application layer should reload the Automa state using \n      `load_from_snapshot()` with the saved `Snapshot` object and call `arun()` again with \n      `feedback_data` containing the user's feedback(s) to finish a complete interaction.\n\n    Parameters\n    ----------\n    args : optional\n        Positional arguments to be passed.\n    feedback_data : Optional[Union[InteractionFeedback, List[InteractionFeedback]]]\n        Feedbacks that are received from one or multiple human interactions occurred before the\n        Automa was paused. This argument may be of type `InteractionFeedback` or \n        `List[InteractionFeedback]`. If only one interaction occurred, `feedback_data` should be\n        of type `InteractionFeedback`. If multiple interactions occurred simultaneously, \n        `feedback_data` should be of type `List[InteractionFeedback]`.\n    kwargs : optional\n        Keyword arguments which may be further propagated to contained workers.\n\n    Returns\n    -------\n    Any\n        The execution result of the output-worker that has the setting `is_output=True`,\n        otherwise None.\n\n    Raises\n    ------\n    InteractionException\n        If the Automa is the top-level Automa and the `interact_with_human()` method is called\n        by one or more workers within the lastest event loop iteration, this exception will be\n        raised to the application layer.\n    \"\"\"\n    if self.is_top_level():\n        # For top-level automa, wrap in a task to ensure context isolation\n        task = asyncio.create_task(\n            self._arun_internal(*args, feedback_data=feedback_data, **kwargs),\n            name=f\"GraphAutoma-{self.name}-arun\"\n        )\n        return await task\n    else:\n        # For nested automa, directly call _arun_internal to avoid redundant task creation\n        return await self._arun_internal(*args, feedback_data=feedback_data, **kwargs)\n</code></pre>"},{"location":"reference/bridgic-core/bridgic/core/automa/#bridgic.core.automa.WorkerSignatureError","title":"WorkerSignatureError","text":"<p>               Bases: <code>Exception</code></p> <p>Raised when invalid signature is detected in the case of defining a worker.</p> Source code in <code>bridgic/core/types/_error.py</code> <pre><code>class WorkerSignatureError(Exception):\n    \"\"\"\n    Raised when invalid signature is detected in the case of defining a worker.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/bridgic-core/bridgic/core/automa/#bridgic.core.automa.WorkerArgsMappingError","title":"WorkerArgsMappingError","text":"<p>               Bases: <code>Exception</code></p> <p>Raised when the parameters declaration of a worker does not meet the requirements of the arguments mapping rule.</p> Source code in <code>bridgic/core/types/_error.py</code> <pre><code>class WorkerArgsMappingError(Exception):\n    \"\"\"\n    Raised when the parameters declaration of a worker does not meet the requirements of the arguments mapping rule.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/bridgic-core/bridgic/core/automa/#bridgic.core.automa.WorkerArgsInjectionError","title":"WorkerArgsInjectionError","text":"<p>               Bases: <code>Exception</code></p> <p>Raised when the arguments injection mechanism encountered an error during operation.</p> Source code in <code>bridgic/core/types/_error.py</code> <pre><code>class WorkerArgsInjectionError(Exception):\n    \"\"\"\n    Raised when the arguments injection mechanism encountered an error during operation.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/bridgic-core/bridgic/core/automa/#bridgic.core.automa.WorkerRuntimeError","title":"WorkerRuntimeError","text":"<p>               Bases: <code>RuntimeError</code></p> <p>Raised when the worker encounters an unexpected error during runtime.</p> Source code in <code>bridgic/core/types/_error.py</code> <pre><code>class WorkerRuntimeError(RuntimeError):\n    \"\"\"\n    Raised when the worker encounters an unexpected error during runtime.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/bridgic-core/bridgic/core/automa/#bridgic.core.automa.AutomaDeclarationError","title":"AutomaDeclarationError","text":"<p>               Bases: <code>Exception</code></p> <p>Raised when the declaration of workers within an Automa is not valid.</p> Source code in <code>bridgic/core/types/_error.py</code> <pre><code>class AutomaDeclarationError(Exception):\n    \"\"\"\n    Raised when the declaration of workers within an Automa is not valid.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/bridgic-core/bridgic/core/automa/#bridgic.core.automa.AutomaCompilationError","title":"AutomaCompilationError","text":"<p>               Bases: <code>Exception</code></p> <p>Raised when the compilation or validation of an Automa fails.</p> Source code in <code>bridgic/core/types/_error.py</code> <pre><code>class AutomaCompilationError(Exception):\n    \"\"\"\n    Raised when the compilation or validation of an Automa fails.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/bridgic-core/bridgic/core/automa/#bridgic.core.automa.AutomaRuntimeError","title":"AutomaRuntimeError","text":"<p>               Bases: <code>RuntimeError</code></p> <p>Raised when the execution of an Automa encounters an unexpected error.</p> Source code in <code>bridgic/core/types/_error.py</code> <pre><code>class AutomaRuntimeError(RuntimeError):\n    \"\"\"\n    Raised when the execution of an Automa encounters an unexpected error.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/bridgic-core/bridgic/core/automa/#bridgic.core.automa.worker._worker_decorator.worker","title":"worker","text":"<pre><code>worker(**kwargs) -&gt; Callable\n</code></pre> <p>Decorator for marking a method inside an Automa class as a worker.</p> <p>To cover the need to declare workers in various Automa classes, this decorator actually  accepts a variable kwargs parameter. Through overloading, it further supports specifying  the parameters that need to be passed in when registering a worker under a specific Automa,  such as <code>ConcurrentAutoma</code>, <code>SequentialAutoma</code> and so on.</p> <p>Parameters:</p> Name Type Description Default <code>kwargs</code> <code>Dict[str, Any]</code> <p>The keyword arguments for the worker decorator.</p> <code>{}</code> Source code in <code>bridgic/core/automa/worker/_worker_decorator.py</code> <pre><code>def worker(**kwargs) -&gt; Callable:\n    \"\"\"\n    Decorator for marking a method inside an Automa class as a worker.\n\n    To cover the need to declare workers in various Automa classes, this decorator actually \n    accepts a variable kwargs parameter. Through overloading, it further supports specifying \n    the parameters that need to be passed in when registering a worker under a specific Automa, \n    such as `ConcurrentAutoma`, `SequentialAutoma` and so on.\n\n    Parameters\n    ----------\n    kwargs : Dict[str, Any]\n        The keyword arguments for the worker decorator.\n    \"\"\"\n    def wrapper(func: Callable):\n        setattr(func, \"__worker_kwargs__\", kwargs)\n        return func\n    return wrapper\n</code></pre>"},{"location":"reference/bridgic-core/bridgic/core/automa/args/","title":"args","text":""},{"location":"reference/bridgic-core/bridgic/core/automa/args/#args","title":"args","text":"<p>The Args module provides Arguments Mapping and Arguments Injection mechanisms in Bridgic.</p>"},{"location":"reference/bridgic-core/bridgic/core/automa/args/#bridgic.core.automa.args.ArgsMappingRule","title":"ArgsMappingRule","text":"<p>               Bases: <code>Enum</code></p> <p>Enumeration of Arguments Mapping rules for worker parameter passing.</p> <p>ArgsMappingRule defines how the return values from predecessor workers are mapped  to the parameters of the current worker. This controls the data flow between workers  in an automa execution graph.</p> <p>Attributes:</p> Name Type Description <code>AS_IS</code> <code>Enum(default)</code> <p>Map the results of the previous workers to the corresponding parameters  in the order of dependency.</p> <code>MERGE</code> <code>Enum</code> <p>Merges all results from previous workers into a single tuple as the  only argument of the current worker.</p> <code>UNPACK</code> <code>Enum</code> <p>Unpacks the result from the previous worker and passes as individual  arguments. Only valid when the current worker has exactly one dependency and  the return value is a list/tuple or dict.</p> <code>SUPPRESSED</code> <code>Enum</code> <p>Suppresses all results from previous workers. No arguments are passed  to the current worker from its dependencies.</p> <p>Examples:</p> <pre><code>class MyAutoma(GraphAutoma):\n    @worker(is_start=True)\n    def worker_0(self, user_input: int) -&gt; int:\n        return user_input + 1\n\n    @worker(dependencies=[\"worker_0\"], args_mapping_rule=ArgsMappingRule.AS_IS)\n    def worker_1(self, worker_0_output: int) -&gt; int:\n        # Receives the exact return value from worker_0\n        return worker_0_output + 1\n\n    @worker(dependencies=[\"worker_0\"], args_mapping_rule=ArgsMappingRule.UNPACK)\n    def worker_2(self, user_input: int, result: int) -&gt; int:\n        # Unpacks the return value from worker_0 (assuming it returns a tuple)\n        return user_input + result\n\n    @worker(dependencies=[\"worker_0\", \"worker_1\"], args_mapping_rule=ArgsMappingRule.MERGE)\n    def worker_3(self, all_results: tuple) -&gt; int:\n        # Receives all results as a single tuple\n        return sum(all_results)\n\n    @worker(dependencies=[\"worker_3\"], args_mapping_rule=ArgsMappingRule.SUPPRESSED)\n    def worker_4(self, custom_input: int = 10) -&gt; int:\n        # Ignores return value from worker_3, uses custom input\n        return custom_input + 1\n</code></pre> Note <ol> <li>AS_IS is the default mapping rule when not specified</li> <li>UNPACK requires exactly one dependency and a list/tuple/dict return value</li> <li>MERGE combines all predecessor outputs into a single tuple argument</li> <li>SUPPRESSED allows workers to ignore dependency outputs completely</li> </ol> Source code in <code>bridgic/core/types/_common.py</code> <pre><code>class ArgsMappingRule(Enum):\n    \"\"\"\n    Enumeration of Arguments Mapping rules for worker parameter passing.\n\n    ArgsMappingRule defines how the return values from predecessor workers are mapped \n    to the parameters of the current worker. This controls the data flow between workers \n    in an automa execution graph.\n\n    Attributes\n    ----------\n    AS_IS: Enum (default)\n        Map the results of the previous workers to the corresponding parameters \n        in the order of dependency.\n    MERGE: Enum\n        Merges all results from previous workers into a single tuple as the \n        only argument of the current worker.\n    UNPACK: Enum\n        Unpacks the result from the previous worker and passes as individual \n        arguments. Only valid when the current worker has exactly one dependency and \n        the return value is a list/tuple or dict.\n    SUPPRESSED: Enum\n        Suppresses all results from previous workers. No arguments are passed \n        to the current worker from its dependencies.\n\n    Examples\n    --------\n    ```python\n    class MyAutoma(GraphAutoma):\n        @worker(is_start=True)\n        def worker_0(self, user_input: int) -&gt; int:\n            return user_input + 1\n\n        @worker(dependencies=[\"worker_0\"], args_mapping_rule=ArgsMappingRule.AS_IS)\n        def worker_1(self, worker_0_output: int) -&gt; int:\n            # Receives the exact return value from worker_0\n            return worker_0_output + 1\n\n        @worker(dependencies=[\"worker_0\"], args_mapping_rule=ArgsMappingRule.UNPACK)\n        def worker_2(self, user_input: int, result: int) -&gt; int:\n            # Unpacks the return value from worker_0 (assuming it returns a tuple)\n            return user_input + result\n\n        @worker(dependencies=[\"worker_0\", \"worker_1\"], args_mapping_rule=ArgsMappingRule.MERGE)\n        def worker_3(self, all_results: tuple) -&gt; int:\n            # Receives all results as a single tuple\n            return sum(all_results)\n\n        @worker(dependencies=[\"worker_3\"], args_mapping_rule=ArgsMappingRule.SUPPRESSED)\n        def worker_4(self, custom_input: int = 10) -&gt; int:\n            # Ignores return value from worker_3, uses custom input\n            return custom_input + 1\n    ```\n\n    Note\n    ----\n    1. AS_IS is the default mapping rule when not specified\n    2. UNPACK requires exactly one dependency and a list/tuple/dict return value\n    3. MERGE combines all predecessor outputs into a single tuple argument\n    4. SUPPRESSED allows workers to ignore dependency outputs completely\n    \"\"\"\n    AS_IS = \"as_is\"\n    MERGE = \"merge\"\n    UNPACK = \"unpack\"\n    SUPPRESSED = \"suppressed\"\n</code></pre>"},{"location":"reference/bridgic-core/bridgic/core/automa/args/#bridgic.core.automa.args.ResultDispatchingRule","title":"ResultDispatchingRule","text":"<p>               Bases: <code>Enum</code></p> <p>Enumeration of Result Dispatch rules for worker result passing.</p> <p>ResultDispatchingRule defines how the result from the current worker is dispatched to the next workers. This controls the data flow between workers in an automa execution graph.</p> <p>Attributes:</p> Name Type Description <code>AS_IS</code> <code>Enum(default)</code> <p>Gathers all results of current worker into a single tuple as the  only result to the next workers.</p> <code>IN_ORDER</code> <code>Enum</code> <p>Dispatch the current worker's results to the corresponding downstream  workers one by one according to the order they are declared or added.</p> Source code in <code>bridgic/core/types/_common.py</code> <pre><code>class ResultDispatchingRule(Enum):\n    \"\"\"\n    Enumeration of Result Dispatch rules for worker result passing.\n\n    ResultDispatchingRule defines how the result from the current worker is dispatched to the next workers.\n    This controls the data flow between workers in an automa execution graph.\n\n    Attributes\n    ----------\n    AS_IS: Enum (default)\n        Gathers all results of current worker into a single tuple as the \n        only result to the next workers.\n    IN_ORDER: Enum\n        Dispatch the current worker's results to the corresponding downstream \n        workers one by one according to the order they are declared or added.\n    \"\"\"\n    AS_IS = \"as_is\"\n    IN_ORDER = \"in_order\"\n</code></pre>"},{"location":"reference/bridgic-core/bridgic/core/automa/args/#bridgic.core.automa.args.From","title":"From  <code>dataclass</code>","text":"<p>               Bases: <code>ArgsDescriptor</code></p> <p>Implementing arguments injection for worker parameters with default value.</p> <p>When a worker needs the output of another worker but does not directly depend on  it in execution, you can use <code>From</code> to declare an arguments injection in  its parameters.</p> <p>Attributes:</p> Name Type Description <code>key</code> <code>str</code> <p>The key of the worker to inject arguments from.</p> <code>default</code> <code>Optional[Any]</code> <p>The default value of the arguments.</p> <p>Examples:</p> <pre><code>class MyAutoma(GraphAutoma):\n    @worker(is_start=True)\n    def worker_0(self, user_input: int) -&gt; int:\n        return user_input + 1\n\n    @worker(dependencies=[\"worker_0\"])\n    def worker_1(self, worker_0_output: int) -&gt; int:\n        return worker_0_output + 1\n\n    @worker(dependencies=[\"worker_1\"], is_output=True)\n    def worker_2(self, worker_1_output: int, worker_0_output: int = From(\"worker_0\", 1)) -&gt; int:\n        # needs the output of worker_0 but does not directly depend on it in execution\n        print(f'worker_0_output: {worker_0_output}')\n        return worker_1_output + 1\n</code></pre> <p>Returns:</p> Type Description <code>Any</code> <p>The output of the worker specified by the key.</p> <p>Raises:</p> Type Description <code>WorkerArgsInjectionError</code> <p>If the worker specified by the key does not exist and no default value is set.</p> Note: <ol> <li>Can set a default value for a <code>From</code> declaration, which will be returned when the specified worker does not exist.</li> <li>Will raise <code>WorkerArgsInjectionError</code> if the worker specified by the key does not exist and no default value is set.</li> </ol> Source code in <code>bridgic/core/automa/args/_args_descriptor.py</code> <pre><code>@dataclass\nclass From(ArgsDescriptor):\n    \"\"\"\n    Implementing arguments injection for worker parameters with default value.\n\n    When a worker needs the output of another worker but does not directly depend on \n    it in execution, you can use `From` to declare an arguments injection in \n    its parameters.\n\n    Attributes\n    ----------\n    key : str\n        The key of the worker to inject arguments from.\n    default : Optional[Any]\n        The default value of the arguments.\n\n    Examples\n    --------\n    ```python\n    class MyAutoma(GraphAutoma):\n        @worker(is_start=True)\n        def worker_0(self, user_input: int) -&gt; int:\n            return user_input + 1\n\n        @worker(dependencies=[\"worker_0\"])\n        def worker_1(self, worker_0_output: int) -&gt; int:\n            return worker_0_output + 1\n\n        @worker(dependencies=[\"worker_1\"], is_output=True)\n        def worker_2(self, worker_1_output: int, worker_0_output: int = From(\"worker_0\", 1)) -&gt; int:\n            # needs the output of worker_0 but does not directly depend on it in execution\n            print(f'worker_0_output: {worker_0_output}')\n            return worker_1_output + 1\n    ```\n\n    Returns\n    -------\n    Any\n        The output of the worker specified by the key.\n\n    Raises\n    ------\n    WorkerArgsInjectionError\n        If the worker specified by the key does not exist and no default value is set.\n\n    Note:\n    ------\n    1. Can set a default value for a `From` declaration, which will be returned when the specified worker does not exist.\n    2. Will raise `WorkerArgsInjectionError` if the worker specified by the key does not exist and no default value is set.\n    \"\"\"\n    key: str\n    default: Optional[Any] = InjectorNone()\n</code></pre>"},{"location":"reference/bridgic-core/bridgic/core/automa/args/#bridgic.core.automa.args.System","title":"System  <code>dataclass</code>","text":"<p>               Bases: <code>ArgsDescriptor</code></p> <p>Implementing system-level arguments injection for worker parameters.</p> <p>System provides access to automa-level resources and context through arguments  injection. It supports pattern matching for different types of system resources.</p> <p>Attributes:</p> Name Type Description <code>key</code> <code>str</code> <p>The system resource key to inject. Supported keys: - \"runtime_context\": Runtime context for data persistence across worker executions. - \"automa\": Current automa instance. - \"automa:worker_key\": Sub-automa instance in current automa.</p> <p>Examples:</p> <pre><code>def worker_1(x: int, current_automa = System(\"automa\")) -&gt; int:\n    # Access current automa instance\n    current_automa.add_worker(\n        key=\"sub_automa\",\n        worker=SubAutoma(),\n        dependencies=[\"worker_1\"]\n    )\n    return x + 1\n\nclass SubAutoma(GraphAutoma):\n    @worker(is_start=True)\n    def worker_0(self, user_input: int) -&gt; int:\n        return user_input + 1\n\nclass MyAutoma(GraphAutoma):\n    @worker(is_start=True)\n    def worker_0(self, user_input: int, rtx = System(\"runtime_context\")) -&gt; int:\n        # Access runtime context for data persistence\n        local_space = self.get_local_space(rtx)\n        count = local_space.get(\"count\", 0)\n        local_space[\"count\"] = count + 1\n\n        self.add_func_as_worker(\n            key=\"worker_1\",\n            func=worker_1,\n            dependencies=[\"worker_0\"]\n        )\n\n        return user_input + count\n\n    @worker(dependencies=[\"worker_1\"])\n    def worker_2(self, worker_1_output: int, sub_automa = System(\"automa:sub_automa\")) -&gt; int:\n        # Access sub-automa from worker_1\n        sub_automa.add_worker(\n            key=\"worker_3\",\n            worker=SubAutoma(),\n            dependencies=[\"worker_2\"],\n            is_output=True,\n        )\n        return worker_1_output + 1\n</code></pre> <p>Returns:</p> Type Description <code>Any</code> <p>The system resource specified by the key: - RuntimeContext: For \"runtime_context\" - AutomaInstance: For current automa instance or a sub-automa instance from the current automa.</p> <p>Raises:</p> Type Description <code>WorkerArgsInjectionError</code> <ul> <li>If the key pattern is not supported.</li> <li>If the specified resource does not exist.</li> <li>If the specified resource is not an Automa.</li> </ul> Note <ol> <li>\"runtime_context\" provides a <code>RuntimeContext</code> instance for data persistence</li> <li>\"automa\" provides access to the current automa instance</li> <li>\"automa:worker_key\" provides access to a sub-automa from the specified worker key</li> </ol> Source code in <code>bridgic/core/automa/args/_args_descriptor.py</code> <pre><code>@dataclass\nclass System(ArgsDescriptor):\n    \"\"\"\n    Implementing system-level arguments injection for worker parameters.\n\n    System provides access to automa-level resources and context through arguments \n    injection. It supports pattern matching for different types of system resources.\n\n    Attributes\n    ----------\n    key : str\n        The system resource key to inject. Supported keys:\n        - \"runtime_context\": Runtime context for data persistence across worker executions.\n        - \"automa\": Current automa instance.\n        - \"automa:worker_key\": Sub-automa instance in current automa.\n\n    Examples\n    --------\n    ```python\n    def worker_1(x: int, current_automa = System(\"automa\")) -&gt; int:\n        # Access current automa instance\n        current_automa.add_worker(\n            key=\"sub_automa\",\n            worker=SubAutoma(),\n            dependencies=[\"worker_1\"]\n        )\n        return x + 1\n\n    class SubAutoma(GraphAutoma):\n        @worker(is_start=True)\n        def worker_0(self, user_input: int) -&gt; int:\n            return user_input + 1\n\n    class MyAutoma(GraphAutoma):\n        @worker(is_start=True)\n        def worker_0(self, user_input: int, rtx = System(\"runtime_context\")) -&gt; int:\n            # Access runtime context for data persistence\n            local_space = self.get_local_space(rtx)\n            count = local_space.get(\"count\", 0)\n            local_space[\"count\"] = count + 1\n\n            self.add_func_as_worker(\n                key=\"worker_1\",\n                func=worker_1,\n                dependencies=[\"worker_0\"]\n            )\n\n            return user_input + count\n\n        @worker(dependencies=[\"worker_1\"])\n        def worker_2(self, worker_1_output: int, sub_automa = System(\"automa:sub_automa\")) -&gt; int:\n            # Access sub-automa from worker_1\n            sub_automa.add_worker(\n                key=\"worker_3\",\n                worker=SubAutoma(),\n                dependencies=[\"worker_2\"],\n                is_output=True,\n            )\n            return worker_1_output + 1\n    ```\n\n    Returns\n    -------\n    Any\n        The system resource specified by the key:\n        - RuntimeContext: For \"runtime_context\"\n        - AutomaInstance: For current automa instance or a sub-automa instance from the current automa.\n\n    Raises\n    ------\n    WorkerArgsInjectionError\n        - If the key pattern is not supported.\n        - If the specified resource does not exist.\n        - If the specified resource is not an Automa.\n\n    Note\n    ----\n    1. \"runtime_context\" provides a `RuntimeContext` instance for data persistence\n    2. \"automa\" provides access to the current automa instance\n    3. \"automa:worker_key\" provides access to a sub-automa from the specified worker key\n    \"\"\"\n    key: str\n\n    def __post_init__(self):\n        allowed_patterns = [\n            r\"^runtime_context$\",\n            r\"^automa:.*$\",\n            r\"^automa$\",\n        ]\n\n        if not any(re.match(pattern, self.key) for pattern in allowed_patterns):\n            raise WorkerArgsInjectionError(\n                f\"Key '{self.key}' is not supported. Supported keys: \\n\"\n                f\"- `runtime_context`: a context for data persistence of the current worker.\\n\"\n                f\"- `automa:&lt;worker_key&gt;`: a sub-automa in current automa.\\n\"\n                f\"- `automa`: the current automa instance.\\n\"\n            )\n</code></pre>"},{"location":"reference/bridgic-core/bridgic/core/automa/args/#bridgic.core.automa.args.InOrder","title":"InOrder  <code>dataclass</code>","text":"<p>A descriptor to indicate that data should be distributed to multiple workers. </p> <p>When is used to input arguments or worker with this descriptor, the data will be distributed to downstream workers instead of being gathered as a single value. Split the returned  Sequence object and dispatching them in-order and element-wise to the downstream workers  as their actual input.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Union[List, Tuple]</code> <p>The data to be distributed. Must be a list or tuple with length matching the number of workers that will receive it.</p> <code>None</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the data is not a list or tuple.</p> Source code in <code>bridgic/core/automa/args/_args_binding.py</code> <pre><code>@dataclass\nclass InOrder:\n    \"\"\"\n    A descriptor to indicate that data should be distributed to multiple workers. \n\n    When is used to input arguments or worker with this descriptor, the data will be distributed\n    to downstream workers instead of being gathered as a single value. Split the returned \n    Sequence object and dispatching them in-order and element-wise to the downstream workers \n    as their actual input.\n\n    Parameters\n    ----------\n    data : Union[List, Tuple]\n        The data to be distributed. Must be a list or tuple with length matching\n        the number of workers that will receive it.\n\n    Raises\n    ------\n    ValueError\n        If the data is not a list or tuple.\n    \"\"\"\n    data: Union[List[Any], Tuple[Any], inspect._empty] = None\n\n    def check_data(self) -&gt; None:\n        if not isinstance(self.data, (List, Tuple)):\n            raise ValueError(f\"The data of the Distribute must be `List` or `Tuple`, but got Type `{type(self.data)}`.\")\n</code></pre>"},{"location":"reference/bridgic-core/bridgic/core/automa/interaction/","title":"interaction","text":""},{"location":"reference/bridgic-core/bridgic/core/automa/interaction/#interaction","title":"interaction","text":"<p>The Interaction module provides mechanisms for handling human-machine interactions in Automa.</p> <p>This module contains several important interface definitions for implementing event handling,  feedback collection, and interaction control during Automa execution.</p> <p>There are two fundamental mechanisms for human-machine interactions in Automa:</p> <ul> <li>Feedback Request Mechanism: For simple interaction scenarios during Automa execution.<ul> <li><code>request_feedback_async()</code>, <code>request_feedback()</code></li> </ul> </li> <li>Human Interaction Mechanism: For long-running interaction scenarios that require interruption and resumption during Automa execution.<ul> <li><code>interact_with_human()</code>, <code>load_from_snapshot()</code></li> </ul> </li> </ul>"},{"location":"reference/bridgic-core/bridgic/core/automa/interaction/#bridgic.core.automa.interaction.EventHandlerType","title":"EventHandlerType  <code>module-attribute</code>","text":"<pre><code>EventHandlerType: TypeAlias = Union[\n    Callable[[Event, FeedbackSender], None],\n    Callable[[Event], None],\n]\n</code></pre> <p>The type of the event handler. It can be a function that takes an Event and a FeedbackSender as arguments, or a function that takes only an Event as an argument.</p>"},{"location":"reference/bridgic-core/bridgic/core/automa/interaction/#bridgic.core.automa.interaction.Event","title":"Event","text":"<p>               Bases: <code>BaseModel</code></p> <p>An event is a message that is sent from one worker inside the Automa to the application layer outside the Automa.</p> Source code in <code>bridgic/core/automa/interaction/_event_handling.py</code> <pre><code>class Event(BaseModel):\n    \"\"\"\n    An event is a message that is sent from one worker inside the Automa to the application layer outside the Automa.\n    \"\"\"\n    event_type: Optional[str] = None\n    \"\"\"The type of the event. The type of the event is used to identify the event handler registered to handle the event.\"\"\"\n    timestamp: datetime = datetime.now()\n    \"\"\"The timestamp of the event.\"\"\"\n    data: Optional[Any] = None\n    \"\"\"The data attached to the event.\"\"\"\n</code></pre>"},{"location":"reference/bridgic-core/bridgic/core/automa/interaction/#bridgic.core.automa.interaction.Event.event_type","title":"event_type  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>event_type: Optional[str] = None\n</code></pre> <p>The type of the event. The type of the event is used to identify the event handler registered to handle the event.</p>"},{"location":"reference/bridgic-core/bridgic/core/automa/interaction/#bridgic.core.automa.interaction.Event.timestamp","title":"timestamp  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>timestamp: datetime = now()\n</code></pre> <p>The timestamp of the event.</p>"},{"location":"reference/bridgic-core/bridgic/core/automa/interaction/#bridgic.core.automa.interaction.Event.data","title":"data  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>data: Optional[Any] = None\n</code></pre> <p>The data attached to the event.</p>"},{"location":"reference/bridgic-core/bridgic/core/automa/interaction/#bridgic.core.automa.interaction.Feedback","title":"Feedback","text":"<p>               Bases: <code>BaseModel</code></p> <p>A feedback is a message that is sent from the application layer outside the Automa to a worker inside the Automa.</p> Source code in <code>bridgic/core/automa/interaction/_event_handling.py</code> <pre><code>class Feedback(BaseModel):\n    \"\"\"\n    A feedback is a message that is sent from the application layer outside the Automa to a worker inside the Automa.\n    \"\"\"\n    data: Any\n    \"\"\"The data attached to the feedback.\"\"\"\n</code></pre>"},{"location":"reference/bridgic-core/bridgic/core/automa/interaction/#bridgic.core.automa.interaction.Feedback.data","title":"data  <code>instance-attribute</code>","text":"<pre><code>data: Any\n</code></pre> <p>The data attached to the feedback.</p>"},{"location":"reference/bridgic-core/bridgic/core/automa/interaction/#bridgic.core.automa.interaction.FeedbackSender","title":"FeedbackSender","text":"<p>               Bases: <code>ABC</code></p> <p>The appliction layer must use <code>FeedbackSender</code> to send back feedback to the worker inside the Automa.</p> Source code in <code>bridgic/core/automa/interaction/_event_handling.py</code> <pre><code>class FeedbackSender(ABC):\n    \"\"\"\n    The appliction layer must use `FeedbackSender` to send back feedback to the worker inside the Automa.\n    \"\"\"\n    @abstractmethod\n    def send(self, feedback: Feedback) -&gt; None:\n        \"\"\"\n        Send feedback to the Automa.\n        This method can be called only once for each event.\n\n        This `send` method can be safely called in several different scenarios:\n        - In the same asyncio Task of the same event loop as the event handler.\n        - In a different asyncio Task of the same event loop as the event handler.\n        - In a different thread from the event handler.\n\n        Parameters\n        ----------\n        feedback: Feedback\n            The feedback to be sent.\n        \"\"\"\n        ...\n</code></pre>"},{"location":"reference/bridgic-core/bridgic/core/automa/interaction/#bridgic.core.automa.interaction.FeedbackSender.send","title":"send","text":"<code>abstractmethod</code> <pre><code>send(feedback: Feedback) -&gt; None\n</code></pre> <p>Send feedback to the Automa. This method can be called only once for each event.</p> <p>This <code>send</code> method can be safely called in several different scenarios: - In the same asyncio Task of the same event loop as the event handler. - In a different asyncio Task of the same event loop as the event handler. - In a different thread from the event handler.</p> <p>Parameters:</p> Name Type Description Default <code>feedback</code> <code>Feedback</code> <p>The feedback to be sent.</p> required Source code in <code>bridgic/core/automa/interaction/_event_handling.py</code> <pre><code>@abstractmethod\ndef send(self, feedback: Feedback) -&gt; None:\n    \"\"\"\n    Send feedback to the Automa.\n    This method can be called only once for each event.\n\n    This `send` method can be safely called in several different scenarios:\n    - In the same asyncio Task of the same event loop as the event handler.\n    - In a different asyncio Task of the same event loop as the event handler.\n    - In a different thread from the event handler.\n\n    Parameters\n    ----------\n    feedback: Feedback\n        The feedback to be sent.\n    \"\"\"\n    ...\n</code></pre>"},{"location":"reference/bridgic-core/bridgic/core/automa/interaction/#bridgic.core.automa.interaction.InteractionFeedback","title":"InteractionFeedback","text":"<p>               Bases: <code>Feedback</code></p> <p>A feedback object that contains both the data provided by the user and the <code>interaction_id</code>, which uniquely identifies the corresponding interaction.</p> Source code in <code>bridgic/core/automa/interaction/_human_interaction.py</code> <pre><code>class InteractionFeedback(Feedback):\n    \"\"\"\n    A feedback object that contains both the data provided by the user and the `interaction_id`, which uniquely identifies the corresponding interaction.\n    \"\"\"\n    interaction_id: str\n    \"\"\" The unique identifier for the interaction.\"\"\"\n    timestamp: datetime = datetime.now()\n    \"\"\"The timestamp of the feedback.\"\"\"\n</code></pre>"},{"location":"reference/bridgic-core/bridgic/core/automa/interaction/#bridgic.core.automa.interaction.InteractionFeedback.interaction_id","title":"interaction_id  <code>instance-attribute</code>","text":"<pre><code>interaction_id: str\n</code></pre> <p>The unique identifier for the interaction.</p>"},{"location":"reference/bridgic-core/bridgic/core/automa/interaction/#bridgic.core.automa.interaction.InteractionFeedback.timestamp","title":"timestamp  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>timestamp: datetime = now()\n</code></pre> <p>The timestamp of the feedback.</p>"},{"location":"reference/bridgic-core/bridgic/core/automa/interaction/#bridgic.core.automa.interaction.InteractionException","title":"InteractionException","text":"<p>               Bases: <code>Exception</code></p> <p>An exception raised when the <code>interact_with_human</code> method is called one or more times within the latest event loop iteration, causing one or multiple human interactions to be triggered.</p> Source code in <code>bridgic/core/automa/interaction/_human_interaction.py</code> <pre><code>class InteractionException(Exception):\n    \"\"\"\n    An exception raised when the `interact_with_human` method is called one or more times within the latest event loop iteration, causing one or multiple human interactions to be triggered.\n    \"\"\"\n    _interactions: List[Interaction]\n    \"\"\"The list of interactions that occurred during the latest event loop iteration.\"\"\"\n    _snapshot: \"Snapshot\"\n    \"\"\"The snapshot of the Automa's current state.\"\"\"\n\n    def __init__(self, interactions: List[Interaction], snapshot: \"Snapshot\"):\n        self._interactions = interactions\n        self._snapshot = snapshot\n\n    @property\n    def interactions(self) -&gt; List[Interaction]:\n        \"\"\"\n        A list of `Interaction` objects that occurred during the latest event loop iteration.\n\n        Multiple `Interaction` objects may be generated because, within the latest event loop iteration, multiple workers calling the `interact_with_human` method might be running concurrently in parallel branches of the graph.\n        \"\"\"\n        return self._interactions\n\n    @property\n    def snapshot(self) -&gt; \"Snapshot\":\n        \"\"\"\n        A `Snapshot` of the Automa's current state.\n        The serialization is triggered automatically by the `interact_with_human` method.\n        \"\"\"\n        return self._snapshot\n</code></pre>"},{"location":"reference/bridgic-core/bridgic/core/automa/interaction/#bridgic.core.automa.interaction.InteractionException.interactions","title":"interactions  <code>property</code>","text":"<pre><code>interactions: List[Interaction]\n</code></pre> <p>A list of <code>Interaction</code> objects that occurred during the latest event loop iteration.</p> <p>Multiple <code>Interaction</code> objects may be generated because, within the latest event loop iteration, multiple workers calling the <code>interact_with_human</code> method might be running concurrently in parallel branches of the graph.</p>"},{"location":"reference/bridgic-core/bridgic/core/automa/interaction/#bridgic.core.automa.interaction.InteractionException.snapshot","title":"snapshot  <code>property</code>","text":"<pre><code>snapshot: Snapshot\n</code></pre> <p>A <code>Snapshot</code> of the Automa's current state. The serialization is triggered automatically by the <code>interact_with_human</code> method.</p>"},{"location":"reference/bridgic-core/bridgic/core/automa/interaction/#bridgic.core.automa.interaction.Interaction","title":"Interaction","text":"<p>               Bases: <code>BaseModel</code></p> <p>An object that represents a single interaction between the Automa and a human.  Each call to <code>interact_with_human</code> will generate an <code>Interaction</code> object which will be included in the <code>InteractionException</code> raised.</p> Source code in <code>bridgic/core/automa/interaction/_human_interaction.py</code> <pre><code>class Interaction(BaseModel):\n    \"\"\"\n    An object that represents a single interaction between the Automa and a human. \n    Each call to `interact_with_human` will generate an `Interaction` object which will be included in the `InteractionException` raised.\n    \"\"\"\n    interaction_id: str\n    \"\"\" The unique identifier for the interaction.\"\"\"\n    event: Event\n    \"\"\"The event that triggered the interaction.\"\"\"\n</code></pre>"},{"location":"reference/bridgic-core/bridgic/core/automa/interaction/#bridgic.core.automa.interaction.Interaction.interaction_id","title":"interaction_id  <code>instance-attribute</code>","text":"<pre><code>interaction_id: str\n</code></pre> <p>The unique identifier for the interaction.</p>"},{"location":"reference/bridgic-core/bridgic/core/automa/interaction/#bridgic.core.automa.interaction.Interaction.event","title":"event  <code>instance-attribute</code>","text":"<pre><code>event: Event\n</code></pre> <p>The event that triggered the interaction.</p>"},{"location":"reference/bridgic-core/bridgic/core/automa/worker/","title":"worker","text":""},{"location":"reference/bridgic-core/bridgic/core/automa/worker/#worker","title":"worker","text":"<p>The Worker module defines the Worker concept and its related implementations in Automa.</p> <p>This module provides the core abstractions and implementations of Worker, including:</p> <ul> <li>Worker: The base class for all workers, which is the basic execution unit in Automa,    defining the execution interface (<code>arun()</code> and <code>run()</code> methods) for nodes</li> <li>CallableWorker: A worker implementation for wrapping callable objects (functions or methods)</li> <li>WorkerCallback: A callback interface during worker execution, supporting validation,    monitoring, and log collection before and after execution</li> <li>WorkerCallbackBuilder: A builder for constructing and configuring worker callbacks</li> </ul>"},{"location":"reference/bridgic-core/bridgic/core/automa/worker/#bridgic.core.automa.worker.T_WorkerCallback","title":"T_WorkerCallback  <code>module-attribute</code>","text":"<pre><code>T_WorkerCallback = TypeVar(\n    \"T_WorkerCallback\", bound=\"WorkerCallback\"\n)\n</code></pre> <p>Type variable for WorkerCallback subclasses.</p>"},{"location":"reference/bridgic-core/bridgic/core/automa/worker/#bridgic.core.automa.worker.Worker","title":"Worker","text":"<p>               Bases: <code>Serializable</code></p> <p>This class is the base class for all workers.</p> <p><code>Worker</code> has two methods that may be overridden by the subclass:</p> <ol> <li> <p><code>arun()</code>: This asynchronous method should be implemented when your worker  does not require almost immediately scheduling after all its task dependencies  are fulfilled, and when overall workflow is not sensitive to the fair sharing  of CPU resources between workers. If workers can afford to retain and occupy  execution resources for their entire execution duration, and there is no  explicit need for fair CPU time-sharing or timely scheduling, you should  implement <code>arun()</code> and allow workers to run to completion as cooperative tasks  within the event loop.</p> </li> <li> <p><code>run()</code>: This synchronous method should be implemented when either of the  following holds:</p> <ul> <li> <p>a. The automa includes other workers that require timely access to CPU  resources (for example, workers that must respond quickly or are sensitive  to scheduling latency).</p> </li> <li> <p>b. The current worker itself should be scheduled as soon as all its task  dependencies are met, to maintain overall workflow responsiveness. In these  cases, <code>run()</code> enables the framework to offload your worker to a thread pool,  ensuring that CPU time is shared fairly among all such workers and the event  loop remains responsive.</p> </li> </ul> </li> </ol> <p>In summary, if you are unsure whether your task require quickly scheduling or not,  it is recommended to implement the <code>arun()</code> method. Otherwise, implement the  <code>run()</code> ONLY if you are certain that you agree to share CPU time slices  with other workers.</p> Source code in <code>bridgic/core/automa/worker/_worker.py</code> <pre><code>class Worker(Serializable):\n    \"\"\"\n    This class is the base class for all workers.\n\n    `Worker` has two methods that may be overridden by the subclass:\n\n    1. `arun()`: This asynchronous method should be implemented when your worker \n    does not require almost immediately scheduling after all its task dependencies \n    are fulfilled, and when overall workflow is not sensitive to the fair sharing \n    of CPU resources between workers. If workers can afford to retain and occupy \n    execution resources for their entire execution duration, and there is no \n    explicit need for fair CPU time-sharing or timely scheduling, you should \n    implement `arun()` and allow workers to run to completion as cooperative tasks \n    within the event loop.\n\n    2. `run()`: This synchronous method should be implemented when either of the \n    following holds:\n\n        - a. The automa includes other workers that require timely access to CPU \n        resources (for example, workers that must respond quickly or are sensitive \n        to scheduling latency).\n\n        - b. The current worker itself should be scheduled as soon as all its task \n        dependencies are met, to maintain overall workflow responsiveness. In these \n        cases, `run()` enables the framework to offload your worker to a thread pool, \n        ensuring that CPU time is shared fairly among all such workers and the event \n        loop remains responsive.\n\n    In summary, if you are unsure whether your task require quickly scheduling or not, \n    it is recommended to implement the `arun()` method. Otherwise, implement the \n    `run()` **ONLY** if you are certain that you agree to share CPU time slices \n    with other workers.\n    \"\"\"\n\n    # TODO : Maybe process pool of the Automa is needed.\n\n    __parent: \"Automa\"\n    __local_space: Dict[str, Any]\n\n    # Cached method signatures, with no need for serialization.\n    __cached_param_names_of_arun: Dict[_ParameterKind, List[Tuple[str, Any]]]\n    __cached_param_names_of_run: Dict[_ParameterKind, List[Tuple[str, Any]]]\n\n    def __init__(self):\n        self.__parent = self\n        self.__local_space = {}\n\n        # Cached method signatures, with no need for serialization.\n        self.__cached_param_names_of_arun = None\n        self.__cached_param_names_of_run = None\n\n    async def arun(self, *args: Tuple[Any, ...], **kwargs: Dict[str, Any]) -&gt; Any:\n        \"\"\"\n        The asynchronous method to run the worker.\n        \"\"\"\n        loop = asyncio.get_running_loop()\n        topest_automa = self._get_top_level_automa()\n        if topest_automa:\n            thread_pool = topest_automa.thread_pool\n            if thread_pool:\n                # kwargs can only be passed by functools.partial.\n                return await loop.run_in_executor(thread_pool, partial(self.run, *args, **kwargs))\n\n        # Unexpected: No thread pool is available.\n        # Case 1: the worker is not inside an Automa (uncommon case).\n        # Case 2: no thread pool is setup by the top-level automa.\n        raise WorkerRuntimeError(f\"No thread pool is available for the worker {type(self)}\")\n\n    def run(self, *args: Tuple[Any, ...], **kwargs: Dict[str, Any]) -&gt; Any:\n        \"\"\"\n        The synchronous method to run the worker.\n        \"\"\"\n        raise NotImplementedError(f\"run() is not implemented in {type(self)}\")\n\n    def is_top_level(self) -&gt; bool:\n        \"\"\"\n        Check if the current worker is the top-level worker.\n\n        Returns\n        -------\n        bool\n            True if the current worker is the top-level worker (parent is self), False otherwise.\n        \"\"\"\n        return self.parent is self\n\n    def _get_top_level_automa(self) -&gt; Optional[\"Automa\"]:\n        \"\"\"\n        Get the top-level automa instance reference.\n        \"\"\"\n        # If the current automa is the top-level automa, return itself.\n        from bridgic.core.automa._automa import Automa\n        if isinstance(self, Automa):\n            top_level_automa = self\n        else:\n            top_level_automa = self.parent\n        while top_level_automa and (not top_level_automa.is_top_level()):\n            top_level_automa = top_level_automa.parent\n        return top_level_automa\n\n    def get_input_param_names(self) -&gt; Dict[_ParameterKind, List[Tuple[str, Any]]]:\n        \"\"\"\n        Get the names of input parameters of the worker.\n        Use cached result if available in order to improve performance.\n\n        This method intelligently detects whether the user has overridden the `run` method\n        or is using the default `arun` method, and returns the appropriate parameter signature.\n\n        Returns\n        -------\n        Dict[_ParameterKind, List[str]]\n            A dictionary of input parameter names by the kind of the parameter.\n            The key is the kind of the parameter, which is one of five possible values:\n\n            - inspect.Parameter.POSITIONAL_ONLY\n            - inspect.Parameter.POSITIONAL_OR_KEYWORD\n            - inspect.Parameter.VAR_POSITIONAL\n            - inspect.Parameter.KEYWORD_ONLY\n            - inspect.Parameter.VAR_KEYWORD\n        \"\"\"\n        # Check if user has overridden the arun method\n        if self._is_arun_overridden():\n            # User overrode arun method, return arun method parameters\n            if self.__cached_param_names_of_arun is None:\n                self.__cached_param_names_of_arun = get_param_names_all_kinds(self.arun)\n            return self.__cached_param_names_of_arun\n        else:\n            # User is using run method, return run method parameters\n            if self.__cached_param_names_of_run is None:\n                self.__cached_param_names_of_run = get_param_names_all_kinds(self.run)\n            return self.__cached_param_names_of_run\n\n    def _is_arun_overridden(self) -&gt; bool:\n        \"\"\"\n        Check if the user has overridden the arun method.\n        \"\"\"\n        # Compare method references - much faster than inspect.getsource()\n        return self.arun.__func__ is not Worker.arun\n\n    @property\n    def parent(self) -&gt; \"Automa\":\n        return self.__parent\n\n    @parent.setter\n    def parent(self, value: \"Automa\"):\n        self.__parent = value\n\n    @property\n    def local_space(self) -&gt; Dict[str, Any]:\n        return self.__local_space\n\n    @local_space.setter\n    def local_space(self, value: Dict[str, Any]):\n        self.__local_space = value\n\n    def get_report_info(self) -&gt; Dict[str, Any]:\n        report_info = {}\n        report_info[\"local_space\"] = self.__local_space\n        return report_info\n\n    @override\n    def dump_to_dict(self) -&gt; Dict[str, Any]:\n        state_dict = {}\n        state_dict[\"local_space\"] = self.__local_space\n        return state_dict\n\n    @override\n    def load_from_dict(self, state_dict: Dict[str, Any]) -&gt; None:\n        self.__parent = self\n        self.__local_space = state_dict[\"local_space\"]\n\n        # Cached method signatures, with no need for serialization.\n        self.__cached_param_names_of_arun = None\n        self.__cached_param_names_of_run = None\n\n    def ferry_to(self, key: str, /, *args, **kwargs):\n        \"\"\"\n        Handoff control flow to the specified worker, passing along any arguments as needed.\n        The specified worker will always start to run asynchronously in the next event loop, regardless of its dependencies.\n\n        Parameters\n        ----------\n        key : str\n            The key of the worker to run.\n        args : optional\n            Positional arguments to be passed.\n        kwargs : optional\n            Keyword arguments to be passed.\n        \"\"\"\n        if self.is_top_level():\n            raise WorkerRuntimeError(f\"`ferry_to` method can only be called by a worker inside an Automa\")\n        self.parent.ferry_to(key, *args, **kwargs)\n\n    def post_event(self, event: Event) -&gt; None:\n        \"\"\"\n        Post an event to the application layer outside the Automa.\n\n        The event handler implemented by the application layer will be called in the same thread as the worker (maybe the main thread or a new thread from the thread pool).\n\n        Note that `post_event` can be called in a non-async method or an async method.\n\n        The event will be bubbled up to the top-level Automa, where it will be processed by the event handler registered with the event type.\n\n        Parameters\n        ----------\n        event: Event\n            The event to be posted.\n        \"\"\"\n        if self.is_top_level():\n            raise WorkerRuntimeError(f\"`post_event` method can only be called by a worker inside an Automa\")\n        self.parent.post_event(event)\n\n    def request_feedback(\n        self, \n        event: Event,\n        timeout: Optional[float] = None\n    ) -&gt; Feedback:\n        \"\"\"\n        Request feedback for the specified event from the application layer outside the Automa. This method blocks the caller until the feedback is received.\n\n        Note that `post_event` should only be called from within a non-async method running in the new thread of the Automa thread pool.\n\n        Parameters\n        ----------\n        event: Event\n            The event to be posted to the event handler implemented by the application layer.\n        timeout: Optional[float]\n            A float or int number of seconds to wait for if the feedback is not received. If None, then there is no limit on the wait time.\n\n        Returns\n        -------\n        Feedback\n            The feedback received from the application layer.\n\n        Raises\n        ------\n        TimeoutError\n            If the feedback is not received before the timeout. Note that the raised exception is the built-in `TimeoutError` exception, instead of asyncio.TimeoutError or concurrent.futures.TimeoutError!\n        \"\"\"\n        if self.is_top_level():\n            raise WorkerRuntimeError(f\"`request_feedback` method can only be called by a worker inside an Automa\")\n        return self.parent.request_feedback(event, timeout)\n\n    async def request_feedback_async(\n        self, \n        event: Event,\n        timeout: Optional[float] = None\n    ) -&gt; Feedback:\n        \"\"\"\n        Request feedback for the specified event from the application layer outside the Automa. This method blocks the caller until the feedback is received.\n\n        The event handler implemented by the application layer will be called in the next event loop, in the main thread.\n\n        Note that `post_event` should only be called from within an asynchronous method running in the main event loop of the top-level Automa.\n\n        Parameters\n        ----------\n        event: Event\n            The event to be posted to the event handler implemented by the application layer.\n        timeout: Optional[float]\n            A float or int number of seconds to wait for if the feedback is not received. If None, then there is no limit on the wait time.\n\n        Returns\n        -------\n        Feedback\n            The feedback received from the application layer.\n\n        Raises\n        ------\n        TimeoutError\n            If the feedback is not received before the timeout. Note that the raised exception is the built-in `TimeoutError` exception, instead of asyncio.TimeoutError!\n        \"\"\"\n        if self.is_top_level():\n            raise WorkerRuntimeError(f\"`request_feedback_async` method can only be called by a worker inside an Automa\")\n        return await self.parent.request_feedback_async(event, timeout)\n\n    def interact_with_human(self, event: Event) -&gt; InteractionFeedback:\n        if self.is_top_level():\n            raise WorkerRuntimeError(f\"`interact_with_human` method can only be called by a worker inside an Automa\")\n        return self.parent.interact_with_human(event, self)\n</code></pre>"},{"location":"reference/bridgic-core/bridgic/core/automa/worker/#bridgic.core.automa.worker.Worker.arun","title":"arun","text":"<code>async</code> <pre><code>arun(\n    *args: Tuple[Any, ...], **kwargs: Dict[str, Any]\n) -&gt; Any\n</code></pre> <p>The asynchronous method to run the worker.</p> Source code in <code>bridgic/core/automa/worker/_worker.py</code> <pre><code>async def arun(self, *args: Tuple[Any, ...], **kwargs: Dict[str, Any]) -&gt; Any:\n    \"\"\"\n    The asynchronous method to run the worker.\n    \"\"\"\n    loop = asyncio.get_running_loop()\n    topest_automa = self._get_top_level_automa()\n    if topest_automa:\n        thread_pool = topest_automa.thread_pool\n        if thread_pool:\n            # kwargs can only be passed by functools.partial.\n            return await loop.run_in_executor(thread_pool, partial(self.run, *args, **kwargs))\n\n    # Unexpected: No thread pool is available.\n    # Case 1: the worker is not inside an Automa (uncommon case).\n    # Case 2: no thread pool is setup by the top-level automa.\n    raise WorkerRuntimeError(f\"No thread pool is available for the worker {type(self)}\")\n</code></pre>"},{"location":"reference/bridgic-core/bridgic/core/automa/worker/#bridgic.core.automa.worker.Worker.run","title":"run","text":"<pre><code>run(\n    *args: Tuple[Any, ...], **kwargs: Dict[str, Any]\n) -&gt; Any\n</code></pre> <p>The synchronous method to run the worker.</p> Source code in <code>bridgic/core/automa/worker/_worker.py</code> <pre><code>def run(self, *args: Tuple[Any, ...], **kwargs: Dict[str, Any]) -&gt; Any:\n    \"\"\"\n    The synchronous method to run the worker.\n    \"\"\"\n    raise NotImplementedError(f\"run() is not implemented in {type(self)}\")\n</code></pre>"},{"location":"reference/bridgic-core/bridgic/core/automa/worker/#bridgic.core.automa.worker.Worker.is_top_level","title":"is_top_level","text":"<pre><code>is_top_level() -&gt; bool\n</code></pre> <p>Check if the current worker is the top-level worker.</p> <p>Returns:</p> Type Description <code>bool</code> <p>True if the current worker is the top-level worker (parent is self), False otherwise.</p> Source code in <code>bridgic/core/automa/worker/_worker.py</code> <pre><code>def is_top_level(self) -&gt; bool:\n    \"\"\"\n    Check if the current worker is the top-level worker.\n\n    Returns\n    -------\n    bool\n        True if the current worker is the top-level worker (parent is self), False otherwise.\n    \"\"\"\n    return self.parent is self\n</code></pre>"},{"location":"reference/bridgic-core/bridgic/core/automa/worker/#bridgic.core.automa.worker.Worker.get_input_param_names","title":"get_input_param_names","text":"<pre><code>get_input_param_names() -&gt; (\n    Dict[_ParameterKind, List[Tuple[str, Any]]]\n)\n</code></pre> <p>Get the names of input parameters of the worker. Use cached result if available in order to improve performance.</p> <p>This method intelligently detects whether the user has overridden the <code>run</code> method or is using the default <code>arun</code> method, and returns the appropriate parameter signature.</p> <p>Returns:</p> Type Description <code>Dict[_ParameterKind, List[str]]</code> <p>A dictionary of input parameter names by the kind of the parameter. The key is the kind of the parameter, which is one of five possible values:</p> <ul> <li>inspect.Parameter.POSITIONAL_ONLY</li> <li>inspect.Parameter.POSITIONAL_OR_KEYWORD</li> <li>inspect.Parameter.VAR_POSITIONAL</li> <li>inspect.Parameter.KEYWORD_ONLY</li> <li>inspect.Parameter.VAR_KEYWORD</li> </ul> Source code in <code>bridgic/core/automa/worker/_worker.py</code> <pre><code>def get_input_param_names(self) -&gt; Dict[_ParameterKind, List[Tuple[str, Any]]]:\n    \"\"\"\n    Get the names of input parameters of the worker.\n    Use cached result if available in order to improve performance.\n\n    This method intelligently detects whether the user has overridden the `run` method\n    or is using the default `arun` method, and returns the appropriate parameter signature.\n\n    Returns\n    -------\n    Dict[_ParameterKind, List[str]]\n        A dictionary of input parameter names by the kind of the parameter.\n        The key is the kind of the parameter, which is one of five possible values:\n\n        - inspect.Parameter.POSITIONAL_ONLY\n        - inspect.Parameter.POSITIONAL_OR_KEYWORD\n        - inspect.Parameter.VAR_POSITIONAL\n        - inspect.Parameter.KEYWORD_ONLY\n        - inspect.Parameter.VAR_KEYWORD\n    \"\"\"\n    # Check if user has overridden the arun method\n    if self._is_arun_overridden():\n        # User overrode arun method, return arun method parameters\n        if self.__cached_param_names_of_arun is None:\n            self.__cached_param_names_of_arun = get_param_names_all_kinds(self.arun)\n        return self.__cached_param_names_of_arun\n    else:\n        # User is using run method, return run method parameters\n        if self.__cached_param_names_of_run is None:\n            self.__cached_param_names_of_run = get_param_names_all_kinds(self.run)\n        return self.__cached_param_names_of_run\n</code></pre>"},{"location":"reference/bridgic-core/bridgic/core/automa/worker/#bridgic.core.automa.worker.Worker.ferry_to","title":"ferry_to","text":"<pre><code>ferry_to(key: str, /, *args, **kwargs)\n</code></pre> <p>Handoff control flow to the specified worker, passing along any arguments as needed. The specified worker will always start to run asynchronously in the next event loop, regardless of its dependencies.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>The key of the worker to run.</p> required <code>args</code> <code>optional</code> <p>Positional arguments to be passed.</p> <code>()</code> <code>kwargs</code> <code>optional</code> <p>Keyword arguments to be passed.</p> <code>{}</code> Source code in <code>bridgic/core/automa/worker/_worker.py</code> <pre><code>def ferry_to(self, key: str, /, *args, **kwargs):\n    \"\"\"\n    Handoff control flow to the specified worker, passing along any arguments as needed.\n    The specified worker will always start to run asynchronously in the next event loop, regardless of its dependencies.\n\n    Parameters\n    ----------\n    key : str\n        The key of the worker to run.\n    args : optional\n        Positional arguments to be passed.\n    kwargs : optional\n        Keyword arguments to be passed.\n    \"\"\"\n    if self.is_top_level():\n        raise WorkerRuntimeError(f\"`ferry_to` method can only be called by a worker inside an Automa\")\n    self.parent.ferry_to(key, *args, **kwargs)\n</code></pre>"},{"location":"reference/bridgic-core/bridgic/core/automa/worker/#bridgic.core.automa.worker.Worker.post_event","title":"post_event","text":"<pre><code>post_event(event: Event) -&gt; None\n</code></pre> <p>Post an event to the application layer outside the Automa.</p> <p>The event handler implemented by the application layer will be called in the same thread as the worker (maybe the main thread or a new thread from the thread pool).</p> <p>Note that <code>post_event</code> can be called in a non-async method or an async method.</p> <p>The event will be bubbled up to the top-level Automa, where it will be processed by the event handler registered with the event type.</p> <p>Parameters:</p> Name Type Description Default <code>event</code> <code>Event</code> <p>The event to be posted.</p> required Source code in <code>bridgic/core/automa/worker/_worker.py</code> <pre><code>def post_event(self, event: Event) -&gt; None:\n    \"\"\"\n    Post an event to the application layer outside the Automa.\n\n    The event handler implemented by the application layer will be called in the same thread as the worker (maybe the main thread or a new thread from the thread pool).\n\n    Note that `post_event` can be called in a non-async method or an async method.\n\n    The event will be bubbled up to the top-level Automa, where it will be processed by the event handler registered with the event type.\n\n    Parameters\n    ----------\n    event: Event\n        The event to be posted.\n    \"\"\"\n    if self.is_top_level():\n        raise WorkerRuntimeError(f\"`post_event` method can only be called by a worker inside an Automa\")\n    self.parent.post_event(event)\n</code></pre>"},{"location":"reference/bridgic-core/bridgic/core/automa/worker/#bridgic.core.automa.worker.Worker.request_feedback","title":"request_feedback","text":"<pre><code>request_feedback(\n    event: Event, timeout: Optional[float] = None\n) -&gt; Feedback\n</code></pre> <p>Request feedback for the specified event from the application layer outside the Automa. This method blocks the caller until the feedback is received.</p> <p>Note that <code>post_event</code> should only be called from within a non-async method running in the new thread of the Automa thread pool.</p> <p>Parameters:</p> Name Type Description Default <code>event</code> <code>Event</code> <p>The event to be posted to the event handler implemented by the application layer.</p> required <code>timeout</code> <code>Optional[float]</code> <p>A float or int number of seconds to wait for if the feedback is not received. If None, then there is no limit on the wait time.</p> <code>None</code> <p>Returns:</p> Type Description <code>Feedback</code> <p>The feedback received from the application layer.</p> <p>Raises:</p> Type Description <code>TimeoutError</code> <p>If the feedback is not received before the timeout. Note that the raised exception is the built-in <code>TimeoutError</code> exception, instead of asyncio.TimeoutError or concurrent.futures.TimeoutError!</p> Source code in <code>bridgic/core/automa/worker/_worker.py</code> <pre><code>def request_feedback(\n    self, \n    event: Event,\n    timeout: Optional[float] = None\n) -&gt; Feedback:\n    \"\"\"\n    Request feedback for the specified event from the application layer outside the Automa. This method blocks the caller until the feedback is received.\n\n    Note that `post_event` should only be called from within a non-async method running in the new thread of the Automa thread pool.\n\n    Parameters\n    ----------\n    event: Event\n        The event to be posted to the event handler implemented by the application layer.\n    timeout: Optional[float]\n        A float or int number of seconds to wait for if the feedback is not received. If None, then there is no limit on the wait time.\n\n    Returns\n    -------\n    Feedback\n        The feedback received from the application layer.\n\n    Raises\n    ------\n    TimeoutError\n        If the feedback is not received before the timeout. Note that the raised exception is the built-in `TimeoutError` exception, instead of asyncio.TimeoutError or concurrent.futures.TimeoutError!\n    \"\"\"\n    if self.is_top_level():\n        raise WorkerRuntimeError(f\"`request_feedback` method can only be called by a worker inside an Automa\")\n    return self.parent.request_feedback(event, timeout)\n</code></pre>"},{"location":"reference/bridgic-core/bridgic/core/automa/worker/#bridgic.core.automa.worker.Worker.request_feedback_async","title":"request_feedback_async","text":"<code>async</code> <pre><code>request_feedback_async(\n    event: Event, timeout: Optional[float] = None\n) -&gt; Feedback\n</code></pre> <p>Request feedback for the specified event from the application layer outside the Automa. This method blocks the caller until the feedback is received.</p> <p>The event handler implemented by the application layer will be called in the next event loop, in the main thread.</p> <p>Note that <code>post_event</code> should only be called from within an asynchronous method running in the main event loop of the top-level Automa.</p> <p>Parameters:</p> Name Type Description Default <code>event</code> <code>Event</code> <p>The event to be posted to the event handler implemented by the application layer.</p> required <code>timeout</code> <code>Optional[float]</code> <p>A float or int number of seconds to wait for if the feedback is not received. If None, then there is no limit on the wait time.</p> <code>None</code> <p>Returns:</p> Type Description <code>Feedback</code> <p>The feedback received from the application layer.</p> <p>Raises:</p> Type Description <code>TimeoutError</code> <p>If the feedback is not received before the timeout. Note that the raised exception is the built-in <code>TimeoutError</code> exception, instead of asyncio.TimeoutError!</p> Source code in <code>bridgic/core/automa/worker/_worker.py</code> <pre><code>async def request_feedback_async(\n    self, \n    event: Event,\n    timeout: Optional[float] = None\n) -&gt; Feedback:\n    \"\"\"\n    Request feedback for the specified event from the application layer outside the Automa. This method blocks the caller until the feedback is received.\n\n    The event handler implemented by the application layer will be called in the next event loop, in the main thread.\n\n    Note that `post_event` should only be called from within an asynchronous method running in the main event loop of the top-level Automa.\n\n    Parameters\n    ----------\n    event: Event\n        The event to be posted to the event handler implemented by the application layer.\n    timeout: Optional[float]\n        A float or int number of seconds to wait for if the feedback is not received. If None, then there is no limit on the wait time.\n\n    Returns\n    -------\n    Feedback\n        The feedback received from the application layer.\n\n    Raises\n    ------\n    TimeoutError\n        If the feedback is not received before the timeout. Note that the raised exception is the built-in `TimeoutError` exception, instead of asyncio.TimeoutError!\n    \"\"\"\n    if self.is_top_level():\n        raise WorkerRuntimeError(f\"`request_feedback_async` method can only be called by a worker inside an Automa\")\n    return await self.parent.request_feedback_async(event, timeout)\n</code></pre>"},{"location":"reference/bridgic-core/bridgic/core/automa/worker/#bridgic.core.automa.worker.CallableWorker","title":"CallableWorker","text":"<p>               Bases: <code>Worker</code></p> <p>This class is a worker that wraps a callable object, such as functions or methods.</p> <p>Parameters:</p> Name Type Description Default <code>func_or_method</code> <code>Optional[Callable]</code> <p>The callable to be wrapped by the worker. If <code>func_or_method</code> is None,  <code>state_dict</code> must be provided.</p> <code>None</code> Source code in <code>bridgic/core/automa/worker/_callable_worker.py</code> <pre><code>class CallableWorker(Worker):\n    \"\"\"\n    This class is a worker that wraps a callable object, such as functions or methods.\n\n    Parameters\n    ----------\n    func_or_method : Optional[Callable]\n        The callable to be wrapped by the worker. If `func_or_method` is None, \n        `state_dict` must be provided.\n    \"\"\"\n    _is_coro_func: bool\n    _is_agen_func: bool\n\n    _callable: Callable\n\n    # Used to deserialization.\n    _expected_bound_parent: bool\n\n    # Cached method signatures, with no need for serialization.\n    __cached_param_names_of_callable: Dict[_ParameterKind, List[str]]\n\n    def __init__(\n        self, \n        func_or_method: Optional[Callable] = None,\n    ):\n        \"\"\"\n        Parameters\n        ----------\n        func_or_method : Optional[Callable]\n            The callable to be wrapped by the worker. If `func_or_method` is None, \n            `state_dict` must be provided.\n        \"\"\"\n        super().__init__()\n        self._is_coro_func = inspect.iscoroutinefunction(func_or_method)\n        self._is_agen_func = inspect.isasyncgenfunction(func_or_method)\n        self._callable = func_or_method\n        self._expected_bound_parent = False\n\n        # Cached method signatures, with no need for serialization.\n        self.__cached_param_names_of_callable = None\n\n    async def arun(self, *args: Tuple[Any, ...], **kwargs: Dict[str, Any]) -&gt; Any:\n        if self._expected_bound_parent:\n            raise WorkerRuntimeError(\n                f\"The callable is expected to be bound to the parent, \"\n                f\"but not bounded yet: {self._callable}\"\n            )\n\n        if self._is_coro_func:\n            return await self._callable(*args, **kwargs)\n        if self._is_agen_func:\n            return self._callable(*args, **kwargs)\n\n        return await super().arun(*args, **kwargs)\n\n    def run(self, *args: Tuple[Any, ...], **kwargs: Dict[str, Any]) -&gt; Any:\n        assert self._is_coro_func is False\n        return self._callable(*args, **kwargs)\n\n    @override\n    def get_input_param_names(self) -&gt; Dict[_ParameterKind, List[str]]:\n        \"\"\"\n        Get the names of input parameters of this callable worker.\n        Use cached result if available in order to improve performance.\n\n        Returns\n        -------\n        Dict[_ParameterKind, List[str]]\n            A dictionary of input parameter names by the kind of the parameter.\n            The key is the kind of the parameter, which is one of five possible values:\n\n            - inspect.Parameter.POSITIONAL_ONLY\n            - inspect.Parameter.POSITIONAL_OR_KEYWORD\n            - inspect.Parameter.VAR_POSITIONAL\n            - inspect.Parameter.KEYWORD_ONLY\n            - inspect.Parameter.VAR_KEYWORD\n        \"\"\"\n        if self.__cached_param_names_of_callable is None:\n            self.__cached_param_names_of_callable = get_param_names_all_kinds(self._callable)\n        return self.__cached_param_names_of_callable\n\n    @override\n    def dump_to_dict(self) -&gt; Dict[str, Any]:\n        state_dict = super().dump_to_dict()\n        state_dict[\"is_coro_func\"] = self._is_coro_func\n        state_dict[\"is_agen_func\"] = self._is_agen_func\n        # Note: Not to use pickle to serialize the callable here.\n        # We customize the serialization method of the callable to avoid creating instance multiple times and to minimize side effects.\n        bounded = isinstance(self._callable, MethodType)\n        state_dict[\"bounded\"] = bounded\n        if bounded:\n            if self._callable.__self__ is self.parent:\n                state_dict[\"callable_name\"] = self._callable.__module__ + \".\" + self._callable.__qualname__\n            else:\n                state_dict[\"pickled_callable\"] = pickle.dumps(self._callable)\n        else:\n            state_dict[\"callable_name\"] = self._callable.__module__ + \".\" + self._callable.__qualname__\n        return state_dict\n\n    @override\n    def load_from_dict(self, state_dict: Dict[str, Any]) -&gt; None:\n        super().load_from_dict(state_dict)\n        # Deserialize from the state_dict.\n        self._is_coro_func = state_dict[\"is_coro_func\"]\n        self._is_agen_func = state_dict[\"is_agen_func\"]\n        bounded = state_dict[\"bounded\"]\n        if bounded:\n            pickled_callable = state_dict.get(\"pickled_callable\", None)\n            if pickled_callable is None:\n                self._callable = load_qualified_class_or_func(state_dict[\"callable_name\"])\n                # Partially deserialized, need to be bound to the parent.\n                self._expected_bound_parent = True\n            else:\n                self._callable = pickle.loads(pickled_callable)\n                self._expected_bound_parent = False\n        else:\n            self._callable = load_qualified_class_or_func(state_dict[\"callable_name\"])\n            self._expected_bound_parent = False\n\n        # Cached method signatures, with no need for serialization.\n        self.__cached_param_names_of_callable = None\n\n    @property\n    def callable(self):\n        return self._callable\n\n    @property\n    def parent(self) -&gt; \"Automa\":\n        return super().parent\n\n    @parent.setter\n    def parent(self, value: \"Automa\"):\n        if self._expected_bound_parent:\n            self._callable = MethodType(self._callable, value)\n            self._expected_bound_parent = False\n        Worker.parent.fset(self, value)\n\n    @override\n    def __str__(self) -&gt; str:\n        return f\"CallableWorker(callable={self._callable.__name__})\"\n</code></pre>"},{"location":"reference/bridgic-core/bridgic/core/automa/worker/#bridgic.core.automa.worker.CallableWorker.get_input_param_names","title":"get_input_param_names","text":"<pre><code>get_input_param_names() -&gt; Dict[_ParameterKind, List[str]]\n</code></pre> <p>Get the names of input parameters of this callable worker. Use cached result if available in order to improve performance.</p> <p>Returns:</p> Type Description <code>Dict[_ParameterKind, List[str]]</code> <p>A dictionary of input parameter names by the kind of the parameter. The key is the kind of the parameter, which is one of five possible values:</p> <ul> <li>inspect.Parameter.POSITIONAL_ONLY</li> <li>inspect.Parameter.POSITIONAL_OR_KEYWORD</li> <li>inspect.Parameter.VAR_POSITIONAL</li> <li>inspect.Parameter.KEYWORD_ONLY</li> <li>inspect.Parameter.VAR_KEYWORD</li> </ul> Source code in <code>bridgic/core/automa/worker/_callable_worker.py</code> <pre><code>@override\ndef get_input_param_names(self) -&gt; Dict[_ParameterKind, List[str]]:\n    \"\"\"\n    Get the names of input parameters of this callable worker.\n    Use cached result if available in order to improve performance.\n\n    Returns\n    -------\n    Dict[_ParameterKind, List[str]]\n        A dictionary of input parameter names by the kind of the parameter.\n        The key is the kind of the parameter, which is one of five possible values:\n\n        - inspect.Parameter.POSITIONAL_ONLY\n        - inspect.Parameter.POSITIONAL_OR_KEYWORD\n        - inspect.Parameter.VAR_POSITIONAL\n        - inspect.Parameter.KEYWORD_ONLY\n        - inspect.Parameter.VAR_KEYWORD\n    \"\"\"\n    if self.__cached_param_names_of_callable is None:\n        self.__cached_param_names_of_callable = get_param_names_all_kinds(self._callable)\n    return self.__cached_param_names_of_callable\n</code></pre>"},{"location":"reference/bridgic-core/bridgic/core/automa/worker/#bridgic.core.automa.worker.WorkerCallback","title":"WorkerCallback","text":"<p>               Bases: <code>Serializable</code></p> <p>Callback for the execution of a worker instance during the running  of a prebult automa.</p> <p>This class defines the interfaces that will be called before or after  the execution of the corresponding worker. Callbacks are typically used  for validating input, monitoring execution, and collecting logs, etc.</p> <p>Methods:</p> Name Description <code>on_worker_start</code> <p>Hook invoked before worker execution.</p> <code>on_worker_end</code> <p>Hook invoked after worker execution.</p> <code>on_worker_error</code> <p>Hook invoked when worker execution raises an exception.</p> Source code in <code>bridgic/core/automa/worker/_worker_callback.py</code> <pre><code>class WorkerCallback(Serializable):\n    \"\"\"\n    Callback for the execution of a worker instance during the running \n    of a prebult automa.\n\n    This class defines the interfaces that will be called before or after \n    the execution of the corresponding worker. Callbacks are typically used \n    for validating input, monitoring execution, and collecting logs, etc.\n\n    Methods\n    -------\n    on_worker_start(key, is_top_level, parent, arguments)\n        Hook invoked before worker execution.\n    on_worker_end(key, is_top_level, parent, arguments, result)\n        Hook invoked after worker execution.\n    on_worker_error(key, is_top_level, parent, arguments, error)\n        Hook invoked when worker execution raises an exception.\n    \"\"\"\n    async def on_worker_start(\n        self, \n        key: str,\n        is_top_level: bool = False,\n        parent: Optional[\"Automa\"] = None,\n        arguments: Dict[str, Any] = None,\n    ) -&gt; None:\n        \"\"\"\n        Hook invoked before worker execution.\n\n        Called immediately before the worker runs. Use for arguments\n        validation, logging, or monitoring. Cannot modify execution\n        arguments or logic.\n\n        Parameters\n        ----------\n        key : str\n            Worker identifier.\n        is_top_level: bool = False\n            Whether the worker is the top-level automa. When True, parent will be the automa itself (parent is self).\n        parent : Optional[Automa] = None\n            Parent automa instance containing this worker. For top-level automa, parent is the automa itself.\n        arguments : Dict[str, Any] = None\n            Execution parameters with keys \"args\" and \"kwargs\".\n        \"\"\"\n        pass\n\n    async def on_worker_end(\n        self,\n        key: str,\n        is_top_level: bool = False,\n        parent: Optional[\"Automa\"] = None,\n        arguments: Dict[str, Any] = None,\n        result: Any = None,\n    ) -&gt; None:\n        \"\"\"\n        Hook invoked after worker execution.\n\n        Called immediately after the worker completes. Use for result\n        monitoring, logging, event publishing, or validation. Cannot\n        modify execution results or logic.\n\n        Parameters\n        ----------\n        key : str\n            Worker identifier.\n        is_top_level: bool = False\n            Whether the worker is the top-level automa. When True, parent will be the automa itself (parent is self).\n        parent : Optional[Automa] = None\n            Parent automa instance containing this worker. For top-level automa, parent is the automa itself.\n        arguments : Dict[str, Any] = None\n            Execution arguments with keys \"args\" and \"kwargs\".\n        result : Any = None\n            Worker execution result.\n        \"\"\"\n        pass\n\n    async def on_worker_error(\n        self,\n        key: str,\n        is_top_level: bool = False,\n        parent: Optional[\"Automa\"] = None,\n        arguments: Dict[str, Any] = None,\n        error: Exception = None,\n    ) -&gt; bool:\n        \"\"\"\n        Hook invoked when worker execution raises an exception.\n\n        Called when the worker execution raises an exception. Use for error handling, logging, \n        or event publishing. Cannot modify execution logic or arguments.\n\n        **Exception Matching Mechanism: How to Handle a Specific Exception**\n\n        The framework enable your callback to handle a given exception based on the \n        **type annotation** of the `error` parameter in your `on_worker_error` method.\n        The matching follows these rules:\n\n        - The parameter name MUST be `error` and the type annotation is critical for the \n          matching mechanism.\n        - If you annotate `error: ValueError`, it will match `ValueError` and all its \n          subclasses (e.g., `UnicodeDecodeError`).\n        - If you annotate `error: Exception`, it will match all exceptions (since all exceptions \n          inherit from Exception).\n        - If you want to match multiple exception types, you can use `Union[Type1, Type2, ...]`.\n\n\n        **Return Value: Whether to Suppress the Exception**\n\n        - If `on_worker_error` returns `True`, the framework will suppress the exception. \n          The framework will then proceed as if there was no error, and the worker result \n          will be set to None.\n        - If `on_worker_error` returns `False`, the framework will simply observe the error; \n          after all matching callbacks are called, the framework will re-raise the exception.\n\n        **Special Case: Interaction Exceptions Cannot Be Suppressed**\n\n        To ensure human-interaction mechanisms work correctly, exceptions of type\n        `_InteractionEventException` or `InteractionException` (including their subclasses) \n        **CANNOT** be suppressed by any callback. Even if your callback returns `True`, the \n        framework will forcibly re-raise the exception. This ensures these exceptions always \n        propagate correctly through the automa hierarchy to trigger necessary human interactions.\n\n        Parameters\n        ----------\n        key : str\n            Worker identifier.\n        is_top_level: bool = False\n            Whether the worker is the top-level automa. When True, parent will be the automa itself (parent is self).\n        parent : Optional[Automa] = None\n            Parent automa instance containing this worker. For top-level automa, parent is the automa itself.\n        arguments : Dict[str, Any] = None\n            Execution arguments with keys \"args\" and \"kwargs\".\n        error : Exception = None\n            The exception raised during worker execution. The type annotation of this\n            parameter determines which exceptions this callback will handle. The matching\n            is based on inheritance relationship (using isinstance), so a callback with\n            `error: ValueError` will match ValueError and all its subclasses.\n\n        Returns\n        -------\n        bool\n            True if the automa should suppress the exception (not re-raise it); False otherwise.\n        \"\"\"\n        return False\n\n    @override\n    def dump_to_dict(self) -&gt; Dict[str, Any]:\n        return {\n            \"callback_cls\": self.__class__.__module__ + \".\" + self.__class__.__qualname__,\n        }\n\n    @override\n    def load_from_dict(self, state_dict: Dict[str, Any]) -&gt; None:\n        pass\n</code></pre>"},{"location":"reference/bridgic-core/bridgic/core/automa/worker/#bridgic.core.automa.worker.WorkerCallback.on_worker_start","title":"on_worker_start","text":"<code>async</code> <pre><code>on_worker_start(\n    key: str,\n    is_top_level: bool = False,\n    parent: Optional[Automa] = None,\n    arguments: Dict[str, Any] = None,\n) -&gt; None\n</code></pre> <p>Hook invoked before worker execution.</p> <p>Called immediately before the worker runs. Use for arguments validation, logging, or monitoring. Cannot modify execution arguments or logic.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>Worker identifier.</p> required <code>is_top_level</code> <code>bool</code> <p>Whether the worker is the top-level automa. When True, parent will be the automa itself (parent is self).</p> <code>False</code> <code>parent</code> <code>Optional[Automa] = None</code> <p>Parent automa instance containing this worker. For top-level automa, parent is the automa itself.</p> <code>None</code> <code>arguments</code> <code>Dict[str, Any] = None</code> <p>Execution parameters with keys \"args\" and \"kwargs\".</p> <code>None</code> Source code in <code>bridgic/core/automa/worker/_worker_callback.py</code> <pre><code>async def on_worker_start(\n    self, \n    key: str,\n    is_top_level: bool = False,\n    parent: Optional[\"Automa\"] = None,\n    arguments: Dict[str, Any] = None,\n) -&gt; None:\n    \"\"\"\n    Hook invoked before worker execution.\n\n    Called immediately before the worker runs. Use for arguments\n    validation, logging, or monitoring. Cannot modify execution\n    arguments or logic.\n\n    Parameters\n    ----------\n    key : str\n        Worker identifier.\n    is_top_level: bool = False\n        Whether the worker is the top-level automa. When True, parent will be the automa itself (parent is self).\n    parent : Optional[Automa] = None\n        Parent automa instance containing this worker. For top-level automa, parent is the automa itself.\n    arguments : Dict[str, Any] = None\n        Execution parameters with keys \"args\" and \"kwargs\".\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/bridgic-core/bridgic/core/automa/worker/#bridgic.core.automa.worker.WorkerCallback.on_worker_end","title":"on_worker_end","text":"<code>async</code> <pre><code>on_worker_end(\n    key: str,\n    is_top_level: bool = False,\n    parent: Optional[Automa] = None,\n    arguments: Dict[str, Any] = None,\n    result: Any = None,\n) -&gt; None\n</code></pre> <p>Hook invoked after worker execution.</p> <p>Called immediately after the worker completes. Use for result monitoring, logging, event publishing, or validation. Cannot modify execution results or logic.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>Worker identifier.</p> required <code>is_top_level</code> <code>bool</code> <p>Whether the worker is the top-level automa. When True, parent will be the automa itself (parent is self).</p> <code>False</code> <code>parent</code> <code>Optional[Automa] = None</code> <p>Parent automa instance containing this worker. For top-level automa, parent is the automa itself.</p> <code>None</code> <code>arguments</code> <code>Dict[str, Any] = None</code> <p>Execution arguments with keys \"args\" and \"kwargs\".</p> <code>None</code> <code>result</code> <code>Any = None</code> <p>Worker execution result.</p> <code>None</code> Source code in <code>bridgic/core/automa/worker/_worker_callback.py</code> <pre><code>async def on_worker_end(\n    self,\n    key: str,\n    is_top_level: bool = False,\n    parent: Optional[\"Automa\"] = None,\n    arguments: Dict[str, Any] = None,\n    result: Any = None,\n) -&gt; None:\n    \"\"\"\n    Hook invoked after worker execution.\n\n    Called immediately after the worker completes. Use for result\n    monitoring, logging, event publishing, or validation. Cannot\n    modify execution results or logic.\n\n    Parameters\n    ----------\n    key : str\n        Worker identifier.\n    is_top_level: bool = False\n        Whether the worker is the top-level automa. When True, parent will be the automa itself (parent is self).\n    parent : Optional[Automa] = None\n        Parent automa instance containing this worker. For top-level automa, parent is the automa itself.\n    arguments : Dict[str, Any] = None\n        Execution arguments with keys \"args\" and \"kwargs\".\n    result : Any = None\n        Worker execution result.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/bridgic-core/bridgic/core/automa/worker/#bridgic.core.automa.worker.WorkerCallback.on_worker_error","title":"on_worker_error","text":"<code>async</code> <pre><code>on_worker_error(\n    key: str,\n    is_top_level: bool = False,\n    parent: Optional[Automa] = None,\n    arguments: Dict[str, Any] = None,\n    error: Exception = None,\n) -&gt; bool\n</code></pre> <p>Hook invoked when worker execution raises an exception.</p> <p>Called when the worker execution raises an exception. Use for error handling, logging,  or event publishing. Cannot modify execution logic or arguments.</p> <p>Exception Matching Mechanism: How to Handle a Specific Exception</p> <p>The framework enable your callback to handle a given exception based on the  type annotation of the <code>error</code> parameter in your <code>on_worker_error</code> method. The matching follows these rules:</p> <ul> <li>The parameter name MUST be <code>error</code> and the type annotation is critical for the    matching mechanism.</li> <li>If you annotate <code>error: ValueError</code>, it will match <code>ValueError</code> and all its    subclasses (e.g., <code>UnicodeDecodeError</code>).</li> <li>If you annotate <code>error: Exception</code>, it will match all exceptions (since all exceptions    inherit from Exception).</li> <li>If you want to match multiple exception types, you can use <code>Union[Type1, Type2, ...]</code>.</li> </ul> <p>Return Value: Whether to Suppress the Exception</p> <ul> <li>If <code>on_worker_error</code> returns <code>True</code>, the framework will suppress the exception.    The framework will then proceed as if there was no error, and the worker result    will be set to None.</li> <li>If <code>on_worker_error</code> returns <code>False</code>, the framework will simply observe the error;    after all matching callbacks are called, the framework will re-raise the exception.</li> </ul> <p>Special Case: Interaction Exceptions Cannot Be Suppressed</p> <p>To ensure human-interaction mechanisms work correctly, exceptions of type <code>_InteractionEventException</code> or <code>InteractionException</code> (including their subclasses)  CANNOT be suppressed by any callback. Even if your callback returns <code>True</code>, the  framework will forcibly re-raise the exception. This ensures these exceptions always  propagate correctly through the automa hierarchy to trigger necessary human interactions.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>Worker identifier.</p> required <code>is_top_level</code> <code>bool</code> <p>Whether the worker is the top-level automa. When True, parent will be the automa itself (parent is self).</p> <code>False</code> <code>parent</code> <code>Optional[Automa] = None</code> <p>Parent automa instance containing this worker. For top-level automa, parent is the automa itself.</p> <code>None</code> <code>arguments</code> <code>Dict[str, Any] = None</code> <p>Execution arguments with keys \"args\" and \"kwargs\".</p> <code>None</code> <code>error</code> <code>Exception = None</code> <p>The exception raised during worker execution. The type annotation of this parameter determines which exceptions this callback will handle. The matching is based on inheritance relationship (using isinstance), so a callback with <code>error: ValueError</code> will match ValueError and all its subclasses.</p> <code>None</code> <p>Returns:</p> Type Description <code>bool</code> <p>True if the automa should suppress the exception (not re-raise it); False otherwise.</p> Source code in <code>bridgic/core/automa/worker/_worker_callback.py</code> <pre><code>async def on_worker_error(\n    self,\n    key: str,\n    is_top_level: bool = False,\n    parent: Optional[\"Automa\"] = None,\n    arguments: Dict[str, Any] = None,\n    error: Exception = None,\n) -&gt; bool:\n    \"\"\"\n    Hook invoked when worker execution raises an exception.\n\n    Called when the worker execution raises an exception. Use for error handling, logging, \n    or event publishing. Cannot modify execution logic or arguments.\n\n    **Exception Matching Mechanism: How to Handle a Specific Exception**\n\n    The framework enable your callback to handle a given exception based on the \n    **type annotation** of the `error` parameter in your `on_worker_error` method.\n    The matching follows these rules:\n\n    - The parameter name MUST be `error` and the type annotation is critical for the \n      matching mechanism.\n    - If you annotate `error: ValueError`, it will match `ValueError` and all its \n      subclasses (e.g., `UnicodeDecodeError`).\n    - If you annotate `error: Exception`, it will match all exceptions (since all exceptions \n      inherit from Exception).\n    - If you want to match multiple exception types, you can use `Union[Type1, Type2, ...]`.\n\n\n    **Return Value: Whether to Suppress the Exception**\n\n    - If `on_worker_error` returns `True`, the framework will suppress the exception. \n      The framework will then proceed as if there was no error, and the worker result \n      will be set to None.\n    - If `on_worker_error` returns `False`, the framework will simply observe the error; \n      after all matching callbacks are called, the framework will re-raise the exception.\n\n    **Special Case: Interaction Exceptions Cannot Be Suppressed**\n\n    To ensure human-interaction mechanisms work correctly, exceptions of type\n    `_InteractionEventException` or `InteractionException` (including their subclasses) \n    **CANNOT** be suppressed by any callback. Even if your callback returns `True`, the \n    framework will forcibly re-raise the exception. This ensures these exceptions always \n    propagate correctly through the automa hierarchy to trigger necessary human interactions.\n\n    Parameters\n    ----------\n    key : str\n        Worker identifier.\n    is_top_level: bool = False\n        Whether the worker is the top-level automa. When True, parent will be the automa itself (parent is self).\n    parent : Optional[Automa] = None\n        Parent automa instance containing this worker. For top-level automa, parent is the automa itself.\n    arguments : Dict[str, Any] = None\n        Execution arguments with keys \"args\" and \"kwargs\".\n    error : Exception = None\n        The exception raised during worker execution. The type annotation of this\n        parameter determines which exceptions this callback will handle. The matching\n        is based on inheritance relationship (using isinstance), so a callback with\n        `error: ValueError` will match ValueError and all its subclasses.\n\n    Returns\n    -------\n    bool\n        True if the automa should suppress the exception (not re-raise it); False otherwise.\n    \"\"\"\n    return False\n</code></pre>"},{"location":"reference/bridgic-core/bridgic/core/automa/worker/#bridgic.core.automa.worker.WorkerCallbackBuilder","title":"WorkerCallbackBuilder","text":"<p>               Bases: <code>Generic[T_WorkerCallback]</code></p> <p>Builder class for creating instances of <code>WorkerCallback</code> subclasses.</p> <p>This builder is designed to construct instances of subclasses of <code>WorkerCallback</code>. The <code>_callback_type</code> parameter should be a subclass of <code>WorkerCallback</code>, and <code>build()</code>  will return an instance of that specific subclass. There is no need to call <code>build()</code>  directly. Instead, the framework calls the <code>build</code> method automatically to create  its own <code>WorkerCallback</code> instance for each worker instance.</p> Notes <p>Register a Callback in Different Scope</p> <p>There are three ways to register a callback for three levels of customization:</p> <ul> <li>Case 1: Use in worker decorator to register the callback for a specific worker.</li> <li>Case 2: Use in RunningOptions to register the callback for a specific Automa instance.</li> <li>Case 3: Use in GlobalSetting to register the callback for all workers.</li> </ul> Notes <p>Shared Instance Mode</p> <ul> <li>When <code>is_shared=True</code> (default), all workers within the same scope will share the same    callback instance. This is useful for scenarios where a single callback instance is needed    to maintain some state across workers within the same scope, such as the connection to    an external service. The scope is determined by where the builder is declared:</li> <li>If declared in GlobalSetting: shared across all workers globally</li> <li>If declared in RunningOptions: shared across all workers within that Automa instance</li> <li>When <code>is_shared=False</code>, each worker will get its own callback instance. This is useful for    scenarios where a independent callback instance is needed for each worker.</li> </ul> <p>Examples:</p> <p>There are three ways to use the builder, for different levels of customization:</p> <pre><code>&gt;&gt;&gt; # Define a custom callback class:\n&gt;&gt;&gt; class MyEmptyCallback(WorkerCallback):\n...     pass\n...\n&gt;&gt;&gt; # Case 1: Use in worker decorator to register the callback for a specific worker:\n&gt;&gt;&gt; class MyGraphAutoma(GraphAutoma):\n...     @worker(callback_builders=[WorkerCallbackBuilder(MyEmptyCallback)])\n...     async def my_worker(self, x: int) -&gt; int:\n...         return x + 1\n...\n&gt;&gt;&gt; # Case 2: Use in RunningOptions to register the callback for a specific Automa instance:\n...     running_options = RunningOptions(callback_builders=[WorkerCallbackBuilder(MyEmptyCallback)])\n...     graph = MyGraphAutoma(running_options=running_options)\n...\n&gt;&gt;&gt; # Case 3: Use in GlobalSetting to register the callback for all workers:\n&gt;&gt;&gt; GlobalSetting.set(callback_builders=[WorkerCallbackBuilder(MyEmptyCallback)])\n</code></pre> Source code in <code>bridgic/core/automa/worker/_worker_callback.py</code> <pre><code>class WorkerCallbackBuilder(Generic[T_WorkerCallback]):\n    \"\"\"\n    Builder class for creating instances of `WorkerCallback` subclasses.\n\n    This builder is designed to construct instances of subclasses of `WorkerCallback`.\n    The `_callback_type` parameter should be a subclass of `WorkerCallback`, and `build()` \n    will return an instance of that specific subclass. There is no need to call `build()` \n    directly. Instead, the framework calls the `build` method automatically to create \n    its own `WorkerCallback` instance for each worker instance.\n\n    Notes\n    -----\n    **Register a Callback in Different Scope**\n\n    There are three ways to register a callback for three levels of customization:\n\n    - Case 1: Use in worker decorator to register the callback for a specific worker.\n    - Case 2: Use in RunningOptions to register the callback for a specific Automa instance.\n    - Case 3: Use in GlobalSetting to register the callback for all workers.\n\n    Notes\n    -----\n    **Shared Instance Mode**\n\n    - When `is_shared=True` (default), all workers within the same scope will share the same \n      callback instance. This is useful for scenarios where a single callback instance is needed \n      to maintain some state across workers within the same scope, such as the connection to \n      an external service. The scope is determined by where the builder is declared:\n      - If declared in GlobalSetting: shared across all workers globally\n      - If declared in RunningOptions: shared across all workers within that Automa instance\n    - When `is_shared=False`, each worker will get its own callback instance. This is useful for \n      scenarios where a independent callback instance is needed for each worker.\n\n    Examples\n    --------\n    There are three ways to use the builder, for different levels of customization:\n\n    &gt;&gt;&gt; # Define a custom callback class:\n    &gt;&gt;&gt; class MyEmptyCallback(WorkerCallback):\n    ...     pass\n    ...\n    &gt;&gt;&gt; # Case 1: Use in worker decorator to register the callback for a specific worker:\n    &gt;&gt;&gt; class MyGraphAutoma(GraphAutoma):\n    ...     @worker(callback_builders=[WorkerCallbackBuilder(MyEmptyCallback)])\n    ...     async def my_worker(self, x: int) -&gt; int:\n    ...         return x + 1\n    ...\n    &gt;&gt;&gt; # Case 2: Use in RunningOptions to register the callback for a specific Automa instance:\n    ...     running_options = RunningOptions(callback_builders=[WorkerCallbackBuilder(MyEmptyCallback)])\n    ...     graph = MyGraphAutoma(running_options=running_options)\n    ...\n    &gt;&gt;&gt; # Case 3: Use in GlobalSetting to register the callback for all workers:\n    &gt;&gt;&gt; GlobalSetting.set(callback_builders=[WorkerCallbackBuilder(MyEmptyCallback)])\n    \"\"\"\n    _callback_type: Type[T_WorkerCallback]\n    \"\"\"The specific subclass of `WorkerCallback` to instantiate.\"\"\"\n    _init_kwargs: Dict[str, Any]\n    \"\"\"The initialization arguments for the instance.\"\"\"\n    _is_shared: bool\n    \"\"\"Whether to use shared instance mode (reuse the same instance within the declaration scope).\"\"\"\n\n    _shared_instance: Optional[T_WorkerCallback] = None\n    \"\"\"Shared instance of the callback within the declaration scope.\"\"\"\n    _shared_lock: Lock = Lock()\n    \"\"\"Lock for thread-safe shared instance creation.\"\"\"\n\n    def __init__(\n        self,\n        callback_type: Type[T_WorkerCallback],\n        init_kwargs: Optional[Dict[str, Any]] = None,\n        is_shared: bool = True,\n    ):\n        \"\"\"\n        Initialize the builder with a `WorkerCallback` subclass type.\n\n        Parameters\n        ----------\n        callback_type : Type[T_WorkerCallback]\n            A subclass of `WorkerCallback` to be instantiated.\n        init_kwargs : Optional[Dict[str, Any]]\n            Keyword arguments to pass to the subclass constructor.\n        is_shared : bool, default True\n            If True, the callback instance will be shared within the declaration scope:\n            If False, each worker will get its own callback instance.\n        \"\"\"\n        self._callback_type = callback_type\n        self._init_kwargs = init_kwargs or {}\n        self._is_shared = is_shared\n\n    def build(self) -&gt; T_WorkerCallback:\n        \"\"\"\n        Build and return an instance of the specified `WorkerCallback` subclass.\n\n        Returns\n        -------\n        T_WorkerCallback\n            An instance of the `WorkerCallback` subclass specified during initialization.\n        \"\"\"\n        if self._is_shared:\n            if self._shared_instance is None:\n                with self._shared_lock:\n                    if self._shared_instance is None:\n                        self._shared_instance = self._callback_type(**self._init_kwargs)\n            return self._shared_instance\n        else:\n            return self._callback_type(**self._init_kwargs)\n\n    @override\n    def dump_to_dict(self) -&gt; Dict[str, Any]:\n        return {\n            \"callback_type\": self._callback_type.__module__ + \".\" + self._callback_type.__qualname__,\n            \"init_kwargs\": self._init_kwargs,\n            \"is_shared\": self._is_shared,\n        }\n\n    @override\n    def load_from_dict(self, state_dict: Dict[str, Any]) -&gt; None:\n        # Load the callback type from its fully qualified name\n        callback_type_name = state_dict[\"callback_type\"]\n        self._callback_type = load_qualified_class_or_func(callback_type_name)\n\n        # Load init_kwargs (default to empty dict if not present or None)\n        init_kwargs = state_dict.get(\"init_kwargs\")\n        self._init_kwargs = init_kwargs if init_kwargs is not None else {}\n\n        # Load is_shared (default to True if not present for backward compatibility)\n        self._is_shared = state_dict.get(\"is_shared\", True)\n\n        # Reset shared instance and lock (they will be recreated when needed)\n        self._shared_instance = None\n        self._shared_lock = Lock()\n</code></pre>"},{"location":"reference/bridgic-core/bridgic/core/automa/worker/#bridgic.core.automa.worker.WorkerCallbackBuilder.build","title":"build","text":"<pre><code>build() -&gt; T_WorkerCallback\n</code></pre> <p>Build and return an instance of the specified <code>WorkerCallback</code> subclass.</p> <p>Returns:</p> Type Description <code>T_WorkerCallback</code> <p>An instance of the <code>WorkerCallback</code> subclass specified during initialization.</p> Source code in <code>bridgic/core/automa/worker/_worker_callback.py</code> <pre><code>def build(self) -&gt; T_WorkerCallback:\n    \"\"\"\n    Build and return an instance of the specified `WorkerCallback` subclass.\n\n    Returns\n    -------\n    T_WorkerCallback\n        An instance of the `WorkerCallback` subclass specified during initialization.\n    \"\"\"\n    if self._is_shared:\n        if self._shared_instance is None:\n            with self._shared_lock:\n                if self._shared_instance is None:\n                    self._shared_instance = self._callback_type(**self._init_kwargs)\n        return self._shared_instance\n    else:\n        return self._callback_type(**self._init_kwargs)\n</code></pre>"},{"location":"reference/bridgic-core/bridgic/core/config/","title":"config","text":""},{"location":"reference/bridgic-core/bridgic/core/config/#config","title":"config","text":"<p>Configuration management module for Bridgic.</p>"},{"location":"reference/bridgic-core/bridgic/core/config/#bridgic.core.config.GlobalSetting","title":"GlobalSetting","text":"<p>               Bases: <code>BaseModel</code></p> <p>Global configuration settings for the Bridgic framework.</p> <p>This class implements a singleton pattern to provide centralized configuration that applies across all Automa instances. The main methods are:</p> <ul> <li><code>GlobalSetting.read()</code>: Get the singleton global setting instance.</li> <li><code>GlobalSetting.set()</code>: Set the specific fields of the global setting instance.</li> </ul> <p>Attributes:</p> Name Type Description <code>callback_builders</code> <code>List[WorkerCallbackBuilder]</code> <p>Callback builders that will be automatically applied to all workers across all Automa instances.</p> Source code in <code>bridgic/core/config/_global_setting.py</code> <pre><code>class GlobalSetting(BaseModel):\n    \"\"\"\n    Global configuration settings for the Bridgic framework.\n\n    This class implements a singleton pattern to provide centralized configuration\n    that applies across all Automa instances. The main methods are:\n\n    - `GlobalSetting.read()`: Get the singleton global setting instance.\n    - `GlobalSetting.set()`: Set the specific fields of the global setting instance.\n\n    Attributes\n    ----------\n    callback_builders : List[WorkerCallbackBuilder]\n        Callback builders that will be automatically applied to all workers\n        across all Automa instances.\n    \"\"\"\n    model_config = {\"arbitrary_types_allowed\": True}\n\n    callback_builders: List[\"WorkerCallbackBuilder\"] = []\n    \"\"\"Global callback builders that will be applied to all workers.\"\"\"\n\n    # Singleton instance\n    _instance: ClassVar[Optional[\"GlobalSetting\"]] = None\n    _lock: ClassVar[Lock] = Lock()\n\n    @classmethod\n    def read(cls) -&gt; \"GlobalSetting\":\n        \"\"\"\n        Get the singleton global setting instance.\n\n        Returns\n        -------\n        GlobalSetting\n            The singleton global setting instance.\n        \"\"\"\n        if cls._instance is None:\n            with cls._lock:\n                if cls._instance is None:\n                    cls._instance = cls()\n        return cls._instance\n\n    @classmethod\n    def set(\n        cls,\n        callback_builders: Optional[List[\"WorkerCallbackBuilder\"]] = None,\n    ) -&gt; None:\n        \"\"\"\n        Set global setting fields.\n\n        This method allows you to update specific fields of the global setting\n        without needing to create a complete GlobalSetting object.\n\n        Parameters\n        ----------\n        callback_builders : Optional[List[WorkerCallbackBuilder]], optional\n            Global callback builders that will be applied to all workers.\n            If None, the current callback_builders are not changed.\n        \"\"\"\n        instance = cls.read()\n        with cls._lock:\n            if callback_builders is not None:\n                instance.callback_builders = callback_builders\n\n    @classmethod\n    def add(cls, callback_builder: Optional[\"WorkerCallbackBuilder\"] = None) -&gt; None:\n        \"\"\"\n        Add new element to the existing field(s) of the `GlobalSetting`.\n\n        Parameters\n        ----------\n        callback_builder : Optional[WorkerCallbackBuilder]\n            The callback builder to add to the global setting callback builders. If None is passed in, nothing will be done.\n        \"\"\"\n        instance = cls.read()\n        with cls._lock:\n            if callback_builder is not None:\n                instance.callback_builders.append(callback_builder)\n</code></pre>"},{"location":"reference/bridgic-core/bridgic/core/config/#bridgic.core.config.GlobalSetting.callback_builders","title":"callback_builders  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>callback_builders: List[WorkerCallbackBuilder] = []\n</code></pre> <p>Global callback builders that will be applied to all workers.</p>"},{"location":"reference/bridgic-core/bridgic/core/config/#bridgic.core.config.GlobalSetting.read","title":"read","text":"<code>classmethod</code> <pre><code>read() -&gt; GlobalSetting\n</code></pre> <p>Get the singleton global setting instance.</p> <p>Returns:</p> Type Description <code>GlobalSetting</code> <p>The singleton global setting instance.</p> Source code in <code>bridgic/core/config/_global_setting.py</code> <pre><code>@classmethod\ndef read(cls) -&gt; \"GlobalSetting\":\n    \"\"\"\n    Get the singleton global setting instance.\n\n    Returns\n    -------\n    GlobalSetting\n        The singleton global setting instance.\n    \"\"\"\n    if cls._instance is None:\n        with cls._lock:\n            if cls._instance is None:\n                cls._instance = cls()\n    return cls._instance\n</code></pre>"},{"location":"reference/bridgic-core/bridgic/core/config/#bridgic.core.config.GlobalSetting.set","title":"set","text":"<code>classmethod</code> <pre><code>set(\n    callback_builders: Optional[\n        List[WorkerCallbackBuilder]\n    ] = None,\n) -&gt; None\n</code></pre> <p>Set global setting fields.</p> <p>This method allows you to update specific fields of the global setting without needing to create a complete GlobalSetting object.</p> <p>Parameters:</p> Name Type Description Default <code>callback_builders</code> <code>Optional[List[WorkerCallbackBuilder]]</code> <p>Global callback builders that will be applied to all workers. If None, the current callback_builders are not changed.</p> <code>None</code> Source code in <code>bridgic/core/config/_global_setting.py</code> <pre><code>@classmethod\ndef set(\n    cls,\n    callback_builders: Optional[List[\"WorkerCallbackBuilder\"]] = None,\n) -&gt; None:\n    \"\"\"\n    Set global setting fields.\n\n    This method allows you to update specific fields of the global setting\n    without needing to create a complete GlobalSetting object.\n\n    Parameters\n    ----------\n    callback_builders : Optional[List[WorkerCallbackBuilder]], optional\n        Global callback builders that will be applied to all workers.\n        If None, the current callback_builders are not changed.\n    \"\"\"\n    instance = cls.read()\n    with cls._lock:\n        if callback_builders is not None:\n            instance.callback_builders = callback_builders\n</code></pre>"},{"location":"reference/bridgic-core/bridgic/core/config/#bridgic.core.config.GlobalSetting.add","title":"add","text":"<code>classmethod</code> <pre><code>add(\n    callback_builder: Optional[\n        WorkerCallbackBuilder\n    ] = None,\n) -&gt; None\n</code></pre> <p>Add new element to the existing field(s) of the <code>GlobalSetting</code>.</p> <p>Parameters:</p> Name Type Description Default <code>callback_builder</code> <code>Optional[WorkerCallbackBuilder]</code> <p>The callback builder to add to the global setting callback builders. If None is passed in, nothing will be done.</p> <code>None</code> Source code in <code>bridgic/core/config/_global_setting.py</code> <pre><code>@classmethod\ndef add(cls, callback_builder: Optional[\"WorkerCallbackBuilder\"] = None) -&gt; None:\n    \"\"\"\n    Add new element to the existing field(s) of the `GlobalSetting`.\n\n    Parameters\n    ----------\n    callback_builder : Optional[WorkerCallbackBuilder]\n        The callback builder to add to the global setting callback builders. If None is passed in, nothing will be done.\n    \"\"\"\n    instance = cls.read()\n    with cls._lock:\n        if callback_builder is not None:\n            instance.callback_builders.append(callback_builder)\n</code></pre>"},{"location":"reference/bridgic-core/bridgic/core/config/#bridgic.core.config.HttpClientConfig","title":"HttpClientConfig","text":"<p>               Bases: <code>TypedDict</code></p> <p>Serializable configuration for creating an HTTP client.</p> <p>This configuration can be serialized and deserialized, allowing the framework to recreate HTTP clients after serialization.</p> <p>Attributes:</p> Name Type Description <code>headers</code> <code>Optional[Dict[str, str]]</code> <p>HTTP headers to include with all requests.</p> <code>timeout</code> <code>Optional[HttpClientTimeoutConfig]</code> <p>Timeout configuration for the HTTP client.</p> <code>auth</code> <code>Optional[HttpClientAuthConfig]</code> <p>Authentication configuration for the HTTP client.</p> Source code in <code>bridgic/core/config/_http_client_config.py</code> <pre><code>class HttpClientConfig(TypedDict, total=False):\n    \"\"\"\n    Serializable configuration for creating an HTTP client.\n\n    This configuration can be serialized and deserialized, allowing the framework\n    to recreate HTTP clients after serialization.\n\n    Attributes\n    ----------\n    headers : Optional[Dict[str, str]]\n        HTTP headers to include with all requests.\n    timeout : Optional[HttpClientTimeoutConfig]\n        Timeout configuration for the HTTP client.\n    auth : Optional[HttpClientAuthConfig]\n        Authentication configuration for the HTTP client.\n    \"\"\"\n    headers: Optional[Dict[str, str]]\n    timeout: Optional[HttpClientTimeoutConfig]\n    auth: Optional[HttpClientAuthConfig]\n</code></pre>"},{"location":"reference/bridgic-core/bridgic/core/config/#bridgic.core.config.HttpClientTimeoutConfig","title":"HttpClientTimeoutConfig","text":"<p>               Bases: <code>TypedDict</code></p> <p>Configuration for HTTP client timeout settings.</p> <p>Attributes:</p> Name Type Description <code>connect</code> <code>Optional[float]</code> <p>Timeout for establishing a connection (seconds).</p> <code>read</code> <code>Optional[float]</code> <p>Timeout for reading data (seconds).</p> <code>write</code> <code>Optional[float]</code> <p>Timeout for writing data (seconds).</p> <code>pool</code> <code>Optional[float]</code> <p>Timeout for acquiring a connection from the pool (seconds).</p> Source code in <code>bridgic/core/config/_http_client_config.py</code> <pre><code>class HttpClientTimeoutConfig(TypedDict, total=False):\n    \"\"\"\n    Configuration for HTTP client timeout settings.\n\n    Attributes\n    ----------\n    connect : Optional[float]\n        Timeout for establishing a connection (seconds).\n    read : Optional[float]\n        Timeout for reading data (seconds).\n    write : Optional[float]\n        Timeout for writing data (seconds).\n    pool : Optional[float]\n        Timeout for acquiring a connection from the pool (seconds).\n    \"\"\"\n    connect: Optional[float]\n    read: Optional[float]\n    write: Optional[float]\n    pool: Optional[float]\n</code></pre>"},{"location":"reference/bridgic-core/bridgic/core/config/#bridgic.core.config.HttpClientAuthConfig","title":"HttpClientAuthConfig","text":"<p>               Bases: <code>TypedDict</code></p> <p>Configuration for HTTP client authentication.</p> <p>Attributes:</p> Name Type Description <code>type</code> <code>Literal['basic', 'bearer']</code> <p>The type of authentication.</p> <code>username</code> <code>Optional[str]</code> <p>Username for basic auth (required if type is \"basic\").</p> <code>password</code> <code>Optional[str]</code> <p>Password for basic auth (required if type is \"basic\").</p> <code>token</code> <code>Optional[str]</code> <p>Bearer token (required if type is \"bearer\").</p> Source code in <code>bridgic/core/config/_http_client_config.py</code> <pre><code>class HttpClientAuthConfig(TypedDict, total=False):\n    \"\"\"\n    Configuration for HTTP client authentication.\n\n    Attributes\n    ----------\n    type : Literal[\"basic\", \"bearer\"]\n        The type of authentication.\n    username : Optional[str]\n        Username for basic auth (required if type is \"basic\").\n    password : Optional[str]\n        Password for basic auth (required if type is \"basic\").\n    token : Optional[str]\n        Bearer token (required if type is \"bearer\").\n    \"\"\"\n    type: Literal[\"basic\", \"bearer\"]\n    username: Optional[str]\n    password: Optional[str]\n    token: Optional[str]\n</code></pre>"},{"location":"reference/bridgic-core/bridgic/core/config/#bridgic.core.config.create_http_client_from_config","title":"create_http_client_from_config","text":"<pre><code>create_http_client_from_config(\n    config: Optional[HttpClientConfig],\n    is_async: bool = True,\n) -&gt; Optional[Union[AsyncClient, Client]]\n</code></pre> <p>Create an HTTP client (sync or async) from a serializable configuration.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>Optional[HttpClientConfig]</code> <p>The HTTP client configuration. If None, returns None.</p> required <code>is_async</code> <code>bool</code> <p>If True, creates an <code>httpx.AsyncClient</code>. If False, creates an <code>httpx.Client</code>. Defaults to True.</p> <code>True</code> <p>Returns:</p> Type Description <code>Optional[Union[AsyncClient, Client]]</code> <p>The created HTTP client, or None if config is None.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the auth configuration is invalid (missing required fields or unsupported type).</p> <p>Examples:</p> <p>Create async client with custom headers:</p> <pre><code>&gt;&gt;&gt; config = {\n...     \"headers\": {\"Authorization\": \"Bearer token123\"}\n... }\n&gt;&gt;&gt; client = create_http_client_from_config(config, is_async=True)\n</code></pre> <p>Create async client with custom timeout:</p> <pre><code>&gt;&gt;&gt; config = {\n...     \"timeout\": {\n...         \"connect\": 10.0,\n...         \"read\": 60.0\n...     }\n... }\n&gt;&gt;&gt; client = create_http_client_from_config(config, is_async=True)\n</code></pre> Source code in <code>bridgic/core/config/_http_client_config.py</code> <pre><code>def create_http_client_from_config(\n    config: Optional[HttpClientConfig],\n    is_async: bool = True,\n) -&gt; Optional[Union[httpx.AsyncClient, httpx.Client]]:\n    \"\"\"\n    Create an HTTP client (sync or async) from a serializable configuration.\n\n    Parameters\n    ----------\n    config : Optional[HttpClientConfig]\n        The HTTP client configuration. If None, returns None.\n    is_async : bool\n        If True, creates an `httpx.AsyncClient`. If False, creates an `httpx.Client`.\n        Defaults to True.\n\n    Returns\n    -------\n    Optional[Union[httpx.AsyncClient, httpx.Client]]\n        The created HTTP client, or None if config is None.\n\n    Raises\n    ------\n    ValueError\n        If the auth configuration is invalid (missing required fields or unsupported type).\n\n    Examples\n    --------\n    Create async client with custom headers:\n    &gt;&gt;&gt; config = {\n    ...     \"headers\": {\"Authorization\": \"Bearer token123\"}\n    ... }\n    &gt;&gt;&gt; client = create_http_client_from_config(config, is_async=True)\n\n    Create async client with custom timeout:\n    &gt;&gt;&gt; config = {\n    ...     \"timeout\": {\n    ...         \"connect\": 10.0,\n    ...         \"read\": 60.0\n    ...     }\n    ... }\n    &gt;&gt;&gt; client = create_http_client_from_config(config, is_async=True)\n    \"\"\"\n    # Return None when config is None.\n    if config is None:\n        return None\n\n    # Extract configuration\n    headers = config.get(\"headers\")\n    timeout_config = config.get(\"timeout\")\n    auth_config = config.get(\"auth\")\n\n    # Build timeout object if configured\n    timeout = None\n    if timeout_config:\n        timeout = httpx.Timeout(\n            connect=timeout_config.get(\"connect\"),\n            read=timeout_config.get(\"read\"),\n            write=timeout_config.get(\"write\"),\n            pool=timeout_config.get(\"pool\"),\n        )\n\n    # Build auth object if configured\n    auth = None\n    if auth_config:\n        auth_type = auth_config.get(\"type\")\n        if auth_type == \"basic\":\n            username = auth_config.get(\"username\")\n            password = auth_config.get(\"password\")\n            if username is None or password is None:\n                raise ValueError(\n                    \"Basic auth requires both 'username' and 'password' in http_client_config['auth'].\"\n                )\n            auth = httpx.BasicAuth(username=username, password=password)\n        elif auth_type == \"bearer\":\n            token = auth_config.get(\"token\")\n            if token is None:\n                raise ValueError(\n                    \"Bearer auth requires 'token' in http_client_config['auth'].\"\n                )\n            # Bearer token is typically passed via headers.\n            # So we add it to headers if not already present.\n            if headers is None:\n                headers = {}\n            if \"Authorization\" not in headers:\n                headers[\"Authorization\"] = f\"Bearer {token}\"\n        else:\n            raise ValueError(\n                f\"Unsupported auth type: {auth_type}. Supported types: 'basic', 'bearer'.\"\n            )\n\n    # Build client kwargs\n    client_kwargs: Dict[str, Any] = {\n        \"follow_redirects\": True,\n    }\n\n    if timeout is not None:\n        client_kwargs[\"timeout\"] = timeout\n\n    if headers is not None:\n        client_kwargs[\"headers\"] = headers\n\n    if auth is not None:\n        client_kwargs[\"auth\"] = auth\n\n    # Create the appropriate client type\n    if is_async:\n        return httpx.AsyncClient(**client_kwargs)\n    else:\n        return httpx.Client(**client_kwargs)\n</code></pre>"},{"location":"reference/bridgic-core/bridgic/core/model/","title":"model","text":""},{"location":"reference/bridgic-core/bridgic/core/model/#model","title":"model","text":"<p>The Model module provides core abstraction entities for LLMs (Large Language Models).</p> <p>This module defines core abstraction entities for interacting with models, providing  foundational type abstractions for different model implementations.</p>"},{"location":"reference/bridgic-core/bridgic/core/model/#bridgic.core.model.BaseLlm","title":"BaseLlm","text":"<p>               Bases: <code>ABC</code>, <code>Serializable</code></p> <p>Base class for Large Language Model implementations.</p> Source code in <code>bridgic/core/model/_base_llm.py</code> <pre><code>class BaseLlm(ABC, Serializable):\n    \"\"\"\n    Base class for Large Language Model implementations.\n    \"\"\"\n\n    @abstractmethod\n    def chat(self, messages: List[Message], **kwargs) -&gt; Response:\n        ...\n\n    @abstractmethod\n    def stream(self, messages: List[Message], **kwargs) -&gt; StreamResponse:\n        ...\n\n    @abstractmethod\n    async def achat(self, messages: List[Message], **kwargs) -&gt; Response:\n        ...\n\n    @abstractmethod\n    async def astream(self, messages: List[Message], **kwargs) -&gt; AsyncStreamResponse:\n        ...\n</code></pre>"},{"location":"reference/bridgic-core/bridgic/core/model/protocols/","title":"protocols","text":""},{"location":"reference/bridgic-core/bridgic/core/model/protocols/#protocols","title":"protocols","text":"<p>The Model Protocols module defines high-level interface protocols for model interaction.</p> <p>This module contains several important interface protocol definitions to provide  capabilities needed in real-world application development, such as tool selection and  structured output. These interfaces have clear input and output definitions and are  \"model-neutral\", aiming to reduce the details developers need to consider when  implementing features, thereby improving development efficiency.</p>"},{"location":"reference/bridgic-core/bridgic/core/model/protocols/#bridgic.core.model.protocols.Constraint","title":"Constraint  <code>module-attribute</code>","text":"<pre><code>Constraint = Union[\n    PydanticModel,\n    JsonSchema,\n    EbnfGrammar,\n    LarkGrammar,\n    Regex,\n    Choice,\n]\n</code></pre> <p>The constraint type for structured LLM output.</p>"},{"location":"reference/bridgic-core/bridgic/core/model/protocols/#bridgic.core.model.protocols.ToolSelection","title":"ToolSelection","text":"<p>               Bases: <code>Protocol</code></p> <p>Protocol for LLM providers that support tool selection and parameter determination.</p> <p>ToolSelection defines the interface for language models that can intelligently  select appropriate tools from a given tools and determine the specific parameters  needed for tool execution.</p> <p>Methods:</p> Name Description <code>select_tool</code> <p>Synchronous method for tool selection based on conversation context.</p> <code>aselect_tool</code> <p>Asynchronous method for tool selection based on conversation context.</p> Notes <ol> <li>Both synchronous and asynchronous methods must be implemented</li> <li>Tool selection should be based on conversation context and available tools</li> <li>Return value includes both selected tool calls and optional response text</li> </ol> Source code in <code>bridgic/core/model/protocols/_tool_selection.py</code> <pre><code>@runtime_checkable\nclass ToolSelection(Protocol):\n    \"\"\"\n    Protocol for LLM providers that support tool selection and parameter determination.\n\n    ToolSelection defines the interface for language models that can intelligently \n    select appropriate tools from a given tools and determine the specific parameters \n    needed for tool execution.\n\n    Methods\n    -------\n    select_tool\n        Synchronous method for tool selection based on conversation context.\n    aselect_tool\n        Asynchronous method for tool selection based on conversation context.\n\n    Notes\n    ----\n    1. Both synchronous and asynchronous methods must be implemented\n    2. Tool selection should be based on conversation context and available tools\n    3. Return value includes both selected tool calls and optional response text\n    \"\"\"\n\n    def select_tool(\n        self,\n        messages: List[Message],\n        tools: List[Tool],\n        **kwargs,\n    ) -&gt; Tuple[List[ToolCall], Optional[str]]:\n        \"\"\"\n        Select appropriate tools and determine their parameters based on conversation context.\n\n        Parameters\n        ----------\n        messages : List[Message]\n            The conversation history and current context.\n        tools : List[Tool]\n            Available tools that can be selected for use.\n        **kwargs\n            Additional keyword arguments for tool selection configuration.\n\n        Returns\n        -------\n        Tuple[List[ToolCall], Optional[str]]\n            A tuple containing:\n            - List of selected tool calls with determined parameters\n            - Optional response text from the LLM\n        \"\"\"\n        ...\n\n    async def aselect_tool(\n        self,\n        messages: List[Message],\n        tools: List[Tool],\n        **kwargs,\n    ) -&gt; Tuple[List[ToolCall], Optional[str]]:\n        \"\"\"\n        Asynchronously select appropriate tools and determine their parameters.\n\n        Parameters\n        ----------\n        messages : List[Message]\n            The conversation history and current context.\n        tools : List[Tool]\n            Available tools that can be selected for use.\n        **kwargs\n            Additional keyword arguments for tool selection configuration.\n\n        Returns\n        -------\n        Tuple[List[ToolCall], Optional[str]]\n            A tuple containing:\n            - List of selected tool calls with determined parameters\n            - Optional response text from the LLM\n        \"\"\"\n        ...\n</code></pre>"},{"location":"reference/bridgic-core/bridgic/core/model/protocols/#bridgic.core.model.protocols.ToolSelection.select_tool","title":"select_tool","text":"<pre><code>select_tool(\n    messages: List[Message], tools: List[Tool], **kwargs\n) -&gt; Tuple[List[ToolCall], Optional[str]]\n</code></pre> <p>Select appropriate tools and determine their parameters based on conversation context.</p> <p>Parameters:</p> Name Type Description Default <code>messages</code> <code>List[Message]</code> <p>The conversation history and current context.</p> required <code>tools</code> <code>List[Tool]</code> <p>Available tools that can be selected for use.</p> required <code>**kwargs</code> <p>Additional keyword arguments for tool selection configuration.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Tuple[List[ToolCall], Optional[str]]</code> <p>A tuple containing: - List of selected tool calls with determined parameters - Optional response text from the LLM</p> Source code in <code>bridgic/core/model/protocols/_tool_selection.py</code> <pre><code>def select_tool(\n    self,\n    messages: List[Message],\n    tools: List[Tool],\n    **kwargs,\n) -&gt; Tuple[List[ToolCall], Optional[str]]:\n    \"\"\"\n    Select appropriate tools and determine their parameters based on conversation context.\n\n    Parameters\n    ----------\n    messages : List[Message]\n        The conversation history and current context.\n    tools : List[Tool]\n        Available tools that can be selected for use.\n    **kwargs\n        Additional keyword arguments for tool selection configuration.\n\n    Returns\n    -------\n    Tuple[List[ToolCall], Optional[str]]\n        A tuple containing:\n        - List of selected tool calls with determined parameters\n        - Optional response text from the LLM\n    \"\"\"\n    ...\n</code></pre>"},{"location":"reference/bridgic-core/bridgic/core/model/protocols/#bridgic.core.model.protocols.ToolSelection.aselect_tool","title":"aselect_tool","text":"<code>async</code> <pre><code>aselect_tool(\n    messages: List[Message], tools: List[Tool], **kwargs\n) -&gt; Tuple[List[ToolCall], Optional[str]]\n</code></pre> <p>Asynchronously select appropriate tools and determine their parameters.</p> <p>Parameters:</p> Name Type Description Default <code>messages</code> <code>List[Message]</code> <p>The conversation history and current context.</p> required <code>tools</code> <code>List[Tool]</code> <p>Available tools that can be selected for use.</p> required <code>**kwargs</code> <p>Additional keyword arguments for tool selection configuration.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Tuple[List[ToolCall], Optional[str]]</code> <p>A tuple containing: - List of selected tool calls with determined parameters - Optional response text from the LLM</p> Source code in <code>bridgic/core/model/protocols/_tool_selection.py</code> <pre><code>async def aselect_tool(\n    self,\n    messages: List[Message],\n    tools: List[Tool],\n    **kwargs,\n) -&gt; Tuple[List[ToolCall], Optional[str]]:\n    \"\"\"\n    Asynchronously select appropriate tools and determine their parameters.\n\n    Parameters\n    ----------\n    messages : List[Message]\n        The conversation history and current context.\n    tools : List[Tool]\n        Available tools that can be selected for use.\n    **kwargs\n        Additional keyword arguments for tool selection configuration.\n\n    Returns\n    -------\n    Tuple[List[ToolCall], Optional[str]]\n        A tuple containing:\n        - List of selected tool calls with determined parameters\n        - Optional response text from the LLM\n    \"\"\"\n    ...\n</code></pre>"},{"location":"reference/bridgic-core/bridgic/core/model/protocols/#bridgic.core.model.protocols.PydanticModel","title":"PydanticModel","text":"<p>               Bases: <code>BaseModel</code></p> <p>A constraint defined as a Pydantic model for structured LLM output.</p> Source code in <code>bridgic/core/model/protocols/_structured_output.py</code> <pre><code>class PydanticModel(BaseModel):\n    \"\"\"\n    A constraint defined as a Pydantic model for structured LLM output.\n    \"\"\"\n    constraint_type: Literal[\"pydantic_model\"] = \"pydantic_model\"\n    \"\"\"The type of the constraint, in this case `pydantic_model`.\"\"\"\n    model: Type[BaseModel] = Field(..., description=\"Model type of the PydanticModel constraint.\")\n    \"\"\"The Pydantic model type of the constraint.\"\"\"\n</code></pre>"},{"location":"reference/bridgic-core/bridgic/core/model/protocols/#bridgic.core.model.protocols.PydanticModel.constraint_type","title":"constraint_type  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>constraint_type: Literal[\"pydantic_model\"] = (\n    \"pydantic_model\"\n)\n</code></pre> <p>The type of the constraint, in this case <code>pydantic_model</code>.</p>"},{"location":"reference/bridgic-core/bridgic/core/model/protocols/#bridgic.core.model.protocols.PydanticModel.model","title":"model  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>model: Type[BaseModel] = Field(\n    ...,\n    description=\"Model type of the PydanticModel constraint.\",\n)\n</code></pre> <p>The Pydantic model type of the constraint.</p>"},{"location":"reference/bridgic-core/bridgic/core/model/protocols/#bridgic.core.model.protocols.JsonSchema","title":"JsonSchema","text":"<p>               Bases: <code>BaseModel</code></p> <p>A constraint defined as a JSON schema for structured LLM output.</p> Source code in <code>bridgic/core/model/protocols/_structured_output.py</code> <pre><code>class JsonSchema(BaseModel):\n    \"\"\"\n    A constraint defined as a JSON schema for structured LLM output.\n    \"\"\"\n    constraint_type: Literal[\"json_schema\"] = \"json_schema\"\n    \"\"\"The type of the constraint, in this case `json_schema`.\"\"\"\n    schema_dict: Dict[str, Any] = Field(..., description=\"Schema of the JsonSchema constraint.\")\n    \"\"\"The JSON schema of the constraint.\"\"\"\n</code></pre>"},{"location":"reference/bridgic-core/bridgic/core/model/protocols/#bridgic.core.model.protocols.JsonSchema.constraint_type","title":"constraint_type  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>constraint_type: Literal['json_schema'] = 'json_schema'\n</code></pre> <p>The type of the constraint, in this case <code>json_schema</code>.</p>"},{"location":"reference/bridgic-core/bridgic/core/model/protocols/#bridgic.core.model.protocols.JsonSchema.schema_dict","title":"schema_dict  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>schema_dict: Dict[str, Any] = Field(\n    ..., description=\"Schema of the JsonSchema constraint.\"\n)\n</code></pre> <p>The JSON schema of the constraint.</p>"},{"location":"reference/bridgic-core/bridgic/core/model/protocols/#bridgic.core.model.protocols.EbnfGrammar","title":"EbnfGrammar","text":"<p>               Bases: <code>BaseModel</code></p> <p>A constraint defined as an EBNF grammar for structured LLM output.</p> Source code in <code>bridgic/core/model/protocols/_structured_output.py</code> <pre><code>class EbnfGrammar(BaseModel):\n    \"\"\"\n    A constraint defined as an EBNF grammar for structured LLM output.\n    \"\"\"\n    constraint_type: Literal[\"ebnf_grammar\"] = \"ebnf_grammar\"\n    \"\"\"The type of the constraint, in this case `ebnf_grammar`.\"\"\"\n    syntax: str = Field(..., description=\"Syntax of the EBNF grammar constraint.\")\n    \"\"\"The syntax of the EBNF grammar constraint.\"\"\"\n</code></pre>"},{"location":"reference/bridgic-core/bridgic/core/model/protocols/#bridgic.core.model.protocols.EbnfGrammar.constraint_type","title":"constraint_type  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>constraint_type: Literal['ebnf_grammar'] = 'ebnf_grammar'\n</code></pre> <p>The type of the constraint, in this case <code>ebnf_grammar</code>.</p>"},{"location":"reference/bridgic-core/bridgic/core/model/protocols/#bridgic.core.model.protocols.EbnfGrammar.syntax","title":"syntax  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>syntax: str = Field(\n    ...,\n    description=\"Syntax of the EBNF grammar constraint.\",\n)\n</code></pre> <p>The syntax of the EBNF grammar constraint.</p>"},{"location":"reference/bridgic-core/bridgic/core/model/protocols/#bridgic.core.model.protocols.LarkGrammar","title":"LarkGrammar","text":"<p>               Bases: <code>BaseModel</code></p> <p>A constraint defined as a Lark grammar for structured LLM output.</p> Source code in <code>bridgic/core/model/protocols/_structured_output.py</code> <pre><code>class LarkGrammar(BaseModel):\n    \"\"\"\n    A constraint defined as a Lark grammar for structured LLM output.\n    \"\"\"\n    constraint_type: Literal[\"lark_grammar\"] = \"lark_grammar\"\n    \"\"\"The type of the constraint, in this case `lark_grammar`.\"\"\"\n    syntax: str = Field(..., description=\"Syntax of the Lark grammar constraint.\")\n    \"\"\"The syntax of the Lark grammar constraint.\"\"\"\n</code></pre>"},{"location":"reference/bridgic-core/bridgic/core/model/protocols/#bridgic.core.model.protocols.LarkGrammar.constraint_type","title":"constraint_type  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>constraint_type: Literal['lark_grammar'] = 'lark_grammar'\n</code></pre> <p>The type of the constraint, in this case <code>lark_grammar</code>.</p>"},{"location":"reference/bridgic-core/bridgic/core/model/protocols/#bridgic.core.model.protocols.LarkGrammar.syntax","title":"syntax  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>syntax: str = Field(\n    ...,\n    description=\"Syntax of the Lark grammar constraint.\",\n)\n</code></pre> <p>The syntax of the Lark grammar constraint.</p>"},{"location":"reference/bridgic-core/bridgic/core/model/protocols/#bridgic.core.model.protocols.Regex","title":"Regex","text":"<p>               Bases: <code>BaseModel</code></p> <p>A constraint defined as a regular expression for structured LLM output.</p> Source code in <code>bridgic/core/model/protocols/_structured_output.py</code> <pre><code>class Regex(BaseModel):\n    \"\"\"\n    A constraint defined as a regular expression for structured LLM output.\n    \"\"\"\n    constraint_type: Literal[\"regex\"] = \"regex\"\n    \"\"\"The type of the constraint, in this case `regex`.\"\"\"\n    pattern: str = Field(..., description=\"Pattern of the Regex constraint.\")\n    \"\"\"The regular expression of the constraint.\"\"\"\n</code></pre>"},{"location":"reference/bridgic-core/bridgic/core/model/protocols/#bridgic.core.model.protocols.Regex.constraint_type","title":"constraint_type  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>constraint_type: Literal['regex'] = 'regex'\n</code></pre> <p>The type of the constraint, in this case <code>regex</code>.</p>"},{"location":"reference/bridgic-core/bridgic/core/model/protocols/#bridgic.core.model.protocols.Regex.pattern","title":"pattern  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>pattern: str = Field(\n    ..., description=\"Pattern of the Regex constraint.\"\n)\n</code></pre> <p>The regular expression of the constraint.</p>"},{"location":"reference/bridgic-core/bridgic/core/model/protocols/#bridgic.core.model.protocols.Choice","title":"Choice","text":"<p>               Bases: <code>BaseModel</code></p> <p>A constraint defined as a predefined set of choices for structured LLM output.</p> Source code in <code>bridgic/core/model/protocols/_structured_output.py</code> <pre><code>class Choice(BaseModel):\n    \"\"\"\n    A constraint defined as a predefined set of choices for structured LLM output.\n    \"\"\"\n    constraint_type: Literal[\"choice\"] = \"choice\"\n    \"\"\"The type of the constraint, in this case `choice`.\"\"\"\n    choices: List[str] = Field(..., description=\"Choices of the choice constraint.\")\n    \"\"\"The choices of the constraint.\"\"\"\n</code></pre>"},{"location":"reference/bridgic-core/bridgic/core/model/protocols/#bridgic.core.model.protocols.Choice.constraint_type","title":"constraint_type  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>constraint_type: Literal['choice'] = 'choice'\n</code></pre> <p>The type of the constraint, in this case <code>choice</code>.</p>"},{"location":"reference/bridgic-core/bridgic/core/model/protocols/#bridgic.core.model.protocols.Choice.choices","title":"choices  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>choices: List[str] = Field(\n    ..., description=\"Choices of the choice constraint.\"\n)\n</code></pre> <p>The choices of the constraint.</p>"},{"location":"reference/bridgic-core/bridgic/core/model/protocols/#bridgic.core.model.protocols.RegexPattern","title":"RegexPattern","text":"<p>Constants that define some common regular expressions for structured LLM output.</p> Source code in <code>bridgic/core/model/protocols/_structured_output.py</code> <pre><code>class RegexPattern:\n    \"\"\"Constants that define some common regular expressions for structured LLM output.\"\"\"\n    INTEGER: ClassVar[Regex] = Regex(pattern=r\"-?\\d+\")\n    \"\"\"A regular expression for integers.\"\"\"\n    FLOAT = Regex(pattern=r\"-?(?:\\d+\\.\\d+|\\d+\\.|\\.\\d+|\\d+)([eE][-+]?\\d+)?\")\n    \"\"\"A regular expression for floats.\"\"\"\n    DATE: ClassVar[Regex] = Regex(pattern=r\"\\d{4}-\\d{2}-\\d{2}\")\n    \"\"\"A regular expression for dates.\"\"\"\n    TIME: ClassVar[Regex] = Regex(pattern=r\"(?:[01]\\d|2[0-3]):[0-5]\\d:[0-5]\\d(?:\\.\\d+)?\")\n    \"\"\"A regular expression for times.\"\"\"\n    DATE_TIME_ISO_8601: ClassVar[Regex] = Regex(pattern=rf\"{DATE.pattern}T{TIME.pattern}(?:Z|[+-](?:[01]\\d|2[0-3]):[0-5]\\d)?\")\n    \"\"\"A regular expression for date-time in ISO 8601 format.\"\"\"\n    IP_V4_ADDRESS: ClassVar[Regex] = Regex(pattern=r\"(?:(?:25[0-5]|2[0-4]\\d|1\\d{2}|[1-9]?\\d)\\.){3}(?:25[0-5]|2[0-4]\\d|1\\d{2}|[1-9]?\\d)\")\n    \"\"\"A regular expression for IPv4 addresses.\"\"\"\n    IP_V6_ADDRESS: ClassVar[Regex] = Regex(pattern=r\"([0-9a-fA-F]{1,4}:){7}[0-9a-fA-F]{1,4}\")\n    \"\"\"A regular expression for IPv6 addresses.\"\"\"\n    EMAIL: ClassVar[Regex] = Regex(pattern=r\"[a-zA-Z0-9_.+-]+@[a-zA-Z0-9-]+\\.[a-zA-Z0-9-.]+\")\n    \"\"\"A regular expression for email addresses.\"\"\"\n</code></pre>"},{"location":"reference/bridgic-core/bridgic/core/model/protocols/#bridgic.core.model.protocols.RegexPattern.INTEGER","title":"INTEGER  <code>class-attribute</code>","text":"<pre><code>INTEGER: Regex = Regex(pattern='-?\\\\d+')\n</code></pre> <p>A regular expression for integers.</p>"},{"location":"reference/bridgic-core/bridgic/core/model/protocols/#bridgic.core.model.protocols.RegexPattern.FLOAT","title":"FLOAT  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>FLOAT = Regex(\n    pattern=\"-?(?:\\\\d+\\\\.\\\\d+|\\\\d+\\\\.|\\\\.\\\\d+|\\\\d+)([eE][-+]?\\\\d+)?\"\n)\n</code></pre> <p>A regular expression for floats.</p>"},{"location":"reference/bridgic-core/bridgic/core/model/protocols/#bridgic.core.model.protocols.RegexPattern.DATE","title":"DATE  <code>class-attribute</code>","text":"<pre><code>DATE: Regex = Regex(pattern='\\\\d{4}-\\\\d{2}-\\\\d{2}')\n</code></pre> <p>A regular expression for dates.</p>"},{"location":"reference/bridgic-core/bridgic/core/model/protocols/#bridgic.core.model.protocols.RegexPattern.TIME","title":"TIME  <code>class-attribute</code>","text":"<pre><code>TIME: Regex = Regex(\n    pattern=\"(?:[01]\\\\d|2[0-3]):[0-5]\\\\d:[0-5]\\\\d(?:\\\\.\\\\d+)?\"\n)\n</code></pre> <p>A regular expression for times.</p>"},{"location":"reference/bridgic-core/bridgic/core/model/protocols/#bridgic.core.model.protocols.RegexPattern.DATE_TIME_ISO_8601","title":"DATE_TIME_ISO_8601  <code>class-attribute</code>","text":"<pre><code>DATE_TIME_ISO_8601: Regex = Regex(\n    pattern=f\"{pattern}T{pattern}(?:Z|[+-](?:[01]\\d|2[0-3]):[0-5]\\d)?\"\n)\n</code></pre> <p>A regular expression for date-time in ISO 8601 format.</p>"},{"location":"reference/bridgic-core/bridgic/core/model/protocols/#bridgic.core.model.protocols.RegexPattern.IP_V4_ADDRESS","title":"IP_V4_ADDRESS  <code>class-attribute</code>","text":"<pre><code>IP_V4_ADDRESS: Regex = Regex(\n    pattern=\"(?:(?:25[0-5]|2[0-4]\\\\d|1\\\\d{2}|[1-9]?\\\\d)\\\\.){3}(?:25[0-5]|2[0-4]\\\\d|1\\\\d{2}|[1-9]?\\\\d)\"\n)\n</code></pre> <p>A regular expression for IPv4 addresses.</p>"},{"location":"reference/bridgic-core/bridgic/core/model/protocols/#bridgic.core.model.protocols.RegexPattern.IP_V6_ADDRESS","title":"IP_V6_ADDRESS  <code>class-attribute</code>","text":"<pre><code>IP_V6_ADDRESS: Regex = Regex(\n    pattern=\"([0-9a-fA-F]{1,4}:){7}[0-9a-fA-F]{1,4}\"\n)\n</code></pre> <p>A regular expression for IPv6 addresses.</p>"},{"location":"reference/bridgic-core/bridgic/core/model/protocols/#bridgic.core.model.protocols.RegexPattern.EMAIL","title":"EMAIL  <code>class-attribute</code>","text":"<pre><code>EMAIL: Regex = Regex(\n    pattern=\"[a-zA-Z0-9_.+-]+@[a-zA-Z0-9-]+\\\\.[a-zA-Z0-9-.]+\"\n)\n</code></pre> <p>A regular expression for email addresses.</p>"},{"location":"reference/bridgic-core/bridgic/core/model/protocols/#bridgic.core.model.protocols.StructuredOutput","title":"StructuredOutput","text":"<p>               Bases: <code>Protocol</code></p> <p>Protocol for LLM providers that support structured output generation.</p> <p>StructuredOutput defines the interface for language models that can generate  responses in specific formats according to given constraints. This protocol  enables controlled output generation for various data structures and formats.</p> <p>Methods:</p> Name Description <code>structured_output</code> <p>Synchronous method for generating structured output based on constraints.</p> <code>astructured_output</code> <p>Asynchronous method for generating structured output based on constraints.</p> Notes <ol> <li>Both synchronous and asynchronous methods must be implemented</li> <li>Supported constraint types depend on the specific LLM provider implementation</li> <li>Output format is determined by the constraint type provided</li> <li>Common constraint types include PydanticModel, JsonSchema, Regex, Choice, etc.</li> </ol> Source code in <code>bridgic/core/model/protocols/_structured_output.py</code> <pre><code>@runtime_checkable\nclass StructuredOutput(Protocol):\n    \"\"\"\n    Protocol for LLM providers that support structured output generation.\n\n    StructuredOutput defines the interface for language models that can generate \n    responses in specific formats according to given constraints. This protocol \n    enables controlled output generation for various data structures and formats.\n\n    Methods\n    -------\n    structured_output\n        Synchronous method for generating structured output based on constraints.\n    astructured_output\n        Asynchronous method for generating structured output based on constraints.\n\n    Notes\n    ----\n    1. Both synchronous and asynchronous methods must be implemented\n    2. Supported constraint types depend on the specific LLM provider implementation\n    3. Output format is determined by the constraint type provided\n    4. Common constraint types include PydanticModel, JsonSchema, Regex, Choice, etc.\n    \"\"\"\n\n    def structured_output(\n        self,\n        messages: List[Message],\n        constraint: Constraint,\n        **kwargs,\n    ) -&gt; Any:\n        \"\"\"\n        Generate structured output based on conversation context and constraints.\n\n        Parameters\n        ----------\n        messages : List[Message]\n            The conversation history and current context.\n        constraint : Constraint\n            The output format constraint. Supported types:\n\n            - PydanticModel: Output as Pydantic model instance\n            - JsonSchema: Output as JSON matching the schema\n            - Regex: Output matching the regex pattern\n            - Choice: Output from predefined choices\n            - EbnfGrammar: Output following EBNF grammar rules\n            - LarkGrammar: Output following Lark grammar rules\n        **kwargs\n            Additional keyword arguments for output generation configuration.\n\n        Returns\n        -------\n        Any\n            The structured output matching the specified constraint format.\n        \"\"\"\n        ...\n\n    async def astructured_output(\n        self,\n        messages: List[Message],\n        constraint: Constraint,\n        **kwargs,\n    ) -&gt; Any:\n        \"\"\"\n        Asynchronously generate structured output based on conversation context and constraints.\n\n        Parameters\n        ----------\n        messages : List[Message]\n            The conversation history and current context.\n        constraint : Constraint\n            The output format constraint. Supported types:\n\n            - PydanticModel: Output as Pydantic model instance\n            - JsonSchema: Output as JSON matching the schema\n            - Regex: Output matching the regex pattern\n            - Choice: Output from predefined choices\n            - EbnfGrammar: Output following EBNF grammar rules\n            - LarkGrammar: Output following Lark grammar rules\n        **kwargs\n            Additional keyword arguments for output generation configuration.\n\n        Returns\n        -------\n        Any\n            The structured output matching the specified constraint format.\n        \"\"\"\n        ...\n</code></pre>"},{"location":"reference/bridgic-core/bridgic/core/model/protocols/#bridgic.core.model.protocols.StructuredOutput.structured_output","title":"structured_output","text":"<pre><code>structured_output(\n    messages: List[Message],\n    constraint: Constraint,\n    **kwargs\n) -&gt; Any\n</code></pre> <p>Generate structured output based on conversation context and constraints.</p> <p>Parameters:</p> Name Type Description Default <code>messages</code> <code>List[Message]</code> <p>The conversation history and current context.</p> required <code>constraint</code> <code>Constraint</code> <p>The output format constraint. Supported types:</p> <ul> <li>PydanticModel: Output as Pydantic model instance</li> <li>JsonSchema: Output as JSON matching the schema</li> <li>Regex: Output matching the regex pattern</li> <li>Choice: Output from predefined choices</li> <li>EbnfGrammar: Output following EBNF grammar rules</li> <li>LarkGrammar: Output following Lark grammar rules</li> </ul> required <code>**kwargs</code> <p>Additional keyword arguments for output generation configuration.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Any</code> <p>The structured output matching the specified constraint format.</p> Source code in <code>bridgic/core/model/protocols/_structured_output.py</code> <pre><code>def structured_output(\n    self,\n    messages: List[Message],\n    constraint: Constraint,\n    **kwargs,\n) -&gt; Any:\n    \"\"\"\n    Generate structured output based on conversation context and constraints.\n\n    Parameters\n    ----------\n    messages : List[Message]\n        The conversation history and current context.\n    constraint : Constraint\n        The output format constraint. Supported types:\n\n        - PydanticModel: Output as Pydantic model instance\n        - JsonSchema: Output as JSON matching the schema\n        - Regex: Output matching the regex pattern\n        - Choice: Output from predefined choices\n        - EbnfGrammar: Output following EBNF grammar rules\n        - LarkGrammar: Output following Lark grammar rules\n    **kwargs\n        Additional keyword arguments for output generation configuration.\n\n    Returns\n    -------\n    Any\n        The structured output matching the specified constraint format.\n    \"\"\"\n    ...\n</code></pre>"},{"location":"reference/bridgic-core/bridgic/core/model/protocols/#bridgic.core.model.protocols.StructuredOutput.astructured_output","title":"astructured_output","text":"<code>async</code> <pre><code>astructured_output(\n    messages: List[Message],\n    constraint: Constraint,\n    **kwargs\n) -&gt; Any\n</code></pre> <p>Asynchronously generate structured output based on conversation context and constraints.</p> <p>Parameters:</p> Name Type Description Default <code>messages</code> <code>List[Message]</code> <p>The conversation history and current context.</p> required <code>constraint</code> <code>Constraint</code> <p>The output format constraint. Supported types:</p> <ul> <li>PydanticModel: Output as Pydantic model instance</li> <li>JsonSchema: Output as JSON matching the schema</li> <li>Regex: Output matching the regex pattern</li> <li>Choice: Output from predefined choices</li> <li>EbnfGrammar: Output following EBNF grammar rules</li> <li>LarkGrammar: Output following Lark grammar rules</li> </ul> required <code>**kwargs</code> <p>Additional keyword arguments for output generation configuration.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Any</code> <p>The structured output matching the specified constraint format.</p> Source code in <code>bridgic/core/model/protocols/_structured_output.py</code> <pre><code>async def astructured_output(\n    self,\n    messages: List[Message],\n    constraint: Constraint,\n    **kwargs,\n) -&gt; Any:\n    \"\"\"\n    Asynchronously generate structured output based on conversation context and constraints.\n\n    Parameters\n    ----------\n    messages : List[Message]\n        The conversation history and current context.\n    constraint : Constraint\n        The output format constraint. Supported types:\n\n        - PydanticModel: Output as Pydantic model instance\n        - JsonSchema: Output as JSON matching the schema\n        - Regex: Output matching the regex pattern\n        - Choice: Output from predefined choices\n        - EbnfGrammar: Output following EBNF grammar rules\n        - LarkGrammar: Output following Lark grammar rules\n    **kwargs\n        Additional keyword arguments for output generation configuration.\n\n    Returns\n    -------\n    Any\n        The structured output matching the specified constraint format.\n    \"\"\"\n    ...\n</code></pre>"},{"location":"reference/bridgic-core/bridgic/core/model/types/","title":"types","text":""},{"location":"reference/bridgic-core/bridgic/core/model/types/#types","title":"types","text":"<p>The Model Types module defines core data types for interacting with models.</p> <p>This module contains type definitions for messages, content blocks, tool calls,  responses, and more, providing a unified data structure representation for model  input and output.</p>"},{"location":"reference/bridgic-core/bridgic/core/model/types/#bridgic.core.model.types.Tool","title":"Tool","text":"<p>               Bases: <code>BaseModel</code></p> <p>A description of a tool that can be used by the LLM. Generally, a list of tools is provided  to the LLM for selection.</p> Source code in <code>bridgic/core/model/types/_tool_use.py</code> <pre><code>class Tool(BaseModel):\n    \"\"\"\n    A description of a tool that can be used by the LLM. Generally, a list of tools is provided \n    to the LLM for selection.\n    \"\"\"\n\n    name: str = Field(..., description=\"Name of the tool.\")\n    \"\"\"The name of the tool to be called.\"\"\"\n    description: str = Field(..., description=\"Description of the tool.\")\n    \"\"\"\n    A description of what the tool does, used by the LLM to choose when and how to call the tool.\n    \"\"\"\n    parameters: Dict[str, Any] = Field(..., description=\"JSON schema object that describes the parameters of the tool.\")\n    \"\"\"\n    A JSON schema dictionary that describes the parameters of the tool.\n    \"\"\"\n</code></pre>"},{"location":"reference/bridgic-core/bridgic/core/model/types/#bridgic.core.model.types.Tool.name","title":"name  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>name: str = Field(..., description='Name of the tool.')\n</code></pre> <p>The name of the tool to be called.</p>"},{"location":"reference/bridgic-core/bridgic/core/model/types/#bridgic.core.model.types.Tool.description","title":"description  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>description: str = Field(\n    ..., description=\"Description of the tool.\"\n)\n</code></pre> <p>A description of what the tool does, used by the LLM to choose when and how to call the tool.</p>"},{"location":"reference/bridgic-core/bridgic/core/model/types/#bridgic.core.model.types.Tool.parameters","title":"parameters  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>parameters: Dict[str, Any] = Field(\n    ...,\n    description=\"JSON schema object that describes the parameters of the tool.\",\n)\n</code></pre> <p>A JSON schema dictionary that describes the parameters of the tool.</p>"},{"location":"reference/bridgic-core/bridgic/core/model/types/#bridgic.core.model.types.ToolCall","title":"ToolCall","text":"<p>               Bases: <code>BaseModel</code></p> <p>A description of a tool call that is generated by the LLM. The LLM can return one or multiple  tool calls at once.</p> Source code in <code>bridgic/core/model/types/_tool_use.py</code> <pre><code>class ToolCall(BaseModel):\n    \"\"\"\n    A description of a tool call that is generated by the LLM. The LLM can return one or multiple \n    tool calls at once.\n    \"\"\"\n\n    id: Optional[str] = Field(..., description=\"ID of the tool call.\")\n    \"\"\"The ID of the tool call.\"\"\"\n    name: str = Field(..., description=\"Name of the tool.\")\n    \"\"\"The name of the tool to be called.\"\"\"\n    arguments: Dict[str, Any] = Field(..., default_factory=dict, description=\"Arguments that are used to call the tool.\")\n    \"\"\"The arguments to call the tool with, as generated by the LLM in JSON format. \"\"\"\n</code></pre>"},{"location":"reference/bridgic-core/bridgic/core/model/types/#bridgic.core.model.types.ToolCall.id","title":"id  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>id: Optional[str] = Field(\n    ..., description=\"ID of the tool call.\"\n)\n</code></pre> <p>The ID of the tool call.</p>"},{"location":"reference/bridgic-core/bridgic/core/model/types/#bridgic.core.model.types.ToolCall.name","title":"name  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>name: str = Field(..., description='Name of the tool.')\n</code></pre> <p>The name of the tool to be called.</p>"},{"location":"reference/bridgic-core/bridgic/core/model/types/#bridgic.core.model.types.ToolCall.arguments","title":"arguments  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>arguments: Dict[str, Any] = Field(\n    ...,\n    default_factory=dict,\n    description=\"Arguments that are used to call the tool.\",\n)\n</code></pre> <p>The arguments to call the tool with, as generated by the LLM in JSON format.</p>"},{"location":"reference/bridgic-core/bridgic/core/model/types/#bridgic.core.model.types.ToolCallDict","title":"ToolCallDict","text":"<p>               Bases: <code>TypedDict</code></p> <p>A dictionary that describes a tool call that is generated by the LLM. The LLM can return one or multiple  tool calls at once. This <code>ToolCallDict</code> is another format of <code>ToolCall</code> that may be easier for developers to use.</p> Source code in <code>bridgic/core/model/types/_tool_use.py</code> <pre><code>class ToolCallDict(TypedDict):\n    \"\"\"\n    A dictionary that describes a tool call that is generated by the LLM. The LLM can return one or multiple \n    tool calls at once. This `ToolCallDict` is another format of `ToolCall` that may be easier for developers to use.\n    \"\"\"\n\n    id: str\n    \"\"\"The ID of the tool call.\"\"\"\n    name: str\n    \"\"\"The name of the tool to be called.\"\"\"\n    arguments: Dict[str, Any]\n    \"\"\"The arguments to call the tool with, as generated by the LLM in JSON format. \"\"\"\n</code></pre>"},{"location":"reference/bridgic-core/bridgic/core/model/types/#bridgic.core.model.types.ToolCallDict.id","title":"id  <code>instance-attribute</code>","text":"<pre><code>id: str\n</code></pre> <p>The ID of the tool call.</p>"},{"location":"reference/bridgic-core/bridgic/core/model/types/#bridgic.core.model.types.ToolCallDict.name","title":"name  <code>instance-attribute</code>","text":"<pre><code>name: str\n</code></pre> <p>The name of the tool to be called.</p>"},{"location":"reference/bridgic-core/bridgic/core/model/types/#bridgic.core.model.types.ToolCallDict.arguments","title":"arguments  <code>instance-attribute</code>","text":"<pre><code>arguments: Dict[str, Any]\n</code></pre> <p>The arguments to call the tool with, as generated by the LLM in JSON format.</p>"},{"location":"reference/bridgic-core/bridgic/core/model/types/#bridgic.core.model.types.TextBlock","title":"TextBlock","text":"<p>               Bases: <code>BaseModel</code></p> <p>Encapsulates plain text data that is passed to or received from language models.</p> <p>Attributes:</p> Name Type Description <code>block_type</code> <code>Literal['text']</code> <p>The type identifier for this content block.</p> <code>text</code> <code>str</code> <p>The actual text content.</p> Source code in <code>bridgic/core/model/types/_content_block.py</code> <pre><code>class TextBlock(BaseModel):\n    \"\"\"\n    Encapsulates plain text data that is passed to or received from language models.\n\n    Attributes\n    ----------\n    block_type : Literal[\"text\"]\n        The type identifier for this content block.\n    text : str\n        The actual text content.\n    \"\"\"\n    block_type: Literal[\"text\"] = Field(default=\"text\")\n    text: str\n</code></pre>"},{"location":"reference/bridgic-core/bridgic/core/model/types/#bridgic.core.model.types.ToolCallBlock","title":"ToolCallBlock","text":"<p>               Bases: <code>BaseModel</code></p> <p>Encapsulates tool invocation data that is received from language models.</p> <p>Attributes:</p> Name Type Description <code>block_type</code> <code>Literal['tool_call']</code> <p>The type identifier for this content block.</p> <code>id</code> <code>str</code> <p>Unique identifier for the tool call instance.</p> <code>name</code> <code>str</code> <p>Name of the tool to be called.</p> <code>arguments</code> <code>Dict[str, Any]</code> <p>Parameters to be passed to the tool function.</p> Source code in <code>bridgic/core/model/types/_content_block.py</code> <pre><code>class ToolCallBlock(BaseModel):\n    \"\"\"\n    Encapsulates tool invocation data that is received from language models.\n\n    Attributes\n    ----------\n    block_type : Literal[\"tool_call\"]\n        The type identifier for this content block.\n    id : str\n        Unique identifier for the tool call instance.\n    name : str\n        Name of the tool to be called.\n    arguments : Dict[str, Any]\n        Parameters to be passed to the tool function.\n    \"\"\"\n    block_type: Literal[\"tool_call\"] = Field(default=\"tool_call\")\n    id: str = Field(..., description=\"The ID of the tool call.\")\n    name: str = Field(..., description=\"The name of the tool call.\")\n    arguments: Dict[str, Any] = Field(..., description=\"The arguments of the tool call.\")\n</code></pre>"},{"location":"reference/bridgic-core/bridgic/core/model/types/#bridgic.core.model.types.ToolResultBlock","title":"ToolResultBlock","text":"<p>               Bases: <code>BaseModel</code></p> <p>Encapsulates the results returned from tool executions.</p> <p>Attributes:</p> Name Type Description <code>block_type</code> <code>Literal['tool_result']</code> <p>The type identifier for this content block.</p> <code>id</code> <code>str</code> <p>Unique identifier matching the corresponding tool call.</p> <code>content</code> <code>str</code> <p>The result content returned from the tool execution.</p> Source code in <code>bridgic/core/model/types/_content_block.py</code> <pre><code>class ToolResultBlock(BaseModel):\n    \"\"\"\n    Encapsulates the results returned from tool executions.\n\n    Attributes\n    ----------\n    block_type : Literal[\"tool_result\"]\n        The type identifier for this content block.\n    id : str\n        Unique identifier matching the corresponding tool call.\n    content : str\n        The result content returned from the tool execution.\n    \"\"\"\n    block_type: Literal[\"tool_result\"] = Field(default=\"tool_result\")\n    id: str = Field(..., description=\"The ID of the tool call.\")\n    content: str = Field(..., description=\"The result content of the tool call.\")\n</code></pre>"},{"location":"reference/bridgic-core/bridgic/core/model/types/#bridgic.core.model.types.Role","title":"Role","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p> <p>Message role enumeration for LLM conversations.</p> <p>Defines the different roles that can be assigned to messages in a conversation with language models, following standard chat completion formats.</p> <p>Attributes:</p> Name Type Description <code>SYSTEM</code> <code>str</code> <p>System role for providing instructions or context to the model.</p> <code>USER</code> <code>str</code> <p>User role for human input and queries.</p> <code>AI</code> <code>str</code> <p>Assistant role for model responses and outputs.</p> <code>TOOL</code> <code>str</code> <p>Tool role for tool execution results and responses.</p> Source code in <code>bridgic/core/model/types/_message.py</code> <pre><code>class Role(str, Enum):\n    \"\"\"\n    Message role enumeration for LLM conversations.\n\n    Defines the different roles that can be assigned to messages in a conversation\n    with language models, following standard chat completion formats.\n\n    Attributes\n    ----------\n    SYSTEM : str\n        System role for providing instructions or context to the model.\n    USER : str\n        User role for human input and queries.\n    AI : str\n        Assistant role for model responses and outputs.\n    TOOL : str\n        Tool role for tool execution results and responses.\n    \"\"\"\n    SYSTEM = \"system\"\n    USER = \"user\"\n    AI = \"assistant\"\n    TOOL = \"tool\"\n\n    @classmethod\n    def get_all_roles(cls) -&gt; List[str]:\n        return [role.value for role in Role]\n</code></pre>"},{"location":"reference/bridgic-core/bridgic/core/model/types/#bridgic.core.model.types.Message","title":"Message","text":"<p>               Bases: <code>BaseModel</code></p> <p>LLM message container for conversation exchanges.</p> <p>Represents a single message in a conversation with language models, containing role information, content blocks, and optional metadata. Supports various content types including text, tool calls, and tool results.</p> <p>Attributes:</p> Name Type Description <code>role</code> <code>Role</code> <p>The role of the message sender (system, user, assistant, or tool).</p> <code>blocks</code> <code>List[ContentBlock]</code> <p>List of content blocks containing the actual message data.</p> <code>extras</code> <code>Dict[str, Any]</code> <p>Additional metadata and custom fields for the message.</p> Source code in <code>bridgic/core/model/types/_message.py</code> <pre><code>class Message(BaseModel):\n    \"\"\"\n    LLM message container for conversation exchanges.\n\n    Represents a single message in a conversation with language models, containing\n    role information, content blocks, and optional metadata. Supports various\n    content types including text, tool calls, and tool results.\n\n    Attributes\n    ----------\n    role : Role\n        The role of the message sender (system, user, assistant, or tool).\n    blocks : List[ContentBlock]\n        List of content blocks containing the actual message data.\n    extras : Dict[str, Any]\n        Additional metadata and custom fields for the message.\n    \"\"\"\n    role: Role = Field(default=Role.USER)\n    blocks: List[ContentBlock] = Field(default=[])\n    extras: Dict[str, Any] = Field(default={})\n\n    @classmethod\n    def from_text(\n        cls,\n        text: str,\n        role: Union[Role, str] = Role.USER,\n        extras: Optional[Dict[str, Any]] = {},\n    ) -&gt; \"Message\":\n        if isinstance(role, str):\n            role = Role(role)\n        return cls(role=role, blocks=[TextBlock(text=text)], extras=extras)\n\n    @classmethod\n    def from_tool_call(\n        cls,\n        tool_calls: Union[\n            \"ToolCallDict\", \n            List[\"ToolCallDict\"], \n            \"ToolCall\",\n            List[\"ToolCall\"]\n        ],\n        text: Optional[str] = None,\n        extras: Optional[Dict[str, Any]] = {},\n    ) -&gt; \"Message\":\n        \"\"\"\n        Create a message with tool call blocks and optional text content.\n\n        Parameters\n        ----------\n        tool_calls : Union[ToolCallDict, List[ToolCallDict], ToolCall, List[ToolCall]]\n            Tool call data in various formats:\n            - Single tool call dict: {\"id\": \"call_123\", \"name\": \"get_weather\", \"arguments\": {...}}\n            - List of tool call dicts: [{\"id\": \"call_123\", ...}, {\"id\": \"call_124\", ...}]\n            - Single ToolCall instance\n            - List of ToolCall instances\n        text : Optional[str], optional\n            Optional text content to include in the message\n\n        extras : Optional[Dict[str, Any]], optional\n            Additional metadata for the message\n\n        Returns\n        -------\n        Message\n            A message containing the tool call blocks and optional text\n\n        Examples\n        --------\n        &gt;&gt;&gt; # Build from single tool call dict.\n        ... message = Message.from_tool_call(\n        ...     tool_calls={\n        ...         \"id\": \"call_id_123\",\n        ...         \"name\": \"get_weather\",\n        ...         \"arguments\": {\"city\": \"Tokyo\", \"unit\": \"celsius\"}\n        ...     },\n        ...     text=\"I will check the weather for you.\"\n        ... )\n\n        &gt;&gt;&gt; # Build from multiple tool call dicts.\n        ... message = Message.from_tool_call(\n        ...     tool_calls=[\n        ...         {\"id\": \"call_id_123\", \"name\": \"get_weather\", \"arguments\": {\"city\": \"Tokyo\"}},\n        ...         {\"id\": \"call_id_456\", \"name\": \"get_news\", \"arguments\": {\"topic\": \"weather\"}},\n        ...     ],\n        ...     text=\"I will get weather and news for you.\"\n        ... )\n\n        &gt;&gt;&gt; # Build from single ToolCall object.\n        ... tool_call = ToolCall(id=\"call_123\", name=\"get_weather\", arguments={\"city\": \"Tokyo\"})\n        ... message = Message.from_tool_call(tool_calls=tool_call, text=\"I will check the weather.\")\n\n        &gt;&gt;&gt; # Build from multiple ToolCall objects.\n        ... tool_calls = [\n        ...     ToolCall(id=\"call_id_123\", name=\"get_weather\", arguments={\"city\": \"Tokyo\"}),\n        ...     ToolCall(id=\"call_id_456\", name=\"get_news\", arguments={\"topic\": \"weather\"}),\n        ... ]\n        ... message = Message.from_tool_call(tool_calls=tool_calls, text=\"I will get weather and news.\")\n        \"\"\"\n        role = Role(Role.AI)\n        blocks = []\n\n        # Add text content if provided\n        if text:\n            blocks.append(TextBlock(text=text))\n\n        # Handle different tool_calls formats\n        if isinstance(tool_calls, dict):\n            # Single tool call dict\n            tool_calls = [tool_calls]\n        if isinstance(tool_calls, list):\n            # List of tool calls (dicts or ToolCall)\n            for tool_call in tool_calls:\n                if isinstance(tool_call, dict):\n                    # Tool call dict\n                    blocks.append(ToolCallBlock(\n                        id=tool_call[\"id\"],\n                        name=tool_call[\"name\"],\n                        arguments=tool_call[\"arguments\"]\n                    ))\n                elif hasattr(tool_call, 'id') and hasattr(tool_call, 'name') and hasattr(tool_call, 'arguments'):\n                    blocks.append(ToolCallBlock(\n                        id=tool_call.id,\n                        name=tool_call.name,\n                        arguments=tool_call.arguments\n                    ))\n                else:\n                    raise ValueError(f\"Invalid tool call format: {tool_call}\")\n        elif hasattr(tool_calls, 'id') and hasattr(tool_calls, 'name') and hasattr(tool_calls, 'arguments'):\n            blocks.append(ToolCallBlock(\n                id=tool_calls.id,\n                name=tool_calls.name,\n                arguments=tool_calls.arguments\n            ))\n        else:\n            raise ValueError(f\"Invalid tool_calls format: {type(tool_calls)}\")\n\n        return cls(role=role, blocks=blocks, extras=extras)\n\n    @classmethod\n    def from_tool_result(\n        cls,\n        tool_id: str,\n        content: str,\n        extras: Optional[Dict[str, Any]] = {},\n    ) -&gt; \"Message\":\n        \"\"\"\n        Create a message with a tool result block.\n\n        Parameters\n        ----------\n        tool_id : str\n            The ID of the tool call that this result corresponds to\n        content : str\n            The result content from the tool execution\n        extras : Optional[Dict[str, Any]], optional\n            Additional metadata for the message\n\n        Returns\n        -------\n        Message\n            A message containing the tool result block\n\n        Examples\n        --------\n        &gt;&gt;&gt; message = Message.from_tool_result(\n        ...     tool_id=\"call_id_123\",\n        ...     content=\"The weather in Tokyo is 22\u00b0C and sunny.\"\n        ... )\n        \"\"\"\n        role = Role(Role.TOOL)\n        return cls(\n            role=role, \n            blocks=[ToolResultBlock(id=tool_id, content=content)], \n            extras=extras\n        )\n\n    @property\n    def content(self) -&gt; str:\n        return \"\\n\\n\".join([block.text for block in self.blocks if isinstance(block, TextBlock)])\n\n    @content.setter\n    def content(self, text: str):\n        if not self.blocks:\n            self.blocks = [TextBlock(text=text)]\n        elif len(self.blocks) == 1 and isinstance(self.blocks[0], TextBlock):\n            self.blocks = [TextBlock(text=text)]\n        else:\n            raise ValueError(\n                \"Message contains multiple blocks or contains a non-text block, thus it could not be \"\n                \"easily set by the property \\\"Message.content\\\". Use \\\"Message.blocks\\\" instead.\"\n            )\n</code></pre>"},{"location":"reference/bridgic-core/bridgic/core/model/types/#bridgic.core.model.types.Message.from_tool_call","title":"from_tool_call","text":"<code>classmethod</code> <pre><code>from_tool_call(\n    tool_calls: Union[\n        ToolCallDict,\n        List[ToolCallDict],\n        ToolCall,\n        List[ToolCall],\n    ],\n    text: Optional[str] = None,\n    extras: Optional[Dict[str, Any]] = {},\n) -&gt; Message\n</code></pre> <p>Create a message with tool call blocks and optional text content.</p> <p>Parameters:</p> Name Type Description Default <code>tool_calls</code> <code>Union[ToolCallDict, List[ToolCallDict], ToolCall, List[ToolCall]]</code> <p>Tool call data in various formats: - Single tool call dict: {\"id\": \"call_123\", \"name\": \"get_weather\", \"arguments\": {...}} - List of tool call dicts: [{\"id\": \"call_123\", ...}, {\"id\": \"call_124\", ...}] - Single ToolCall instance - List of ToolCall instances</p> required <code>text</code> <code>Optional[str]</code> <p>Optional text content to include in the message</p> <code>None</code> <code>extras</code> <code>Optional[Dict[str, Any]]</code> <p>Additional metadata for the message</p> <code>{}</code> <p>Returns:</p> Type Description <code>Message</code> <p>A message containing the tool call blocks and optional text</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; # Build from single tool call dict.\n... message = Message.from_tool_call(\n...     tool_calls={\n...         \"id\": \"call_id_123\",\n...         \"name\": \"get_weather\",\n...         \"arguments\": {\"city\": \"Tokyo\", \"unit\": \"celsius\"}\n...     },\n...     text=\"I will check the weather for you.\"\n... )\n</code></pre> <pre><code>&gt;&gt;&gt; # Build from multiple tool call dicts.\n... message = Message.from_tool_call(\n...     tool_calls=[\n...         {\"id\": \"call_id_123\", \"name\": \"get_weather\", \"arguments\": {\"city\": \"Tokyo\"}},\n...         {\"id\": \"call_id_456\", \"name\": \"get_news\", \"arguments\": {\"topic\": \"weather\"}},\n...     ],\n...     text=\"I will get weather and news for you.\"\n... )\n</code></pre> <pre><code>&gt;&gt;&gt; # Build from single ToolCall object.\n... tool_call = ToolCall(id=\"call_123\", name=\"get_weather\", arguments={\"city\": \"Tokyo\"})\n... message = Message.from_tool_call(tool_calls=tool_call, text=\"I will check the weather.\")\n</code></pre> <pre><code>&gt;&gt;&gt; # Build from multiple ToolCall objects.\n... tool_calls = [\n...     ToolCall(id=\"call_id_123\", name=\"get_weather\", arguments={\"city\": \"Tokyo\"}),\n...     ToolCall(id=\"call_id_456\", name=\"get_news\", arguments={\"topic\": \"weather\"}),\n... ]\n... message = Message.from_tool_call(tool_calls=tool_calls, text=\"I will get weather and news.\")\n</code></pre> Source code in <code>bridgic/core/model/types/_message.py</code> <pre><code>@classmethod\ndef from_tool_call(\n    cls,\n    tool_calls: Union[\n        \"ToolCallDict\", \n        List[\"ToolCallDict\"], \n        \"ToolCall\",\n        List[\"ToolCall\"]\n    ],\n    text: Optional[str] = None,\n    extras: Optional[Dict[str, Any]] = {},\n) -&gt; \"Message\":\n    \"\"\"\n    Create a message with tool call blocks and optional text content.\n\n    Parameters\n    ----------\n    tool_calls : Union[ToolCallDict, List[ToolCallDict], ToolCall, List[ToolCall]]\n        Tool call data in various formats:\n        - Single tool call dict: {\"id\": \"call_123\", \"name\": \"get_weather\", \"arguments\": {...}}\n        - List of tool call dicts: [{\"id\": \"call_123\", ...}, {\"id\": \"call_124\", ...}]\n        - Single ToolCall instance\n        - List of ToolCall instances\n    text : Optional[str], optional\n        Optional text content to include in the message\n\n    extras : Optional[Dict[str, Any]], optional\n        Additional metadata for the message\n\n    Returns\n    -------\n    Message\n        A message containing the tool call blocks and optional text\n\n    Examples\n    --------\n    &gt;&gt;&gt; # Build from single tool call dict.\n    ... message = Message.from_tool_call(\n    ...     tool_calls={\n    ...         \"id\": \"call_id_123\",\n    ...         \"name\": \"get_weather\",\n    ...         \"arguments\": {\"city\": \"Tokyo\", \"unit\": \"celsius\"}\n    ...     },\n    ...     text=\"I will check the weather for you.\"\n    ... )\n\n    &gt;&gt;&gt; # Build from multiple tool call dicts.\n    ... message = Message.from_tool_call(\n    ...     tool_calls=[\n    ...         {\"id\": \"call_id_123\", \"name\": \"get_weather\", \"arguments\": {\"city\": \"Tokyo\"}},\n    ...         {\"id\": \"call_id_456\", \"name\": \"get_news\", \"arguments\": {\"topic\": \"weather\"}},\n    ...     ],\n    ...     text=\"I will get weather and news for you.\"\n    ... )\n\n    &gt;&gt;&gt; # Build from single ToolCall object.\n    ... tool_call = ToolCall(id=\"call_123\", name=\"get_weather\", arguments={\"city\": \"Tokyo\"})\n    ... message = Message.from_tool_call(tool_calls=tool_call, text=\"I will check the weather.\")\n\n    &gt;&gt;&gt; # Build from multiple ToolCall objects.\n    ... tool_calls = [\n    ...     ToolCall(id=\"call_id_123\", name=\"get_weather\", arguments={\"city\": \"Tokyo\"}),\n    ...     ToolCall(id=\"call_id_456\", name=\"get_news\", arguments={\"topic\": \"weather\"}),\n    ... ]\n    ... message = Message.from_tool_call(tool_calls=tool_calls, text=\"I will get weather and news.\")\n    \"\"\"\n    role = Role(Role.AI)\n    blocks = []\n\n    # Add text content if provided\n    if text:\n        blocks.append(TextBlock(text=text))\n\n    # Handle different tool_calls formats\n    if isinstance(tool_calls, dict):\n        # Single tool call dict\n        tool_calls = [tool_calls]\n    if isinstance(tool_calls, list):\n        # List of tool calls (dicts or ToolCall)\n        for tool_call in tool_calls:\n            if isinstance(tool_call, dict):\n                # Tool call dict\n                blocks.append(ToolCallBlock(\n                    id=tool_call[\"id\"],\n                    name=tool_call[\"name\"],\n                    arguments=tool_call[\"arguments\"]\n                ))\n            elif hasattr(tool_call, 'id') and hasattr(tool_call, 'name') and hasattr(tool_call, 'arguments'):\n                blocks.append(ToolCallBlock(\n                    id=tool_call.id,\n                    name=tool_call.name,\n                    arguments=tool_call.arguments\n                ))\n            else:\n                raise ValueError(f\"Invalid tool call format: {tool_call}\")\n    elif hasattr(tool_calls, 'id') and hasattr(tool_calls, 'name') and hasattr(tool_calls, 'arguments'):\n        blocks.append(ToolCallBlock(\n            id=tool_calls.id,\n            name=tool_calls.name,\n            arguments=tool_calls.arguments\n        ))\n    else:\n        raise ValueError(f\"Invalid tool_calls format: {type(tool_calls)}\")\n\n    return cls(role=role, blocks=blocks, extras=extras)\n</code></pre>"},{"location":"reference/bridgic-core/bridgic/core/model/types/#bridgic.core.model.types.Message.from_tool_result","title":"from_tool_result","text":"<code>classmethod</code> <pre><code>from_tool_result(\n    tool_id: str,\n    content: str,\n    extras: Optional[Dict[str, Any]] = {},\n) -&gt; Message\n</code></pre> <p>Create a message with a tool result block.</p> <p>Parameters:</p> Name Type Description Default <code>tool_id</code> <code>str</code> <p>The ID of the tool call that this result corresponds to</p> required <code>content</code> <code>str</code> <p>The result content from the tool execution</p> required <code>extras</code> <code>Optional[Dict[str, Any]]</code> <p>Additional metadata for the message</p> <code>{}</code> <p>Returns:</p> Type Description <code>Message</code> <p>A message containing the tool result block</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; message = Message.from_tool_result(\n...     tool_id=\"call_id_123\",\n...     content=\"The weather in Tokyo is 22\u00b0C and sunny.\"\n... )\n</code></pre> Source code in <code>bridgic/core/model/types/_message.py</code> <pre><code>@classmethod\ndef from_tool_result(\n    cls,\n    tool_id: str,\n    content: str,\n    extras: Optional[Dict[str, Any]] = {},\n) -&gt; \"Message\":\n    \"\"\"\n    Create a message with a tool result block.\n\n    Parameters\n    ----------\n    tool_id : str\n        The ID of the tool call that this result corresponds to\n    content : str\n        The result content from the tool execution\n    extras : Optional[Dict[str, Any]], optional\n        Additional metadata for the message\n\n    Returns\n    -------\n    Message\n        A message containing the tool result block\n\n    Examples\n    --------\n    &gt;&gt;&gt; message = Message.from_tool_result(\n    ...     tool_id=\"call_id_123\",\n    ...     content=\"The weather in Tokyo is 22\u00b0C and sunny.\"\n    ... )\n    \"\"\"\n    role = Role(Role.TOOL)\n    return cls(\n        role=role, \n        blocks=[ToolResultBlock(id=tool_id, content=content)], \n        extras=extras\n    )\n</code></pre>"},{"location":"reference/bridgic-core/bridgic/core/model/types/#bridgic.core.model.types.MessageChunk","title":"MessageChunk","text":"<p>               Bases: <code>BaseModel</code></p> <p>Streaming message chunk for real-time LLM responses.</p> <p>Represents a partial message chunk received during streaming responses from language models, allowing for real-time processing of incremental content.</p> <p>Attributes:</p> Name Type Description <code>delta</code> <code>Optional[str]</code> <p>The incremental text content of this chunk.</p> <code>raw</code> <code>Optional[Any]</code> <p>Raw response data from the LLM provider.</p> Source code in <code>bridgic/core/model/types/_message.py</code> <pre><code>class MessageChunk(BaseModel):\n    \"\"\"\n    Streaming message chunk for real-time LLM responses.\n\n    Represents a partial message chunk received during streaming responses from\n    language models, allowing for real-time processing of incremental content.\n\n    Attributes\n    ----------\n    delta : Optional[str]\n        The incremental text content of this chunk.\n    raw : Optional[Any]\n        Raw response data from the LLM provider.\n    \"\"\"\n    delta: Optional[str] = None\n    raw: Optional[Any] = None\n</code></pre>"},{"location":"reference/bridgic-core/bridgic/core/model/types/#bridgic.core.model.types.Response","title":"Response","text":"<p>               Bases: <code>BaseModel</code></p> <p>LLM response container for model outputs.</p> <p>Represents the complete response from a language model, containing both the message content and the raw response data from the underlying model  provider.</p> <p>Attributes:</p> Name Type Description <code>message</code> <code>Optional[Message]</code> <p>The structured message containing the model's response content.</p> <code>raw</code> <code>Optional[Any]</code> <p>Raw response data from the LLM provider for debugging or custom processing.</p> Source code in <code>bridgic/core/model/types/_response.py</code> <pre><code>class Response(BaseModel):\n    \"\"\"\n    LLM response container for model outputs.\n\n    Represents the complete response from a language model, containing both\n    the message content and the raw response data from the underlying model \n    provider.\n\n    Attributes\n    ----------\n    message : Optional[Message]\n        The structured message containing the model's response content.\n    raw : Optional[Any]\n        Raw response data from the LLM provider for debugging or custom processing.\n    \"\"\"\n    message: Optional[Message] = None\n    raw: Optional[Any] = None\n</code></pre>"},{"location":"reference/bridgic-core/bridgic/core/prompt/","title":"prompt","text":""},{"location":"reference/bridgic-core/bridgic/core/prompt/#prompt","title":"prompt","text":"<p>The Prompt module provides core functionality for managing and rendering prompt templates.</p> <p>This module contains multiple prompt template implementations for more convenient  construction of dynamic LLM prompt content.</p>"},{"location":"reference/bridgic-core/bridgic/core/prompt/#bridgic.core.prompt.BasePromptTemplate","title":"BasePromptTemplate","text":"<p>Abstract base class for prompt templates.</p> Source code in <code>bridgic/core/prompt/_base_template.py</code> <pre><code>class BasePromptTemplate:\n    \"\"\"\n    Abstract base class for prompt templates.\n    \"\"\"\n\n    def format_message(self, role: Union[Role, str] = Role.USER, **kwargs) -&gt; Message:\n        \"\"\"\n        Format a single message from the template.\n\n        Parameters\n        ----------\n        role : Union[Role, str], default=Role.USER\n            The role of the message (e.g., 'user', 'assistant', 'system').\n        **kwargs\n            Additional keyword arguments to be substituted into the template.\n\n        Returns\n        -------\n        Message\n            A formatted message object.\n\n        Raises\n        ------\n        NotImplementedError\n            This method must be implemented by subclasses.\n        \"\"\"\n        raise NotImplementedError(f\"format_message is not implemented in class {self.__class__.__name__}\")\n\n    def format_messages(self, **kwargs) -&gt; List[Message]:\n        \"\"\"\n        Format multiple messages from the template.\n\n        Parameters\n        ----------\n        **kwargs\n            Additional keyword arguments to be substituted into the template.\n\n        Returns\n        -------\n        List[Message]\n            A list of formatted message objects.\n\n        Raises\n        ------\n        NotImplementedError\n            This method must be implemented by subclasses.\n        \"\"\"\n        raise NotImplementedError(f\"format_messages is not implemented in class {self.__class__.__name__}\")\n</code></pre>"},{"location":"reference/bridgic-core/bridgic/core/prompt/#bridgic.core.prompt.BasePromptTemplate.format_message","title":"format_message","text":"<pre><code>format_message(\n    role: Union[Role, str] = USER, **kwargs\n) -&gt; Message\n</code></pre> <p>Format a single message from the template.</p> <p>Parameters:</p> Name Type Description Default <code>role</code> <code>Union[Role, str]</code> <p>The role of the message (e.g., 'user', 'assistant', 'system').</p> <code>Role.USER</code> <code>**kwargs</code> <p>Additional keyword arguments to be substituted into the template.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Message</code> <p>A formatted message object.</p> <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>This method must be implemented by subclasses.</p> Source code in <code>bridgic/core/prompt/_base_template.py</code> <pre><code>def format_message(self, role: Union[Role, str] = Role.USER, **kwargs) -&gt; Message:\n    \"\"\"\n    Format a single message from the template.\n\n    Parameters\n    ----------\n    role : Union[Role, str], default=Role.USER\n        The role of the message (e.g., 'user', 'assistant', 'system').\n    **kwargs\n        Additional keyword arguments to be substituted into the template.\n\n    Returns\n    -------\n    Message\n        A formatted message object.\n\n    Raises\n    ------\n    NotImplementedError\n        This method must be implemented by subclasses.\n    \"\"\"\n    raise NotImplementedError(f\"format_message is not implemented in class {self.__class__.__name__}\")\n</code></pre>"},{"location":"reference/bridgic-core/bridgic/core/prompt/#bridgic.core.prompt.BasePromptTemplate.format_messages","title":"format_messages","text":"<pre><code>format_messages(**kwargs) -&gt; List[Message]\n</code></pre> <p>Format multiple messages from the template.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <p>Additional keyword arguments to be substituted into the template.</p> <code>{}</code> <p>Returns:</p> Type Description <code>List[Message]</code> <p>A list of formatted message objects.</p> <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>This method must be implemented by subclasses.</p> Source code in <code>bridgic/core/prompt/_base_template.py</code> <pre><code>def format_messages(self, **kwargs) -&gt; List[Message]:\n    \"\"\"\n    Format multiple messages from the template.\n\n    Parameters\n    ----------\n    **kwargs\n        Additional keyword arguments to be substituted into the template.\n\n    Returns\n    -------\n    List[Message]\n        A list of formatted message objects.\n\n    Raises\n    ------\n    NotImplementedError\n        This method must be implemented by subclasses.\n    \"\"\"\n    raise NotImplementedError(f\"format_messages is not implemented in class {self.__class__.__name__}\")\n</code></pre>"},{"location":"reference/bridgic-core/bridgic/core/prompt/#bridgic.core.prompt.FstringPromptTemplate","title":"FstringPromptTemplate","text":"<p>               Bases: <code>BasePromptTemplate</code>, <code>Serializable</code></p> <p>This template implementation uses Python's f-string syntax (braces <code>{}</code>).</p> <p>Methods:</p> Name Description <code>format_message</code> <p>Format a single message from the template.</p> Notes <p>This template supports single message rendering via <code>format_message()</code>. The template uses Python's built-in <code>str.format()</code> method for variable substitution, which provides basic formatting capabilities.</p> <p>Examples:</p> <p>Basic usage:</p> <pre><code>&gt;&gt;&gt; template = FstringPromptTemplate(\"Hello {name}, you are {age} years old.\")\n&gt;&gt;&gt; message = template.format_message(role=\"user\", name=\"Alice\", age=25)\n</code></pre> <p>With context:</p> <pre><code>&gt;&gt;&gt; template = FstringPromptTemplate('''\n... Context: {context}\n... Question: {question}\n... Please provide a helpful answer.\n... ''')\n&gt;&gt;&gt; message = template.format_message(\n...     role=\"system\", \n...     context=\"Python programming\", \n...     question=\"What is a decorator?\"\n... )\n</code></pre> <p>Multiple variables:</p> <pre><code>&gt;&gt;&gt; template = FstringPromptTemplate(\"{greeting} {name}! Today is {date}.\")\n&gt;&gt;&gt; message = template.format_message(\n...     role=\"assistant\",\n...     greeting=\"Good morning\",\n...     name=\"Bob\", \n...     date=\"Monday\"\n... )\n</code></pre> Source code in <code>bridgic/core/prompt/_fstring_template.py</code> <pre><code>class FstringPromptTemplate(BasePromptTemplate, Serializable):\n    \"\"\"    \n    This template implementation uses Python's f-string syntax (braces `{}`).\n\n    Methods\n    -------\n    format_message(role, **kwargs)\n        Format a single message from the template.\n\n    Notes\n    -----\n    This template supports single message rendering via `format_message()`.\n    The template uses Python's built-in `str.format()` method for variable\n    substitution, which provides basic formatting capabilities.\n\n    Examples\n    --------\n    Basic usage:\n    &gt;&gt;&gt; template = FstringPromptTemplate(\"Hello {name}, you are {age} years old.\")\n    &gt;&gt;&gt; message = template.format_message(role=\"user\", name=\"Alice\", age=25)\n\n    With context:\n    &gt;&gt;&gt; template = FstringPromptTemplate('''\n    ... Context: {context}\n    ... Question: {question}\n    ... Please provide a helpful answer.\n    ... ''')\n    &gt;&gt;&gt; message = template.format_message(\n    ...     role=\"system\", \n    ...     context=\"Python programming\", \n    ...     question=\"What is a decorator?\"\n    ... )\n\n    Multiple variables:\n    &gt;&gt;&gt; template = FstringPromptTemplate(\"{greeting} {name}! Today is {date}.\")\n    &gt;&gt;&gt; message = template.format_message(\n    ...     role=\"assistant\",\n    ...     greeting=\"Good morning\",\n    ...     name=\"Bob\", \n    ...     date=\"Monday\"\n    ... )\n    \"\"\"\n\n    template_str: str\n\n    def __init__(self, template_str: str):\n        super().__init__()\n        self.template_str = template_str\n\n    def format_message(self, role: Union[Role, str], **kwargs) -&gt; Message:\n        \"\"\"\n        Format a single message from the template.\n\n        Parameters\n        ----------\n        role : Union[Role, str]\n            The role of the message (e.g., 'user', 'assistant', 'system').\n            Required parameter for this template implementation.\n        **kwargs\n            Keyword arguments containing values for all variables referenced\n            in the template string. All variables must be provided.\n\n        Returns\n        -------\n        Message\n            A formatted message object with the specified role and rendered content.\n\n        Raises\n        ------\n        PromptRenderError\n            If any variables referenced in the template are missing from\n            the provided keyword arguments.\n        \"\"\"\n        if isinstance(role, str):\n            role = Role(role)\n\n        all_vars = self._find_variables()\n        missing_vars = set(all_vars) - set(kwargs.keys())\n        if missing_vars:\n            raise PromptRenderError(f\"Missing variables that are required to render the prompt template: {', '.join(missing_vars)}\")\n\n        rendered = self.template_str.format(**kwargs)\n        return Message.from_text(text=rendered, role=role)\n\n    def _find_variables(self) -&gt; List[str]:\n        \"\"\"\n        Extract variable names from the template string.\n\n        Returns\n        -------\n        List[str]\n            A list of unique variable names found in the template string,\n            in the order they first appear. Variable names are extracted\n            from curly brace syntax `{variable_name}`.\n        \"\"\"\n        var_list = re.findall(r'{([^}]+)}', self.template_str)\n        var_list = [var.strip() for var in var_list]\n        return unique_list_in_order(var_list)\n\n    @override\n    def dump_to_dict(self) -&gt; Dict[str, Any]:\n        return {\n            \"template_str\": self.template_str,\n        }\n\n    @override\n    def load_from_dict(self, state_dict: Dict[str, Any]) -&gt; None:\n        self.template_str = state_dict[\"template_str\"]\n</code></pre>"},{"location":"reference/bridgic-core/bridgic/core/prompt/#bridgic.core.prompt.FstringPromptTemplate.format_message","title":"format_message","text":"<pre><code>format_message(role: Union[Role, str], **kwargs) -&gt; Message\n</code></pre> <p>Format a single message from the template.</p> <p>Parameters:</p> Name Type Description Default <code>role</code> <code>Union[Role, str]</code> <p>The role of the message (e.g., 'user', 'assistant', 'system'). Required parameter for this template implementation.</p> required <code>**kwargs</code> <p>Keyword arguments containing values for all variables referenced in the template string. All variables must be provided.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Message</code> <p>A formatted message object with the specified role and rendered content.</p> <p>Raises:</p> Type Description <code>PromptRenderError</code> <p>If any variables referenced in the template are missing from the provided keyword arguments.</p> Source code in <code>bridgic/core/prompt/_fstring_template.py</code> <pre><code>def format_message(self, role: Union[Role, str], **kwargs) -&gt; Message:\n    \"\"\"\n    Format a single message from the template.\n\n    Parameters\n    ----------\n    role : Union[Role, str]\n        The role of the message (e.g., 'user', 'assistant', 'system').\n        Required parameter for this template implementation.\n    **kwargs\n        Keyword arguments containing values for all variables referenced\n        in the template string. All variables must be provided.\n\n    Returns\n    -------\n    Message\n        A formatted message object with the specified role and rendered content.\n\n    Raises\n    ------\n    PromptRenderError\n        If any variables referenced in the template are missing from\n        the provided keyword arguments.\n    \"\"\"\n    if isinstance(role, str):\n        role = Role(role)\n\n    all_vars = self._find_variables()\n    missing_vars = set(all_vars) - set(kwargs.keys())\n    if missing_vars:\n        raise PromptRenderError(f\"Missing variables that are required to render the prompt template: {', '.join(missing_vars)}\")\n\n    rendered = self.template_str.format(**kwargs)\n    return Message.from_text(text=rendered, role=role)\n</code></pre>"},{"location":"reference/bridgic-core/bridgic/core/prompt/#bridgic.core.prompt.EjinjaPromptTemplate","title":"EjinjaPromptTemplate","text":"<p>               Bases: <code>BasePromptTemplate</code>, <code>Serializable</code></p> <p>Extended Jinja2-based prompt template with custom message blocks.</p> <p>This template implementation extends the standard Jinja2 syntax with custom <code>{% msg %}</code> blocks to create structured Message objects. It supports both single message and multiple message rendering with variable substitution and content block parsing.</p> <p>Attributes:</p> Name Type Description <code>_env_template</code> <code>Template</code> <p>The compiled Jinja2 template object.</p> <code>_render_cache</code> <code>MemoryCache</code> <p>Cache for rendered template results to improve performance.</p> <p>Methods:</p> Name Description <code>format_message</code> <p>Format a single message from the template.</p> <code>format_messages</code> <p>Format multiple messages from the template.</p> Notes <p>This template supports two rendering modes:</p> <ol> <li>Single Message Mode: Use <code>format_message()</code> to render one message.    </li> <li>Multiple Messages Mode: Use <code>format_messages()</code> to render multiple messages.</li> </ol> <p>Examples:</p> <p>Single message with role in template:</p> <pre><code>&gt;&gt;&gt; template = EjinjaPromptTemplate('''\n... {% msg role=\"system\" %}\n... You are a helpful assistant. User name: {{ name }}\n... {% endmsg %}\n... ''')\n&gt;&gt;&gt; message = template.format_message(name=\"Alice\")\n</code></pre> <p>Single message with role as parameter:</p> <pre><code>&gt;&gt;&gt; template = EjinjaPromptTemplate(\"Hello {{ name }}, how are you?\")\n&gt;&gt;&gt; message = template.format_message(role=\"user\", name=\"Bob\")\n</code></pre> <p>Multiple messages:</p> <pre><code>&gt;&gt;&gt; template = EjinjaPromptTemplate('''\n... {% msg role=\"system\" %}You are helpful{% endmsg %}\n... {% msg role=\"user\" %}Hello {{ name }}{% endmsg %}\n... ''')\n&gt;&gt;&gt; messages = template.format_messages(name=\"Charlie\")\n</code></pre> Source code in <code>bridgic/core/prompt/_ejinja_template.py</code> <pre><code>class EjinjaPromptTemplate(BasePromptTemplate, Serializable):\n    \"\"\"\n    Extended Jinja2-based prompt template with custom message blocks.\n\n    This template implementation extends the standard Jinja2 syntax with custom\n    `{% msg %}` blocks to create structured Message objects. It supports both\n    single message and multiple message rendering with variable substitution\n    and content block parsing.\n\n    Attributes\n    ----------\n    _env_template : Template\n        The compiled Jinja2 template object.\n    _render_cache : MemoryCache\n        Cache for rendered template results to improve performance.\n\n    Methods\n    -------\n    format_message(role, **kwargs)\n        Format a single message from the template.\n    format_messages(**kwargs)\n        Format multiple messages from the template.\n\n    Notes\n    -----\n    This template supports two rendering modes:\n\n    1. **Single Message Mode**: Use `format_message()` to render one message.    \n    2. **Multiple Messages Mode**: Use `format_messages()` to render multiple messages.\n\n    Examples\n    --------\n    Single message with role in template:\n    &gt;&gt;&gt; template = EjinjaPromptTemplate('''\n    ... {% msg role=\"system\" %}\n    ... You are a helpful assistant. User name: {{ name }}\n    ... {% endmsg %}\n    ... ''')\n    &gt;&gt;&gt; message = template.format_message(name=\"Alice\")\n\n    Single message with role as parameter:\n    &gt;&gt;&gt; template = EjinjaPromptTemplate(\"Hello {{ name }}, how are you?\")\n    &gt;&gt;&gt; message = template.format_message(role=\"user\", name=\"Bob\")\n\n    Multiple messages:\n    &gt;&gt;&gt; template = EjinjaPromptTemplate('''\n    ... {% msg role=\"system\" %}You are helpful{% endmsg %}\n    ... {% msg role=\"user\" %}Hello {{ name }}{% endmsg %}\n    ... ''')\n    &gt;&gt;&gt; messages = template.format_messages(name=\"Charlie\")\n    \"\"\"\n\n    template_str: str\n\n    _env_template: Template\n    _render_cache: MemoryCache\n\n    def __init__(self, template_str: str):\n        \"\"\"\n        Initialize the EjinjaPromptTemplate.\n\n        Parameters\n        ----------\n        template_str : str\n            The template string using extended Jinja2 syntax.\n        \"\"\"\n        super().__init__()\n        self.template_str = template_str\n        self._env_template = env.from_string(template_str)\n        self._render_cache = MemoryCache()\n\n    def format_message(self, role: Union[Role, str] = None, **kwargs) -&gt; Message:\n        \"\"\"\n        Format a single message from the template.\n\n        Parameters\n        ----------\n        role : Union[Role, str], optional\n            The role of the message. If the template contains a `{% msg %}` block,\n            this parameter should be None as the role will be extracted from\n            the template. If no `{% msg %}` block exists, this parameter is required.\n        **kwargs\n            Additional keyword arguments to be substituted into the template.\n\n        Returns\n        -------\n        Message\n            A formatted message object with the specified role and content.\n\n        Raises\n        ------\n        PromptSyntaxError\n            If the template contains more than one `{% msg %}` block.\n        PromptRenderError\n            If role parameter conflicts with template-defined role, or if\n            no role is specified when template has no `{% msg %}` block.\n        \"\"\"\n        if isinstance(role, str):\n            role = Role(role)\n\n        rendered = self._env_template.render(**kwargs)\n        match_list = re.findall(r\"{%\\s*msg\\s*role=\\\"(.*?)\\\"\\s*%}(.*?){%\\s*endmsg\\s*%}\", rendered)\n        if len(match_list) &gt; 1:\n            raise PromptSyntaxError(\n                f\"It is required to just have one {{% msg %}} block in the template, \"\n                f\"but got {len(match_list)}\"\n            )\n        elif len(match_list) == 1:\n            if role is not None:\n                raise PromptRenderError(\n                    f\"If you want to render a single message, the role has to be only specified in the template \"\n                    f\"and not be passed as an argument to the \\\"format_message\\\" method in {type(self).__name__}\"\n                )\n            role, content = match_list[0][0], match_list[0][1]\n        else:\n            if role is None:\n                raise PromptRenderError(\n                    f\"If you want to render a template without {{% msg %}} blocks, the role has to be specified \"\n                    f\"as an argument to the \\\"format_message\\\" method in {type(self).__name__}\"\n                )\n            role, content = role, rendered\n        return Message.from_text(text=content, role=role)\n\n    def format_messages(self, **kwargs) -&gt; List[Message]:\n        \"\"\"\n        Format multiple messages from the template.\n\n        Parameters\n        ----------\n        **kwargs\n            Additional keyword arguments to be substituted into the template.\n\n        Returns\n        -------\n        List[Message]\n            A list of formatted message objects. Each line of the rendered\n            template should be a valid JSON representation of a Message object.\n            If no valid messages are found but content exists, a default user\n            message is created.\n\n        Raises\n        ------\n        PromptRenderError\n            If any line in the rendered template is not a valid JSON\n            representation of a Message object.\n\n        Notes\n        -----\n        This method uses caching to improve performance for repeated calls\n        with the same parameters. The rendered template is cached based on\n        the provided keyword arguments.\n        \"\"\"\n        rendered = self._render_cache.get(kwargs)\n        if not rendered:\n            rendered = self._env_template.render(kwargs)\n            self._render_cache.set(kwargs, rendered)\n\n        messages: List[Message] = []\n        for line in rendered.strip().split(\"\\n\"):\n            try:\n                if len(line.strip()) &gt; 0:\n                    messages.append(Message.model_validate_json(line))\n            except Exception:\n                raise PromptRenderError(\n                    f\"It is required to wrap each content in a {{% msg %}} block when calling the \"\n                    f\"\\\"format_messages\\\" method of {type(self).__name__}, but got: {line}\"\n                )\n\n        if not messages and rendered.strip():\n            messages.append(_chat_message_from_text(role=\"user\", content=rendered))\n        return messages\n\n    @override\n    def dump_to_dict(self) -&gt; Dict[str, Any]:\n        return {\n            \"template_str\": self.template_str,\n        }\n\n    @override\n    def load_from_dict(self, state_dict: Dict[str, Any]) -&gt; None:\n        self.template_str = state_dict[\"template_str\"]\n        self._env_template = env.from_string(self.template_str)\n        self._render_cache = MemoryCache()\n</code></pre>"},{"location":"reference/bridgic-core/bridgic/core/prompt/#bridgic.core.prompt.EjinjaPromptTemplate.format_message","title":"format_message","text":"<pre><code>format_message(\n    role: Union[Role, str] = None, **kwargs\n) -&gt; Message\n</code></pre> <p>Format a single message from the template.</p> <p>Parameters:</p> Name Type Description Default <code>role</code> <code>Union[Role, str]</code> <p>The role of the message. If the template contains a <code>{% msg %}</code> block, this parameter should be None as the role will be extracted from the template. If no <code>{% msg %}</code> block exists, this parameter is required.</p> <code>None</code> <code>**kwargs</code> <p>Additional keyword arguments to be substituted into the template.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Message</code> <p>A formatted message object with the specified role and content.</p> <p>Raises:</p> Type Description <code>PromptSyntaxError</code> <p>If the template contains more than one <code>{% msg %}</code> block.</p> <code>PromptRenderError</code> <p>If role parameter conflicts with template-defined role, or if no role is specified when template has no <code>{% msg %}</code> block.</p> Source code in <code>bridgic/core/prompt/_ejinja_template.py</code> <pre><code>def format_message(self, role: Union[Role, str] = None, **kwargs) -&gt; Message:\n    \"\"\"\n    Format a single message from the template.\n\n    Parameters\n    ----------\n    role : Union[Role, str], optional\n        The role of the message. If the template contains a `{% msg %}` block,\n        this parameter should be None as the role will be extracted from\n        the template. If no `{% msg %}` block exists, this parameter is required.\n    **kwargs\n        Additional keyword arguments to be substituted into the template.\n\n    Returns\n    -------\n    Message\n        A formatted message object with the specified role and content.\n\n    Raises\n    ------\n    PromptSyntaxError\n        If the template contains more than one `{% msg %}` block.\n    PromptRenderError\n        If role parameter conflicts with template-defined role, or if\n        no role is specified when template has no `{% msg %}` block.\n    \"\"\"\n    if isinstance(role, str):\n        role = Role(role)\n\n    rendered = self._env_template.render(**kwargs)\n    match_list = re.findall(r\"{%\\s*msg\\s*role=\\\"(.*?)\\\"\\s*%}(.*?){%\\s*endmsg\\s*%}\", rendered)\n    if len(match_list) &gt; 1:\n        raise PromptSyntaxError(\n            f\"It is required to just have one {{% msg %}} block in the template, \"\n            f\"but got {len(match_list)}\"\n        )\n    elif len(match_list) == 1:\n        if role is not None:\n            raise PromptRenderError(\n                f\"If you want to render a single message, the role has to be only specified in the template \"\n                f\"and not be passed as an argument to the \\\"format_message\\\" method in {type(self).__name__}\"\n            )\n        role, content = match_list[0][0], match_list[0][1]\n    else:\n        if role is None:\n            raise PromptRenderError(\n                f\"If you want to render a template without {{% msg %}} blocks, the role has to be specified \"\n                f\"as an argument to the \\\"format_message\\\" method in {type(self).__name__}\"\n            )\n        role, content = role, rendered\n    return Message.from_text(text=content, role=role)\n</code></pre>"},{"location":"reference/bridgic-core/bridgic/core/prompt/#bridgic.core.prompt.EjinjaPromptTemplate.format_messages","title":"format_messages","text":"<pre><code>format_messages(**kwargs) -&gt; List[Message]\n</code></pre> <p>Format multiple messages from the template.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <p>Additional keyword arguments to be substituted into the template.</p> <code>{}</code> <p>Returns:</p> Type Description <code>List[Message]</code> <p>A list of formatted message objects. Each line of the rendered template should be a valid JSON representation of a Message object. If no valid messages are found but content exists, a default user message is created.</p> <p>Raises:</p> Type Description <code>PromptRenderError</code> <p>If any line in the rendered template is not a valid JSON representation of a Message object.</p> Notes <p>This method uses caching to improve performance for repeated calls with the same parameters. The rendered template is cached based on the provided keyword arguments.</p> Source code in <code>bridgic/core/prompt/_ejinja_template.py</code> <pre><code>def format_messages(self, **kwargs) -&gt; List[Message]:\n    \"\"\"\n    Format multiple messages from the template.\n\n    Parameters\n    ----------\n    **kwargs\n        Additional keyword arguments to be substituted into the template.\n\n    Returns\n    -------\n    List[Message]\n        A list of formatted message objects. Each line of the rendered\n        template should be a valid JSON representation of a Message object.\n        If no valid messages are found but content exists, a default user\n        message is created.\n\n    Raises\n    ------\n    PromptRenderError\n        If any line in the rendered template is not a valid JSON\n        representation of a Message object.\n\n    Notes\n    -----\n    This method uses caching to improve performance for repeated calls\n    with the same parameters. The rendered template is cached based on\n    the provided keyword arguments.\n    \"\"\"\n    rendered = self._render_cache.get(kwargs)\n    if not rendered:\n        rendered = self._env_template.render(kwargs)\n        self._render_cache.set(kwargs, rendered)\n\n    messages: List[Message] = []\n    for line in rendered.strip().split(\"\\n\"):\n        try:\n            if len(line.strip()) &gt; 0:\n                messages.append(Message.model_validate_json(line))\n        except Exception:\n            raise PromptRenderError(\n                f\"It is required to wrap each content in a {{% msg %}} block when calling the \"\n                f\"\\\"format_messages\\\" method of {type(self).__name__}, but got: {line}\"\n            )\n\n    if not messages and rendered.strip():\n        messages.append(_chat_message_from_text(role=\"user\", content=rendered))\n    return messages\n</code></pre>"},{"location":"reference/bridgic-core/bridgic/core/types/","title":"types","text":""},{"location":"reference/bridgic-core/bridgic/core/types/#types","title":"types","text":"<p>The Types module defines several basic data types for the framework.</p>"},{"location":"reference/bridgic-core/bridgic/core/types/#bridgic.core.types.Serializable","title":"Serializable","text":"<p>               Bases: <code>Protocol</code></p> <p>Serializable is a protocol that defines the interfaces that customizes serialization.</p> Source code in <code>bridgic/core/types/_serialization.py</code> <pre><code>@runtime_checkable\nclass Serializable(Protocol):\n    \"\"\"\n    Serializable is a protocol that defines the interfaces that customizes serialization.\n    \"\"\"\n    @abstractmethod\n    def dump_to_dict(self) -&gt; Dict[str, Any]:\n        \"\"\"\n        Dump the object to a dictionary, which will finally be serialized to bytes.\n        \"\"\"\n        ...\n\n    @abstractmethod\n    def load_from_dict(self, state_dict: Dict[str, Any]) -&gt; None:\n        \"\"\"\n        Load the object state from a dictionary previously obtained by deserializing from bytes.\n        \"\"\"\n        ...\n</code></pre>"},{"location":"reference/bridgic-core/bridgic/core/types/#bridgic.core.types.Serializable.dump_to_dict","title":"dump_to_dict","text":"<code>abstractmethod</code> <pre><code>dump_to_dict() -&gt; Dict[str, Any]\n</code></pre> <p>Dump the object to a dictionary, which will finally be serialized to bytes.</p> Source code in <code>bridgic/core/types/_serialization.py</code> <pre><code>@abstractmethod\ndef dump_to_dict(self) -&gt; Dict[str, Any]:\n    \"\"\"\n    Dump the object to a dictionary, which will finally be serialized to bytes.\n    \"\"\"\n    ...\n</code></pre>"},{"location":"reference/bridgic-core/bridgic/core/types/#bridgic.core.types.Serializable.load_from_dict","title":"load_from_dict","text":"<code>abstractmethod</code> <pre><code>load_from_dict(state_dict: Dict[str, Any]) -&gt; None\n</code></pre> <p>Load the object state from a dictionary previously obtained by deserializing from bytes.</p> Source code in <code>bridgic/core/types/_serialization.py</code> <pre><code>@abstractmethod\ndef load_from_dict(self, state_dict: Dict[str, Any]) -&gt; None:\n    \"\"\"\n    Load the object state from a dictionary previously obtained by deserializing from bytes.\n    \"\"\"\n    ...\n</code></pre>"},{"location":"reference/bridgic-core/bridgic/core/types/#bridgic.core.types.Picklable","title":"Picklable","text":"<p>               Bases: <code>Protocol</code></p> <p>Picklable is a protocol that defines the interfaces that customizes serialization using pickle.</p> Notes <p>If a class implements both Serializable and Picklable, the object of the class will be  serialized using the implementation provided by Serializable instead of using pickle.</p> Source code in <code>bridgic/core/types/_serialization.py</code> <pre><code>@runtime_checkable\nclass Picklable(Protocol):\n    \"\"\"\n    Picklable is a protocol that defines the interfaces that customizes serialization using pickle.\n\n    Notes\n    -----\n    If a class implements both Serializable and Picklable, the object of the class will be \n    serialized using the implementation provided by Serializable instead of using pickle.\n    \"\"\"\n\n    def __picklable_marker__(self) -&gt; None:\n        \"\"\"\n        This is just a marker method to distinguish Picklable objects from other objects.\n        Since it is not necessary to implement this method in the subclass, thus no \n        @abstractmethod is used here.\n        \"\"\"\n        ...\n</code></pre>"},{"location":"reference/bridgic-llms-openai/bridgic/llms/openai/","title":"bridgic.llms.openai","text":""},{"location":"reference/bridgic-llms-openai/bridgic/llms/openai/#openai","title":"openai","text":"<p>The OpenAI integration module provides support for the OpenAI API.</p> <p>This module implements integration interfaces with OpenAI language models, supporting  calls to large language models provided by OpenAI such as the GPT series, and provides  several wrappers for advanced functionality.</p> <p>You can install the OpenAI integration package for Bridgic by running:</p> <pre><code>pip install bridgic-llms-openai\n</code></pre>"},{"location":"reference/bridgic-llms-openai/bridgic/llms/openai/#bridgic.llms.openai.OpenAIConfiguration","title":"OpenAIConfiguration","text":"<p>               Bases: <code>OpenAILikeConfiguration</code></p> <p>Configuration for OpenAI chat completions.</p> Source code in <code>bridgic/llms/openai/_openai_llm.py</code> <pre><code>class OpenAIConfiguration(OpenAILikeConfiguration):\n    \"\"\"\n    Configuration for OpenAI chat completions.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/bridgic-llms-openai/bridgic/llms/openai/#bridgic.llms.openai.OpenAILlm","title":"OpenAILlm","text":"<p>               Bases: <code>BaseLlm</code>, <code>StructuredOutput</code>, <code>ToolSelection</code></p> <p>Wrapper class for OpenAI, providing common chat and stream calling interfaces for OpenAI model and implementing the common protocols in the Bridgic framework.</p> <p>Parameters:</p> Name Type Description Default <code>api_key</code> <code>str</code> <p>The API key for OpenAI services. Required for authentication.</p> required <code>api_base</code> <code>Optional[str]</code> <p>The base URL for the OpenAI API. If None, uses the default OpenAI endpoint.</p> <code>None</code> <code>configuration</code> <code>Optional[OpenAIConfiguration]</code> <p>The configuration for the OpenAI API. If None, uses the default configuration.</p> <code>OpenAIConfiguration()</code> <code>timeout</code> <code>Optional[float]</code> <p>Request timeout in seconds. If None, no timeout is applied.</p> <code>None</code> <code>http_client</code> <code>Optional[Client]</code> <p>Custom synchronous HTTP client for requests. If None, creates a default client.</p> <code>None</code> <code>http_async_client</code> <code>Optional[AsyncClient]</code> <p>Custom asynchronous HTTP client for requests. If None, creates a default client.</p> <code>None</code> <p>Examples:</p> <p>Basic usage for chat completion:</p> <pre><code>llm = OpenAILlm(api_key=\"your-api-key\")\nmessages = [Message.from_text(\"Hello!\", role=Role.USER)]\nresponse = llm.chat(messages=messages, model=\"gpt-4\")\n</code></pre> <p>Structured output with Pydantic model:</p> <pre><code>class Answer(BaseModel):\n    reasoning: str\n    result: int\n\nconstraint = PydanticModel(model=Answer)\nstructured_response = llm.structured_output(\n    messages=messages,\n    constraint=constraint,\n    model=\"gpt-4\"\n)\n</code></pre> <p>Tool calling:</p> <pre><code>tools = [Tool(name=\"calculator\", description=\"Calculate math\", parameters={})]\ntool_calls, tool_call_response = llm.select_tool(messages=messages, tools=tools, model=\"gpt-4\")\n</code></pre> Source code in <code>bridgic/llms/openai/_openai_llm.py</code> <pre><code>class OpenAILlm(BaseLlm, StructuredOutput, ToolSelection):\n    \"\"\"\n    Wrapper class for OpenAI, providing common chat and stream calling interfaces for OpenAI model\n    and implementing the common protocols in the Bridgic framework.\n\n    Parameters\n    ----------\n    api_key : str\n        The API key for OpenAI services. Required for authentication.\n    api_base : Optional[str]\n        The base URL for the OpenAI API. If None, uses the default OpenAI endpoint.\n    configuration : Optional[OpenAIConfiguration]\n        The configuration for the OpenAI API. If None, uses the default configuration.\n    timeout : Optional[float]\n        Request timeout in seconds. If None, no timeout is applied.\n    http_client : Optional[httpx.Client]\n        Custom synchronous HTTP client for requests. If None, creates a default client.\n    http_async_client : Optional[httpx.AsyncClient]\n        Custom asynchronous HTTP client for requests. If None, creates a default client.\n\n    Examples\n    --------\n    Basic usage for chat completion:\n\n    ```python\n    llm = OpenAILlm(api_key=\"your-api-key\")\n    messages = [Message.from_text(\"Hello!\", role=Role.USER)]\n    response = llm.chat(messages=messages, model=\"gpt-4\")\n    ```\n\n    Structured output with Pydantic model:\n\n    ```python\n    class Answer(BaseModel):\n        reasoning: str\n        result: int\n\n    constraint = PydanticModel(model=Answer)\n    structured_response = llm.structured_output(\n        messages=messages,\n        constraint=constraint,\n        model=\"gpt-4\"\n    )\n    ```\n\n    Tool calling:\n\n    ```python\n    tools = [Tool(name=\"calculator\", description=\"Calculate math\", parameters={})]\n    tool_calls, tool_call_response = llm.select_tool(messages=messages, tools=tools, model=\"gpt-4\")\n    ```\n    \"\"\"\n\n    api_base: str\n    api_key: str\n    configuration: OpenAIConfiguration\n    timeout: float\n    http_client: httpx.Client\n    http_async_client: httpx.AsyncClient\n\n    client: OpenAI\n    async_client: AsyncOpenAI\n\n    def __init__(\n        self,\n        api_key: str,\n        api_base: Optional[str] = None,\n        configuration: Optional[OpenAIConfiguration] = OpenAIConfiguration(),\n        timeout: Optional[float] = None,\n        http_client: Optional[httpx.Client] = None,\n        http_async_client: Optional[httpx.AsyncClient] = None,\n    ):\n        \"\"\"\n        Initialize the OpenAI LLM client with configuration parameters.\n\n        Parameters\n        ----------\n        api_key : str\n            The API key for OpenAI services. Required for authentication.\n        api_base : Optional[str]\n            The base URL for the OpenAI API. If None, uses the default OpenAI endpoint.\n        configuration : Optional[OpenAIConfiguration]\n            The configuration for the OpenAI API. If None, uses the default configuration.\n        timeout : Optional[float]\n            Request timeout in seconds. If None, no timeout is applied.\n        http_client : Optional[httpx.Client]\n            Custom synchronous HTTP client for requests. If None, creates a default client.\n        http_async_client : Optional[httpx.AsyncClient]\n            Custom asynchronous HTTP client for requests. If None, creates a default client.\n        \"\"\"\n        # Record for serialization / deserialization.\n        self.api_base = api_base\n        self.api_key = api_key\n        self.configuration = configuration\n        self.timeout = timeout\n        self.http_client = http_client\n        self.http_async_client = http_async_client\n\n        # Initialize clients.\n        self.client = OpenAI(base_url=api_base, api_key=api_key, timeout=timeout, http_client=http_client)\n        self.async_client = AsyncOpenAI(base_url=api_base, api_key=api_key, timeout=timeout, http_client=http_async_client)\n\n    def chat(\n        self,\n        messages: List[Message],\n        model: Optional[str] = None,\n        temperature: Optional[float] = None,\n        top_p: Optional[float] = None,\n        presence_penalty: Optional[float] = None,\n        frequency_penalty: Optional[float] = None,\n        max_tokens: Optional[int] = None,\n        stop: Optional[List[str]] = None,\n        tools: Optional[List[Tool]] = None,\n        extra_body: Optional[Dict[str, Any]] = None,\n        **kwargs,\n    ) -&gt; Response:\n        \"\"\"\n        Send a synchronous chat completion request to OpenAI.\n\n        Parameters\n        ----------\n        messages : List[Message]\n            A list of messages comprising the conversation so far.\n        model : str\n            Model ID used to generate the response, like `gpt-4o` or `gpt-4`.\n        temperature : Optional[float]\n            What sampling temperature to use, between 0 and 2. Higher values like 0.8 will\n            make the output more random, while lower values like 0.2 will make it more\n            focused and deterministic.\n        top_p : Optional[float]\n            An alternative to sampling with temperature, called nucleus sampling, where the\n            model considers the results of the tokens with top_p probability mass.\n        presence_penalty : Optional[float]\n            Number between -2.0 and 2.0. Positive values penalize new tokens based on\n            whether they appear in the text so far, increasing the model's likelihood to\n            talk about new topics.\n        frequency_penalty : Optional[float]\n            Number between -2.0 and 2.0. Positive values penalize new tokens based on their\n            existing frequency in the text so far, decreasing the model's likelihood to\n            repeat the same line verbatim.\n        max_tokens : Optional[int]\n            The maximum number of tokens that can be generated in the chat completion.\n            This value is now deprecated in favor of `max_completion_tokens`.\n        stop : Optional[List[str]]\n            Up to 4 sequences where the API will stop generating further tokens.\n            Not supported with latest reasoning models `o3` and `o3-mini`.\n        tools : Optional[List[Tool]]\n            A list of tools to use in the chat completion.\n        extra_body : Optional[Dict[str, Any]]\n            Add additional JSON properties to the request.\n        **kwargs\n            Additional keyword arguments passed to the OpenAI API.\n\n        Returns\n        -------\n        Response\n            A response object containing the generated message and raw API response.\n        \"\"\"\n        params = self._build_parameters(\n            messages=messages,\n            model=model,\n            temperature=temperature,\n            top_p=top_p,\n            presence_penalty=presence_penalty,\n            frequency_penalty=frequency_penalty,\n            max_tokens=max_tokens,\n            stop=stop,\n            extra_body=extra_body,\n            **kwargs,\n        )\n        # Validate required parameters for non-streaming chat completion\n        validate_required_params(params, [\"messages\", \"model\"])\n\n        response: ChatCompletion = self.client.chat.completions.create(**params)\n        return self._handle_chat_response(response)\n\n    def stream(\n        self,\n        messages: List[Message],\n        model: Optional[str] = None,\n        temperature: Optional[float] = None,\n        top_p: Optional[float] = None,\n        presence_penalty: Optional[float] = None,\n        frequency_penalty: Optional[float] = None,\n        max_tokens: Optional[int] = None,\n        stop: Optional[List[str]] = None,\n        extra_body: Optional[Dict[str, Any]] = None,\n        **kwargs,\n    ) -&gt; StreamResponse:\n        \"\"\"\n        Send a streaming chat completion request to OpenAI.\n\n        Parameters\n        ----------\n        messages : List[Message]\n            A list of messages comprising the conversation so far.\n        model : str\n            Model ID used to generate the response, like `gpt-4o` or `gpt-4`.\n        temperature : Optional[float]\n            What sampling temperature to use, between 0 and 2. Higher values like 0.8 will\n            make the output more random, while lower values like 0.2 will make it more\n            focused and deterministic.\n        top_p : Optional[float]\n            An alternative to sampling with temperature, called nucleus sampling, where the\n            model considers the results of the tokens with top_p probability mass.\n        presence_penalty : Optional[float]\n            Number between -2.0 and 2.0. Positive values penalize new tokens based on\n            whether they appear in the text so far, increasing the model's likelihood to\n            talk about new topics.\n        frequency_penalty : Optional[float]\n            Number between -2.0 and 2.0. Positive values penalize new tokens based on their\n            existing frequency in the text so far, decreasing the model's likelihood to\n            repeat the same line verbatim.\n        max_tokens : Optional[int]\n            The maximum number of tokens that can be generated in the chat completion.\n            This value is now deprecated in favor of `max_completion_tokens`.\n        stop : Optional[List[str]]\n            Up to 4 sequences where the API will stop generating further tokens.\n            Not supported with latest reasoning models `o3` and `o3-mini`.\n        extra_body : Optional[Dict[str, Any]]\n            Add additional JSON properties to the request.\n        **kwargs\n            Additional keyword arguments passed to the OpenAI API.\n\n        Yields\n        ------\n        MessageChunk\n            Individual chunks of the response as they are received from the API.\n            Each chunk contains a delta (partial content) and the raw response.\n\n        Notes\n        -----\n        This method enables real-time streaming of the model's response,\n        useful for providing incremental updates to users as the response is generated.\n        \"\"\"\n        params = self._build_parameters(\n            messages=messages,\n            model=model,\n            temperature=temperature,\n            top_p=top_p,\n            presence_penalty=presence_penalty,\n            frequency_penalty=frequency_penalty,\n            max_tokens=max_tokens,\n            stop=stop,\n            extra_body=extra_body,\n            stream=True,\n            **kwargs,\n        )\n        # Validate required parameters for streaming chat completion\n        validate_required_params(params, [\"messages\", \"model\", \"stream\"])\n\n        response: Stream[ChatCompletionChunk] = self.client.chat.completions.create(**params)\n        for chunk in response:\n            if chunk.choices and chunk.choices[0].delta.content:\n                delta_content = chunk.choices[0].delta.content\n                delta_content = delta_content if delta_content else \"\"\n                yield MessageChunk(delta=delta_content, raw=chunk)\n\n    async def achat(\n        self,\n        messages: List[Message],\n        model: Optional[str] = None,\n        temperature: Optional[float] = None,\n        top_p: Optional[float] = None,\n        presence_penalty: Optional[float] = None,\n        frequency_penalty: Optional[float] = None,\n        max_tokens: Optional[int] = None,\n        stop: Optional[List[str]] = None,\n        tools: Optional[List[Tool]] = None,\n        extra_body: Optional[Dict[str, Any]] = None,\n        **kwargs,\n    ) -&gt; Response:\n        \"\"\"\n        Send an asynchronous chat completion request to OpenAI.\n\n        Parameters\n        ----------\n        messages : List[Message]\n            A list of messages comprising the conversation so far.\n        model : str\n            Model ID used to generate the response, like `gpt-4o` or `gpt-4`.\n        temperature : Optional[float]\n            What sampling temperature to use, between 0 and 2. Higher values like 0.8 will\n            make the output more random, while lower values like 0.2 will make it more\n            focused and deterministic.\n        top_p : Optional[float]\n            An alternative to sampling with temperature, called nucleus sampling, where the\n            model considers the results of the tokens with top_p probability mass.\n        presence_penalty : Optional[float]\n            Number between -2.0 and 2.0. Positive values penalize new tokens based on\n            whether they appear in the text so far, increasing the model's likelihood to\n            talk about new topics.\n        frequency_penalty : Optional[float]\n            Number between -2.0 and 2.0. Positive values penalize new tokens based on their\n            existing frequency in the text so far, decreasing the model's likelihood to\n            repeat the same line verbatim.\n        max_tokens : Optional[int]\n            The maximum number of tokens that can be generated in the chat completion.\n            This value is now deprecated in favor of `max_completion_tokens`.\n        stop : Optional[List[str]]\n            Up to 4 sequences where the API will stop generating further tokens.\n            Not supported with latest reasoning models `o3` and `o3-mini`.\n        tools : Optional[List[Tool]]\n            A list of tools to use in the chat completion.\n        extra_body : Optional[Dict[str, Any]]\n            Add additional JSON properties to the request.\n        **kwargs\n            Additional keyword arguments passed to the OpenAI API.\n\n        Returns\n        -------\n        Response\n            A response object containing the generated message and raw API response.\n\n        Notes\n        -----\n        This is the asynchronous version of the chat method, suitable for\n        concurrent processing and non-blocking I/O operations.\n        \"\"\"\n        params = self._build_parameters(\n            messages=messages,\n            model=model,\n            temperature=temperature,\n            top_p=top_p,\n            presence_penalty=presence_penalty,\n            frequency_penalty=frequency_penalty,\n            max_tokens=max_tokens,\n            stop=stop,\n            extra_body=extra_body,\n            **kwargs,\n        )\n        # Validate required parameters for non-streaming chat completion\n        validate_required_params(params, [\"messages\", \"model\"])\n\n        response = await self.async_client.chat.completions.create(**params)\n        return self._handle_chat_response(response)\n\n    async def astream(\n        self,\n        messages: List[Message],\n        model: Optional[str] = None,\n        temperature: Optional[float] = None,\n        top_p: Optional[float] = None,\n        presence_penalty: Optional[float] = None,\n        frequency_penalty: Optional[float] = None,\n        max_tokens: Optional[int] = None,\n        stop: Optional[List[str]] = None,\n        extra_body: Optional[Dict[str, Any]] = None,\n        **kwargs,\n    ) -&gt; AsyncStreamResponse:\n        \"\"\"\n        Send an asynchronous streaming chat completion request to OpenAI.\n\n        Parameters\n        ----------\n        messages : List[Message]\n            A list of messages comprising the conversation so far.\n        model : str\n            Model ID used to generate the response, like `gpt-4o` or `gpt-4`.\n        temperature : Optional[float]\n            What sampling temperature to use, between 0 and 2. Higher values like 0.8 will\n            make the output more random, while lower values like 0.2 will make it more\n            focused and deterministic.\n        top_p : Optional[float]\n            An alternative to sampling with temperature, called nucleus sampling, where the\n            model considers the results of the tokens with top_p probability mass.\n        presence_penalty : Optional[float]\n            Number between -2.0 and 2.0. Positive values penalize new tokens based on\n            whether they appear in the text so far, increasing the model's likelihood to\n            talk about new topics.\n        frequency_penalty : Optional[float]\n            Number between -2.0 and 2.0. Positive values penalize new tokens based on their\n            existing frequency in the text so far, decreasing the model's likelihood to\n            repeat the same line verbatim.\n        max_tokens : Optional[int]\n            The maximum number of tokens that can be generated in the chat completion.\n            This value is now deprecated in favor of `max_completion_tokens`.\n        stop : Optional[List[str]]\n            Up to 4 sequences where the API will stop generating further tokens.\n            Not supported with latest reasoning models `o3` and `o3-mini`.\n        extra_body : Optional[Dict[str, Any]]\n            Add additional JSON properties to the request.\n        **kwargs\n            Additional keyword arguments passed to the OpenAI API.\n\n        Yields\n        ------\n        MessageChunk\n            Individual chunks of the response as they are received from the API.\n            Each chunk contains a delta (partial content) and the raw response.\n\n        Notes\n        -----\n        This is the asynchronous version of the stream method, suitable for\n        concurrent processing and non-blocking streaming operations.\n        \"\"\"\n        params = self._build_parameters(\n            messages=messages,\n            model=model,\n            temperature=temperature,\n            top_p=top_p,\n            presence_penalty=presence_penalty,\n            frequency_penalty=frequency_penalty,\n            max_tokens=max_tokens,\n            stop=stop,\n            extra_body=extra_body,\n            stream=True,\n            **kwargs,\n        )\n        # Validate required parameters for streaming chat completion\n        validate_required_params(params, [\"messages\", \"model\", \"stream\"])\n\n        response = await self.async_client.chat.completions.create(**params)\n        async for chunk in response:\n            if chunk.choices and chunk.choices[0].delta.content:\n                delta_content = chunk.choices[0].delta.content\n                delta_content = delta_content if delta_content else \"\"\n                yield MessageChunk(delta=delta_content, raw=chunk)\n\n    def _build_parameters(\n        self,\n        messages: List[Message],\n        model: Optional[str] = None,\n        temperature: Optional[float] = None,\n        top_p: Optional[float] = None,\n        presence_penalty: Optional[float] = None,\n        frequency_penalty: Optional[float] = None,\n        max_tokens: Optional[int] = None,\n        stop: Optional[List[str]] = None,\n        tools: Optional[List[Tool]] = None,\n        extra_body: Optional[Dict[str, Any]] = None,\n        stream: Optional[bool] = None,\n        response_format: Optional[Dict[str, Any]] = None,\n        tool_choice: Optional[ChatCompletionToolChoiceOptionParam] = None,\n        parallel_tool_calls: Optional[bool] = None,\n        **kwargs,\n    ) -&gt; Dict[str, Any]:\n        msgs: List[ChatCompletionMessageParam] = [self._convert_chat_completions_message(msg) for msg in messages]\n\n        # Handle tools parameter - convert to list if provided, otherwise use empty list\n        json_desc_tools = [self._convert_tool_to_json(tool) for tool in tools] if tools is not None else None\n\n        # Build parameters dictionary and filter out None values\n        # The priority order is as follows: configuration passed through the interface &gt; configuration of the instance itself.\n        merge_params = merge_dict(self.configuration.model_dump(), {\n            \"messages\": msgs,\n            \"model\": model,\n            \"temperature\": temperature,\n            \"top_p\": top_p,\n            \"presence_penalty\": presence_penalty,\n            \"frequency_penalty\": frequency_penalty,\n            \"max_tokens\": max_tokens,\n            \"stop\": stop,\n            \"tools\": json_desc_tools,\n            \"extra_body\": extra_body,\n            \"stream\": stream,\n            \"response_format\": response_format,\n            \"tool_choice\": tool_choice,\n            \"parallel_tool_calls\": parallel_tool_calls,\n            **kwargs,\n        })\n\n        params = filter_dict(merge_params, exclude_none=True)\n        return params\n\n    def _handle_chat_response(self, response: ChatCompletion) -&gt; Response:\n        openai_message = response.choices[0].message\n        text = openai_message.content if openai_message.content else \"\"\n\n        if openai_message.refusal:\n            warnings.warn(openai_message.refusal, RuntimeWarning)\n\n        # Handle tool calls in the response\n        # if openai_message.tool_calls:\n        #     # Create a message with both text content and tool calls\n        #     blocks = []\n        #     if text:\n        #         blocks.append(TextBlock(text=text))\n        #     else:\n        #         # Ensure there's always some text content, even if empty\n        #         blocks.append(TextBlock(text=\"\"))\n\n        #     for tool_call in openai_message.tool_calls:\n        #         tool_call_block = ToolCallBlock(\n        #             id=tool_call.id,\n        #             name=tool_call.function.name,\n        #             arguments=json.loads(tool_call.function.arguments)\n        #         )\n        #         blocks.append(tool_call_block)\n\n        #     message = Message(role=Role.AI, blocks=blocks)\n        # else:\n        #     # Regular text response\n        #     message = Message.from_text(text, role=Role.AI)\n\n        return Response(\n            message=Message.from_text(text, role=Role.AI),\n            raw=response,\n        )\n\n    def _convert_chat_completions_message(self, message: Message) -&gt; ChatCompletionMessageParam:\n        \"\"\"\n        Convert a Bridgic Message to OpenAI ChatCompletionMessageParam.\n\n        This method handles different message types including:\n        - Text messages\n        - Messages with tool calls (ToolCallBlock)\n        - Messages with tool results (ToolResultBlock)\n\n        Parameters\n        ----\n        message : Message\n            The Bridgic message to convert\n\n        Returns\n        ----\n        ChatCompletionMessageParam\n            The converted OpenAI message parameter\n        \"\"\"\n        # Extract text content from TextBlocks and ToolResultBlocks\n        content_list = []\n        for block in message.blocks:\n            if isinstance(block, TextBlock):\n                content_list.append(block.text)\n            elif isinstance(block, ToolResultBlock):\n                content_list.append(block.content)\n        content_txt = \"\\n\\n\".join(content_list) if content_list else \"\"\n\n        # Extract tool calls from ToolCallBlocks\n        tool_calls = []\n        for block in message.blocks:\n            if isinstance(block, ToolCallBlock):\n                tool_call = ChatCompletionMessageToolCallParam(\n                    id=block.id,\n                    type=\"function\",\n                    function=Function(\n                        name=block.name,\n                        arguments=json.dumps(block.arguments)\n                    )\n                )\n                tool_calls.append(tool_call)\n\n        # Handle different message roles\n        if message.role == Role.SYSTEM:\n            return ChatCompletionSystemMessageParam(content=content_txt, role=\"system\", **message.extras)\n        elif message.role == Role.USER:\n            return ChatCompletionUserMessageParam(content=content_txt, role=\"user\", **message.extras)\n        elif message.role == Role.AI:\n            # For AI messages, include tool calls if present\n            if tool_calls:\n                return ChatCompletionAssistantMessageParam(\n                    content=content_txt, \n                    role=\"assistant\", \n                    tool_calls=tool_calls,\n                    **message.extras\n                )\n            else:\n                return ChatCompletionAssistantMessageParam(content=content_txt, role=\"assistant\", **message.extras)\n        elif message.role == Role.TOOL:\n            # For tool messages, extract tool_call_id from ToolResultBlock\n            tool_call_id = None\n            for block in message.blocks:\n                if isinstance(block, ToolResultBlock):\n                    tool_call_id = block.id\n                    break\n\n            if tool_call_id is None:\n                raise ValueError(\"Tool message must contain a ToolResultBlock with an ID\")\n\n            return ChatCompletionToolMessageParam(\n                content=content_txt, \n                role=\"tool\", \n                tool_call_id=tool_call_id,\n                **message.extras\n            )\n        else:\n            raise ValueError(f\"Invalid role: {message.role}\")\n\n    @overload\n    def structured_output(\n        self,\n        messages: List[Message],\n        constraint: PydanticModel,\n        model: Optional[str] = None,\n        temperature: Optional[float] = ...,\n        top_p: Optional[float] = ...,\n        presence_penalty: Optional[float] = ...,\n        frequency_penalty: Optional[float] = ...,\n        extra_body: Optional[Dict[str, Any]] = ...,\n        **kwargs,\n    ) -&gt; BaseModel: ...\n\n    @overload\n    def structured_output(\n        self,\n        messages: List[Message],\n        constraint: JsonSchema,\n        model: Optional[str] = None,\n        temperature: Optional[float] = ...,\n        top_p: Optional[float] = ...,\n        presence_penalty: Optional[float] = ...,\n        frequency_penalty: Optional[float] = ...,\n        extra_body: Optional[Dict[str, Any]] = ...,\n        **kwargs,\n    ) -&gt; Dict[str, Any]: ...\n\n\n    def structured_output(\n        self,\n        messages: List[Message],\n        constraint: Union[PydanticModel, JsonSchema],\n        model: Optional[str] = None,\n        temperature: Optional[float] = None,\n        top_p: Optional[float] = None,\n        presence_penalty: Optional[float] = None,\n        frequency_penalty: Optional[float] = None,\n        extra_body: Optional[Dict[str, Any]] = None,\n        **kwargs,\n    ) -&gt; Union[BaseModel, Dict[str, Any]]:\n        \"\"\"\n        Generate structured output in a specified format using OpenAI's structured output API.\n\n        This method leverages OpenAI's structured output capabilities to ensure the model\n        response conforms to a specified schema. Recommended for use with GPT-4o and later models.\n\n        Parameters\n        ----------\n        messages : List[Message]\n            A list of messages comprising the conversation so far.\n        constraint : Constraint\n            The constraint defining the desired output format (PydanticModel or JsonSchema).\n        model : str\n            Model ID used to generate the response. Structured outputs work best with GPT-4o and later.\n        temperature : Optional[float]\n            What sampling temperature to use, between 0 and 2. Higher values like 0.8 will\n            make the output more random, while lower values like 0.2 will make it more\n            focused and deterministic.\n        top_p : Optional[float]\n            An alternative to sampling with temperature, called nucleus sampling, where the\n            model considers the results of the tokens with top_p probability mass.\n        presence_penalty : Optional[float]\n            Number between -2.0 and 2.0. Positive values penalize new tokens based on\n            whether they appear in the text so far, increasing the model's likelihood to\n            talk about new topics.\n        frequency_penalty : Optional[float]\n            Number between -2.0 and 2.0. Positive values penalize new tokens based on their\n            existing frequency in the text so far, decreasing the model's likelihood to\n            repeat the same line verbatim.\n        extra_body : Optional[Dict[str, Any]]\n            Add additional JSON properties to the request.\n        **kwargs\n            Additional keyword arguments passed to the OpenAI API.\n\n        Returns\n        -------\n        Union[BaseModel, Dict[str, Any]]\n            The structured response in the format specified by the constraint:\n            - BaseModel instance if constraint is PydanticModel\n            - Dict[str, Any] if constraint is JsonSchema\n\n        Examples\n        --------\n        Using a Pydantic model constraint:\n\n        ```python\n        class Answer(BaseModel):\n            reasoning: str\n            result: int\n\n        constraint = PydanticModel(model=Answer)\n        response = llm.structured_output(\n            messages=[Message.from_text(\"What is 2+2?\", role=Role.USER)],\n            constraint=constraint,\n            model=\"gpt-4o\"\n        )\n        print(response.reasoning, response.result)\n        ```\n\n        Using a JSON schema constraint:\n\n        ```python\n        schema = {\"type\": \"object\", \"properties\": {\"answer\": {\"type\": \"string\"}}}\n        constraint = JsonSchema(schema=schema)\n        response = llm.structured_output(\n            messages=[Message.from_text(\"Hello\", role=Role.USER)],\n            constraint=constraint,\n            model=\"gpt-4o\"\n        )\n        print(response[\"answer\"])\n        ```\n\n        Notes\n        -----\n        - Utilizes OpenAI's native structured output API with strict schema validation\n        - All schemas automatically have additionalProperties set to False\n        - Best performance achieved with GPT-4o and later models (gpt-4o-mini, gpt-4o-2024-08-06, and later)\n        \"\"\"\n        params = self._build_parameters(\n            messages=messages,\n            model=model,\n            temperature=temperature,\n            top_p=top_p,\n            presence_penalty=presence_penalty,\n            frequency_penalty=frequency_penalty,\n            extra_body=extra_body,\n            response_format=self._get_response_format(constraint),\n            **kwargs,\n        )\n        # Validate required parameters for structured output\n        validate_required_params(params, [\"messages\", \"model\"])\n\n        response = self.client.chat.completions.parse(**params)\n        return self._convert_response(constraint, response.choices[0].message.content)\n\n    async def astructured_output(\n        self,\n        messages: List[Message],\n        constraint: Union[PydanticModel, JsonSchema],\n        model: Optional[str] = None,\n        temperature: Optional[float] = None,\n        top_p: Optional[float] = None,\n        presence_penalty: Optional[float] = None,\n        frequency_penalty: Optional[float] = None,\n        extra_body: Optional[Dict[str, Any]] = None,\n        **kwargs,\n    ) -&gt; Union[BaseModel, Dict[str, Any]]:\n        \"\"\"\n        Asynchronously generate structured output in a specified format using OpenAI's API.\n\n        This is the asynchronous version of structured_output, suitable for concurrent\n        processing and non-blocking operations. It leverages OpenAI's structured output\n        capabilities to ensure the model response conforms to a specified schema.\n\n        Parameters\n        ----------\n        messages : List[Message]\n            A list of messages comprising the conversation so far.\n        constraint : Constraint\n            The constraint defining the desired output format (PydanticModel or JsonSchema).\n        model : str\n            Model ID used to generate the response. Structured outputs work best with GPT-4o and later.\n        temperature : Optional[float]\n            What sampling temperature to use, between 0 and 2. Higher values like 0.8 will\n            make the output more random, while lower values like 0.2 will make it more\n            focused and deterministic.\n        top_p : Optional[float]\n            An alternative to sampling with temperature, called nucleus sampling, where the\n            model considers the results of the tokens with top_p probability mass.\n        presence_penalty : Optional[float]\n            Number between -2.0 and 2.0. Positive values penalize new tokens based on\n            whether they appear in the text so far, increasing the model's likelihood to\n            talk about new topics.\n        frequency_penalty : Optional[float]\n            Number between -2.0 and 2.0. Positive values penalize new tokens based on their\n            existing frequency in the text so far, decreasing the model's likelihood to\n            repeat the same line verbatim.\n        extra_body : Optional[Dict[str, Any]]\n            Add additional JSON properties to the request.\n        **kwargs\n            Additional keyword arguments passed to the OpenAI API.\n\n        Returns\n        -------\n        Union[BaseModel, Dict[str, Any]]\n            The structured response in the format specified by the constraint:\n            - BaseModel instance if constraint is PydanticModel\n            - Dict[str, Any] if constraint is JsonSchema\n\n        Examples\n        --------\n        Using asynchronous structured output:\n\n        ```python\n        async def get_structured_response():\n            llm = OpenAILlm(api_key=\"your-key\")\n            constraint = PydanticModel(model=Answer)\n            response = await llm.astructured_output(\n                messages=[Message.from_text(\"Calculate 5+3\", role=Role.USER)],\n                constraint=constraint,\n                model=\"gpt-4o\"\n            )\n            return response\n        ```\n\n        Notes\n        -----\n        - This is the asynchronous version of structured_output\n        - Utilizes OpenAI's native structured output API with strict schema validation\n        - Suitable for concurrent processing and high-throughput applications\n        - Best performance achieved with GPT-4o and later models (gpt-4o-mini, gpt-4o-2024-08-06, and later)\n        \"\"\"\n        params = self._build_parameters(\n            messages=messages,\n            model=model,\n            temperature=temperature,\n            top_p=top_p,\n            presence_penalty=presence_penalty,\n            frequency_penalty=frequency_penalty,\n            extra_body=extra_body,\n            response_format=self._get_response_format(constraint),\n            **kwargs,\n        )\n        # Validate required parameters for structured output\n        validate_required_params(params, [\"messages\", \"model\"])\n\n        response = await self.async_client.chat.completions.parse(**params)\n        return self._convert_response(constraint, response.choices[0].message.content)\n\n    def _add_schema_properties(self, schema: Dict[str, Any]) -&gt; Dict[str, Any]:\n        \"\"\"\n        OpenAI requires additionalProperties to be set to False for all objects\n        in structured output schemas. See:\n        [AdditionalProperties False Must Always Be Set in Objects](https://platform.openai.com/docs/guides/structured-outputs?example=moderation#additionalproperties-false-must-always-be-set-in-objects)\n        \"\"\"\n        schema[\"additionalProperties\"] = False\n        return schema\n\n    def _get_response_format(self, constraint: Union[PydanticModel, JsonSchema]) -&gt; Dict[str, Any]:\n        if isinstance(constraint, PydanticModel):\n            result = {\n                \"type\": \"json_schema\",\n                \"json_schema\": {\n                    \"schema\": self._add_schema_properties(constraint.model.model_json_schema()),\n                    \"name\": constraint.model.__name__,\n                    \"strict\": True,\n                },\n            }\n            return result\n        elif isinstance(constraint, JsonSchema):\n            return {\n                \"type\": \"json_schema\",\n                \"json_schema\": {\n                    \"schema\": self._add_schema_properties(constraint.schema_dict),\n                    # default name for schema\n                    \"name\": \"schema\",\n                    \"strict\": True,\n                },\n            }\n        else:\n            raise ValueError(f\"Unsupported constraint type '{constraint.constraint_type}'. More info about OpenAI structured output: https://platform.openai.com/docs/guides/structured-outputs\")\n\n    def _convert_response(\n        self,\n        constraint: Union[PydanticModel, JsonSchema],\n        content: str,\n    ) -&gt; Union[BaseModel, Dict[str, Any]]:\n        if isinstance(constraint, PydanticModel):\n            return constraint.model.model_validate_json(content)\n        elif isinstance(constraint, JsonSchema):\n            return json.loads(content)\n        else:\n            raise ValueError(f\"Unsupported constraint type '{constraint.constraint_type}'. More info about OpenAI structured output: https://platform.openai.com/docs/guides/structured-outputs\")\n\n    def select_tool(\n        self,\n        messages: List[Message],\n        tools: List[Tool],\n        model: Optional[str] = None,\n        temperature: Optional[float] = None,\n        top_p: Optional[float] = None,\n        presence_penalty: Optional[float] = None,\n        frequency_penalty: Optional[float] = None,\n        extra_body: Optional[Dict[str, Any]] = None,\n        parallel_tool_calls: Optional[bool] = None,\n        tool_choice: Union[Literal[\"auto\", \"required\", \"none\"], ChatCompletionNamedToolChoiceParam] = None,\n        **kwargs,\n    ) -&gt; Tuple[List[ToolCall], Optional[str]]:\n        \"\"\"\n        Select and invoke tools from a list based on conversation context.\n\n        This method enables the model to intelligently select and call appropriate tools\n        from a provided list based on the conversation context. It supports OpenAI's\n        function calling capabilities with parallel execution and various control options.\n\n        More OpenAI information: [function-calling](https://platform.openai.com/docs/guides/function-calling)\n\n        Parameters\n        ----------\n        messages : List[Message]\n            A list of messages comprising the conversation so far providing context for tool selection.\n        tools : List[Tool]\n            A list of tools the model may call.\n        model : str\n            Model ID used to generate the response. Function calling requires compatible models.\n        temperature : Optional[float]\n            What sampling temperature to use, between 0 and 2. Higher values like 0.8 will\n            make the output more random, while lower values like 0.2 will make it more\n            focused and deterministic.\n        top_p : Optional[float]\n            An alternative to sampling with temperature, called nucleus sampling, where the\n            model considers the results of the tokens with top_p probability mass.\n        presence_penalty : Optional[float]\n            Number between -2.0 and 2.0. Positive values penalize new tokens based on\n            whether they appear in the text so far, increasing the model's likelihood to\n            talk about new topics.\n        frequency_penalty : Optional[float]\n            Number between -2.0 and 2.0. Positive values penalize new tokens based on their\n            existing frequency in the text so far, decreasing the model's likelihood to\n            repeat the same line verbatim.\n        extra_body : Optional[Dict[str, Any]]\n            Add additional JSON properties to the request.\n        parallel_tool_calls : Optional[bool]\n            Whether to enable parallel function calling during tool use.\n        tool_choice : Union[Literal[\"auto\", \"required\", \"none\"], ChatCompletionNamedToolChoiceParam]\n            Controls which tool, if any, the model may call.\n            - `none`: The model will not call any tool and will instead generate a message. This is the default when no tools are provided.\n            - `auto`: The model may choose to generate a message or call one or more tools. This is the default when tools are provided.\n            - `required`: The model must call one or more tools.\n            - To force a specific tool, pass `{\"type\": \"function\", \"function\": {\"name\": \"my_function\"}}`.\n        **kwargs\n            Additional keyword arguments passed to the OpenAI API.\n\n        Returns\n        -------\n        List[ToolCall]\n            List of selected tool calls with their IDs, names, and parsed arguments.\n        Union[str, None]\n            The content of the message from the model.\n        \"\"\"\n        params = self._build_parameters(\n            messages=messages,\n            model=model,\n            temperature=temperature,\n            top_p=top_p,\n            presence_penalty=presence_penalty,\n            frequency_penalty=frequency_penalty,\n            tools=tools,\n            tool_choice=tool_choice,\n            parallel_tool_calls=parallel_tool_calls,\n            extra_body=extra_body,\n            **kwargs,\n        )\n        # Validate required parameters for tool selection\n        validate_required_params(params, [\"messages\", \"model\"])\n\n        response: ChatCompletion = self.client.chat.completions.create(**params)\n        tool_calls = response.choices[0].message.tool_calls\n        content = response.choices[0].message.content\n        return (self._convert_tool_calls(tool_calls), content)\n\n    async def aselect_tool(\n        self,\n        messages: List[Message],\n        tools: List[Tool],\n        model: Optional[str] = None,\n        temperature: Optional[float] = None,\n        top_p: Optional[float] = None,\n        presence_penalty: Optional[float] = None,\n        frequency_penalty: Optional[float] = None,\n        extra_body: Optional[Dict[str, Any]] = None,\n        parallel_tool_calls: Optional[bool] = None,\n        tool_choice: Union[Literal[\"auto\", \"required\", \"none\"], ChatCompletionNamedToolChoiceParam] = None,\n        **kwargs,\n    )-&gt; Tuple[List[ToolCall], Optional[str]]:\n        \"\"\"\n        Select and invoke tools from a list based on conversation context.\n\n        This method enables the model to intelligently select and call appropriate tools\n        from a provided list based on the conversation context. It supports OpenAI's\n        function calling capabilities with parallel execution and various control options.\n\n        More OpenAI information: [function-calling](https://platform.openai.com/docs/guides/function-calling)\n\n        Parameters\n        ----------\n        messages : List[Message]\n            A list of messages comprising the conversation so far providing context for tool selection.\n        tools : List[Tool]\n            A list of tools the model may call.\n        model : str\n            Model ID used to generate the response. Function calling requires compatible models.\n        temperature : Optional[float]\n            What sampling temperature to use, between 0 and 2. Higher values like 0.8 will\n            make the output more random, while lower values like 0.2 will make it more\n            focused and deterministic.\n        top_p : Optional[float]\n            An alternative to sampling with temperature, called nucleus sampling, where the\n            model considers the results of the tokens with top_p probability mass.\n        presence_penalty : Optional[float]\n            Number between -2.0 and 2.0. Positive values penalize new tokens based on\n            whether they appear in the text so far, increasing the model's likelihood to\n            talk about new topics.\n        frequency_penalty : Optional[float]\n            Number between -2.0 and 2.0. Positive values penalize new tokens based on their\n            existing frequency in the text so far, decreasing the model's likelihood to\n            repeat the same line verbatim.\n        extra_body : Optional[Dict[str, Any]]\n            Add additional JSON properties to the request.\n        parallel_tool_calls : Optional[bool]\n            Whether to enable parallel function calling during tool use.\n        tool_choice : Union[Literal[\"auto\", \"required\", \"none\"], ChatCompletionNamedToolChoiceParam]\n            Controls which tool, if any, the model may call.\n            - `none`: The model will not call any tool and will instead generate a message. This is the default when no tools are provided.\n            - `auto`: The model may choose to generate a message or call one or more tools. This is the default when tools are provided.\n            - `required`: The model must call one or more tools.\n            - To force a specific tool, pass `{\"type\": \"function\", \"function\": {\"name\": \"my_function\"}}`.\n\n        **kwargs\n            Additional keyword arguments passed to the OpenAI API.\n\n        Returns\n        -------\n        List[ToolCall]\n            List of selected tool calls with their IDs, names, and parsed arguments.\n        Union[str, None]\n            The content of the message from the model.\n        \"\"\"\n        params = self._build_parameters(\n            messages=messages,\n            model=model,\n            temperature=temperature,\n            top_p=top_p,\n            presence_penalty=presence_penalty,\n            frequency_penalty=frequency_penalty,\n            tools=tools,\n            tool_choice=tool_choice,\n            parallel_tool_calls=parallel_tool_calls,\n            extra_body=extra_body,\n            **kwargs,\n        )\n        # Validate required parameters for tool selection\n        validate_required_params(params, [\"messages\", \"model\"])\n\n        response: ChatCompletion = await self.async_client.chat.completions.create(**params)\n        tool_calls = response.choices[0].message.tool_calls\n        content = response.choices[0].message.content\n        return (self._convert_tool_calls(tool_calls), content)\n\n    def _convert_parameters(self, parameters: Dict[str, Any]) -&gt; Dict[str, Any]:\n        return {\n            \"type\": \"object\",\n            \"properties\": parameters.get(\"properties\", {}),\n            \"required\": parameters.get(\"required\", []),\n            \"additionalProperties\": False\n        }\n\n    def _convert_tool_to_json(self, tool: Tool) -&gt; Dict[str, Any]:\n        return {\n            \"type\": \"function\",\n            \"function\": {\n                \"name\": tool.name,\n                \"description\": tool.description,\n                \"parameters\": self._convert_parameters(tool.parameters),\n            }\n        }\n\n    def _convert_tool_calls(self, tool_calls: List[ChatCompletionMessageFunctionToolCall]) -&gt; List[ToolCall]:\n        return [] if tool_calls is None else [\n            ToolCall(\n                id=generate_tool_id(),\n                name=tool_call.function.name,\n                arguments=json.loads(tool_call.function.arguments),\n            ) for tool_call in tool_calls\n        ]\n\n    @override\n    def dump_to_dict(self) -&gt; Dict[str, Any]:\n        state_dict = {\n            \"api_base\": self.api_base,\n            \"api_key\": self.api_key,\n            \"timeout\": self.timeout,\n            \"configuration\": self.configuration.model_dump(),\n        }\n        if self.http_client:\n            warnings.warn(\n                \"httpx.Client is not serializable, so it will be set to None in the deserialization.\",\n                RuntimeWarning,\n            )\n        if self.http_async_client:\n            warnings.warn(\n                \"httpx.AsyncClient is not serializable, so it will be set to None in the deserialization.\",\n                RuntimeWarning,\n            )\n        return state_dict\n\n    @override\n    def load_from_dict(self, state_dict: Dict[str, Any]) -&gt; None:\n        self.api_base = state_dict[\"api_base\"]\n        self.api_key = state_dict[\"api_key\"]\n        self.timeout = state_dict[\"timeout\"]\n        self.configuration = OpenAIConfiguration(**state_dict.get(\"configuration\", {}))\n        self.http_client = None\n        self.http_async_client = None\n\n        self.client = OpenAI(\n            base_url=self.api_base,\n            api_key=self.api_key,\n            timeout=self.timeout,\n            http_client=self.http_client,\n        )\n        self.async_client = AsyncOpenAI(\n            base_url=self.api_base,\n            api_key=self.api_key,\n            timeout=self.timeout,\n            http_client=self.http_async_client,\n        )\n</code></pre>"},{"location":"reference/bridgic-llms-openai/bridgic/llms/openai/#bridgic.llms.openai.OpenAILlm.chat","title":"chat","text":"<pre><code>chat(\n    messages: List[Message],\n    model: Optional[str] = None,\n    temperature: Optional[float] = None,\n    top_p: Optional[float] = None,\n    presence_penalty: Optional[float] = None,\n    frequency_penalty: Optional[float] = None,\n    max_tokens: Optional[int] = None,\n    stop: Optional[List[str]] = None,\n    tools: Optional[List[Tool]] = None,\n    extra_body: Optional[Dict[str, Any]] = None,\n    **kwargs\n) -&gt; Response\n</code></pre> <p>Send a synchronous chat completion request to OpenAI.</p> <p>Parameters:</p> Name Type Description Default <code>messages</code> <code>List[Message]</code> <p>A list of messages comprising the conversation so far.</p> required <code>model</code> <code>str</code> <p>Model ID used to generate the response, like <code>gpt-4o</code> or <code>gpt-4</code>.</p> <code>None</code> <code>temperature</code> <code>Optional[float]</code> <p>What sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic.</p> <code>None</code> <code>top_p</code> <code>Optional[float]</code> <p>An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass.</p> <code>None</code> <code>presence_penalty</code> <code>Optional[float]</code> <p>Number between -2.0 and 2.0. Positive values penalize new tokens based on whether they appear in the text so far, increasing the model's likelihood to talk about new topics.</p> <code>None</code> <code>frequency_penalty</code> <code>Optional[float]</code> <p>Number between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model's likelihood to repeat the same line verbatim.</p> <code>None</code> <code>max_tokens</code> <code>Optional[int]</code> <p>The maximum number of tokens that can be generated in the chat completion. This value is now deprecated in favor of <code>max_completion_tokens</code>.</p> <code>None</code> <code>stop</code> <code>Optional[List[str]]</code> <p>Up to 4 sequences where the API will stop generating further tokens. Not supported with latest reasoning models <code>o3</code> and <code>o3-mini</code>.</p> <code>None</code> <code>tools</code> <code>Optional[List[Tool]]</code> <p>A list of tools to use in the chat completion.</p> <code>None</code> <code>extra_body</code> <code>Optional[Dict[str, Any]]</code> <p>Add additional JSON properties to the request.</p> <code>None</code> <code>**kwargs</code> <p>Additional keyword arguments passed to the OpenAI API.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Response</code> <p>A response object containing the generated message and raw API response.</p> Source code in <code>bridgic/llms/openai/_openai_llm.py</code> <pre><code>def chat(\n    self,\n    messages: List[Message],\n    model: Optional[str] = None,\n    temperature: Optional[float] = None,\n    top_p: Optional[float] = None,\n    presence_penalty: Optional[float] = None,\n    frequency_penalty: Optional[float] = None,\n    max_tokens: Optional[int] = None,\n    stop: Optional[List[str]] = None,\n    tools: Optional[List[Tool]] = None,\n    extra_body: Optional[Dict[str, Any]] = None,\n    **kwargs,\n) -&gt; Response:\n    \"\"\"\n    Send a synchronous chat completion request to OpenAI.\n\n    Parameters\n    ----------\n    messages : List[Message]\n        A list of messages comprising the conversation so far.\n    model : str\n        Model ID used to generate the response, like `gpt-4o` or `gpt-4`.\n    temperature : Optional[float]\n        What sampling temperature to use, between 0 and 2. Higher values like 0.8 will\n        make the output more random, while lower values like 0.2 will make it more\n        focused and deterministic.\n    top_p : Optional[float]\n        An alternative to sampling with temperature, called nucleus sampling, where the\n        model considers the results of the tokens with top_p probability mass.\n    presence_penalty : Optional[float]\n        Number between -2.0 and 2.0. Positive values penalize new tokens based on\n        whether they appear in the text so far, increasing the model's likelihood to\n        talk about new topics.\n    frequency_penalty : Optional[float]\n        Number between -2.0 and 2.0. Positive values penalize new tokens based on their\n        existing frequency in the text so far, decreasing the model's likelihood to\n        repeat the same line verbatim.\n    max_tokens : Optional[int]\n        The maximum number of tokens that can be generated in the chat completion.\n        This value is now deprecated in favor of `max_completion_tokens`.\n    stop : Optional[List[str]]\n        Up to 4 sequences where the API will stop generating further tokens.\n        Not supported with latest reasoning models `o3` and `o3-mini`.\n    tools : Optional[List[Tool]]\n        A list of tools to use in the chat completion.\n    extra_body : Optional[Dict[str, Any]]\n        Add additional JSON properties to the request.\n    **kwargs\n        Additional keyword arguments passed to the OpenAI API.\n\n    Returns\n    -------\n    Response\n        A response object containing the generated message and raw API response.\n    \"\"\"\n    params = self._build_parameters(\n        messages=messages,\n        model=model,\n        temperature=temperature,\n        top_p=top_p,\n        presence_penalty=presence_penalty,\n        frequency_penalty=frequency_penalty,\n        max_tokens=max_tokens,\n        stop=stop,\n        extra_body=extra_body,\n        **kwargs,\n    )\n    # Validate required parameters for non-streaming chat completion\n    validate_required_params(params, [\"messages\", \"model\"])\n\n    response: ChatCompletion = self.client.chat.completions.create(**params)\n    return self._handle_chat_response(response)\n</code></pre>"},{"location":"reference/bridgic-llms-openai/bridgic/llms/openai/#bridgic.llms.openai.OpenAILlm.stream","title":"stream","text":"<pre><code>stream(\n    messages: List[Message],\n    model: Optional[str] = None,\n    temperature: Optional[float] = None,\n    top_p: Optional[float] = None,\n    presence_penalty: Optional[float] = None,\n    frequency_penalty: Optional[float] = None,\n    max_tokens: Optional[int] = None,\n    stop: Optional[List[str]] = None,\n    extra_body: Optional[Dict[str, Any]] = None,\n    **kwargs\n) -&gt; StreamResponse\n</code></pre> <p>Send a streaming chat completion request to OpenAI.</p> <p>Parameters:</p> Name Type Description Default <code>messages</code> <code>List[Message]</code> <p>A list of messages comprising the conversation so far.</p> required <code>model</code> <code>str</code> <p>Model ID used to generate the response, like <code>gpt-4o</code> or <code>gpt-4</code>.</p> <code>None</code> <code>temperature</code> <code>Optional[float]</code> <p>What sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic.</p> <code>None</code> <code>top_p</code> <code>Optional[float]</code> <p>An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass.</p> <code>None</code> <code>presence_penalty</code> <code>Optional[float]</code> <p>Number between -2.0 and 2.0. Positive values penalize new tokens based on whether they appear in the text so far, increasing the model's likelihood to talk about new topics.</p> <code>None</code> <code>frequency_penalty</code> <code>Optional[float]</code> <p>Number between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model's likelihood to repeat the same line verbatim.</p> <code>None</code> <code>max_tokens</code> <code>Optional[int]</code> <p>The maximum number of tokens that can be generated in the chat completion. This value is now deprecated in favor of <code>max_completion_tokens</code>.</p> <code>None</code> <code>stop</code> <code>Optional[List[str]]</code> <p>Up to 4 sequences where the API will stop generating further tokens. Not supported with latest reasoning models <code>o3</code> and <code>o3-mini</code>.</p> <code>None</code> <code>extra_body</code> <code>Optional[Dict[str, Any]]</code> <p>Add additional JSON properties to the request.</p> <code>None</code> <code>**kwargs</code> <p>Additional keyword arguments passed to the OpenAI API.</p> <code>{}</code> <p>Yields:</p> Type Description <code>MessageChunk</code> <p>Individual chunks of the response as they are received from the API. Each chunk contains a delta (partial content) and the raw response.</p> Notes <p>This method enables real-time streaming of the model's response, useful for providing incremental updates to users as the response is generated.</p> Source code in <code>bridgic/llms/openai/_openai_llm.py</code> <pre><code>def stream(\n    self,\n    messages: List[Message],\n    model: Optional[str] = None,\n    temperature: Optional[float] = None,\n    top_p: Optional[float] = None,\n    presence_penalty: Optional[float] = None,\n    frequency_penalty: Optional[float] = None,\n    max_tokens: Optional[int] = None,\n    stop: Optional[List[str]] = None,\n    extra_body: Optional[Dict[str, Any]] = None,\n    **kwargs,\n) -&gt; StreamResponse:\n    \"\"\"\n    Send a streaming chat completion request to OpenAI.\n\n    Parameters\n    ----------\n    messages : List[Message]\n        A list of messages comprising the conversation so far.\n    model : str\n        Model ID used to generate the response, like `gpt-4o` or `gpt-4`.\n    temperature : Optional[float]\n        What sampling temperature to use, between 0 and 2. Higher values like 0.8 will\n        make the output more random, while lower values like 0.2 will make it more\n        focused and deterministic.\n    top_p : Optional[float]\n        An alternative to sampling with temperature, called nucleus sampling, where the\n        model considers the results of the tokens with top_p probability mass.\n    presence_penalty : Optional[float]\n        Number between -2.0 and 2.0. Positive values penalize new tokens based on\n        whether they appear in the text so far, increasing the model's likelihood to\n        talk about new topics.\n    frequency_penalty : Optional[float]\n        Number between -2.0 and 2.0. Positive values penalize new tokens based on their\n        existing frequency in the text so far, decreasing the model's likelihood to\n        repeat the same line verbatim.\n    max_tokens : Optional[int]\n        The maximum number of tokens that can be generated in the chat completion.\n        This value is now deprecated in favor of `max_completion_tokens`.\n    stop : Optional[List[str]]\n        Up to 4 sequences where the API will stop generating further tokens.\n        Not supported with latest reasoning models `o3` and `o3-mini`.\n    extra_body : Optional[Dict[str, Any]]\n        Add additional JSON properties to the request.\n    **kwargs\n        Additional keyword arguments passed to the OpenAI API.\n\n    Yields\n    ------\n    MessageChunk\n        Individual chunks of the response as they are received from the API.\n        Each chunk contains a delta (partial content) and the raw response.\n\n    Notes\n    -----\n    This method enables real-time streaming of the model's response,\n    useful for providing incremental updates to users as the response is generated.\n    \"\"\"\n    params = self._build_parameters(\n        messages=messages,\n        model=model,\n        temperature=temperature,\n        top_p=top_p,\n        presence_penalty=presence_penalty,\n        frequency_penalty=frequency_penalty,\n        max_tokens=max_tokens,\n        stop=stop,\n        extra_body=extra_body,\n        stream=True,\n        **kwargs,\n    )\n    # Validate required parameters for streaming chat completion\n    validate_required_params(params, [\"messages\", \"model\", \"stream\"])\n\n    response: Stream[ChatCompletionChunk] = self.client.chat.completions.create(**params)\n    for chunk in response:\n        if chunk.choices and chunk.choices[0].delta.content:\n            delta_content = chunk.choices[0].delta.content\n            delta_content = delta_content if delta_content else \"\"\n            yield MessageChunk(delta=delta_content, raw=chunk)\n</code></pre>"},{"location":"reference/bridgic-llms-openai/bridgic/llms/openai/#bridgic.llms.openai.OpenAILlm.achat","title":"achat","text":"<code>async</code> <pre><code>achat(\n    messages: List[Message],\n    model: Optional[str] = None,\n    temperature: Optional[float] = None,\n    top_p: Optional[float] = None,\n    presence_penalty: Optional[float] = None,\n    frequency_penalty: Optional[float] = None,\n    max_tokens: Optional[int] = None,\n    stop: Optional[List[str]] = None,\n    tools: Optional[List[Tool]] = None,\n    extra_body: Optional[Dict[str, Any]] = None,\n    **kwargs\n) -&gt; Response\n</code></pre> <p>Send an asynchronous chat completion request to OpenAI.</p> <p>Parameters:</p> Name Type Description Default <code>messages</code> <code>List[Message]</code> <p>A list of messages comprising the conversation so far.</p> required <code>model</code> <code>str</code> <p>Model ID used to generate the response, like <code>gpt-4o</code> or <code>gpt-4</code>.</p> <code>None</code> <code>temperature</code> <code>Optional[float]</code> <p>What sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic.</p> <code>None</code> <code>top_p</code> <code>Optional[float]</code> <p>An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass.</p> <code>None</code> <code>presence_penalty</code> <code>Optional[float]</code> <p>Number between -2.0 and 2.0. Positive values penalize new tokens based on whether they appear in the text so far, increasing the model's likelihood to talk about new topics.</p> <code>None</code> <code>frequency_penalty</code> <code>Optional[float]</code> <p>Number between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model's likelihood to repeat the same line verbatim.</p> <code>None</code> <code>max_tokens</code> <code>Optional[int]</code> <p>The maximum number of tokens that can be generated in the chat completion. This value is now deprecated in favor of <code>max_completion_tokens</code>.</p> <code>None</code> <code>stop</code> <code>Optional[List[str]]</code> <p>Up to 4 sequences where the API will stop generating further tokens. Not supported with latest reasoning models <code>o3</code> and <code>o3-mini</code>.</p> <code>None</code> <code>tools</code> <code>Optional[List[Tool]]</code> <p>A list of tools to use in the chat completion.</p> <code>None</code> <code>extra_body</code> <code>Optional[Dict[str, Any]]</code> <p>Add additional JSON properties to the request.</p> <code>None</code> <code>**kwargs</code> <p>Additional keyword arguments passed to the OpenAI API.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Response</code> <p>A response object containing the generated message and raw API response.</p> Notes <p>This is the asynchronous version of the chat method, suitable for concurrent processing and non-blocking I/O operations.</p> Source code in <code>bridgic/llms/openai/_openai_llm.py</code> <pre><code>async def achat(\n    self,\n    messages: List[Message],\n    model: Optional[str] = None,\n    temperature: Optional[float] = None,\n    top_p: Optional[float] = None,\n    presence_penalty: Optional[float] = None,\n    frequency_penalty: Optional[float] = None,\n    max_tokens: Optional[int] = None,\n    stop: Optional[List[str]] = None,\n    tools: Optional[List[Tool]] = None,\n    extra_body: Optional[Dict[str, Any]] = None,\n    **kwargs,\n) -&gt; Response:\n    \"\"\"\n    Send an asynchronous chat completion request to OpenAI.\n\n    Parameters\n    ----------\n    messages : List[Message]\n        A list of messages comprising the conversation so far.\n    model : str\n        Model ID used to generate the response, like `gpt-4o` or `gpt-4`.\n    temperature : Optional[float]\n        What sampling temperature to use, between 0 and 2. Higher values like 0.8 will\n        make the output more random, while lower values like 0.2 will make it more\n        focused and deterministic.\n    top_p : Optional[float]\n        An alternative to sampling with temperature, called nucleus sampling, where the\n        model considers the results of the tokens with top_p probability mass.\n    presence_penalty : Optional[float]\n        Number between -2.0 and 2.0. Positive values penalize new tokens based on\n        whether they appear in the text so far, increasing the model's likelihood to\n        talk about new topics.\n    frequency_penalty : Optional[float]\n        Number between -2.0 and 2.0. Positive values penalize new tokens based on their\n        existing frequency in the text so far, decreasing the model's likelihood to\n        repeat the same line verbatim.\n    max_tokens : Optional[int]\n        The maximum number of tokens that can be generated in the chat completion.\n        This value is now deprecated in favor of `max_completion_tokens`.\n    stop : Optional[List[str]]\n        Up to 4 sequences where the API will stop generating further tokens.\n        Not supported with latest reasoning models `o3` and `o3-mini`.\n    tools : Optional[List[Tool]]\n        A list of tools to use in the chat completion.\n    extra_body : Optional[Dict[str, Any]]\n        Add additional JSON properties to the request.\n    **kwargs\n        Additional keyword arguments passed to the OpenAI API.\n\n    Returns\n    -------\n    Response\n        A response object containing the generated message and raw API response.\n\n    Notes\n    -----\n    This is the asynchronous version of the chat method, suitable for\n    concurrent processing and non-blocking I/O operations.\n    \"\"\"\n    params = self._build_parameters(\n        messages=messages,\n        model=model,\n        temperature=temperature,\n        top_p=top_p,\n        presence_penalty=presence_penalty,\n        frequency_penalty=frequency_penalty,\n        max_tokens=max_tokens,\n        stop=stop,\n        extra_body=extra_body,\n        **kwargs,\n    )\n    # Validate required parameters for non-streaming chat completion\n    validate_required_params(params, [\"messages\", \"model\"])\n\n    response = await self.async_client.chat.completions.create(**params)\n    return self._handle_chat_response(response)\n</code></pre>"},{"location":"reference/bridgic-llms-openai/bridgic/llms/openai/#bridgic.llms.openai.OpenAILlm.astream","title":"astream","text":"<code>async</code> <pre><code>astream(\n    messages: List[Message],\n    model: Optional[str] = None,\n    temperature: Optional[float] = None,\n    top_p: Optional[float] = None,\n    presence_penalty: Optional[float] = None,\n    frequency_penalty: Optional[float] = None,\n    max_tokens: Optional[int] = None,\n    stop: Optional[List[str]] = None,\n    extra_body: Optional[Dict[str, Any]] = None,\n    **kwargs\n) -&gt; AsyncStreamResponse\n</code></pre> <p>Send an asynchronous streaming chat completion request to OpenAI.</p> <p>Parameters:</p> Name Type Description Default <code>messages</code> <code>List[Message]</code> <p>A list of messages comprising the conversation so far.</p> required <code>model</code> <code>str</code> <p>Model ID used to generate the response, like <code>gpt-4o</code> or <code>gpt-4</code>.</p> <code>None</code> <code>temperature</code> <code>Optional[float]</code> <p>What sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic.</p> <code>None</code> <code>top_p</code> <code>Optional[float]</code> <p>An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass.</p> <code>None</code> <code>presence_penalty</code> <code>Optional[float]</code> <p>Number between -2.0 and 2.0. Positive values penalize new tokens based on whether they appear in the text so far, increasing the model's likelihood to talk about new topics.</p> <code>None</code> <code>frequency_penalty</code> <code>Optional[float]</code> <p>Number between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model's likelihood to repeat the same line verbatim.</p> <code>None</code> <code>max_tokens</code> <code>Optional[int]</code> <p>The maximum number of tokens that can be generated in the chat completion. This value is now deprecated in favor of <code>max_completion_tokens</code>.</p> <code>None</code> <code>stop</code> <code>Optional[List[str]]</code> <p>Up to 4 sequences where the API will stop generating further tokens. Not supported with latest reasoning models <code>o3</code> and <code>o3-mini</code>.</p> <code>None</code> <code>extra_body</code> <code>Optional[Dict[str, Any]]</code> <p>Add additional JSON properties to the request.</p> <code>None</code> <code>**kwargs</code> <p>Additional keyword arguments passed to the OpenAI API.</p> <code>{}</code> <p>Yields:</p> Type Description <code>MessageChunk</code> <p>Individual chunks of the response as they are received from the API. Each chunk contains a delta (partial content) and the raw response.</p> Notes <p>This is the asynchronous version of the stream method, suitable for concurrent processing and non-blocking streaming operations.</p> Source code in <code>bridgic/llms/openai/_openai_llm.py</code> <pre><code>async def astream(\n    self,\n    messages: List[Message],\n    model: Optional[str] = None,\n    temperature: Optional[float] = None,\n    top_p: Optional[float] = None,\n    presence_penalty: Optional[float] = None,\n    frequency_penalty: Optional[float] = None,\n    max_tokens: Optional[int] = None,\n    stop: Optional[List[str]] = None,\n    extra_body: Optional[Dict[str, Any]] = None,\n    **kwargs,\n) -&gt; AsyncStreamResponse:\n    \"\"\"\n    Send an asynchronous streaming chat completion request to OpenAI.\n\n    Parameters\n    ----------\n    messages : List[Message]\n        A list of messages comprising the conversation so far.\n    model : str\n        Model ID used to generate the response, like `gpt-4o` or `gpt-4`.\n    temperature : Optional[float]\n        What sampling temperature to use, between 0 and 2. Higher values like 0.8 will\n        make the output more random, while lower values like 0.2 will make it more\n        focused and deterministic.\n    top_p : Optional[float]\n        An alternative to sampling with temperature, called nucleus sampling, where the\n        model considers the results of the tokens with top_p probability mass.\n    presence_penalty : Optional[float]\n        Number between -2.0 and 2.0. Positive values penalize new tokens based on\n        whether they appear in the text so far, increasing the model's likelihood to\n        talk about new topics.\n    frequency_penalty : Optional[float]\n        Number between -2.0 and 2.0. Positive values penalize new tokens based on their\n        existing frequency in the text so far, decreasing the model's likelihood to\n        repeat the same line verbatim.\n    max_tokens : Optional[int]\n        The maximum number of tokens that can be generated in the chat completion.\n        This value is now deprecated in favor of `max_completion_tokens`.\n    stop : Optional[List[str]]\n        Up to 4 sequences where the API will stop generating further tokens.\n        Not supported with latest reasoning models `o3` and `o3-mini`.\n    extra_body : Optional[Dict[str, Any]]\n        Add additional JSON properties to the request.\n    **kwargs\n        Additional keyword arguments passed to the OpenAI API.\n\n    Yields\n    ------\n    MessageChunk\n        Individual chunks of the response as they are received from the API.\n        Each chunk contains a delta (partial content) and the raw response.\n\n    Notes\n    -----\n    This is the asynchronous version of the stream method, suitable for\n    concurrent processing and non-blocking streaming operations.\n    \"\"\"\n    params = self._build_parameters(\n        messages=messages,\n        model=model,\n        temperature=temperature,\n        top_p=top_p,\n        presence_penalty=presence_penalty,\n        frequency_penalty=frequency_penalty,\n        max_tokens=max_tokens,\n        stop=stop,\n        extra_body=extra_body,\n        stream=True,\n        **kwargs,\n    )\n    # Validate required parameters for streaming chat completion\n    validate_required_params(params, [\"messages\", \"model\", \"stream\"])\n\n    response = await self.async_client.chat.completions.create(**params)\n    async for chunk in response:\n        if chunk.choices and chunk.choices[0].delta.content:\n            delta_content = chunk.choices[0].delta.content\n            delta_content = delta_content if delta_content else \"\"\n            yield MessageChunk(delta=delta_content, raw=chunk)\n</code></pre>"},{"location":"reference/bridgic-llms-openai/bridgic/llms/openai/#bridgic.llms.openai.OpenAILlm.structured_output","title":"structured_output","text":"<pre><code>structured_output(\n    messages: List[Message],\n    constraint: Union[PydanticModel, JsonSchema],\n    model: Optional[str] = None,\n    temperature: Optional[float] = None,\n    top_p: Optional[float] = None,\n    presence_penalty: Optional[float] = None,\n    frequency_penalty: Optional[float] = None,\n    extra_body: Optional[Dict[str, Any]] = None,\n    **kwargs\n) -&gt; Union[BaseModel, Dict[str, Any]]\n</code></pre> <p>Generate structured output in a specified format using OpenAI's structured output API.</p> <p>This method leverages OpenAI's structured output capabilities to ensure the model response conforms to a specified schema. Recommended for use with GPT-4o and later models.</p> <p>Parameters:</p> Name Type Description Default <code>messages</code> <code>List[Message]</code> <p>A list of messages comprising the conversation so far.</p> required <code>constraint</code> <code>Constraint</code> <p>The constraint defining the desired output format (PydanticModel or JsonSchema).</p> required <code>model</code> <code>str</code> <p>Model ID used to generate the response. Structured outputs work best with GPT-4o and later.</p> <code>None</code> <code>temperature</code> <code>Optional[float]</code> <p>What sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic.</p> <code>None</code> <code>top_p</code> <code>Optional[float]</code> <p>An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass.</p> <code>None</code> <code>presence_penalty</code> <code>Optional[float]</code> <p>Number between -2.0 and 2.0. Positive values penalize new tokens based on whether they appear in the text so far, increasing the model's likelihood to talk about new topics.</p> <code>None</code> <code>frequency_penalty</code> <code>Optional[float]</code> <p>Number between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model's likelihood to repeat the same line verbatim.</p> <code>None</code> <code>extra_body</code> <code>Optional[Dict[str, Any]]</code> <p>Add additional JSON properties to the request.</p> <code>None</code> <code>**kwargs</code> <p>Additional keyword arguments passed to the OpenAI API.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Union[BaseModel, Dict[str, Any]]</code> <p>The structured response in the format specified by the constraint: - BaseModel instance if constraint is PydanticModel - Dict[str, Any] if constraint is JsonSchema</p> <p>Examples:</p> <p>Using a Pydantic model constraint:</p> <pre><code>class Answer(BaseModel):\n    reasoning: str\n    result: int\n\nconstraint = PydanticModel(model=Answer)\nresponse = llm.structured_output(\n    messages=[Message.from_text(\"What is 2+2?\", role=Role.USER)],\n    constraint=constraint,\n    model=\"gpt-4o\"\n)\nprint(response.reasoning, response.result)\n</code></pre> <p>Using a JSON schema constraint:</p> <pre><code>schema = {\"type\": \"object\", \"properties\": {\"answer\": {\"type\": \"string\"}}}\nconstraint = JsonSchema(schema=schema)\nresponse = llm.structured_output(\n    messages=[Message.from_text(\"Hello\", role=Role.USER)],\n    constraint=constraint,\n    model=\"gpt-4o\"\n)\nprint(response[\"answer\"])\n</code></pre> Notes <ul> <li>Utilizes OpenAI's native structured output API with strict schema validation</li> <li>All schemas automatically have additionalProperties set to False</li> <li>Best performance achieved with GPT-4o and later models (gpt-4o-mini, gpt-4o-2024-08-06, and later)</li> </ul> Source code in <code>bridgic/llms/openai/_openai_llm.py</code> <pre><code>def structured_output(\n    self,\n    messages: List[Message],\n    constraint: Union[PydanticModel, JsonSchema],\n    model: Optional[str] = None,\n    temperature: Optional[float] = None,\n    top_p: Optional[float] = None,\n    presence_penalty: Optional[float] = None,\n    frequency_penalty: Optional[float] = None,\n    extra_body: Optional[Dict[str, Any]] = None,\n    **kwargs,\n) -&gt; Union[BaseModel, Dict[str, Any]]:\n    \"\"\"\n    Generate structured output in a specified format using OpenAI's structured output API.\n\n    This method leverages OpenAI's structured output capabilities to ensure the model\n    response conforms to a specified schema. Recommended for use with GPT-4o and later models.\n\n    Parameters\n    ----------\n    messages : List[Message]\n        A list of messages comprising the conversation so far.\n    constraint : Constraint\n        The constraint defining the desired output format (PydanticModel or JsonSchema).\n    model : str\n        Model ID used to generate the response. Structured outputs work best with GPT-4o and later.\n    temperature : Optional[float]\n        What sampling temperature to use, between 0 and 2. Higher values like 0.8 will\n        make the output more random, while lower values like 0.2 will make it more\n        focused and deterministic.\n    top_p : Optional[float]\n        An alternative to sampling with temperature, called nucleus sampling, where the\n        model considers the results of the tokens with top_p probability mass.\n    presence_penalty : Optional[float]\n        Number between -2.0 and 2.0. Positive values penalize new tokens based on\n        whether they appear in the text so far, increasing the model's likelihood to\n        talk about new topics.\n    frequency_penalty : Optional[float]\n        Number between -2.0 and 2.0. Positive values penalize new tokens based on their\n        existing frequency in the text so far, decreasing the model's likelihood to\n        repeat the same line verbatim.\n    extra_body : Optional[Dict[str, Any]]\n        Add additional JSON properties to the request.\n    **kwargs\n        Additional keyword arguments passed to the OpenAI API.\n\n    Returns\n    -------\n    Union[BaseModel, Dict[str, Any]]\n        The structured response in the format specified by the constraint:\n        - BaseModel instance if constraint is PydanticModel\n        - Dict[str, Any] if constraint is JsonSchema\n\n    Examples\n    --------\n    Using a Pydantic model constraint:\n\n    ```python\n    class Answer(BaseModel):\n        reasoning: str\n        result: int\n\n    constraint = PydanticModel(model=Answer)\n    response = llm.structured_output(\n        messages=[Message.from_text(\"What is 2+2?\", role=Role.USER)],\n        constraint=constraint,\n        model=\"gpt-4o\"\n    )\n    print(response.reasoning, response.result)\n    ```\n\n    Using a JSON schema constraint:\n\n    ```python\n    schema = {\"type\": \"object\", \"properties\": {\"answer\": {\"type\": \"string\"}}}\n    constraint = JsonSchema(schema=schema)\n    response = llm.structured_output(\n        messages=[Message.from_text(\"Hello\", role=Role.USER)],\n        constraint=constraint,\n        model=\"gpt-4o\"\n    )\n    print(response[\"answer\"])\n    ```\n\n    Notes\n    -----\n    - Utilizes OpenAI's native structured output API with strict schema validation\n    - All schemas automatically have additionalProperties set to False\n    - Best performance achieved with GPT-4o and later models (gpt-4o-mini, gpt-4o-2024-08-06, and later)\n    \"\"\"\n    params = self._build_parameters(\n        messages=messages,\n        model=model,\n        temperature=temperature,\n        top_p=top_p,\n        presence_penalty=presence_penalty,\n        frequency_penalty=frequency_penalty,\n        extra_body=extra_body,\n        response_format=self._get_response_format(constraint),\n        **kwargs,\n    )\n    # Validate required parameters for structured output\n    validate_required_params(params, [\"messages\", \"model\"])\n\n    response = self.client.chat.completions.parse(**params)\n    return self._convert_response(constraint, response.choices[0].message.content)\n</code></pre>"},{"location":"reference/bridgic-llms-openai/bridgic/llms/openai/#bridgic.llms.openai.OpenAILlm.astructured_output","title":"astructured_output","text":"<code>async</code> <pre><code>astructured_output(\n    messages: List[Message],\n    constraint: Union[PydanticModel, JsonSchema],\n    model: Optional[str] = None,\n    temperature: Optional[float] = None,\n    top_p: Optional[float] = None,\n    presence_penalty: Optional[float] = None,\n    frequency_penalty: Optional[float] = None,\n    extra_body: Optional[Dict[str, Any]] = None,\n    **kwargs\n) -&gt; Union[BaseModel, Dict[str, Any]]\n</code></pre> <p>Asynchronously generate structured output in a specified format using OpenAI's API.</p> <p>This is the asynchronous version of structured_output, suitable for concurrent processing and non-blocking operations. It leverages OpenAI's structured output capabilities to ensure the model response conforms to a specified schema.</p> <p>Parameters:</p> Name Type Description Default <code>messages</code> <code>List[Message]</code> <p>A list of messages comprising the conversation so far.</p> required <code>constraint</code> <code>Constraint</code> <p>The constraint defining the desired output format (PydanticModel or JsonSchema).</p> required <code>model</code> <code>str</code> <p>Model ID used to generate the response. Structured outputs work best with GPT-4o and later.</p> <code>None</code> <code>temperature</code> <code>Optional[float]</code> <p>What sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic.</p> <code>None</code> <code>top_p</code> <code>Optional[float]</code> <p>An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass.</p> <code>None</code> <code>presence_penalty</code> <code>Optional[float]</code> <p>Number between -2.0 and 2.0. Positive values penalize new tokens based on whether they appear in the text so far, increasing the model's likelihood to talk about new topics.</p> <code>None</code> <code>frequency_penalty</code> <code>Optional[float]</code> <p>Number between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model's likelihood to repeat the same line verbatim.</p> <code>None</code> <code>extra_body</code> <code>Optional[Dict[str, Any]]</code> <p>Add additional JSON properties to the request.</p> <code>None</code> <code>**kwargs</code> <p>Additional keyword arguments passed to the OpenAI API.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Union[BaseModel, Dict[str, Any]]</code> <p>The structured response in the format specified by the constraint: - BaseModel instance if constraint is PydanticModel - Dict[str, Any] if constraint is JsonSchema</p> <p>Examples:</p> <p>Using asynchronous structured output:</p> <pre><code>async def get_structured_response():\n    llm = OpenAILlm(api_key=\"your-key\")\n    constraint = PydanticModel(model=Answer)\n    response = await llm.astructured_output(\n        messages=[Message.from_text(\"Calculate 5+3\", role=Role.USER)],\n        constraint=constraint,\n        model=\"gpt-4o\"\n    )\n    return response\n</code></pre> Notes <ul> <li>This is the asynchronous version of structured_output</li> <li>Utilizes OpenAI's native structured output API with strict schema validation</li> <li>Suitable for concurrent processing and high-throughput applications</li> <li>Best performance achieved with GPT-4o and later models (gpt-4o-mini, gpt-4o-2024-08-06, and later)</li> </ul> Source code in <code>bridgic/llms/openai/_openai_llm.py</code> <pre><code>async def astructured_output(\n    self,\n    messages: List[Message],\n    constraint: Union[PydanticModel, JsonSchema],\n    model: Optional[str] = None,\n    temperature: Optional[float] = None,\n    top_p: Optional[float] = None,\n    presence_penalty: Optional[float] = None,\n    frequency_penalty: Optional[float] = None,\n    extra_body: Optional[Dict[str, Any]] = None,\n    **kwargs,\n) -&gt; Union[BaseModel, Dict[str, Any]]:\n    \"\"\"\n    Asynchronously generate structured output in a specified format using OpenAI's API.\n\n    This is the asynchronous version of structured_output, suitable for concurrent\n    processing and non-blocking operations. It leverages OpenAI's structured output\n    capabilities to ensure the model response conforms to a specified schema.\n\n    Parameters\n    ----------\n    messages : List[Message]\n        A list of messages comprising the conversation so far.\n    constraint : Constraint\n        The constraint defining the desired output format (PydanticModel or JsonSchema).\n    model : str\n        Model ID used to generate the response. Structured outputs work best with GPT-4o and later.\n    temperature : Optional[float]\n        What sampling temperature to use, between 0 and 2. Higher values like 0.8 will\n        make the output more random, while lower values like 0.2 will make it more\n        focused and deterministic.\n    top_p : Optional[float]\n        An alternative to sampling with temperature, called nucleus sampling, where the\n        model considers the results of the tokens with top_p probability mass.\n    presence_penalty : Optional[float]\n        Number between -2.0 and 2.0. Positive values penalize new tokens based on\n        whether they appear in the text so far, increasing the model's likelihood to\n        talk about new topics.\n    frequency_penalty : Optional[float]\n        Number between -2.0 and 2.0. Positive values penalize new tokens based on their\n        existing frequency in the text so far, decreasing the model's likelihood to\n        repeat the same line verbatim.\n    extra_body : Optional[Dict[str, Any]]\n        Add additional JSON properties to the request.\n    **kwargs\n        Additional keyword arguments passed to the OpenAI API.\n\n    Returns\n    -------\n    Union[BaseModel, Dict[str, Any]]\n        The structured response in the format specified by the constraint:\n        - BaseModel instance if constraint is PydanticModel\n        - Dict[str, Any] if constraint is JsonSchema\n\n    Examples\n    --------\n    Using asynchronous structured output:\n\n    ```python\n    async def get_structured_response():\n        llm = OpenAILlm(api_key=\"your-key\")\n        constraint = PydanticModel(model=Answer)\n        response = await llm.astructured_output(\n            messages=[Message.from_text(\"Calculate 5+3\", role=Role.USER)],\n            constraint=constraint,\n            model=\"gpt-4o\"\n        )\n        return response\n    ```\n\n    Notes\n    -----\n    - This is the asynchronous version of structured_output\n    - Utilizes OpenAI's native structured output API with strict schema validation\n    - Suitable for concurrent processing and high-throughput applications\n    - Best performance achieved with GPT-4o and later models (gpt-4o-mini, gpt-4o-2024-08-06, and later)\n    \"\"\"\n    params = self._build_parameters(\n        messages=messages,\n        model=model,\n        temperature=temperature,\n        top_p=top_p,\n        presence_penalty=presence_penalty,\n        frequency_penalty=frequency_penalty,\n        extra_body=extra_body,\n        response_format=self._get_response_format(constraint),\n        **kwargs,\n    )\n    # Validate required parameters for structured output\n    validate_required_params(params, [\"messages\", \"model\"])\n\n    response = await self.async_client.chat.completions.parse(**params)\n    return self._convert_response(constraint, response.choices[0].message.content)\n</code></pre>"},{"location":"reference/bridgic-llms-openai/bridgic/llms/openai/#bridgic.llms.openai.OpenAILlm.select_tool","title":"select_tool","text":"<pre><code>select_tool(\n    messages: List[Message],\n    tools: List[Tool],\n    model: Optional[str] = None,\n    temperature: Optional[float] = None,\n    top_p: Optional[float] = None,\n    presence_penalty: Optional[float] = None,\n    frequency_penalty: Optional[float] = None,\n    extra_body: Optional[Dict[str, Any]] = None,\n    parallel_tool_calls: Optional[bool] = None,\n    tool_choice: Union[\n        Literal[\"auto\", \"required\", \"none\"],\n        ChatCompletionNamedToolChoiceParam,\n    ] = None,\n    **kwargs\n) -&gt; Tuple[List[ToolCall], Optional[str]]\n</code></pre> <p>Select and invoke tools from a list based on conversation context.</p> <p>This method enables the model to intelligently select and call appropriate tools from a provided list based on the conversation context. It supports OpenAI's function calling capabilities with parallel execution and various control options.</p> <p>More OpenAI information: function-calling</p> <p>Parameters:</p> Name Type Description Default <code>messages</code> <code>List[Message]</code> <p>A list of messages comprising the conversation so far providing context for tool selection.</p> required <code>tools</code> <code>List[Tool]</code> <p>A list of tools the model may call.</p> required <code>model</code> <code>str</code> <p>Model ID used to generate the response. Function calling requires compatible models.</p> <code>None</code> <code>temperature</code> <code>Optional[float]</code> <p>What sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic.</p> <code>None</code> <code>top_p</code> <code>Optional[float]</code> <p>An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass.</p> <code>None</code> <code>presence_penalty</code> <code>Optional[float]</code> <p>Number between -2.0 and 2.0. Positive values penalize new tokens based on whether they appear in the text so far, increasing the model's likelihood to talk about new topics.</p> <code>None</code> <code>frequency_penalty</code> <code>Optional[float]</code> <p>Number between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model's likelihood to repeat the same line verbatim.</p> <code>None</code> <code>extra_body</code> <code>Optional[Dict[str, Any]]</code> <p>Add additional JSON properties to the request.</p> <code>None</code> <code>parallel_tool_calls</code> <code>Optional[bool]</code> <p>Whether to enable parallel function calling during tool use.</p> <code>None</code> <code>tool_choice</code> <code>Union[Literal['auto', 'required', 'none'], ChatCompletionNamedToolChoiceParam]</code> <p>Controls which tool, if any, the model may call. - <code>none</code>: The model will not call any tool and will instead generate a message. This is the default when no tools are provided. - <code>auto</code>: The model may choose to generate a message or call one or more tools. This is the default when tools are provided. - <code>required</code>: The model must call one or more tools. - To force a specific tool, pass <code>{\"type\": \"function\", \"function\": {\"name\": \"my_function\"}}</code>.</p> <code>None</code> <code>**kwargs</code> <p>Additional keyword arguments passed to the OpenAI API.</p> <code>{}</code> <p>Returns:</p> Type Description <code>List[ToolCall]</code> <p>List of selected tool calls with their IDs, names, and parsed arguments.</p> <code>Union[str, None]</code> <p>The content of the message from the model.</p> Source code in <code>bridgic/llms/openai/_openai_llm.py</code> <pre><code>def select_tool(\n    self,\n    messages: List[Message],\n    tools: List[Tool],\n    model: Optional[str] = None,\n    temperature: Optional[float] = None,\n    top_p: Optional[float] = None,\n    presence_penalty: Optional[float] = None,\n    frequency_penalty: Optional[float] = None,\n    extra_body: Optional[Dict[str, Any]] = None,\n    parallel_tool_calls: Optional[bool] = None,\n    tool_choice: Union[Literal[\"auto\", \"required\", \"none\"], ChatCompletionNamedToolChoiceParam] = None,\n    **kwargs,\n) -&gt; Tuple[List[ToolCall], Optional[str]]:\n    \"\"\"\n    Select and invoke tools from a list based on conversation context.\n\n    This method enables the model to intelligently select and call appropriate tools\n    from a provided list based on the conversation context. It supports OpenAI's\n    function calling capabilities with parallel execution and various control options.\n\n    More OpenAI information: [function-calling](https://platform.openai.com/docs/guides/function-calling)\n\n    Parameters\n    ----------\n    messages : List[Message]\n        A list of messages comprising the conversation so far providing context for tool selection.\n    tools : List[Tool]\n        A list of tools the model may call.\n    model : str\n        Model ID used to generate the response. Function calling requires compatible models.\n    temperature : Optional[float]\n        What sampling temperature to use, between 0 and 2. Higher values like 0.8 will\n        make the output more random, while lower values like 0.2 will make it more\n        focused and deterministic.\n    top_p : Optional[float]\n        An alternative to sampling with temperature, called nucleus sampling, where the\n        model considers the results of the tokens with top_p probability mass.\n    presence_penalty : Optional[float]\n        Number between -2.0 and 2.0. Positive values penalize new tokens based on\n        whether they appear in the text so far, increasing the model's likelihood to\n        talk about new topics.\n    frequency_penalty : Optional[float]\n        Number between -2.0 and 2.0. Positive values penalize new tokens based on their\n        existing frequency in the text so far, decreasing the model's likelihood to\n        repeat the same line verbatim.\n    extra_body : Optional[Dict[str, Any]]\n        Add additional JSON properties to the request.\n    parallel_tool_calls : Optional[bool]\n        Whether to enable parallel function calling during tool use.\n    tool_choice : Union[Literal[\"auto\", \"required\", \"none\"], ChatCompletionNamedToolChoiceParam]\n        Controls which tool, if any, the model may call.\n        - `none`: The model will not call any tool and will instead generate a message. This is the default when no tools are provided.\n        - `auto`: The model may choose to generate a message or call one or more tools. This is the default when tools are provided.\n        - `required`: The model must call one or more tools.\n        - To force a specific tool, pass `{\"type\": \"function\", \"function\": {\"name\": \"my_function\"}}`.\n    **kwargs\n        Additional keyword arguments passed to the OpenAI API.\n\n    Returns\n    -------\n    List[ToolCall]\n        List of selected tool calls with their IDs, names, and parsed arguments.\n    Union[str, None]\n        The content of the message from the model.\n    \"\"\"\n    params = self._build_parameters(\n        messages=messages,\n        model=model,\n        temperature=temperature,\n        top_p=top_p,\n        presence_penalty=presence_penalty,\n        frequency_penalty=frequency_penalty,\n        tools=tools,\n        tool_choice=tool_choice,\n        parallel_tool_calls=parallel_tool_calls,\n        extra_body=extra_body,\n        **kwargs,\n    )\n    # Validate required parameters for tool selection\n    validate_required_params(params, [\"messages\", \"model\"])\n\n    response: ChatCompletion = self.client.chat.completions.create(**params)\n    tool_calls = response.choices[0].message.tool_calls\n    content = response.choices[0].message.content\n    return (self._convert_tool_calls(tool_calls), content)\n</code></pre>"},{"location":"reference/bridgic-llms-openai/bridgic/llms/openai/#bridgic.llms.openai.OpenAILlm.aselect_tool","title":"aselect_tool","text":"<code>async</code> <pre><code>aselect_tool(\n    messages: List[Message],\n    tools: List[Tool],\n    model: Optional[str] = None,\n    temperature: Optional[float] = None,\n    top_p: Optional[float] = None,\n    presence_penalty: Optional[float] = None,\n    frequency_penalty: Optional[float] = None,\n    extra_body: Optional[Dict[str, Any]] = None,\n    parallel_tool_calls: Optional[bool] = None,\n    tool_choice: Union[\n        Literal[\"auto\", \"required\", \"none\"],\n        ChatCompletionNamedToolChoiceParam,\n    ] = None,\n    **kwargs\n) -&gt; Tuple[List[ToolCall], Optional[str]]\n</code></pre> <p>Select and invoke tools from a list based on conversation context.</p> <p>This method enables the model to intelligently select and call appropriate tools from a provided list based on the conversation context. It supports OpenAI's function calling capabilities with parallel execution and various control options.</p> <p>More OpenAI information: function-calling</p> <p>Parameters:</p> Name Type Description Default <code>messages</code> <code>List[Message]</code> <p>A list of messages comprising the conversation so far providing context for tool selection.</p> required <code>tools</code> <code>List[Tool]</code> <p>A list of tools the model may call.</p> required <code>model</code> <code>str</code> <p>Model ID used to generate the response. Function calling requires compatible models.</p> <code>None</code> <code>temperature</code> <code>Optional[float]</code> <p>What sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic.</p> <code>None</code> <code>top_p</code> <code>Optional[float]</code> <p>An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass.</p> <code>None</code> <code>presence_penalty</code> <code>Optional[float]</code> <p>Number between -2.0 and 2.0. Positive values penalize new tokens based on whether they appear in the text so far, increasing the model's likelihood to talk about new topics.</p> <code>None</code> <code>frequency_penalty</code> <code>Optional[float]</code> <p>Number between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model's likelihood to repeat the same line verbatim.</p> <code>None</code> <code>extra_body</code> <code>Optional[Dict[str, Any]]</code> <p>Add additional JSON properties to the request.</p> <code>None</code> <code>parallel_tool_calls</code> <code>Optional[bool]</code> <p>Whether to enable parallel function calling during tool use.</p> <code>None</code> <code>tool_choice</code> <code>Union[Literal['auto', 'required', 'none'], ChatCompletionNamedToolChoiceParam]</code> <p>Controls which tool, if any, the model may call. - <code>none</code>: The model will not call any tool and will instead generate a message. This is the default when no tools are provided. - <code>auto</code>: The model may choose to generate a message or call one or more tools. This is the default when tools are provided. - <code>required</code>: The model must call one or more tools. - To force a specific tool, pass <code>{\"type\": \"function\", \"function\": {\"name\": \"my_function\"}}</code>.</p> <code>None</code> <code>**kwargs</code> <p>Additional keyword arguments passed to the OpenAI API.</p> <code>{}</code> <p>Returns:</p> Type Description <code>List[ToolCall]</code> <p>List of selected tool calls with their IDs, names, and parsed arguments.</p> <code>Union[str, None]</code> <p>The content of the message from the model.</p> Source code in <code>bridgic/llms/openai/_openai_llm.py</code> <pre><code>async def aselect_tool(\n    self,\n    messages: List[Message],\n    tools: List[Tool],\n    model: Optional[str] = None,\n    temperature: Optional[float] = None,\n    top_p: Optional[float] = None,\n    presence_penalty: Optional[float] = None,\n    frequency_penalty: Optional[float] = None,\n    extra_body: Optional[Dict[str, Any]] = None,\n    parallel_tool_calls: Optional[bool] = None,\n    tool_choice: Union[Literal[\"auto\", \"required\", \"none\"], ChatCompletionNamedToolChoiceParam] = None,\n    **kwargs,\n)-&gt; Tuple[List[ToolCall], Optional[str]]:\n    \"\"\"\n    Select and invoke tools from a list based on conversation context.\n\n    This method enables the model to intelligently select and call appropriate tools\n    from a provided list based on the conversation context. It supports OpenAI's\n    function calling capabilities with parallel execution and various control options.\n\n    More OpenAI information: [function-calling](https://platform.openai.com/docs/guides/function-calling)\n\n    Parameters\n    ----------\n    messages : List[Message]\n        A list of messages comprising the conversation so far providing context for tool selection.\n    tools : List[Tool]\n        A list of tools the model may call.\n    model : str\n        Model ID used to generate the response. Function calling requires compatible models.\n    temperature : Optional[float]\n        What sampling temperature to use, between 0 and 2. Higher values like 0.8 will\n        make the output more random, while lower values like 0.2 will make it more\n        focused and deterministic.\n    top_p : Optional[float]\n        An alternative to sampling with temperature, called nucleus sampling, where the\n        model considers the results of the tokens with top_p probability mass.\n    presence_penalty : Optional[float]\n        Number between -2.0 and 2.0. Positive values penalize new tokens based on\n        whether they appear in the text so far, increasing the model's likelihood to\n        talk about new topics.\n    frequency_penalty : Optional[float]\n        Number between -2.0 and 2.0. Positive values penalize new tokens based on their\n        existing frequency in the text so far, decreasing the model's likelihood to\n        repeat the same line verbatim.\n    extra_body : Optional[Dict[str, Any]]\n        Add additional JSON properties to the request.\n    parallel_tool_calls : Optional[bool]\n        Whether to enable parallel function calling during tool use.\n    tool_choice : Union[Literal[\"auto\", \"required\", \"none\"], ChatCompletionNamedToolChoiceParam]\n        Controls which tool, if any, the model may call.\n        - `none`: The model will not call any tool and will instead generate a message. This is the default when no tools are provided.\n        - `auto`: The model may choose to generate a message or call one or more tools. This is the default when tools are provided.\n        - `required`: The model must call one or more tools.\n        - To force a specific tool, pass `{\"type\": \"function\", \"function\": {\"name\": \"my_function\"}}`.\n\n    **kwargs\n        Additional keyword arguments passed to the OpenAI API.\n\n    Returns\n    -------\n    List[ToolCall]\n        List of selected tool calls with their IDs, names, and parsed arguments.\n    Union[str, None]\n        The content of the message from the model.\n    \"\"\"\n    params = self._build_parameters(\n        messages=messages,\n        model=model,\n        temperature=temperature,\n        top_p=top_p,\n        presence_penalty=presence_penalty,\n        frequency_penalty=frequency_penalty,\n        tools=tools,\n        tool_choice=tool_choice,\n        parallel_tool_calls=parallel_tool_calls,\n        extra_body=extra_body,\n        **kwargs,\n    )\n    # Validate required parameters for tool selection\n    validate_required_params(params, [\"messages\", \"model\"])\n\n    response: ChatCompletion = await self.async_client.chat.completions.create(**params)\n    tool_calls = response.choices[0].message.tool_calls\n    content = response.choices[0].message.content\n    return (self._convert_tool_calls(tool_calls), content)\n</code></pre>"},{"location":"reference/bridgic-llms-openai-like/bridgic/llms/openai_like/","title":"bridgic.llms.openai_like","text":""},{"location":"reference/bridgic-llms-openai-like/bridgic/llms/openai_like/#openai_like","title":"openai_like","text":"<p>The OpenAI-Like integration module provides support for third-party services  compatible with the OpenAI API.</p> <p>This package is a thin wrapper for the OpenAI API, designed to meet the needs  for calling third-party model services compatible with the OpenAI API.</p> <p>Note that this integration does not adapt to specific model providers, but  provides general-purpose interfaces. Therefore, it is not fully comprehensive  in functionality and only supports basic chat/stream operations and their  corresponding async interfaces.</p> <p>You can install the OpenAI-Like integration package for Bridgic by running:</p> <pre><code>pip install bridgic-llms-openai-like\n</code></pre>"},{"location":"reference/bridgic-llms-openai-like/bridgic/llms/openai_like/#bridgic.llms.openai_like.OpenAILikeConfiguration","title":"OpenAILikeConfiguration","text":"<p>               Bases: <code>BaseModel</code></p> <p>Default configuration for OpenAI-compatible chat completions.</p> Source code in <code>bridgic/llms/openai_like/_openai_like_llm.py</code> <pre><code>class OpenAILikeConfiguration(BaseModel):\n    \"\"\"\n    Default configuration for OpenAI-compatible chat completions.\n    \"\"\"\n    model: Optional[str] = None\n    \"\"\"Default model to use when a call-time `model` is not provided.\"\"\"\n    temperature: Optional[float] = None\n    \"\"\"Sampling temperature in [0, 2]. Higher is more random, lower is more deterministic.\"\"\"\n    top_p: Optional[float] = None\n    \"\"\"Nucleus sampling probability mass in (0, 1]. Alternative to temperature.\"\"\"\n    presence_penalty: Optional[float] = None\n    \"\"\"Penalize new tokens based on whether they appear so far. [-2.0, 2.0].\"\"\"\n    frequency_penalty: Optional[float] = None\n    \"\"\"Penalize new tokens based on their frequency so far. [-2.0, 2.0].\"\"\"\n    max_tokens: Optional[int] = None\n    \"\"\"Maximum number of tokens to generate for the completion.\"\"\"\n    stop: Optional[List[str]] = None\n    \"\"\"Up to 4 sequences where generation will stop.\"\"\"\n</code></pre>"},{"location":"reference/bridgic-llms-openai-like/bridgic/llms/openai_like/#bridgic.llms.openai_like.OpenAILikeConfiguration.model","title":"model  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>model: Optional[str] = None\n</code></pre> <p>Default model to use when a call-time <code>model</code> is not provided.</p>"},{"location":"reference/bridgic-llms-openai-like/bridgic/llms/openai_like/#bridgic.llms.openai_like.OpenAILikeConfiguration.temperature","title":"temperature  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>temperature: Optional[float] = None\n</code></pre> <p>Sampling temperature in [0, 2]. Higher is more random, lower is more deterministic.</p>"},{"location":"reference/bridgic-llms-openai-like/bridgic/llms/openai_like/#bridgic.llms.openai_like.OpenAILikeConfiguration.top_p","title":"top_p  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>top_p: Optional[float] = None\n</code></pre> <p>Nucleus sampling probability mass in (0, 1]. Alternative to temperature.</p>"},{"location":"reference/bridgic-llms-openai-like/bridgic/llms/openai_like/#bridgic.llms.openai_like.OpenAILikeConfiguration.presence_penalty","title":"presence_penalty  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>presence_penalty: Optional[float] = None\n</code></pre> <p>Penalize new tokens based on whether they appear so far. [-2.0, 2.0].</p>"},{"location":"reference/bridgic-llms-openai-like/bridgic/llms/openai_like/#bridgic.llms.openai_like.OpenAILikeConfiguration.frequency_penalty","title":"frequency_penalty  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>frequency_penalty: Optional[float] = None\n</code></pre> <p>Penalize new tokens based on their frequency so far. [-2.0, 2.0].</p>"},{"location":"reference/bridgic-llms-openai-like/bridgic/llms/openai_like/#bridgic.llms.openai_like.OpenAILikeConfiguration.max_tokens","title":"max_tokens  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>max_tokens: Optional[int] = None\n</code></pre> <p>Maximum number of tokens to generate for the completion.</p>"},{"location":"reference/bridgic-llms-openai-like/bridgic/llms/openai_like/#bridgic.llms.openai_like.OpenAILikeConfiguration.stop","title":"stop  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>stop: Optional[List[str]] = None\n</code></pre> <p>Up to 4 sequences where generation will stop.</p>"},{"location":"reference/bridgic-llms-openai-like/bridgic/llms/openai_like/#bridgic.llms.openai_like.OpenAILikeLlm","title":"OpenAILikeLlm","text":"<p>               Bases: <code>BaseLlm</code></p> <p>OpenAILikeLlm is a thin wrapper around the LLM providers that makes it compatible with the  services that provide OpenAI compatible API. To support the widest range of model providers,  this wrapper only supports text-modal usage.</p> <p>Parameters:</p> Name Type Description Default <code>api_base</code> <code>str</code> <p>The base URL of the LLM provider.</p> required <code>api_key</code> <code>str</code> <p>The API key of the LLM provider.</p> required <code>configuration</code> <code>Optional[OpenAILikeConfiguration]</code> <p>The configuration for the OpenAI-compatible API. If None, uses the default configuration.</p> <code>OpenAILikeConfiguration()</code> <code>timeout</code> <code>Optional[float]</code> <p>The timeout in seconds. If None, no timeout is applied.</p> <code>None</code> <code>http_client</code> <code>Optional[Client]</code> <p>Custom synchronous HTTP client for requests. If None, creates a default client.</p> <code>None</code> <code>http_async_client</code> <code>Optional[AsyncClient]</code> <p>Custom asynchronous HTTP client for requests. If None, creates a default client.</p> <code>None</code> Source code in <code>bridgic/llms/openai_like/_openai_like_llm.py</code> <pre><code>class OpenAILikeLlm(BaseLlm):\n    \"\"\"\n    OpenAILikeLlm is a thin wrapper around the LLM providers that makes it compatible with the \n    services that provide OpenAI compatible API. To support the widest range of model providers, \n    this wrapper only supports text-modal usage.\n\n    Parameters\n    ----------\n    api_base: str\n        The base URL of the LLM provider.\n    api_key: str\n        The API key of the LLM provider.\n    configuration: Optional[OpenAILikeConfiguration]\n        The configuration for the OpenAI-compatible API. If None, uses the default configuration.\n    timeout: Optional[float]\n        The timeout in seconds. If None, no timeout is applied.\n    http_client : Optional[httpx.Client]\n        Custom synchronous HTTP client for requests. If None, creates a default client.\n    http_async_client : Optional[httpx.AsyncClient]\n        Custom asynchronous HTTP client for requests. If None, creates a default client.\n    \"\"\"\n\n    api_base: str\n    api_key: str\n    configuration: OpenAILikeConfiguration\n    timeout: float\n    http_client: httpx.Client\n    http_async_client: httpx.AsyncClient\n\n    client: OpenAI\n    async_client: AsyncOpenAI\n\n    def __init__(\n        self,\n        api_base: str,\n        api_key: str,\n        configuration: Optional[OpenAILikeConfiguration] = OpenAILikeConfiguration(),\n        timeout: Optional[float] = None,\n        http_client: Optional[httpx.Client] = None,\n        http_async_client: Optional[httpx.AsyncClient] = None,\n    ):\n        # Record for serialization / deserialization.\n        self.api_base = api_base\n        self.api_key = api_key\n        self.configuration = configuration\n        self.timeout = timeout\n        self.http_client = http_client\n        self.http_async_client = http_async_client\n\n        # Initialize clients.\n        self.client = OpenAI(base_url=api_base, api_key=api_key, timeout=timeout, http_client=http_client)\n        self.async_client = AsyncOpenAI(base_url=api_base, api_key=api_key, timeout=timeout, http_client=http_async_client)\n\n    def chat(\n        self,\n        messages: List[Message],\n        model: Optional[str] = None,\n        temperature: Optional[float] = None,\n        top_p: Optional[float] = None,\n        presence_penalty: Optional[float] = None,\n        frequency_penalty: Optional[float] = None,\n        max_tokens: Optional[int] = None,\n        stop: Optional[List[str]] = None,\n        extra_body: Optional[Dict[str, Any]] = None,\n        **kwargs,\n    ) -&gt; Response:\n        \"\"\"\n        Send a synchronous chat completion request to an OpenAI-compatible provider.\n\n        Parameters\n        ----------\n        messages : list[Message]\n            Conversation messages.\n        model : str, optional\n            Model ID to use. Required unless provided in `configuration.model`.\n        temperature : float, optional\n            Sampling temperature in [0, 2]. Higher is more random, lower is more deterministic.\n        top_p : float, optional\n            Nucleus sampling probability mass in (0, 1]. Alternative to temperature.\n        presence_penalty : float, optional\n            Penalize new tokens based on whether they appear so far. [-2.0, 2.0].\n        frequency_penalty : float, optional\n            Penalize new tokens based on their frequency so far. [-2.0, 2.0].\n        max_tokens : int, optional\n            Maximum tokens to generate for completion.\n        stop : list[str], optional\n            Up to 4 sequences where generation will stop.\n        extra_body : dict, optional\n            Extra JSON payload sent to the provider.\n        **kwargs\n            Additional provider-specific arguments.\n\n        Returns\n        -------\n        Response\n            Bridgic response containing the generated message and raw API response.\n\n        Notes\n        -----\n        - Required parameter validation ensures `messages` and final `model` are present\n          (from either the call or `configuration`).\n        \"\"\"\n        params = self._build_parameters(\n            messages=messages,\n            model=model,\n            temperature=temperature,\n            top_p=top_p,\n            presence_penalty=presence_penalty,\n            frequency_penalty=frequency_penalty,\n            max_tokens=max_tokens,\n            stop=stop,\n            extra_body=extra_body,\n            **kwargs,\n        )\n        validate_required_params(params, [\"messages\", \"model\"])\n        response = self.client.chat.completions.create(**params)\n        openai_message: ChatCompletionMessage = response.choices[0].message\n        text: str = openai_message.content if openai_message.content else \"\"\n\n        if openai_message.refusal:\n            warnings.warn(openai_message.refusal, RuntimeWarning)\n\n        return Response(\n            message=Message.from_text(text, role=Role.AI),\n            raw=response,\n        )\n\n    def stream(\n        self,\n        messages: List[Message],\n        model: Optional[str] = None,\n        temperature: Optional[float] = None,\n        top_p: Optional[float] = None,\n        presence_penalty: Optional[float] = None,\n        frequency_penalty: Optional[float] = None,\n        max_tokens: Optional[int] = None,\n        stop: Optional[List[str]] = None,\n        extra_body: Optional[Dict[str, Any]] = None,\n        **kwargs,\n    ) -&gt; StreamResponse:\n        \"\"\"\n        Stream a chat completion response incrementally.\n\n        Parameters\n        ----------\n        messages : list[Message]\n            Conversation messages.\n        model : str, optional\n            Model ID to use. Required unless provided in `configuration.model`.\n        temperature, top_p, presence_penalty, frequency_penalty, max_tokens, stop, extra_body\n            See `chat` for details.\n        **kwargs\n            Additional provider-specific arguments.\n\n        Yields\n        ------\n        MessageChunk\n            Delta chunks as they arrive from the provider.\n\n        Notes\n        -----\n        - Validates `messages`, final `model`, and `stream=True`.\n        \"\"\"\n        params = self._build_parameters(\n            messages=messages,\n            model=model,\n            temperature=temperature,\n            top_p=top_p,\n            presence_penalty=presence_penalty,\n            frequency_penalty=frequency_penalty,\n            max_tokens=max_tokens,\n            stop=stop,\n            extra_body=extra_body,\n            stream=True,\n            **kwargs,\n        )\n        validate_required_params(params, [\"messages\", \"model\", \"stream\"])\n        response = self.client.chat.completions.create(**params)\n        for chunk in response:\n            delta_content = chunk.choices[0].delta.content\n            delta_content = delta_content if delta_content else \"\"\n            yield MessageChunk(delta=delta_content, raw=chunk)\n\n    async def achat(\n        self,\n        messages: List[Message],\n        model: Optional[str] = None,\n        temperature: Optional[float] = None,\n        top_p: Optional[float] = None,\n        presence_penalty: Optional[float] = None,\n        frequency_penalty: Optional[float] = None,\n        max_tokens: Optional[int] = None,\n        stop: Optional[List[str]] = None,\n        extra_body: Optional[Dict[str, Any]] = None,\n        **kwargs,\n    ) -&gt; Response:\n        \"\"\"\n        Asynchronously send a chat completion request to an OpenAI-compatible provider.\n\n        Parameters\n        ----------\n        messages : list[Message]\n            Conversation messages.\n        model : str, optional\n            Model ID to use. Required unless provided in `configuration.model`.\n        temperature, top_p, presence_penalty, frequency_penalty, max_tokens, stop, extra_body\n            See `chat` for details.\n        **kwargs\n            Additional provider-specific arguments.\n\n        Returns\n        -------\n        Response\n            Bridgic response containing the generated message and raw API response.\n        \"\"\"\n        params = self._build_parameters(\n            messages=messages,\n            model=model,\n            temperature=temperature,\n            top_p=top_p,\n            presence_penalty=presence_penalty,\n            frequency_penalty=frequency_penalty,\n            max_tokens=max_tokens,\n            stop=stop,\n            extra_body=extra_body,\n            **kwargs,\n        )\n        validate_required_params(params, [\"messages\", \"model\"])\n        response = await self.async_client.chat.completions.create(**params)\n        openai_message: ChatCompletionMessage = response.choices[0].message\n        text: str = openai_message.content if openai_message.content else \"\"\n\n        if openai_message.refusal:\n            warnings.warn(openai_message.refusal, RuntimeWarning)\n\n        return Response(\n            message=Message.from_text(text, role=Role.AI),\n            raw=response,\n        )\n\n    async def astream(\n        self,\n        messages: List[Message],\n        model: Optional[str] = None,\n        temperature: Optional[float] = None,\n        top_p: Optional[float] = None,\n        presence_penalty: Optional[float] = None,\n        frequency_penalty: Optional[float] = None,\n        max_tokens: Optional[int] = None,\n        stop: Optional[List[str]] = None,\n        extra_body: Optional[Dict[str, Any]] = None,\n        **kwargs,\n    ) -&gt; AsyncStreamResponse:\n        \"\"\"\n        Asynchronously stream a chat completion response incrementally.\n\n        Parameters\n        ----------\n        messages : list[Message]\n            Conversation messages.\n        model : str, optional\n            Model ID to use. Required unless provided in `configuration.model`.\n        temperature, top_p, presence_penalty, frequency_penalty, max_tokens, stop, extra_body\n            See `chat` for details.\n        **kwargs\n            Additional provider-specific arguments.\n\n        Yields\n        ------\n        MessageChunk\n            Delta chunks as they arrive from the provider.\n\n        Notes\n        -----\n        - Validates `messages`, final `model`, and `stream=True`.\n        \"\"\"\n        params = self._build_parameters(\n            messages=messages,\n            model=model,\n            temperature=temperature,\n            top_p=top_p,\n            presence_penalty=presence_penalty,\n            frequency_penalty=frequency_penalty,\n            max_tokens=max_tokens,\n            stop=stop,\n            extra_body=extra_body,\n            stream=True,\n            **kwargs,\n        )\n        validate_required_params(params, [\"messages\", \"model\", \"stream\"])\n        response = await self.async_client.chat.completions.create(**params)\n        async for chunk in response:\n            delta_content = chunk.choices[0].delta.content\n            delta_content = delta_content if delta_content else \"\"\n            yield MessageChunk(delta=delta_content, raw=chunk)\n\n    def _build_parameters(\n        self,\n        messages: List[Message],\n        model: Optional[str] = None,\n        temperature: Optional[float] = None,\n        top_p: Optional[float] = None,\n        presence_penalty: Optional[float] = None,\n        frequency_penalty: Optional[float] = None,\n        max_tokens: Optional[int] = None,\n        stop: Optional[List[str]] = None,\n        extra_body: Optional[Dict[str, Any]] = None,\n        stream: Optional[bool] = None,\n        **kwargs,\n    ) -&gt; Dict[str, Any]:\n        \"\"\"\n        Merge configuration defaults with per-call parameters and remove None values.\n\n        Parameters\n        ----------\n        messages : list[Message]\n            Conversation messages to send.\n        model : str, optional\n            Model identifier. May be omitted if `configuration.model` is set.\n        temperature, top_p, presence_penalty, frequency_penalty, max_tokens, stop, extra_body, stream\n            Standard OpenAI chat parameters.\n        **kwargs\n            Additional provider-specific parameters.\n\n        Returns\n        -------\n        dict\n            Final parameter dictionary for the OpenAI-compatible API.\n        \"\"\"\n        msgs: List[ChatCompletionMessageParam] = [self._convert_message(msg) for msg in messages]\n        merge_params = merge_dict(self.configuration.model_dump(), {\n            \"messages\": msgs,\n            \"model\": model,\n            \"temperature\": temperature,\n            \"top_p\": top_p,\n            \"presence_penalty\": presence_penalty,\n            \"frequency_penalty\": frequency_penalty,\n            \"max_tokens\": max_tokens,\n            \"stop\": stop,\n            \"extra_body\": extra_body,\n            \"stream\": stream,\n            **kwargs,\n        })\n        return filter_dict(merge_params, exclude_none=True)\n\n    def _convert_message(self, message: Message, strict: bool = False) -&gt; ChatCompletionMessageParam:\n        if strict:\n            return self._convert_message_strict(message)\n        else:\n            return self._convert_message_normal(message)\n\n    def _convert_message_normal(self, message: Message) -&gt; ChatCompletionMessageParam:\n        content_list = []\n        for block in message.blocks:\n            if isinstance(block, TextBlock):\n                content_list.append(block.text)\n            if isinstance(block, ToolCallBlock):\n                content_list.append(\n                    f\"Tool call:\\n\"\n                    f\"- id: {block.id}\\n\"\n                    f\"- name: {block.name}\\n\"\n                    f\"- arguments: {block.arguments}\"\n                )\n            if isinstance(block, ToolResultBlock):\n                content_list.append(f\"Tool result: {block.content}\")\n        content_txt = \"\\n\\n\".join(content_list)\n\n        if message.role == Role.SYSTEM:\n            return ChatCompletionSystemMessageParam(content=content_txt, role=\"system\")\n        elif message.role == Role.USER:\n            return ChatCompletionUserMessageParam(content=content_txt, role=\"user\")\n        elif message.role == Role.AI:\n            return ChatCompletionAssistantMessageParam(content=content_txt, role=\"assistant\")\n        elif message.role == Role.TOOL:\n            return ChatCompletionToolMessageParam(content=content_txt, role=\"tool\")\n        else:\n            raise ValueError(f\"Invalid role: {message.role}\")\n\n    def _convert_message_strict(self, message: Message) -&gt; ChatCompletionMessageParam:\n        content_list = []\n        tool_call_list = []\n        tool_result = \"\"\n        tool_result_call_id = None\n\n        for block in message.blocks:\n            if isinstance(block, TextBlock):\n                content_list.append(block.text)\n            if isinstance(block, ToolCallBlock):\n                tool_call: ChatCompletionMessageFunctionToolCallParam = {\n                    \"type\": \"function\",\n                    \"id\": block.id,\n                    \"function\": {\n                        \"name\": block.name,\n                        \"arguments\": json.dumps(block.arguments),\n                    },\n                }\n                tool_call_list.append(tool_call)\n            if isinstance(block, ToolResultBlock):\n                tool_result = block.content\n                tool_result_call_id = block.id\n\n        content_txt = \"\\n\\n\".join(content_list)\n\n        if message.role == Role.SYSTEM:\n            return ChatCompletionSystemMessageParam(content=content_txt, role=\"system\")\n        elif message.role == Role.USER:\n            return ChatCompletionUserMessageParam(content=content_txt, role=\"user\")\n        elif message.role == Role.AI:\n            return ChatCompletionAssistantMessageParam(content=content_txt, tool_calls=tool_call_list, role=\"assistant\")\n        elif message.role == Role.TOOL:\n            content_txt = \"\\n\\n\".join([content_txt, tool_result])\n            return ChatCompletionToolMessageParam(content=content_txt, tool_call_id=tool_result_call_id, role=\"tool\")\n        else:\n            raise ValueError(f\"Invalid role: {message.role}\")\n\n    @override\n    def dump_to_dict(self) -&gt; Dict[str, Any]:\n        state_dict = {\n            \"api_base\": self.api_base,\n            \"api_key\": self.api_key,\n            \"timeout\": self.timeout,\n            \"configuration\": self.configuration.model_dump(),\n        }\n        if self.http_client:\n            warnings.warn(\n                \"httpx.Client is not serializable, so it will be set to None in the deserialization.\",\n                RuntimeWarning,\n            )\n        if self.http_async_client:\n            warnings.warn(\n                \"httpx.AsyncClient is not serializable, so it will be set to None in the deserialization.\",\n                RuntimeWarning,\n            )\n        return state_dict\n\n    @override\n    def load_from_dict(self, state_dict: Dict[str, Any]) -&gt; None:\n        self.api_base = state_dict[\"api_base\"]\n        self.api_key = state_dict[\"api_key\"]\n        self.timeout = state_dict[\"timeout\"]\n        self.configuration = OpenAILikeConfiguration(**state_dict.get(\"configuration\", {}))\n\n        self.http_client = None\n        self.http_async_client = None\n\n        self.client = OpenAI(\n            base_url=self.api_base,\n            api_key=self.api_key,\n            timeout=self.timeout,\n            http_client=self.http_client,\n        )\n        self.async_client = AsyncOpenAI(\n            base_url=self.api_base,\n            api_key=self.api_key,\n            timeout=self.timeout,\n            http_client=self.http_async_client,\n        )\n</code></pre>"},{"location":"reference/bridgic-llms-openai-like/bridgic/llms/openai_like/#bridgic.llms.openai_like.OpenAILikeLlm.chat","title":"chat","text":"<pre><code>chat(\n    messages: List[Message],\n    model: Optional[str] = None,\n    temperature: Optional[float] = None,\n    top_p: Optional[float] = None,\n    presence_penalty: Optional[float] = None,\n    frequency_penalty: Optional[float] = None,\n    max_tokens: Optional[int] = None,\n    stop: Optional[List[str]] = None,\n    extra_body: Optional[Dict[str, Any]] = None,\n    **kwargs\n) -&gt; Response\n</code></pre> <p>Send a synchronous chat completion request to an OpenAI-compatible provider.</p> <p>Parameters:</p> Name Type Description Default <code>messages</code> <code>list[Message]</code> <p>Conversation messages.</p> required <code>model</code> <code>str</code> <p>Model ID to use. Required unless provided in <code>configuration.model</code>.</p> <code>None</code> <code>temperature</code> <code>float</code> <p>Sampling temperature in [0, 2]. Higher is more random, lower is more deterministic.</p> <code>None</code> <code>top_p</code> <code>float</code> <p>Nucleus sampling probability mass in (0, 1]. Alternative to temperature.</p> <code>None</code> <code>presence_penalty</code> <code>float</code> <p>Penalize new tokens based on whether they appear so far. [-2.0, 2.0].</p> <code>None</code> <code>frequency_penalty</code> <code>float</code> <p>Penalize new tokens based on their frequency so far. [-2.0, 2.0].</p> <code>None</code> <code>max_tokens</code> <code>int</code> <p>Maximum tokens to generate for completion.</p> <code>None</code> <code>stop</code> <code>list[str]</code> <p>Up to 4 sequences where generation will stop.</p> <code>None</code> <code>extra_body</code> <code>dict</code> <p>Extra JSON payload sent to the provider.</p> <code>None</code> <code>**kwargs</code> <p>Additional provider-specific arguments.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Response</code> <p>Bridgic response containing the generated message and raw API response.</p> Notes <ul> <li>Required parameter validation ensures <code>messages</code> and final <code>model</code> are present   (from either the call or <code>configuration</code>).</li> </ul> Source code in <code>bridgic/llms/openai_like/_openai_like_llm.py</code> <pre><code>def chat(\n    self,\n    messages: List[Message],\n    model: Optional[str] = None,\n    temperature: Optional[float] = None,\n    top_p: Optional[float] = None,\n    presence_penalty: Optional[float] = None,\n    frequency_penalty: Optional[float] = None,\n    max_tokens: Optional[int] = None,\n    stop: Optional[List[str]] = None,\n    extra_body: Optional[Dict[str, Any]] = None,\n    **kwargs,\n) -&gt; Response:\n    \"\"\"\n    Send a synchronous chat completion request to an OpenAI-compatible provider.\n\n    Parameters\n    ----------\n    messages : list[Message]\n        Conversation messages.\n    model : str, optional\n        Model ID to use. Required unless provided in `configuration.model`.\n    temperature : float, optional\n        Sampling temperature in [0, 2]. Higher is more random, lower is more deterministic.\n    top_p : float, optional\n        Nucleus sampling probability mass in (0, 1]. Alternative to temperature.\n    presence_penalty : float, optional\n        Penalize new tokens based on whether they appear so far. [-2.0, 2.0].\n    frequency_penalty : float, optional\n        Penalize new tokens based on their frequency so far. [-2.0, 2.0].\n    max_tokens : int, optional\n        Maximum tokens to generate for completion.\n    stop : list[str], optional\n        Up to 4 sequences where generation will stop.\n    extra_body : dict, optional\n        Extra JSON payload sent to the provider.\n    **kwargs\n        Additional provider-specific arguments.\n\n    Returns\n    -------\n    Response\n        Bridgic response containing the generated message and raw API response.\n\n    Notes\n    -----\n    - Required parameter validation ensures `messages` and final `model` are present\n      (from either the call or `configuration`).\n    \"\"\"\n    params = self._build_parameters(\n        messages=messages,\n        model=model,\n        temperature=temperature,\n        top_p=top_p,\n        presence_penalty=presence_penalty,\n        frequency_penalty=frequency_penalty,\n        max_tokens=max_tokens,\n        stop=stop,\n        extra_body=extra_body,\n        **kwargs,\n    )\n    validate_required_params(params, [\"messages\", \"model\"])\n    response = self.client.chat.completions.create(**params)\n    openai_message: ChatCompletionMessage = response.choices[0].message\n    text: str = openai_message.content if openai_message.content else \"\"\n\n    if openai_message.refusal:\n        warnings.warn(openai_message.refusal, RuntimeWarning)\n\n    return Response(\n        message=Message.from_text(text, role=Role.AI),\n        raw=response,\n    )\n</code></pre>"},{"location":"reference/bridgic-llms-openai-like/bridgic/llms/openai_like/#bridgic.llms.openai_like.OpenAILikeLlm.stream","title":"stream","text":"<pre><code>stream(\n    messages: List[Message],\n    model: Optional[str] = None,\n    temperature: Optional[float] = None,\n    top_p: Optional[float] = None,\n    presence_penalty: Optional[float] = None,\n    frequency_penalty: Optional[float] = None,\n    max_tokens: Optional[int] = None,\n    stop: Optional[List[str]] = None,\n    extra_body: Optional[Dict[str, Any]] = None,\n    **kwargs\n) -&gt; StreamResponse\n</code></pre> <p>Stream a chat completion response incrementally.</p> <p>Parameters:</p> Name Type Description Default <code>messages</code> <code>list[Message]</code> <p>Conversation messages.</p> required <code>model</code> <code>str</code> <p>Model ID to use. Required unless provided in <code>configuration.model</code>.</p> <code>None</code> <code>temperature</code> <code>Optional[float]</code> <p>See <code>chat</code> for details.</p> <code>None</code> <code>top_p</code> <code>Optional[float]</code> <p>See <code>chat</code> for details.</p> <code>None</code> <code>presence_penalty</code> <code>Optional[float]</code> <p>See <code>chat</code> for details.</p> <code>None</code> <code>frequency_penalty</code> <code>Optional[float]</code> <p>See <code>chat</code> for details.</p> <code>None</code> <code>max_tokens</code> <code>Optional[float]</code> <p>See <code>chat</code> for details.</p> <code>None</code> <code>stop</code> <code>Optional[float]</code> <p>See <code>chat</code> for details.</p> <code>None</code> <code>extra_body</code> <code>Optional[float]</code> <p>See <code>chat</code> for details.</p> <code>None</code> <code>**kwargs</code> <p>Additional provider-specific arguments.</p> <code>{}</code> <p>Yields:</p> Type Description <code>MessageChunk</code> <p>Delta chunks as they arrive from the provider.</p> Notes <ul> <li>Validates <code>messages</code>, final <code>model</code>, and <code>stream=True</code>.</li> </ul> Source code in <code>bridgic/llms/openai_like/_openai_like_llm.py</code> <pre><code>def stream(\n    self,\n    messages: List[Message],\n    model: Optional[str] = None,\n    temperature: Optional[float] = None,\n    top_p: Optional[float] = None,\n    presence_penalty: Optional[float] = None,\n    frequency_penalty: Optional[float] = None,\n    max_tokens: Optional[int] = None,\n    stop: Optional[List[str]] = None,\n    extra_body: Optional[Dict[str, Any]] = None,\n    **kwargs,\n) -&gt; StreamResponse:\n    \"\"\"\n    Stream a chat completion response incrementally.\n\n    Parameters\n    ----------\n    messages : list[Message]\n        Conversation messages.\n    model : str, optional\n        Model ID to use. Required unless provided in `configuration.model`.\n    temperature, top_p, presence_penalty, frequency_penalty, max_tokens, stop, extra_body\n        See `chat` for details.\n    **kwargs\n        Additional provider-specific arguments.\n\n    Yields\n    ------\n    MessageChunk\n        Delta chunks as they arrive from the provider.\n\n    Notes\n    -----\n    - Validates `messages`, final `model`, and `stream=True`.\n    \"\"\"\n    params = self._build_parameters(\n        messages=messages,\n        model=model,\n        temperature=temperature,\n        top_p=top_p,\n        presence_penalty=presence_penalty,\n        frequency_penalty=frequency_penalty,\n        max_tokens=max_tokens,\n        stop=stop,\n        extra_body=extra_body,\n        stream=True,\n        **kwargs,\n    )\n    validate_required_params(params, [\"messages\", \"model\", \"stream\"])\n    response = self.client.chat.completions.create(**params)\n    for chunk in response:\n        delta_content = chunk.choices[0].delta.content\n        delta_content = delta_content if delta_content else \"\"\n        yield MessageChunk(delta=delta_content, raw=chunk)\n</code></pre>"},{"location":"reference/bridgic-llms-openai-like/bridgic/llms/openai_like/#bridgic.llms.openai_like.OpenAILikeLlm.achat","title":"achat","text":"<code>async</code> <pre><code>achat(\n    messages: List[Message],\n    model: Optional[str] = None,\n    temperature: Optional[float] = None,\n    top_p: Optional[float] = None,\n    presence_penalty: Optional[float] = None,\n    frequency_penalty: Optional[float] = None,\n    max_tokens: Optional[int] = None,\n    stop: Optional[List[str]] = None,\n    extra_body: Optional[Dict[str, Any]] = None,\n    **kwargs\n) -&gt; Response\n</code></pre> <p>Asynchronously send a chat completion request to an OpenAI-compatible provider.</p> <p>Parameters:</p> Name Type Description Default <code>messages</code> <code>list[Message]</code> <p>Conversation messages.</p> required <code>model</code> <code>str</code> <p>Model ID to use. Required unless provided in <code>configuration.model</code>.</p> <code>None</code> <code>temperature</code> <code>Optional[float]</code> <p>See <code>chat</code> for details.</p> <code>None</code> <code>top_p</code> <code>Optional[float]</code> <p>See <code>chat</code> for details.</p> <code>None</code> <code>presence_penalty</code> <code>Optional[float]</code> <p>See <code>chat</code> for details.</p> <code>None</code> <code>frequency_penalty</code> <code>Optional[float]</code> <p>See <code>chat</code> for details.</p> <code>None</code> <code>max_tokens</code> <code>Optional[float]</code> <p>See <code>chat</code> for details.</p> <code>None</code> <code>stop</code> <code>Optional[float]</code> <p>See <code>chat</code> for details.</p> <code>None</code> <code>extra_body</code> <code>Optional[float]</code> <p>See <code>chat</code> for details.</p> <code>None</code> <code>**kwargs</code> <p>Additional provider-specific arguments.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Response</code> <p>Bridgic response containing the generated message and raw API response.</p> Source code in <code>bridgic/llms/openai_like/_openai_like_llm.py</code> <pre><code>async def achat(\n    self,\n    messages: List[Message],\n    model: Optional[str] = None,\n    temperature: Optional[float] = None,\n    top_p: Optional[float] = None,\n    presence_penalty: Optional[float] = None,\n    frequency_penalty: Optional[float] = None,\n    max_tokens: Optional[int] = None,\n    stop: Optional[List[str]] = None,\n    extra_body: Optional[Dict[str, Any]] = None,\n    **kwargs,\n) -&gt; Response:\n    \"\"\"\n    Asynchronously send a chat completion request to an OpenAI-compatible provider.\n\n    Parameters\n    ----------\n    messages : list[Message]\n        Conversation messages.\n    model : str, optional\n        Model ID to use. Required unless provided in `configuration.model`.\n    temperature, top_p, presence_penalty, frequency_penalty, max_tokens, stop, extra_body\n        See `chat` for details.\n    **kwargs\n        Additional provider-specific arguments.\n\n    Returns\n    -------\n    Response\n        Bridgic response containing the generated message and raw API response.\n    \"\"\"\n    params = self._build_parameters(\n        messages=messages,\n        model=model,\n        temperature=temperature,\n        top_p=top_p,\n        presence_penalty=presence_penalty,\n        frequency_penalty=frequency_penalty,\n        max_tokens=max_tokens,\n        stop=stop,\n        extra_body=extra_body,\n        **kwargs,\n    )\n    validate_required_params(params, [\"messages\", \"model\"])\n    response = await self.async_client.chat.completions.create(**params)\n    openai_message: ChatCompletionMessage = response.choices[0].message\n    text: str = openai_message.content if openai_message.content else \"\"\n\n    if openai_message.refusal:\n        warnings.warn(openai_message.refusal, RuntimeWarning)\n\n    return Response(\n        message=Message.from_text(text, role=Role.AI),\n        raw=response,\n    )\n</code></pre>"},{"location":"reference/bridgic-llms-openai-like/bridgic/llms/openai_like/#bridgic.llms.openai_like.OpenAILikeLlm.astream","title":"astream","text":"<code>async</code> <pre><code>astream(\n    messages: List[Message],\n    model: Optional[str] = None,\n    temperature: Optional[float] = None,\n    top_p: Optional[float] = None,\n    presence_penalty: Optional[float] = None,\n    frequency_penalty: Optional[float] = None,\n    max_tokens: Optional[int] = None,\n    stop: Optional[List[str]] = None,\n    extra_body: Optional[Dict[str, Any]] = None,\n    **kwargs\n) -&gt; AsyncStreamResponse\n</code></pre> <p>Asynchronously stream a chat completion response incrementally.</p> <p>Parameters:</p> Name Type Description Default <code>messages</code> <code>list[Message]</code> <p>Conversation messages.</p> required <code>model</code> <code>str</code> <p>Model ID to use. Required unless provided in <code>configuration.model</code>.</p> <code>None</code> <code>temperature</code> <code>Optional[float]</code> <p>See <code>chat</code> for details.</p> <code>None</code> <code>top_p</code> <code>Optional[float]</code> <p>See <code>chat</code> for details.</p> <code>None</code> <code>presence_penalty</code> <code>Optional[float]</code> <p>See <code>chat</code> for details.</p> <code>None</code> <code>frequency_penalty</code> <code>Optional[float]</code> <p>See <code>chat</code> for details.</p> <code>None</code> <code>max_tokens</code> <code>Optional[float]</code> <p>See <code>chat</code> for details.</p> <code>None</code> <code>stop</code> <code>Optional[float]</code> <p>See <code>chat</code> for details.</p> <code>None</code> <code>extra_body</code> <code>Optional[float]</code> <p>See <code>chat</code> for details.</p> <code>None</code> <code>**kwargs</code> <p>Additional provider-specific arguments.</p> <code>{}</code> <p>Yields:</p> Type Description <code>MessageChunk</code> <p>Delta chunks as they arrive from the provider.</p> Notes <ul> <li>Validates <code>messages</code>, final <code>model</code>, and <code>stream=True</code>.</li> </ul> Source code in <code>bridgic/llms/openai_like/_openai_like_llm.py</code> <pre><code>async def astream(\n    self,\n    messages: List[Message],\n    model: Optional[str] = None,\n    temperature: Optional[float] = None,\n    top_p: Optional[float] = None,\n    presence_penalty: Optional[float] = None,\n    frequency_penalty: Optional[float] = None,\n    max_tokens: Optional[int] = None,\n    stop: Optional[List[str]] = None,\n    extra_body: Optional[Dict[str, Any]] = None,\n    **kwargs,\n) -&gt; AsyncStreamResponse:\n    \"\"\"\n    Asynchronously stream a chat completion response incrementally.\n\n    Parameters\n    ----------\n    messages : list[Message]\n        Conversation messages.\n    model : str, optional\n        Model ID to use. Required unless provided in `configuration.model`.\n    temperature, top_p, presence_penalty, frequency_penalty, max_tokens, stop, extra_body\n        See `chat` for details.\n    **kwargs\n        Additional provider-specific arguments.\n\n    Yields\n    ------\n    MessageChunk\n        Delta chunks as they arrive from the provider.\n\n    Notes\n    -----\n    - Validates `messages`, final `model`, and `stream=True`.\n    \"\"\"\n    params = self._build_parameters(\n        messages=messages,\n        model=model,\n        temperature=temperature,\n        top_p=top_p,\n        presence_penalty=presence_penalty,\n        frequency_penalty=frequency_penalty,\n        max_tokens=max_tokens,\n        stop=stop,\n        extra_body=extra_body,\n        stream=True,\n        **kwargs,\n    )\n    validate_required_params(params, [\"messages\", \"model\", \"stream\"])\n    response = await self.async_client.chat.completions.create(**params)\n    async for chunk in response:\n        delta_content = chunk.choices[0].delta.content\n        delta_content = delta_content if delta_content else \"\"\n        yield MessageChunk(delta=delta_content, raw=chunk)\n</code></pre>"},{"location":"reference/bridgic-llms-vllm/bridgic/llms/vllm/","title":"bridgic.llms.vllm","text":""},{"location":"reference/bridgic-llms-vllm/bridgic/llms/vllm/#vllm","title":"vllm","text":"<p>The vLLM integration module provides support for the vLLM inference engine.</p> <p>This module implements communication interfaces with vLLM inference services, supporting  highly reliable calls to large language models deployed via vLLM, and provides several  encapsulations for common seen high-level functionality.</p> <p>You can install the vLLM integration package for Bridgic by running:</p> <pre><code>pip install bridgic-llms-vllm\n</code></pre>"},{"location":"reference/bridgic-llms-vllm/bridgic/llms/vllm/#bridgic.llms.vllm.VllmServerLlm","title":"VllmServerLlm","text":"<p>               Bases: <code>OpenAILikeLlm</code>, <code>StructuredOutput</code>, <code>ToolSelection</code></p> <p>VllmServerLlm is a wrapper around the vLLM server, providing common calling interfaces for  self-hosted LLM service, such as chat, stream, as well as with encapsulation of common  seen high-level functionality.</p> <p>Parameters:</p> Name Type Description Default <code>api_base</code> <code>str</code> <p>The base URL of the LLM provider.</p> required <code>api_key</code> <code>str</code> <p>The API key of the LLM provider.</p> required <code>configuration</code> <code>Optional[VllmServerConfiguration]</code> <p>The configuration for the OpenAI-compatible API. If None, uses the default configuration.</p> <code>VllmServerConfiguration()</code> <code>timeout</code> <code>Optional[float]</code> <p>The timeout in seconds. If None, no timeout is applied.</p> <code>None</code> <code>http_client</code> <code>Optional[Client]</code> <p>Custom synchronous HTTP client for requests. If None, creates a default client.</p> <code>None</code> <code>http_async_client</code> <code>Optional[AsyncClient]</code> <p>Custom asynchronous HTTP client for requests. If None, creates a default client.</p> <code>None</code> Source code in <code>bridgic/llms/vllm/_vllm_server_llm.py</code> <pre><code>class VllmServerLlm(OpenAILikeLlm, StructuredOutput, ToolSelection):\n    \"\"\"\n    VllmServerLlm is a wrapper around the vLLM server, providing common calling interfaces for \n    self-hosted LLM service, such as chat, stream, as well as with encapsulation of common \n    seen high-level functionality.\n\n    Parameters\n    ----------\n    api_base: str\n        The base URL of the LLM provider.\n    api_key: str\n        The API key of the LLM provider.\n    configuration: Optional[VllmServerConfiguration]\n        The configuration for the OpenAI-compatible API. If None, uses the default configuration.\n    timeout: Optional[float]\n        The timeout in seconds. If None, no timeout is applied.\n    http_client : Optional[httpx.Client]\n        Custom synchronous HTTP client for requests. If None, creates a default client.\n    http_async_client : Optional[httpx.AsyncClient]\n        Custom asynchronous HTTP client for requests. If None, creates a default client.\n    \"\"\"\n\n    def __init__(\n        self,\n        api_base: str,\n        api_key: str,\n        configuration: Optional[VllmServerConfiguration] = VllmServerConfiguration(),\n        timeout: Optional[float] = None,\n        http_client: Optional[httpx.Client] = None,\n        http_async_client: Optional[httpx.AsyncClient] = None,\n    ):\n        super().__init__(\n            api_base=api_base,\n            api_key=api_key,\n            configuration=configuration,\n            timeout=timeout,\n            http_client=http_client,\n            http_async_client=http_async_client,\n        )\n\n    @override\n    def dump_to_dict(self) -&gt; Dict[str, Any]:\n        return super().dump_to_dict()\n\n    @override\n    def load_from_dict(self, state_dict: Dict[str, Any]) -&gt; None:\n        super().load_from_dict(state_dict)\n\n    @overload\n    def structured_output(\n        self,\n        messages: List[Message],\n        constraint: PydanticModel,\n        model: Optional[str] = None,\n        temperature: Optional[float] = ...,\n        top_p: Optional[float] = ...,\n        presence_penalty: Optional[float] = ...,\n        frequency_penalty: Optional[float] = ...,\n        extra_body: Optional[Dict[str, Any]] = ...,\n        **kwargs,\n    ) -&gt; BaseModel: ...\n\n    @overload\n    def structured_output(\n        self,\n        messages: List[Message],\n        constraint: JsonSchema,\n        model: Optional[str] = None,\n        temperature: Optional[float] = ...,\n        top_p: Optional[float] = ...,\n        presence_penalty: Optional[float] = ...,\n        frequency_penalty: Optional[float] = ...,\n        extra_body: Optional[Dict[str, Any]] = ...,\n        **kwargs,\n    ) -&gt; Dict[str, Any]: ...\n\n    @overload\n    def structured_output(\n        self,\n        messages: List[Message],\n        constraint: Choice,\n        model: Optional[str] = None,\n        temperature: Optional[float] = ...,\n        top_p: Optional[float] = ...,\n        presence_penalty: Optional[float] = ...,\n        frequency_penalty: Optional[float] = ...,\n        extra_body: Optional[Dict[str, Any]] = ...,\n        **kwargs,\n    ) -&gt; str: ...\n\n    def structured_output(\n        self,\n        messages: List[Message],\n        constraint: Constraint,\n        model: Optional[str] = None,\n        temperature: Optional[float] = None,\n        top_p: Optional[float] = None,\n        presence_penalty: Optional[float] = None,\n        frequency_penalty: Optional[float] = None,\n        extra_body: Optional[Dict[str, Any]] = None,\n        **kwargs,\n    ) -&gt; Union[BaseModel, Dict[str, Any], str]:\n        '''\n        Structured output in a specified format. This part of the functionality is provided based on the \n        capabilities of [vLLM Structured Output](https://docs.vllm.ai/en/latest/features/structured_outputs.html).\n\n        Parameters\n        ----------\n        messages: List[Message]\n            The messages to send to the LLM.\n        constraint: Constraint\n            The constraint to use for the structured output.\n        model: Optional[str]\n            The model to use for the structured output.\n        temperature: Optional[float]\n            The temperature to use for the structured output.\n        top_p: Optional[float]\n            The top_p to use for the structured output.\n        presence_penalty: Optional[float]\n            The presence_penalty to use for the structured output.\n        frequency_penalty: Optional[float]\n            The frequency_penalty to use for the structured output.\n        extra_body: Optional[Dict[str, Any]]\n            The extra_body to use for the structured output.\n        **kwargs: Any\n            The kwargs to use for the structured output.\n\n        Returns\n        -------\n        Union[BaseModel, Dict[str, Any], str]\n            The return type is based on the constraint type:\n            * If the constraint is PydanticModel, return an instance of the corresponding Pydantic model.\n            * If the constraint is JsonSchema, return a Dict[str, Any] that is the parsed JSON.\n            * Otherwise, return a str.\n        '''\n        response = self.chat(\n            messages=messages,\n            model=model,\n            temperature=temperature,\n            top_p=top_p,\n            presence_penalty=presence_penalty,\n            frequency_penalty=frequency_penalty,\n            extra_body=self._convert_constraint(constraint, extra_body),\n            **kwargs,\n        )\n        return self._convert_response(constraint, response)\n\n    async def astructured_output(\n        self,\n        messages: List[Message],\n        constraint: Constraint,\n        model: Optional[str] = None,\n        temperature: Optional[float] = None,\n        top_p: Optional[float] = None,\n        presence_penalty: Optional[float] = None,\n        frequency_penalty: Optional[float] = None,\n        extra_body: Optional[Dict[str, Any]] = None,\n        **kwargs,\n    ) -&gt; Union[BaseModel, Dict[str, Any], str]:\n        '''\n        Structured output in a specified format. This part of the functionality is provided based on the \n        capabilities of [vLLM Structured Output](https://docs.vllm.ai/en/latest/features/structured_outputs.html).\n\n        Parameters\n        ----------\n        messages: List[Message]\n            The messages to send to the LLM.\n        constraint: Constraint\n            The constraint to use for the structured output.\n        model: Optional[str]\n            The model to use for the structured output.\n        temperature: Optional[float]\n            The temperature to use for the structured output.\n        top_p: Optional[float]\n            The top_p to use for the structured output.\n        presence_penalty: Optional[float]\n            The presence_penalty to use for the structured output.\n        frequency_penalty: Optional[float]\n            The frequency_penalty to use for the structured output.\n        extra_body: Optional[Dict[str, Any]]\n            The extra_body to use for the structured output.\n        **kwargs: Any\n            The kwargs to use for the structured output.\n\n        Returns\n        -------\n        Union[BaseModel, Dict[str, Any], str]\n            The return type is based on the constraint type:\n            * If the constraint is PydanticModel, return an instance of the corresponding Pydantic model.\n            * If the constraint is JsonSchema, return a Dict[str, Any] that is the parsed JSON.\n            * Otherwise, return a str.\n        '''\n        response = await self.achat(\n            messages=messages,\n            model=model,\n            temperature=temperature,\n            top_p=top_p,\n            presence_penalty=presence_penalty,\n            frequency_penalty=frequency_penalty,\n            extra_body=self._convert_constraint(constraint, extra_body),\n            **kwargs,\n        )\n        return self._convert_response(constraint, response)\n\n    def _convert_constraint(\n        self,\n        constraint: Constraint,\n        extra_body: Optional[Dict[str, Any]] = None,\n    ) -&gt; Dict[str, Any]:\n        extra_body = {} if extra_body is None else extra_body\n\n        if isinstance(constraint, PydanticModel):\n            extra_body[\"guided_json\"] = constraint.model.model_json_schema()\n        elif isinstance(constraint, JsonSchema):\n            extra_body[\"guided_json\"] = constraint.schema_dict\n        elif isinstance(constraint, Regex):\n            extra_body[\"guided_regex\"] = constraint.pattern\n        elif isinstance(constraint, Choice):\n            extra_body[\"guided_choice\"] = constraint.choices\n        elif isinstance(constraint, EbnfGrammar):\n            extra_body[\"guided_grammar\"] = constraint.syntax\n        else:\n            raise ValueError(f\"Invalid constraint: {constraint}\")\n\n        return extra_body\n\n    def _convert_response(\n        self,\n        constraint: Constraint,\n        response: Response,\n    ) -&gt; Union[BaseModel, Dict[str, Any], str]:\n        content = response.message.content\n\n        if isinstance(constraint, PydanticModel):\n            return constraint.model.model_validate_json(content)\n        elif isinstance(constraint, JsonSchema):\n            return json.loads(content)\n        return content\n\n    def select_tool(\n        self,\n        messages: List[Message],\n        tools: List[Tool],\n        model: Optional[str] = None,\n        temperature: Optional[float] = None,\n        top_p: Optional[float] = None,\n        presence_penalty: Optional[float] = None,\n        frequency_penalty: Optional[float] = None,\n        extra_body: Optional[Dict[str, Any]] = None,\n        tool_choice: Union[Literal[\"auto\", \"required\", \"none\"], ChatCompletionNamedToolChoiceParam] = \"auto\",\n        **kwargs,\n    ) -&gt; Tuple[List[ToolCall], Optional[Dict]]:\n        \"\"\"\n        Select tools from a specified list of tools.\n\n        Parameters\n        ----------\n        messages: List[Message]\n            The messages to send to the LLM.\n        tools: List[Tool]\n            The tools to use for the tool select.\n        model: Optional[str]\n            The model to use for the tool select.\n        temperature: Optional[float]\n            The temperature to use for the tool select.\n        top_p: Optional[float]\n            The top_p to use for the tool select.\n        presence_penalty: Optional[float]\n            The presence_penalty to use for the tool select.\n        frequency_penalty: Optional[float]\n            The frequency_penalty to use for the tool select.\n        extra_body: Optional[Dict[str, Any]]\n            The extra_body to use for the tool select.\n        tool_choice : Union[Literal[\"auto\", \"required\", \"none\"], ChatCompletionNamedToolChoiceParam]\n            Tool choice mode for tool calling. There are 4 choices that are supported:\n            - `auto` means the model can pick between generating a message or calling one or more tools.\n            To enable this feature, you should set the tags `--enable-auto-tool-choice` and `--tool-call-parser` \n            when starting the vLLM server.\n            - `required` means the model must generate one or more tool calls based on the specified tool list \n            in the `tools` parameter. The number of tool calls depends on the user's query.\n            - `none` means the model will not call any tool and instead generates a message. When tools are \n            specified in the request, vLLM includes tool definitions in the prompt by default, regardless \n            of the tool_choice setting. To exclude tool definitions when tool_choice='none', use the \n            `--exclude-tools-when-tool-choice-none` option when starting the vLLM server.\n            - You can also specify a particular function using named function calling by setting `tool_choice` \n            parameter to a json object, like `tool_choice={\"type\": \"function\", \"function\": {\"name\": \"get_weather\"}}`.\n\n        **kwargs: Any\n            The kwargs to use for the tool select.\n\n        Returns\n        -------\n        Tuple[List[ToolCall], Optional[str]]\n            A list that contains the selected tools and their arguments.\n\n        Notes\n        -----\n        See more on [Tool Calling](https://docs.vllm.ai/en/stable/features/tool_calling.html).\n        \"\"\"\n        # Build parameters dictionary for validation\n        params = filter_dict(merge_dict(self.configuration.model_dump(), {\n            \"model\": model,\n            \"temperature\": temperature,\n            \"top_p\": top_p,\n            \"presence_penalty\": presence_penalty,\n            \"frequency_penalty\": frequency_penalty,\n            \"extra_body\": extra_body,\n            **kwargs,\n        }))\n\n        # Validate required parameters for tool selection\n        validate_required_params(params, [\"model\"])\n\n        input_messages = [self._convert_message(message=msg, strict=True) for msg in messages]\n        input_tools = [\n            {\n                \"type\": \"function\",\n                \"function\": {\n                    \"name\": tool.name,\n                    \"description\": tool.description,\n                    \"parameters\": tool.parameters,\n                },\n            } for tool in tools\n        ]\n\n        response = self.client.chat.completions.create(\n            model=model,\n            messages=input_messages,\n            tools=input_tools,\n            tool_choice=tool_choice,\n            **kwargs,\n        )\n        tool_calls = response.choices[0].message.tool_calls\n\n        output_content = \"\"\n        if response.choices[0].message.content:\n            output_content = response.choices[0].message.content\n\n        output_tool_calls = []\n        if tool_calls:\n            output_tool_calls = self._convert_tool_calls(tool_calls)\n\n        return (output_tool_calls, output_content)\n\n    async def aselect_tool(\n        self,\n        messages: List[Message],\n        tools: List[Tool],\n        model: Optional[str] = None,\n        temperature: Optional[float] = None,\n        top_p: Optional[float] = None,\n        presence_penalty: Optional[float] = None,\n        frequency_penalty: Optional[float] = None,\n        extra_body: Optional[Dict[str, Any]] = None,\n        tool_choice: Union[Literal[\"auto\", \"required\", \"none\"], ChatCompletionNamedToolChoiceParam] = \"auto\",\n        **kwargs,\n    ) -&gt; Tuple[List[ToolCall], Optional[str]]:\n        \"\"\"\n        Select tools from a specified list of tools.\n\n        Parameters\n        ----------\n        messages: List[Message]\n            The messages to send to the LLM.\n        tools: List[Tool]\n            The tools to use for the tool select.\n        model: Optional[str]\n            The model to use for the tool select.\n        temperature: Optional[float]\n            The temperature to use for the tool select.\n        top_p: Optional[float]\n            The top_p to use for the tool select.\n        presence_penalty: Optional[float]\n            The presence_penalty to use for the tool select.\n        frequency_penalty: Optional[float]\n            The frequency_penalty to use for the tool select.\n        extra_body: Optional[Dict[str, Any]]\n            The extra_body to use for the tool select.\n        tool_choice : Union[Literal[\"auto\", \"required\", \"none\"], ChatCompletionNamedToolChoiceParam]\n            Tool choice mode for tool calling. There are 4 choices that are supported:\n            - `auto` means the model can pick between generating a message or calling one or more tools.\n            To enable this feature, you should set the tags `--enable-auto-tool-choice` and `--tool-call-parser` \n            when starting the vLLM server.\n            - `required` means the model must generate one or more tool calls based on the specified tool list \n            in the `tools` parameter. The number of tool calls depends on the user's query.\n            - `none` means the model will not call any tool and instead generates a message. When tools are \n            specified in the request, vLLM includes tool definitions in the prompt by default, regardless \n            of the tool_choice setting. To exclude tool definitions when tool_choice='none', use the \n            `--exclude-tools-when-tool-choice-none` option when starting the vLLM server.\n            - You can also specify a particular function using named function calling by setting `tool_choice` \n            parameter to a json object, like `tool_choice={\"type\": \"function\", \"function\": {\"name\": \"get_weather\"}}`.\n\n        **kwargs: Any\n            The kwargs to use for the tool select.\n\n        Returns\n        -------\n        Tuple[List[ToolCall], Optional[str]]\n            A list that contains the selected tools and their arguments.\n\n        Notes\n        -----\n        See more on [Tool Calling](https://docs.vllm.ai/en/stable/features/tool_calling.html).\n        \"\"\"\n        # Build parameters dictionary for validation\n        params = filter_dict(merge_dict(self.configuration.model_dump(), {\n            \"model\": model,\n            \"temperature\": temperature,\n            \"top_p\": top_p,\n            \"presence_penalty\": presence_penalty,\n            \"frequency_penalty\": frequency_penalty,\n            \"extra_body\": extra_body,\n            **kwargs,\n        }))\n\n        # Validate required parameters for tool selection\n        validate_required_params(params, [\"model\"])\n\n        input_messages = [self._convert_message(message=msg, strict=True) for msg in messages]\n        input_tools = [\n            {\n                \"type\": \"function\",\n                \"function\": {\n                    \"name\": tool.name,\n                    \"description\": tool.description,\n                    \"parameters\": tool.parameters,\n                },\n            } for tool in tools\n        ]\n\n        response = self.client.chat.completions.create(\n            model=model,\n            messages=input_messages,\n            tools=input_tools,\n            tool_choice=tool_choice,\n            **kwargs,\n        )\n        tool_calls = response.choices[0].message.tool_calls\n\n        output_content = \"\"\n        if response.choices[0].message.content:\n            output_content = response.choices[0].message.content\n\n        output_tool_calls = []\n        if tool_calls:\n            output_tool_calls = self._convert_tool_calls(tool_calls)\n\n        return (output_tool_calls, output_content)\n\n    def _convert_tool_calls(self, tool_calls: List[ChatCompletionMessageFunctionToolCall]) -&gt; List[ToolCall]:\n        return [\n            ToolCall(\n                id=generate_tool_id(),\n                name=tool_call.function.name,\n                arguments=json.loads(tool_call.function.arguments),\n            ) for tool_call in tool_calls\n        ]\n</code></pre>"},{"location":"reference/bridgic-llms-vllm/bridgic/llms/vllm/#bridgic.llms.vllm.VllmServerLlm.structured_output","title":"structured_output","text":"<pre><code>structured_output(\n    messages: List[Message],\n    constraint: Constraint,\n    model: Optional[str] = None,\n    temperature: Optional[float] = None,\n    top_p: Optional[float] = None,\n    presence_penalty: Optional[float] = None,\n    frequency_penalty: Optional[float] = None,\n    extra_body: Optional[Dict[str, Any]] = None,\n    **kwargs\n) -&gt; Union[BaseModel, Dict[str, Any], str]\n</code></pre> <p>Structured output in a specified format. This part of the functionality is provided based on the  capabilities of vLLM Structured Output.</p> <p>Parameters:</p> Name Type Description Default <code>messages</code> <code>List[Message]</code> <p>The messages to send to the LLM.</p> required <code>constraint</code> <code>Constraint</code> <p>The constraint to use for the structured output.</p> required <code>model</code> <code>Optional[str]</code> <p>The model to use for the structured output.</p> <code>None</code> <code>temperature</code> <code>Optional[float]</code> <p>The temperature to use for the structured output.</p> <code>None</code> <code>top_p</code> <code>Optional[float]</code> <p>The top_p to use for the structured output.</p> <code>None</code> <code>presence_penalty</code> <code>Optional[float]</code> <p>The presence_penalty to use for the structured output.</p> <code>None</code> <code>frequency_penalty</code> <code>Optional[float]</code> <p>The frequency_penalty to use for the structured output.</p> <code>None</code> <code>extra_body</code> <code>Optional[Dict[str, Any]]</code> <p>The extra_body to use for the structured output.</p> <code>None</code> <code>**kwargs</code> <p>The kwargs to use for the structured output.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Union[BaseModel, Dict[str, Any], str]</code> <p>The return type is based on the constraint type: * If the constraint is PydanticModel, return an instance of the corresponding Pydantic model. * If the constraint is JsonSchema, return a Dict[str, Any] that is the parsed JSON. * Otherwise, return a str.</p> Source code in <code>bridgic/llms/vllm/_vllm_server_llm.py</code> <pre><code>def structured_output(\n    self,\n    messages: List[Message],\n    constraint: Constraint,\n    model: Optional[str] = None,\n    temperature: Optional[float] = None,\n    top_p: Optional[float] = None,\n    presence_penalty: Optional[float] = None,\n    frequency_penalty: Optional[float] = None,\n    extra_body: Optional[Dict[str, Any]] = None,\n    **kwargs,\n) -&gt; Union[BaseModel, Dict[str, Any], str]:\n    '''\n    Structured output in a specified format. This part of the functionality is provided based on the \n    capabilities of [vLLM Structured Output](https://docs.vllm.ai/en/latest/features/structured_outputs.html).\n\n    Parameters\n    ----------\n    messages: List[Message]\n        The messages to send to the LLM.\n    constraint: Constraint\n        The constraint to use for the structured output.\n    model: Optional[str]\n        The model to use for the structured output.\n    temperature: Optional[float]\n        The temperature to use for the structured output.\n    top_p: Optional[float]\n        The top_p to use for the structured output.\n    presence_penalty: Optional[float]\n        The presence_penalty to use for the structured output.\n    frequency_penalty: Optional[float]\n        The frequency_penalty to use for the structured output.\n    extra_body: Optional[Dict[str, Any]]\n        The extra_body to use for the structured output.\n    **kwargs: Any\n        The kwargs to use for the structured output.\n\n    Returns\n    -------\n    Union[BaseModel, Dict[str, Any], str]\n        The return type is based on the constraint type:\n        * If the constraint is PydanticModel, return an instance of the corresponding Pydantic model.\n        * If the constraint is JsonSchema, return a Dict[str, Any] that is the parsed JSON.\n        * Otherwise, return a str.\n    '''\n    response = self.chat(\n        messages=messages,\n        model=model,\n        temperature=temperature,\n        top_p=top_p,\n        presence_penalty=presence_penalty,\n        frequency_penalty=frequency_penalty,\n        extra_body=self._convert_constraint(constraint, extra_body),\n        **kwargs,\n    )\n    return self._convert_response(constraint, response)\n</code></pre>"},{"location":"reference/bridgic-llms-vllm/bridgic/llms/vllm/#bridgic.llms.vllm.VllmServerLlm.astructured_output","title":"astructured_output","text":"<code>async</code> <pre><code>astructured_output(\n    messages: List[Message],\n    constraint: Constraint,\n    model: Optional[str] = None,\n    temperature: Optional[float] = None,\n    top_p: Optional[float] = None,\n    presence_penalty: Optional[float] = None,\n    frequency_penalty: Optional[float] = None,\n    extra_body: Optional[Dict[str, Any]] = None,\n    **kwargs\n) -&gt; Union[BaseModel, Dict[str, Any], str]\n</code></pre> <p>Structured output in a specified format. This part of the functionality is provided based on the  capabilities of vLLM Structured Output.</p> <p>Parameters:</p> Name Type Description Default <code>messages</code> <code>List[Message]</code> <p>The messages to send to the LLM.</p> required <code>constraint</code> <code>Constraint</code> <p>The constraint to use for the structured output.</p> required <code>model</code> <code>Optional[str]</code> <p>The model to use for the structured output.</p> <code>None</code> <code>temperature</code> <code>Optional[float]</code> <p>The temperature to use for the structured output.</p> <code>None</code> <code>top_p</code> <code>Optional[float]</code> <p>The top_p to use for the structured output.</p> <code>None</code> <code>presence_penalty</code> <code>Optional[float]</code> <p>The presence_penalty to use for the structured output.</p> <code>None</code> <code>frequency_penalty</code> <code>Optional[float]</code> <p>The frequency_penalty to use for the structured output.</p> <code>None</code> <code>extra_body</code> <code>Optional[Dict[str, Any]]</code> <p>The extra_body to use for the structured output.</p> <code>None</code> <code>**kwargs</code> <p>The kwargs to use for the structured output.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Union[BaseModel, Dict[str, Any], str]</code> <p>The return type is based on the constraint type: * If the constraint is PydanticModel, return an instance of the corresponding Pydantic model. * If the constraint is JsonSchema, return a Dict[str, Any] that is the parsed JSON. * Otherwise, return a str.</p> Source code in <code>bridgic/llms/vllm/_vllm_server_llm.py</code> <pre><code>async def astructured_output(\n    self,\n    messages: List[Message],\n    constraint: Constraint,\n    model: Optional[str] = None,\n    temperature: Optional[float] = None,\n    top_p: Optional[float] = None,\n    presence_penalty: Optional[float] = None,\n    frequency_penalty: Optional[float] = None,\n    extra_body: Optional[Dict[str, Any]] = None,\n    **kwargs,\n) -&gt; Union[BaseModel, Dict[str, Any], str]:\n    '''\n    Structured output in a specified format. This part of the functionality is provided based on the \n    capabilities of [vLLM Structured Output](https://docs.vllm.ai/en/latest/features/structured_outputs.html).\n\n    Parameters\n    ----------\n    messages: List[Message]\n        The messages to send to the LLM.\n    constraint: Constraint\n        The constraint to use for the structured output.\n    model: Optional[str]\n        The model to use for the structured output.\n    temperature: Optional[float]\n        The temperature to use for the structured output.\n    top_p: Optional[float]\n        The top_p to use for the structured output.\n    presence_penalty: Optional[float]\n        The presence_penalty to use for the structured output.\n    frequency_penalty: Optional[float]\n        The frequency_penalty to use for the structured output.\n    extra_body: Optional[Dict[str, Any]]\n        The extra_body to use for the structured output.\n    **kwargs: Any\n        The kwargs to use for the structured output.\n\n    Returns\n    -------\n    Union[BaseModel, Dict[str, Any], str]\n        The return type is based on the constraint type:\n        * If the constraint is PydanticModel, return an instance of the corresponding Pydantic model.\n        * If the constraint is JsonSchema, return a Dict[str, Any] that is the parsed JSON.\n        * Otherwise, return a str.\n    '''\n    response = await self.achat(\n        messages=messages,\n        model=model,\n        temperature=temperature,\n        top_p=top_p,\n        presence_penalty=presence_penalty,\n        frequency_penalty=frequency_penalty,\n        extra_body=self._convert_constraint(constraint, extra_body),\n        **kwargs,\n    )\n    return self._convert_response(constraint, response)\n</code></pre>"},{"location":"reference/bridgic-llms-vllm/bridgic/llms/vllm/#bridgic.llms.vllm.VllmServerLlm.select_tool","title":"select_tool","text":"<pre><code>select_tool(\n    messages: List[Message],\n    tools: List[Tool],\n    model: Optional[str] = None,\n    temperature: Optional[float] = None,\n    top_p: Optional[float] = None,\n    presence_penalty: Optional[float] = None,\n    frequency_penalty: Optional[float] = None,\n    extra_body: Optional[Dict[str, Any]] = None,\n    tool_choice: Union[\n        Literal[\"auto\", \"required\", \"none\"],\n        ChatCompletionNamedToolChoiceParam,\n    ] = \"auto\",\n    **kwargs\n) -&gt; Tuple[List[ToolCall], Optional[Dict]]\n</code></pre> <p>Select tools from a specified list of tools.</p> <p>Parameters:</p> Name Type Description Default <code>messages</code> <code>List[Message]</code> <p>The messages to send to the LLM.</p> required <code>tools</code> <code>List[Tool]</code> <p>The tools to use for the tool select.</p> required <code>model</code> <code>Optional[str]</code> <p>The model to use for the tool select.</p> <code>None</code> <code>temperature</code> <code>Optional[float]</code> <p>The temperature to use for the tool select.</p> <code>None</code> <code>top_p</code> <code>Optional[float]</code> <p>The top_p to use for the tool select.</p> <code>None</code> <code>presence_penalty</code> <code>Optional[float]</code> <p>The presence_penalty to use for the tool select.</p> <code>None</code> <code>frequency_penalty</code> <code>Optional[float]</code> <p>The frequency_penalty to use for the tool select.</p> <code>None</code> <code>extra_body</code> <code>Optional[Dict[str, Any]]</code> <p>The extra_body to use for the tool select.</p> <code>None</code> <code>tool_choice</code> <code>Union[Literal['auto', 'required', 'none'], ChatCompletionNamedToolChoiceParam]</code> <p>Tool choice mode for tool calling. There are 4 choices that are supported: - <code>auto</code> means the model can pick between generating a message or calling one or more tools. To enable this feature, you should set the tags <code>--enable-auto-tool-choice</code> and <code>--tool-call-parser</code>  when starting the vLLM server. - <code>required</code> means the model must generate one or more tool calls based on the specified tool list  in the <code>tools</code> parameter. The number of tool calls depends on the user's query. - <code>none</code> means the model will not call any tool and instead generates a message. When tools are  specified in the request, vLLM includes tool definitions in the prompt by default, regardless  of the tool_choice setting. To exclude tool definitions when tool_choice='none', use the  <code>--exclude-tools-when-tool-choice-none</code> option when starting the vLLM server. - You can also specify a particular function using named function calling by setting <code>tool_choice</code>  parameter to a json object, like <code>tool_choice={\"type\": \"function\", \"function\": {\"name\": \"get_weather\"}}</code>.</p> <code>'auto'</code> <code>**kwargs</code> <p>The kwargs to use for the tool select.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Tuple[List[ToolCall], Optional[str]]</code> <p>A list that contains the selected tools and their arguments.</p> Notes <p>See more on Tool Calling.</p> Source code in <code>bridgic/llms/vllm/_vllm_server_llm.py</code> <pre><code>def select_tool(\n    self,\n    messages: List[Message],\n    tools: List[Tool],\n    model: Optional[str] = None,\n    temperature: Optional[float] = None,\n    top_p: Optional[float] = None,\n    presence_penalty: Optional[float] = None,\n    frequency_penalty: Optional[float] = None,\n    extra_body: Optional[Dict[str, Any]] = None,\n    tool_choice: Union[Literal[\"auto\", \"required\", \"none\"], ChatCompletionNamedToolChoiceParam] = \"auto\",\n    **kwargs,\n) -&gt; Tuple[List[ToolCall], Optional[Dict]]:\n    \"\"\"\n    Select tools from a specified list of tools.\n\n    Parameters\n    ----------\n    messages: List[Message]\n        The messages to send to the LLM.\n    tools: List[Tool]\n        The tools to use for the tool select.\n    model: Optional[str]\n        The model to use for the tool select.\n    temperature: Optional[float]\n        The temperature to use for the tool select.\n    top_p: Optional[float]\n        The top_p to use for the tool select.\n    presence_penalty: Optional[float]\n        The presence_penalty to use for the tool select.\n    frequency_penalty: Optional[float]\n        The frequency_penalty to use for the tool select.\n    extra_body: Optional[Dict[str, Any]]\n        The extra_body to use for the tool select.\n    tool_choice : Union[Literal[\"auto\", \"required\", \"none\"], ChatCompletionNamedToolChoiceParam]\n        Tool choice mode for tool calling. There are 4 choices that are supported:\n        - `auto` means the model can pick between generating a message or calling one or more tools.\n        To enable this feature, you should set the tags `--enable-auto-tool-choice` and `--tool-call-parser` \n        when starting the vLLM server.\n        - `required` means the model must generate one or more tool calls based on the specified tool list \n        in the `tools` parameter. The number of tool calls depends on the user's query.\n        - `none` means the model will not call any tool and instead generates a message. When tools are \n        specified in the request, vLLM includes tool definitions in the prompt by default, regardless \n        of the tool_choice setting. To exclude tool definitions when tool_choice='none', use the \n        `--exclude-tools-when-tool-choice-none` option when starting the vLLM server.\n        - You can also specify a particular function using named function calling by setting `tool_choice` \n        parameter to a json object, like `tool_choice={\"type\": \"function\", \"function\": {\"name\": \"get_weather\"}}`.\n\n    **kwargs: Any\n        The kwargs to use for the tool select.\n\n    Returns\n    -------\n    Tuple[List[ToolCall], Optional[str]]\n        A list that contains the selected tools and their arguments.\n\n    Notes\n    -----\n    See more on [Tool Calling](https://docs.vllm.ai/en/stable/features/tool_calling.html).\n    \"\"\"\n    # Build parameters dictionary for validation\n    params = filter_dict(merge_dict(self.configuration.model_dump(), {\n        \"model\": model,\n        \"temperature\": temperature,\n        \"top_p\": top_p,\n        \"presence_penalty\": presence_penalty,\n        \"frequency_penalty\": frequency_penalty,\n        \"extra_body\": extra_body,\n        **kwargs,\n    }))\n\n    # Validate required parameters for tool selection\n    validate_required_params(params, [\"model\"])\n\n    input_messages = [self._convert_message(message=msg, strict=True) for msg in messages]\n    input_tools = [\n        {\n            \"type\": \"function\",\n            \"function\": {\n                \"name\": tool.name,\n                \"description\": tool.description,\n                \"parameters\": tool.parameters,\n            },\n        } for tool in tools\n    ]\n\n    response = self.client.chat.completions.create(\n        model=model,\n        messages=input_messages,\n        tools=input_tools,\n        tool_choice=tool_choice,\n        **kwargs,\n    )\n    tool_calls = response.choices[0].message.tool_calls\n\n    output_content = \"\"\n    if response.choices[0].message.content:\n        output_content = response.choices[0].message.content\n\n    output_tool_calls = []\n    if tool_calls:\n        output_tool_calls = self._convert_tool_calls(tool_calls)\n\n    return (output_tool_calls, output_content)\n</code></pre>"},{"location":"reference/bridgic-llms-vllm/bridgic/llms/vllm/#bridgic.llms.vllm.VllmServerLlm.aselect_tool","title":"aselect_tool","text":"<code>async</code> <pre><code>aselect_tool(\n    messages: List[Message],\n    tools: List[Tool],\n    model: Optional[str] = None,\n    temperature: Optional[float] = None,\n    top_p: Optional[float] = None,\n    presence_penalty: Optional[float] = None,\n    frequency_penalty: Optional[float] = None,\n    extra_body: Optional[Dict[str, Any]] = None,\n    tool_choice: Union[\n        Literal[\"auto\", \"required\", \"none\"],\n        ChatCompletionNamedToolChoiceParam,\n    ] = \"auto\",\n    **kwargs\n) -&gt; Tuple[List[ToolCall], Optional[str]]\n</code></pre> <p>Select tools from a specified list of tools.</p> <p>Parameters:</p> Name Type Description Default <code>messages</code> <code>List[Message]</code> <p>The messages to send to the LLM.</p> required <code>tools</code> <code>List[Tool]</code> <p>The tools to use for the tool select.</p> required <code>model</code> <code>Optional[str]</code> <p>The model to use for the tool select.</p> <code>None</code> <code>temperature</code> <code>Optional[float]</code> <p>The temperature to use for the tool select.</p> <code>None</code> <code>top_p</code> <code>Optional[float]</code> <p>The top_p to use for the tool select.</p> <code>None</code> <code>presence_penalty</code> <code>Optional[float]</code> <p>The presence_penalty to use for the tool select.</p> <code>None</code> <code>frequency_penalty</code> <code>Optional[float]</code> <p>The frequency_penalty to use for the tool select.</p> <code>None</code> <code>extra_body</code> <code>Optional[Dict[str, Any]]</code> <p>The extra_body to use for the tool select.</p> <code>None</code> <code>tool_choice</code> <code>Union[Literal['auto', 'required', 'none'], ChatCompletionNamedToolChoiceParam]</code> <p>Tool choice mode for tool calling. There are 4 choices that are supported: - <code>auto</code> means the model can pick between generating a message or calling one or more tools. To enable this feature, you should set the tags <code>--enable-auto-tool-choice</code> and <code>--tool-call-parser</code>  when starting the vLLM server. - <code>required</code> means the model must generate one or more tool calls based on the specified tool list  in the <code>tools</code> parameter. The number of tool calls depends on the user's query. - <code>none</code> means the model will not call any tool and instead generates a message. When tools are  specified in the request, vLLM includes tool definitions in the prompt by default, regardless  of the tool_choice setting. To exclude tool definitions when tool_choice='none', use the  <code>--exclude-tools-when-tool-choice-none</code> option when starting the vLLM server. - You can also specify a particular function using named function calling by setting <code>tool_choice</code>  parameter to a json object, like <code>tool_choice={\"type\": \"function\", \"function\": {\"name\": \"get_weather\"}}</code>.</p> <code>'auto'</code> <code>**kwargs</code> <p>The kwargs to use for the tool select.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Tuple[List[ToolCall], Optional[str]]</code> <p>A list that contains the selected tools and their arguments.</p> Notes <p>See more on Tool Calling.</p> Source code in <code>bridgic/llms/vllm/_vllm_server_llm.py</code> <pre><code>async def aselect_tool(\n    self,\n    messages: List[Message],\n    tools: List[Tool],\n    model: Optional[str] = None,\n    temperature: Optional[float] = None,\n    top_p: Optional[float] = None,\n    presence_penalty: Optional[float] = None,\n    frequency_penalty: Optional[float] = None,\n    extra_body: Optional[Dict[str, Any]] = None,\n    tool_choice: Union[Literal[\"auto\", \"required\", \"none\"], ChatCompletionNamedToolChoiceParam] = \"auto\",\n    **kwargs,\n) -&gt; Tuple[List[ToolCall], Optional[str]]:\n    \"\"\"\n    Select tools from a specified list of tools.\n\n    Parameters\n    ----------\n    messages: List[Message]\n        The messages to send to the LLM.\n    tools: List[Tool]\n        The tools to use for the tool select.\n    model: Optional[str]\n        The model to use for the tool select.\n    temperature: Optional[float]\n        The temperature to use for the tool select.\n    top_p: Optional[float]\n        The top_p to use for the tool select.\n    presence_penalty: Optional[float]\n        The presence_penalty to use for the tool select.\n    frequency_penalty: Optional[float]\n        The frequency_penalty to use for the tool select.\n    extra_body: Optional[Dict[str, Any]]\n        The extra_body to use for the tool select.\n    tool_choice : Union[Literal[\"auto\", \"required\", \"none\"], ChatCompletionNamedToolChoiceParam]\n        Tool choice mode for tool calling. There are 4 choices that are supported:\n        - `auto` means the model can pick between generating a message or calling one or more tools.\n        To enable this feature, you should set the tags `--enable-auto-tool-choice` and `--tool-call-parser` \n        when starting the vLLM server.\n        - `required` means the model must generate one or more tool calls based on the specified tool list \n        in the `tools` parameter. The number of tool calls depends on the user's query.\n        - `none` means the model will not call any tool and instead generates a message. When tools are \n        specified in the request, vLLM includes tool definitions in the prompt by default, regardless \n        of the tool_choice setting. To exclude tool definitions when tool_choice='none', use the \n        `--exclude-tools-when-tool-choice-none` option when starting the vLLM server.\n        - You can also specify a particular function using named function calling by setting `tool_choice` \n        parameter to a json object, like `tool_choice={\"type\": \"function\", \"function\": {\"name\": \"get_weather\"}}`.\n\n    **kwargs: Any\n        The kwargs to use for the tool select.\n\n    Returns\n    -------\n    Tuple[List[ToolCall], Optional[str]]\n        A list that contains the selected tools and their arguments.\n\n    Notes\n    -----\n    See more on [Tool Calling](https://docs.vllm.ai/en/stable/features/tool_calling.html).\n    \"\"\"\n    # Build parameters dictionary for validation\n    params = filter_dict(merge_dict(self.configuration.model_dump(), {\n        \"model\": model,\n        \"temperature\": temperature,\n        \"top_p\": top_p,\n        \"presence_penalty\": presence_penalty,\n        \"frequency_penalty\": frequency_penalty,\n        \"extra_body\": extra_body,\n        **kwargs,\n    }))\n\n    # Validate required parameters for tool selection\n    validate_required_params(params, [\"model\"])\n\n    input_messages = [self._convert_message(message=msg, strict=True) for msg in messages]\n    input_tools = [\n        {\n            \"type\": \"function\",\n            \"function\": {\n                \"name\": tool.name,\n                \"description\": tool.description,\n                \"parameters\": tool.parameters,\n            },\n        } for tool in tools\n    ]\n\n    response = self.client.chat.completions.create(\n        model=model,\n        messages=input_messages,\n        tools=input_tools,\n        tool_choice=tool_choice,\n        **kwargs,\n    )\n    tool_calls = response.choices[0].message.tool_calls\n\n    output_content = \"\"\n    if response.choices[0].message.content:\n        output_content = response.choices[0].message.content\n\n    output_tool_calls = []\n    if tool_calls:\n        output_tool_calls = self._convert_tool_calls(tool_calls)\n\n    return (output_tool_calls, output_content)\n</code></pre>"},{"location":"reference/bridgic-llms-vllm/bridgic/llms/vllm/#bridgic.llms.vllm.VllmServerConfiguration","title":"VllmServerConfiguration","text":"<p>               Bases: <code>OpenAILikeConfiguration</code></p> <p>Configuration for the vLLM server.</p> Source code in <code>bridgic/llms/vllm/_vllm_server_llm.py</code> <pre><code>class VllmServerConfiguration(OpenAILikeConfiguration):\n    \"\"\"\n    Configuration for the vLLM server.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/bridgic-protocols-mcp/bridgic/protocols/mcp/","title":"bridgic.protocols.mcp","text":""},{"location":"reference/bridgic-protocols-mcp/bridgic/protocols/mcp/#mcp","title":"mcp","text":"<p>The MCP module provides integration with the Model Context Protocol (MCP) for Bridgic.</p> <p>This module enables Bridgic to connect to and interact with MCP servers, allowing:</p> <ul> <li>Connection Management: Establish and manage connections to MCP servers via different   transport protocols (stdio, streamable HTTP, etc.)</li> <li>Tool Integration: Use MCP tools as callable tools within Bridgic agentic systems</li> <li>Prompt Integration: Access and use prompts from MCP servers in Bridgic workflows</li> <li>Worker Integration: Execute MCP tools through Bridgic's worker system</li> </ul> Core Components <ul> <li><code>McpServerConnection</code>: Abstract base class for MCP server connections</li> <li><code>McpServerConnectionStdio</code>: Connection implementation using stdio transport</li> <li><code>McpServerConnectionStreamableHttp</code>: Connection implementation using streamable HTTP</li> <li><code>McpServerConnectionManager</code>: Manager for multiple MCP connections with shared event loop</li> <li><code>McpToolSpec</code>: Tool specification for MCP tools</li> <li><code>McpToolSetBuilder</code>: Builder for creating exclusive McpToolSpec instances for ReCentAutoma</li> <li><code>McpToolWorker</code>: Worker implementation for executing MCP tools</li> <li><code>McpPromptTemplate</code>: Prompt template implementation for MCP prompts</li> </ul> <p>Examples:</p> <p>Create a connection to an MCP server using stdio transport and list available tools:</p> <pre><code>&gt;&gt;&gt; from bridgic.protocols.mcp import McpServerConnectionStdio\n&gt;&gt;&gt; from bridgic.core.automa import GraphAutoma\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Create and connect to an MCP server\n&gt;&gt;&gt; connection = McpServerConnectionStdio(\n...     name=\"my-mcp-server\",\n...     command=\"npx\",\n...     args=[\"-y\", \"@modelcontextprotocol/server-filesystem\", \"/path/to/dir\"],\n... )\n&gt;&gt;&gt; connection.connect()\n&gt;&gt;&gt;\n&gt;&gt;&gt; # List available tools\n&gt;&gt;&gt; tools = connection.list_tools()\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Use tools in an automa\n&gt;&gt;&gt; automa = GraphAutoma()\n&gt;&gt;&gt; for tool_spec in tools:\n...     automa.add_worker(tool_spec.create_worker())\n</code></pre>"},{"location":"reference/bridgic-protocols-mcp/bridgic/protocols/mcp/#bridgic.protocols.mcp.McpServerConnection","title":"McpServerConnection","text":"<p>               Bases: <code>ABC</code></p> <p>The abstract base class for Connection to an MCP server.</p> <p>This class is responsible for establishing a connection to an MCP server and providing a  session to interact with the server. The connection can be established using different  transport protocols, which depends on the specific implementation.</p> <p>Methods:</p> Name Description <code>connect</code> <p>Establish a connection to an MCP server.</p> <code>get_mcp_client</code> <p>Get a MCP client to interact with the server.</p> Source code in <code>bridgic/protocols/mcp/_mcp_server_connection.py</code> <pre><code>class McpServerConnection(ABC):\n    \"\"\"\n    The abstract base class for Connection to an MCP server.\n\n    This class is responsible for establishing a connection to an MCP server and providing a \n    session to interact with the server. The connection can be established using different \n    transport protocols, which depends on the specific implementation.\n\n    Methods\n    -------\n    connect\n        Establish a connection to an MCP server.\n    get_mcp_client\n        Get a MCP client to interact with the server.\n    \"\"\"\n\n    name: str\n    \"\"\"The name of the connected MCP server.\"\"\"\n\n    request_timeout: int\n    \"\"\"The timeout in seconds for the requests to the MCP server. Default is 30 seconds.\"\"\"\n\n    encoding: str\n    \"\"\"The encoding to use for the connection.\"\"\"\n\n    client_kwargs: Dict[str, Any]\n    \"\"\"The keyword arguments to pass to the MCP client.\"\"\"\n\n    _manager: McpServerConnectionManager\n    _session: ClientSession\n    _exit_stack: AsyncExitStack\n\n    _lifecycle_task: asyncio.Task\n    _connection_ready_event: asyncio.Event\n    _connection_stop_event: asyncio.Event\n    _connection_error: Exception\n\n    is_connected: bool\n    \"\"\"Whether the connection is established.\"\"\"\n\n    def __init__(\n        self,\n        name: str,\n        *,\n        request_timeout: Optional[int] = None,\n        **kwargs: Any,\n    ):\n        self.name = name\n        self.request_timeout = request_timeout or 30\n        self.client_kwargs = kwargs\n\n        self._manager = None\n        self._session = None\n        self._exit_stack = None\n\n        self._lifecycle_task = None\n        self._connection_stop_event = asyncio.Event()\n        self._connection_ready_event = asyncio.Event()\n\n        self.is_connected = False\n        self._connection_error = None\n\n    def _get_manager(self) -&gt; McpServerConnectionManager:\n        if self._manager is None:\n            manager = McpServerConnectionManager.get_instance()\n            manager.register_connection(self)\n            assert manager is self._manager\n        return self._manager\n\n    def connect(self):\n        \"\"\"\n        Establish a connection to the MCP server. Call this method once before using the connection.\n\n        If the connection is not registered in a specific manager explicitly, it will be registered\n        in the default manager (manager_name=\"default-mcp-manager\"). If the connection needs to be\n        registered in a specific manager, the `connect` method should be called after the registration.\n\n        Notes\n        -----\n        The event loop responsible for managing the session is determined at the time when `connect()` is called.\n        Therefore, it is required to register the connection to the desired manager *before* calling `connect()`.\n        Otherwise, the connection will be registered to the default manager. All registrations could not be changed later.\n\n        Examples\n        --------\n        Create a connection to a streamable HTTP MCP server and register it to a manager:\n        &gt;&gt;&gt; connection = McpServerConnectionStreamableHttp(\n        ...     name=\"streamable-http-server-connection\",\n        ...     url=\"http://localhost:8000\",\n        ...     request_timeout=5,\n        ... )\n        &gt;&gt;&gt; manager = McpServerConnectionManager.get_instance(\"my-manager\")\n        &gt;&gt;&gt; manager.register_connection(connection)\n        &gt;&gt;&gt; connection.connect()\n        \"\"\"\n        if self.is_connected:\n            return\n\n        # Create events and lifecycle task in the manager's event loop.\n        async def setup_and_wait():\n            # Create events in the manager's event loop.\n            self._connection_stop_event.clear()\n            self._connection_ready_event.clear()\n            self._connection_error = None\n\n            # Create the lifecycle task.\n            self._lifecycle_task = asyncio.create_task(self._lifecycle_task_coro())\n\n            # Wait for connection to be ready\n            await self._connection_ready_event.wait()\n\n            # Check if there was an error\n            if self._connection_error:\n                raise self._connection_error\n\n        # Setup the connection and make sure it could be closed within the same lifecycle task.\n        self._get_manager().run_sync(\n            coro=setup_and_wait(),\n            timeout=self.request_timeout + 1,\n        )\n\n    def close(self):\n        \"\"\"\n        Close the connection to the MCP server.\n        \"\"\"\n        if not self.is_connected:\n            return\n\n        # Signal the lifecycle task to stop and wait for it to complete.\n        async def stop_and_wait():\n            if self._connection_stop_event is not None:\n                self._connection_stop_event.set()\n\n            if self._lifecycle_task is not None:\n                try:\n                    await self._lifecycle_task\n                except Exception as e:\n                    warnings.warn(f\"Exception occurred while waiting for the lifecycle task to complete: {e}\")\n                finally:\n                    self.is_connected = False\n                    self._lifecycle_task = None\n                    self._manager.unregister_connection(self)\n\n        self._get_manager().run_sync(\n            coro=stop_and_wait(),\n            timeout=self.request_timeout + 1,\n        )\n\n    @abstractmethod\n    def get_mcp_client(self) -&gt; _AsyncGeneratorContextManager[Any, None]:\n        \"\"\"\n        Get an MCP client.\n\n        Returns\n        -------\n        _AsyncGeneratorContextManager[Any, None]\n            An async context manager for the MCP client transport.\n        \"\"\"\n        ...\n\n    ###########################################################################\n    # Protected methods that should be called within the dedicated event loop.\n    ###########################################################################\n\n    async def _lifecycle_task_coro(self):\n        \"\"\"\n        The lifecycle task coroutine that manages the connection lifecycle.\n\n        This coroutine runs in the manager's dedicated event loop and is responsible for:\n        1. Establishing the connection to the MCP server\n        2. Waiting for the stop event\n        3. Closing the connection when the stop event is set\n\n        This ensures that the connection is created and closed within the same Task,\n        which is required by anyio's cancel scope mechanism.\n        \"\"\"\n        try:\n            # Establish connection\n            self._exit_stack = AsyncExitStack()\n            try:\n                transport = await self._exit_stack.enter_async_context(self.get_mcp_client())\n                session = await self._exit_stack.enter_async_context(\n                    ClientSession(\n                        read_stream=transport[0],\n                        write_stream=transport[1],\n                        read_timeout_seconds=timedelta(seconds=self.request_timeout),\n                        # TODO : Callback will be added in the future to support more advanced features.\n                        # message_handler=...,\n                        # logging_callback=...,\n                        # sampling_callback=...,\n                    )\n                )\n                await session.initialize()\n            except Exception as ex:\n                session = None\n                self._connection_error = McpServerConnectionError(\n                    f\"Failed to create session to MCP server: name={self.name}\"\n                ).with_traceback(ex.__traceback__)\n\n            # Hold the connected session for later use.\n            self._session = session\n            self.is_connected = True if session is not None else False\n\n            # Signal that the connection is ready.\n            self._connection_ready_event.set()\n\n            # Wait for the stop signal.\n            await self._connection_stop_event.wait()\n        finally:\n            # At the final moment, the exit stack must be closed.\n            if self._exit_stack is not None:\n                await self._exit_stack.aclose()\n                self._exit_stack = None\n\n            self._session = None\n            self.is_connected = False\n\n    async def _list_prompts_unsafe(self) -&gt; ListPromptsResult:\n        \"\"\"\n        Asynchronously list the prompts from the MCP server.\n\n        Since the session used to communicate with the MCP server is bound to a specific event \n        loop, this method should be called within the designated event loop for the connection.\n        \"\"\"\n        if not self.is_connected or self._session is None:\n            raise McpServerConnectionError(\n                f\"Connection to MCP server is not established: name={self.name}\"\n            )\n        return await self._session.list_prompts()\n\n    async def _get_prompt_unsafe(\n        self,\n        prompt_name: str,\n        arguments: Optional[Dict[str, Any]] = None,\n    ) -&gt; GetPromptResult:\n        \"\"\"\n        Asynchronously get a prompt from the MCP server.\n\n        Since the session used to communicate with the MCP server is bound to a specific event \n        loop, this method should be called within the designated event loop for the connection.\n        \"\"\"\n        if not self.is_connected or self._session is None:\n            raise McpServerConnectionError(\n                f\"Connection to MCP server is not established: name={self.name}\"\n            )\n        return await self._session.get_prompt(name=prompt_name, arguments=arguments or {})\n\n    async def _list_tools_unsafe(self) -&gt; ListToolsResult:\n        \"\"\"\n        Asynchronously list the tools from the MCP server.\n\n        Since the session used to communicate with the MCP server is bound to a specific event \n        loop, this method should be called within the designated event loop for the connection.\n        \"\"\"\n        if not self.is_connected or self._session is None:\n            raise McpServerConnectionError(\n                f\"Connection to MCP server is not established: name={self.name}\"\n            )\n        try:\n            result = await self._session.list_tools()\n        except McpError as e:\n            is_timeout = False\n            for content in [\"timed out\", \"timeout\"]:\n                if content in str.lower(e.error.message):\n                    result = ListToolsResult(tools=[])\n                    is_timeout = True\n                    warnings.warn(f\"Timeout occurred while listing tools from MCP server: name={self.name}\")\n                    break\n            if not is_timeout:\n                raise e\n        return result\n\n    async def _call_tool_unsafe(\n        self,\n        tool_name: str,\n        arguments: Optional[Dict[str, Any]] = None,\n    ) -&gt; CallToolResult:\n        \"\"\"\n        Asynchronously call a tool on the MCP server.\n\n        Since the session used to communicate with the MCP server is bound to a specific event \n        loop, this method should be called within the designated event loop for the connection.\n        \"\"\"\n        if not self.is_connected or self._session is None:\n            raise McpServerConnectionError(\n                f\"Connection to MCP server is not established: name={self.name}\"\n            )\n        return await self._session.call_tool(name=tool_name, arguments=arguments or {})\n\n    ###########################################################################\n    # Public methods that are safely wrapped and could be called anywhere.\n    ###########################################################################\n\n    def list_prompts(self) -&gt; List[\"McpPromptTemplate\"]:\n        \"\"\"\n        List the prompts from the MCP server.\n\n        Returns\n        -------\n        List[McpPromptTemplate]\n            The list of prompt template instances from the server.\n        \"\"\"\n        from bridgic.protocols.mcp._mcp_template import McpPromptTemplate\n\n        result = self._get_manager().run_sync(\n            coro=self._list_prompts_unsafe(),\n            timeout=self.request_timeout + 1,\n        )\n\n        return [\n            McpPromptTemplate(\n                prompt_name=prompt.name,\n                prompt_info=prompt,\n                server_connection=self\n            )\n            for prompt in result.prompts\n        ]\n\n    async def alist_prompts(self) -&gt; List[\"McpPromptTemplate\"]:\n        \"\"\"\n        Asynchronously list the prompts from the MCP server.\n\n        Returns\n        -------\n        List[McpPromptTemplate]\n            The list of prompt template instances from the server.\n        \"\"\"\n        from bridgic.protocols.mcp._mcp_template import McpPromptTemplate\n\n        result = await self._get_manager().run_async(\n            coro=self._list_prompts_unsafe(),\n            timeout=self.request_timeout + 1,\n        )\n\n        return [\n            McpPromptTemplate(\n                prompt_name=prompt.name,\n                prompt_info=prompt,\n                server_connection=self\n            )\n            for prompt in result.prompts\n        ]\n\n    def get_prompt(\n        self,\n        prompt_name: str,\n        arguments: Optional[Dict[str, Any]] = None,\n    ) -&gt; GetPromptResult:\n        \"\"\"\n        Synchronously get a prompt from the MCP server.\n\n        Parameters\n        ----------\n        prompt_name : str\n            The name of the prompt to retrieve.\n        arguments : Optional[Dict[str, Any]]\n            Arguments to pass to the prompt.\n\n        Returns\n        -------\n        GetPromptResult\n            The prompt result from the server.\n\n        Raises\n        ------\n        RuntimeError\n            If the connection is not established.\n        \"\"\"\n        return self._get_manager().run_sync(\n            coro=self._get_prompt_unsafe(prompt_name=prompt_name, arguments=arguments),\n            timeout=self.request_timeout + 1,\n        )\n\n    async def aget_prompt(\n        self,\n        prompt_name: str,\n        arguments: Optional[Dict[str, Any]] = None,\n    ) -&gt; GetPromptResult:\n        \"\"\"\n        Asynchronously get a prompt from the MCP server.\n\n        Parameters\n        ----------\n        prompt_name : str\n            The name of the prompt to retrieve.\n        arguments : Optional[Dict[str, Any]]\n            Arguments to pass to the prompt.\n\n        Returns\n        -------\n        GetPromptResult\n            The prompt result from the server.\n\n        Raises\n        ------\n        RuntimeError\n            If the connection is not established.\n        \"\"\"\n        return await self._get_manager().run_async(\n            coro=self._get_prompt_unsafe(prompt_name=prompt_name, arguments=arguments),\n            timeout=self.request_timeout + 1,\n        )\n\n    def list_tools(self) -&gt; List[\"McpToolSpec\"]:\n        \"\"\"\n        List the tools from the MCP server.\n\n        This method synchronously retrieves the list of tools available from the connected\n        MCP server and wraps each tool in an `McpToolSpec` instance for use within the\n        bridgic framework.\n\n        Returns\n        -------\n        List[McpToolSpec]\n            The list of tool specification instances from the server.\n\n        Raises\n        ------\n        RuntimeError\n            If the connection is not established and cannot be established.\n        \"\"\"\n        from bridgic.protocols.mcp._mcp_tool_spec import McpToolSpec\n\n        result = self._get_manager().run_sync(\n            coro=self._list_tools_unsafe(),\n            timeout=self.request_timeout + 1,\n        )\n\n        return [\n            McpToolSpec(\n                tool_name=tool.name,\n                tool_info=tool,\n                server_connection=self\n            )\n            for tool in result.tools\n        ]\n\n    async def alist_tools(self) -&gt; List[\"McpToolSpec\"]:\n        \"\"\"\n        Asynchronously list the tools from the MCP server.\n\n        This method asynchronously retrieves the list of tools available from the connected\n        MCP server and wraps each tool in an `McpToolSpec` instance for use within the\n        bridgic framework.\n\n        Returns\n        -------\n        List[McpToolSpec]\n            The list of tool specification instances from the server.\n\n        Raises\n        ------\n        RuntimeError\n            If the connection is not established and cannot be established.\n        \"\"\"\n        from bridgic.protocols.mcp._mcp_tool_spec import McpToolSpec\n\n        result = await self._get_manager().run_async(\n            coro=self._list_tools_unsafe(),\n            timeout=self.request_timeout + 1,\n        )\n\n        return [\n            McpToolSpec(\n                tool_name=tool.name,\n                tool_info=tool,\n                server_connection=self\n            )\n            for tool in result.tools\n        ]\n\n    def call_tool(\n        self,\n        tool_name: str,\n        arguments: Optional[Dict[str, Any]] = None,\n    ) -&gt; CallToolResult:\n        \"\"\"\n        Synchronously call a tool on the MCP server.\n\n        This method synchronously invokes a tool on the connected MCP server with the\n        specified arguments and returns the result.\n\n        Parameters\n        ----------\n        tool_name : str\n            The name of the tool to call.\n        arguments : Optional[Dict[str, Any]]\n            The arguments to pass to the tool. If None, an empty dictionary will be used.\n\n        Returns\n        -------\n        CallToolResult\n            The result of the tool call from the server, containing content and optionally\n            structured content.\n\n        Raises\n        ------\n        RuntimeError\n            If the connection is not established and cannot be established.\n        \"\"\"\n        return self._get_manager().run_sync(\n            coro=self._call_tool_unsafe(tool_name=tool_name, arguments=arguments),\n            timeout=self.request_timeout + 1,\n        )\n\n    async def acall_tool(\n        self,\n        tool_name: str,\n        arguments: Optional[Dict[str, Any]] = None,\n    ) -&gt; CallToolResult:\n        \"\"\"\n        Asynchronously call a tool on the MCP server.\n\n        This method asynchronously invokes a tool on the connected MCP server with the\n        specified arguments and returns the result.\n\n        Parameters\n        ----------\n        tool_name : str\n            The name of the tool to call.\n        arguments : Optional[Dict[str, Any]]\n            The arguments to pass to the tool. If None, an empty dictionary will be used.\n\n        Returns\n        -------\n        CallToolResult\n            The result of the tool call from the server, containing content and optionally\n            structured content.\n\n        Raises\n        ------\n        RuntimeError\n            If the connection is not established and cannot be established.\n        \"\"\"\n        return await self._get_manager().run_async(\n            coro=self._call_tool_unsafe(tool_name=tool_name, arguments=arguments),\n            timeout=self.request_timeout + 1,\n        )\n</code></pre>"},{"location":"reference/bridgic-protocols-mcp/bridgic/protocols/mcp/#bridgic.protocols.mcp.McpServerConnection.name","title":"name  <code>instance-attribute</code>","text":"<pre><code>name: str = name\n</code></pre> <p>The name of the connected MCP server.</p>"},{"location":"reference/bridgic-protocols-mcp/bridgic/protocols/mcp/#bridgic.protocols.mcp.McpServerConnection.request_timeout","title":"request_timeout  <code>instance-attribute</code>","text":"<pre><code>request_timeout: int = request_timeout or 30\n</code></pre> <p>The timeout in seconds for the requests to the MCP server. Default is 30 seconds.</p>"},{"location":"reference/bridgic-protocols-mcp/bridgic/protocols/mcp/#bridgic.protocols.mcp.McpServerConnection.encoding","title":"encoding  <code>instance-attribute</code>","text":"<pre><code>encoding: str\n</code></pre> <p>The encoding to use for the connection.</p>"},{"location":"reference/bridgic-protocols-mcp/bridgic/protocols/mcp/#bridgic.protocols.mcp.McpServerConnection.client_kwargs","title":"client_kwargs  <code>instance-attribute</code>","text":"<pre><code>client_kwargs: Dict[str, Any] = kwargs\n</code></pre> <p>The keyword arguments to pass to the MCP client.</p>"},{"location":"reference/bridgic-protocols-mcp/bridgic/protocols/mcp/#bridgic.protocols.mcp.McpServerConnection.is_connected","title":"is_connected  <code>instance-attribute</code>","text":"<pre><code>is_connected: bool = False\n</code></pre> <p>Whether the connection is established.</p>"},{"location":"reference/bridgic-protocols-mcp/bridgic/protocols/mcp/#bridgic.protocols.mcp.McpServerConnection.connect","title":"connect","text":"<pre><code>connect()\n</code></pre> <p>Establish a connection to the MCP server. Call this method once before using the connection.</p> <p>If the connection is not registered in a specific manager explicitly, it will be registered in the default manager (manager_name=\"default-mcp-manager\"). If the connection needs to be registered in a specific manager, the <code>connect</code> method should be called after the registration.</p> Notes <p>The event loop responsible for managing the session is determined at the time when <code>connect()</code> is called. Therefore, it is required to register the connection to the desired manager before calling <code>connect()</code>. Otherwise, the connection will be registered to the default manager. All registrations could not be changed later.</p> <p>Examples:</p> <p>Create a connection to a streamable HTTP MCP server and register it to a manager:</p> <pre><code>&gt;&gt;&gt; connection = McpServerConnectionStreamableHttp(\n...     name=\"streamable-http-server-connection\",\n...     url=\"http://localhost:8000\",\n...     request_timeout=5,\n... )\n&gt;&gt;&gt; manager = McpServerConnectionManager.get_instance(\"my-manager\")\n&gt;&gt;&gt; manager.register_connection(connection)\n&gt;&gt;&gt; connection.connect()\n</code></pre> Source code in <code>bridgic/protocols/mcp/_mcp_server_connection.py</code> <pre><code>def connect(self):\n    \"\"\"\n    Establish a connection to the MCP server. Call this method once before using the connection.\n\n    If the connection is not registered in a specific manager explicitly, it will be registered\n    in the default manager (manager_name=\"default-mcp-manager\"). If the connection needs to be\n    registered in a specific manager, the `connect` method should be called after the registration.\n\n    Notes\n    -----\n    The event loop responsible for managing the session is determined at the time when `connect()` is called.\n    Therefore, it is required to register the connection to the desired manager *before* calling `connect()`.\n    Otherwise, the connection will be registered to the default manager. All registrations could not be changed later.\n\n    Examples\n    --------\n    Create a connection to a streamable HTTP MCP server and register it to a manager:\n    &gt;&gt;&gt; connection = McpServerConnectionStreamableHttp(\n    ...     name=\"streamable-http-server-connection\",\n    ...     url=\"http://localhost:8000\",\n    ...     request_timeout=5,\n    ... )\n    &gt;&gt;&gt; manager = McpServerConnectionManager.get_instance(\"my-manager\")\n    &gt;&gt;&gt; manager.register_connection(connection)\n    &gt;&gt;&gt; connection.connect()\n    \"\"\"\n    if self.is_connected:\n        return\n\n    # Create events and lifecycle task in the manager's event loop.\n    async def setup_and_wait():\n        # Create events in the manager's event loop.\n        self._connection_stop_event.clear()\n        self._connection_ready_event.clear()\n        self._connection_error = None\n\n        # Create the lifecycle task.\n        self._lifecycle_task = asyncio.create_task(self._lifecycle_task_coro())\n\n        # Wait for connection to be ready\n        await self._connection_ready_event.wait()\n\n        # Check if there was an error\n        if self._connection_error:\n            raise self._connection_error\n\n    # Setup the connection and make sure it could be closed within the same lifecycle task.\n    self._get_manager().run_sync(\n        coro=setup_and_wait(),\n        timeout=self.request_timeout + 1,\n    )\n</code></pre>"},{"location":"reference/bridgic-protocols-mcp/bridgic/protocols/mcp/#bridgic.protocols.mcp.McpServerConnection.close","title":"close","text":"<pre><code>close()\n</code></pre> <p>Close the connection to the MCP server.</p> Source code in <code>bridgic/protocols/mcp/_mcp_server_connection.py</code> <pre><code>def close(self):\n    \"\"\"\n    Close the connection to the MCP server.\n    \"\"\"\n    if not self.is_connected:\n        return\n\n    # Signal the lifecycle task to stop and wait for it to complete.\n    async def stop_and_wait():\n        if self._connection_stop_event is not None:\n            self._connection_stop_event.set()\n\n        if self._lifecycle_task is not None:\n            try:\n                await self._lifecycle_task\n            except Exception as e:\n                warnings.warn(f\"Exception occurred while waiting for the lifecycle task to complete: {e}\")\n            finally:\n                self.is_connected = False\n                self._lifecycle_task = None\n                self._manager.unregister_connection(self)\n\n    self._get_manager().run_sync(\n        coro=stop_and_wait(),\n        timeout=self.request_timeout + 1,\n    )\n</code></pre>"},{"location":"reference/bridgic-protocols-mcp/bridgic/protocols/mcp/#bridgic.protocols.mcp.McpServerConnection.get_mcp_client","title":"get_mcp_client","text":"<code>abstractmethod</code> <pre><code>get_mcp_client() -&gt; (\n    _AsyncGeneratorContextManager[Any, None]\n)\n</code></pre> <p>Get an MCP client.</p> <p>Returns:</p> Type Description <code>_AsyncGeneratorContextManager[Any, None]</code> <p>An async context manager for the MCP client transport.</p> Source code in <code>bridgic/protocols/mcp/_mcp_server_connection.py</code> <pre><code>@abstractmethod\ndef get_mcp_client(self) -&gt; _AsyncGeneratorContextManager[Any, None]:\n    \"\"\"\n    Get an MCP client.\n\n    Returns\n    -------\n    _AsyncGeneratorContextManager[Any, None]\n        An async context manager for the MCP client transport.\n    \"\"\"\n    ...\n</code></pre>"},{"location":"reference/bridgic-protocols-mcp/bridgic/protocols/mcp/#bridgic.protocols.mcp.McpServerConnection.list_prompts","title":"list_prompts","text":"<pre><code>list_prompts() -&gt; List[McpPromptTemplate]\n</code></pre> <p>List the prompts from the MCP server.</p> <p>Returns:</p> Type Description <code>List[McpPromptTemplate]</code> <p>The list of prompt template instances from the server.</p> Source code in <code>bridgic/protocols/mcp/_mcp_server_connection.py</code> <pre><code>def list_prompts(self) -&gt; List[\"McpPromptTemplate\"]:\n    \"\"\"\n    List the prompts from the MCP server.\n\n    Returns\n    -------\n    List[McpPromptTemplate]\n        The list of prompt template instances from the server.\n    \"\"\"\n    from bridgic.protocols.mcp._mcp_template import McpPromptTemplate\n\n    result = self._get_manager().run_sync(\n        coro=self._list_prompts_unsafe(),\n        timeout=self.request_timeout + 1,\n    )\n\n    return [\n        McpPromptTemplate(\n            prompt_name=prompt.name,\n            prompt_info=prompt,\n            server_connection=self\n        )\n        for prompt in result.prompts\n    ]\n</code></pre>"},{"location":"reference/bridgic-protocols-mcp/bridgic/protocols/mcp/#bridgic.protocols.mcp.McpServerConnection.alist_prompts","title":"alist_prompts","text":"<code>async</code> <pre><code>alist_prompts() -&gt; List[McpPromptTemplate]\n</code></pre> <p>Asynchronously list the prompts from the MCP server.</p> <p>Returns:</p> Type Description <code>List[McpPromptTemplate]</code> <p>The list of prompt template instances from the server.</p> Source code in <code>bridgic/protocols/mcp/_mcp_server_connection.py</code> <pre><code>async def alist_prompts(self) -&gt; List[\"McpPromptTemplate\"]:\n    \"\"\"\n    Asynchronously list the prompts from the MCP server.\n\n    Returns\n    -------\n    List[McpPromptTemplate]\n        The list of prompt template instances from the server.\n    \"\"\"\n    from bridgic.protocols.mcp._mcp_template import McpPromptTemplate\n\n    result = await self._get_manager().run_async(\n        coro=self._list_prompts_unsafe(),\n        timeout=self.request_timeout + 1,\n    )\n\n    return [\n        McpPromptTemplate(\n            prompt_name=prompt.name,\n            prompt_info=prompt,\n            server_connection=self\n        )\n        for prompt in result.prompts\n    ]\n</code></pre>"},{"location":"reference/bridgic-protocols-mcp/bridgic/protocols/mcp/#bridgic.protocols.mcp.McpServerConnection.get_prompt","title":"get_prompt","text":"<pre><code>get_prompt(\n    prompt_name: str,\n    arguments: Optional[Dict[str, Any]] = None,\n) -&gt; GetPromptResult\n</code></pre> <p>Synchronously get a prompt from the MCP server.</p> <p>Parameters:</p> Name Type Description Default <code>prompt_name</code> <code>str</code> <p>The name of the prompt to retrieve.</p> required <code>arguments</code> <code>Optional[Dict[str, Any]]</code> <p>Arguments to pass to the prompt.</p> <code>None</code> <p>Returns:</p> Type Description <code>GetPromptResult</code> <p>The prompt result from the server.</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If the connection is not established.</p> Source code in <code>bridgic/protocols/mcp/_mcp_server_connection.py</code> <pre><code>def get_prompt(\n    self,\n    prompt_name: str,\n    arguments: Optional[Dict[str, Any]] = None,\n) -&gt; GetPromptResult:\n    \"\"\"\n    Synchronously get a prompt from the MCP server.\n\n    Parameters\n    ----------\n    prompt_name : str\n        The name of the prompt to retrieve.\n    arguments : Optional[Dict[str, Any]]\n        Arguments to pass to the prompt.\n\n    Returns\n    -------\n    GetPromptResult\n        The prompt result from the server.\n\n    Raises\n    ------\n    RuntimeError\n        If the connection is not established.\n    \"\"\"\n    return self._get_manager().run_sync(\n        coro=self._get_prompt_unsafe(prompt_name=prompt_name, arguments=arguments),\n        timeout=self.request_timeout + 1,\n    )\n</code></pre>"},{"location":"reference/bridgic-protocols-mcp/bridgic/protocols/mcp/#bridgic.protocols.mcp.McpServerConnection.aget_prompt","title":"aget_prompt","text":"<code>async</code> <pre><code>aget_prompt(\n    prompt_name: str,\n    arguments: Optional[Dict[str, Any]] = None,\n) -&gt; GetPromptResult\n</code></pre> <p>Asynchronously get a prompt from the MCP server.</p> <p>Parameters:</p> Name Type Description Default <code>prompt_name</code> <code>str</code> <p>The name of the prompt to retrieve.</p> required <code>arguments</code> <code>Optional[Dict[str, Any]]</code> <p>Arguments to pass to the prompt.</p> <code>None</code> <p>Returns:</p> Type Description <code>GetPromptResult</code> <p>The prompt result from the server.</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If the connection is not established.</p> Source code in <code>bridgic/protocols/mcp/_mcp_server_connection.py</code> <pre><code>async def aget_prompt(\n    self,\n    prompt_name: str,\n    arguments: Optional[Dict[str, Any]] = None,\n) -&gt; GetPromptResult:\n    \"\"\"\n    Asynchronously get a prompt from the MCP server.\n\n    Parameters\n    ----------\n    prompt_name : str\n        The name of the prompt to retrieve.\n    arguments : Optional[Dict[str, Any]]\n        Arguments to pass to the prompt.\n\n    Returns\n    -------\n    GetPromptResult\n        The prompt result from the server.\n\n    Raises\n    ------\n    RuntimeError\n        If the connection is not established.\n    \"\"\"\n    return await self._get_manager().run_async(\n        coro=self._get_prompt_unsafe(prompt_name=prompt_name, arguments=arguments),\n        timeout=self.request_timeout + 1,\n    )\n</code></pre>"},{"location":"reference/bridgic-protocols-mcp/bridgic/protocols/mcp/#bridgic.protocols.mcp.McpServerConnection.list_tools","title":"list_tools","text":"<pre><code>list_tools() -&gt; List[McpToolSpec]\n</code></pre> <p>List the tools from the MCP server.</p> <p>This method synchronously retrieves the list of tools available from the connected MCP server and wraps each tool in an <code>McpToolSpec</code> instance for use within the bridgic framework.</p> <p>Returns:</p> Type Description <code>List[McpToolSpec]</code> <p>The list of tool specification instances from the server.</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If the connection is not established and cannot be established.</p> Source code in <code>bridgic/protocols/mcp/_mcp_server_connection.py</code> <pre><code>def list_tools(self) -&gt; List[\"McpToolSpec\"]:\n    \"\"\"\n    List the tools from the MCP server.\n\n    This method synchronously retrieves the list of tools available from the connected\n    MCP server and wraps each tool in an `McpToolSpec` instance for use within the\n    bridgic framework.\n\n    Returns\n    -------\n    List[McpToolSpec]\n        The list of tool specification instances from the server.\n\n    Raises\n    ------\n    RuntimeError\n        If the connection is not established and cannot be established.\n    \"\"\"\n    from bridgic.protocols.mcp._mcp_tool_spec import McpToolSpec\n\n    result = self._get_manager().run_sync(\n        coro=self._list_tools_unsafe(),\n        timeout=self.request_timeout + 1,\n    )\n\n    return [\n        McpToolSpec(\n            tool_name=tool.name,\n            tool_info=tool,\n            server_connection=self\n        )\n        for tool in result.tools\n    ]\n</code></pre>"},{"location":"reference/bridgic-protocols-mcp/bridgic/protocols/mcp/#bridgic.protocols.mcp.McpServerConnection.alist_tools","title":"alist_tools","text":"<code>async</code> <pre><code>alist_tools() -&gt; List[McpToolSpec]\n</code></pre> <p>Asynchronously list the tools from the MCP server.</p> <p>This method asynchronously retrieves the list of tools available from the connected MCP server and wraps each tool in an <code>McpToolSpec</code> instance for use within the bridgic framework.</p> <p>Returns:</p> Type Description <code>List[McpToolSpec]</code> <p>The list of tool specification instances from the server.</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If the connection is not established and cannot be established.</p> Source code in <code>bridgic/protocols/mcp/_mcp_server_connection.py</code> <pre><code>async def alist_tools(self) -&gt; List[\"McpToolSpec\"]:\n    \"\"\"\n    Asynchronously list the tools from the MCP server.\n\n    This method asynchronously retrieves the list of tools available from the connected\n    MCP server and wraps each tool in an `McpToolSpec` instance for use within the\n    bridgic framework.\n\n    Returns\n    -------\n    List[McpToolSpec]\n        The list of tool specification instances from the server.\n\n    Raises\n    ------\n    RuntimeError\n        If the connection is not established and cannot be established.\n    \"\"\"\n    from bridgic.protocols.mcp._mcp_tool_spec import McpToolSpec\n\n    result = await self._get_manager().run_async(\n        coro=self._list_tools_unsafe(),\n        timeout=self.request_timeout + 1,\n    )\n\n    return [\n        McpToolSpec(\n            tool_name=tool.name,\n            tool_info=tool,\n            server_connection=self\n        )\n        for tool in result.tools\n    ]\n</code></pre>"},{"location":"reference/bridgic-protocols-mcp/bridgic/protocols/mcp/#bridgic.protocols.mcp.McpServerConnection.call_tool","title":"call_tool","text":"<pre><code>call_tool(\n    tool_name: str,\n    arguments: Optional[Dict[str, Any]] = None,\n) -&gt; CallToolResult\n</code></pre> <p>Synchronously call a tool on the MCP server.</p> <p>This method synchronously invokes a tool on the connected MCP server with the specified arguments and returns the result.</p> <p>Parameters:</p> Name Type Description Default <code>tool_name</code> <code>str</code> <p>The name of the tool to call.</p> required <code>arguments</code> <code>Optional[Dict[str, Any]]</code> <p>The arguments to pass to the tool. If None, an empty dictionary will be used.</p> <code>None</code> <p>Returns:</p> Type Description <code>CallToolResult</code> <p>The result of the tool call from the server, containing content and optionally structured content.</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If the connection is not established and cannot be established.</p> Source code in <code>bridgic/protocols/mcp/_mcp_server_connection.py</code> <pre><code>def call_tool(\n    self,\n    tool_name: str,\n    arguments: Optional[Dict[str, Any]] = None,\n) -&gt; CallToolResult:\n    \"\"\"\n    Synchronously call a tool on the MCP server.\n\n    This method synchronously invokes a tool on the connected MCP server with the\n    specified arguments and returns the result.\n\n    Parameters\n    ----------\n    tool_name : str\n        The name of the tool to call.\n    arguments : Optional[Dict[str, Any]]\n        The arguments to pass to the tool. If None, an empty dictionary will be used.\n\n    Returns\n    -------\n    CallToolResult\n        The result of the tool call from the server, containing content and optionally\n        structured content.\n\n    Raises\n    ------\n    RuntimeError\n        If the connection is not established and cannot be established.\n    \"\"\"\n    return self._get_manager().run_sync(\n        coro=self._call_tool_unsafe(tool_name=tool_name, arguments=arguments),\n        timeout=self.request_timeout + 1,\n    )\n</code></pre>"},{"location":"reference/bridgic-protocols-mcp/bridgic/protocols/mcp/#bridgic.protocols.mcp.McpServerConnection.acall_tool","title":"acall_tool","text":"<code>async</code> <pre><code>acall_tool(\n    tool_name: str,\n    arguments: Optional[Dict[str, Any]] = None,\n) -&gt; CallToolResult\n</code></pre> <p>Asynchronously call a tool on the MCP server.</p> <p>This method asynchronously invokes a tool on the connected MCP server with the specified arguments and returns the result.</p> <p>Parameters:</p> Name Type Description Default <code>tool_name</code> <code>str</code> <p>The name of the tool to call.</p> required <code>arguments</code> <code>Optional[Dict[str, Any]]</code> <p>The arguments to pass to the tool. If None, an empty dictionary will be used.</p> <code>None</code> <p>Returns:</p> Type Description <code>CallToolResult</code> <p>The result of the tool call from the server, containing content and optionally structured content.</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If the connection is not established and cannot be established.</p> Source code in <code>bridgic/protocols/mcp/_mcp_server_connection.py</code> <pre><code>async def acall_tool(\n    self,\n    tool_name: str,\n    arguments: Optional[Dict[str, Any]] = None,\n) -&gt; CallToolResult:\n    \"\"\"\n    Asynchronously call a tool on the MCP server.\n\n    This method asynchronously invokes a tool on the connected MCP server with the\n    specified arguments and returns the result.\n\n    Parameters\n    ----------\n    tool_name : str\n        The name of the tool to call.\n    arguments : Optional[Dict[str, Any]]\n        The arguments to pass to the tool. If None, an empty dictionary will be used.\n\n    Returns\n    -------\n    CallToolResult\n        The result of the tool call from the server, containing content and optionally\n        structured content.\n\n    Raises\n    ------\n    RuntimeError\n        If the connection is not established and cannot be established.\n    \"\"\"\n    return await self._get_manager().run_async(\n        coro=self._call_tool_unsafe(tool_name=tool_name, arguments=arguments),\n        timeout=self.request_timeout + 1,\n    )\n</code></pre>"},{"location":"reference/bridgic-protocols-mcp/bridgic/protocols/mcp/#bridgic.protocols.mcp.McpServerConnectionStdio","title":"McpServerConnectionStdio","text":"<p>               Bases: <code>McpServerConnection</code></p> <p>The connection to an MCP server using stdio.</p> Source code in <code>bridgic/protocols/mcp/_mcp_server_connection.py</code> <pre><code>class McpServerConnectionStdio(McpServerConnection):\n    \"\"\"\n    The connection to an MCP server using stdio.\n    \"\"\"\n\n    command: str\n    \"\"\"The command to use for the connection.\"\"\"\n\n    encoding: str\n    \"\"\"The encoding to use for the connection.\"\"\"\n\n    args: List[str]\n    \"\"\"The arguments to use for the connection.\"\"\"\n\n    env: Dict[str, str]\n    \"\"\"The environment variables to use for the connection.\"\"\"\n\n    def __init__(\n        self,\n        name: str,\n        command: str,\n        *,\n        args: Optional[List[str]] = None,\n        env: Optional[Dict[str, str]] = None,\n        encoding: Optional[str] = None,\n        request_timeout: Optional[int] = None,\n        **kwargs: Any,\n    ):\n        super().__init__(\n            name,\n            request_timeout=request_timeout,\n            **kwargs,\n        )\n        self.command = command\n        self.encoding = encoding or \"utf-8\"\n        self.args = args\n        self.env = env\n\n    def get_mcp_client(self) -&gt; _AsyncGeneratorContextManager[Any, None]:\n        \"\"\"\n        Get an MCP client transport for stdio.\n\n        Returns\n        -------\n        _AsyncGeneratorContextManager[Any, None]\n            An async context manager for the stdio client transport.\n        \"\"\"\n        start_args = {\n            \"command\": self.command,\n            \"args\": self.args,\n            \"env\": self.env,\n        }\n        if self.encoding:\n            start_args[\"encoding\"] = self.encoding\n        if self.client_kwargs:\n            start_args.update(self.client_kwargs)\n\n        return stdio_client(server=StdioServerParameters(**start_args))\n</code></pre>"},{"location":"reference/bridgic-protocols-mcp/bridgic/protocols/mcp/#bridgic.protocols.mcp.McpServerConnectionStdio.command","title":"command  <code>instance-attribute</code>","text":"<pre><code>command: str = command\n</code></pre> <p>The command to use for the connection.</p>"},{"location":"reference/bridgic-protocols-mcp/bridgic/protocols/mcp/#bridgic.protocols.mcp.McpServerConnectionStdio.encoding","title":"encoding  <code>instance-attribute</code>","text":"<pre><code>encoding: str = encoding or 'utf-8'\n</code></pre> <p>The encoding to use for the connection.</p>"},{"location":"reference/bridgic-protocols-mcp/bridgic/protocols/mcp/#bridgic.protocols.mcp.McpServerConnectionStdio.args","title":"args  <code>instance-attribute</code>","text":"<pre><code>args: List[str] = args\n</code></pre> <p>The arguments to use for the connection.</p>"},{"location":"reference/bridgic-protocols-mcp/bridgic/protocols/mcp/#bridgic.protocols.mcp.McpServerConnectionStdio.env","title":"env  <code>instance-attribute</code>","text":"<pre><code>env: Dict[str, str] = env\n</code></pre> <p>The environment variables to use for the connection.</p>"},{"location":"reference/bridgic-protocols-mcp/bridgic/protocols/mcp/#bridgic.protocols.mcp.McpServerConnectionStdio.get_mcp_client","title":"get_mcp_client","text":"<pre><code>get_mcp_client() -&gt; (\n    _AsyncGeneratorContextManager[Any, None]\n)\n</code></pre> <p>Get an MCP client transport for stdio.</p> <p>Returns:</p> Type Description <code>_AsyncGeneratorContextManager[Any, None]</code> <p>An async context manager for the stdio client transport.</p> Source code in <code>bridgic/protocols/mcp/_mcp_server_connection.py</code> <pre><code>def get_mcp_client(self) -&gt; _AsyncGeneratorContextManager[Any, None]:\n    \"\"\"\n    Get an MCP client transport for stdio.\n\n    Returns\n    -------\n    _AsyncGeneratorContextManager[Any, None]\n        An async context manager for the stdio client transport.\n    \"\"\"\n    start_args = {\n        \"command\": self.command,\n        \"args\": self.args,\n        \"env\": self.env,\n    }\n    if self.encoding:\n        start_args[\"encoding\"] = self.encoding\n    if self.client_kwargs:\n        start_args.update(self.client_kwargs)\n\n    return stdio_client(server=StdioServerParameters(**start_args))\n</code></pre>"},{"location":"reference/bridgic-protocols-mcp/bridgic/protocols/mcp/#bridgic.protocols.mcp.McpServerConnectionStreamableHttp","title":"McpServerConnectionStreamableHttp","text":"<p>               Bases: <code>McpServerConnection</code></p> <p>The connection to an MCP server using streamable http.</p> Source code in <code>bridgic/protocols/mcp/_mcp_server_connection.py</code> <pre><code>class McpServerConnectionStreamableHttp(McpServerConnection):\n    \"\"\"\n    The connection to an MCP server using streamable http.\n    \"\"\"\n\n    url: str\n    \"\"\"The URL of the MCP server.\"\"\"\n\n    http_client: httpx.AsyncClient\n    \"\"\"The HTTP client to use for the connection.\"\"\"\n\n    terminate_on_close: bool\n    \"\"\"Whether to terminate the session when the connection is closed.\"\"\"\n\n    def __init__(\n        self,\n        name: str,\n        url: str,\n        *,\n        http_client: Optional[httpx.AsyncClient] = None,\n        terminate_on_close: Optional[bool] = None,\n        request_timeout: Optional[int] = None,\n    ):\n        \"\"\"\n        Initialize a streamable HTTP connection to an MCP server.\n\n        Parameters\n        ----------\n        name : str\n            The name of the connection.\n        url : str\n            The URL of the MCP server.\n        http_client : Optional[httpx.AsyncClient]\n            Optional pre-configured httpx.AsyncClient. If None, a default\n            client with recommended MCP timeouts will be created. To configure headers,\n            authentication, or other HTTP settings, create an httpx.AsyncClient and pass it here.\n        terminate_on_close : Optional[bool]\n            If True, send a DELETE request to terminate the session when the connection\n            is closed. Defaults to True.\n        request_timeout : Optional[int]\n            The timeout in seconds for MCP requests. Default is 30 seconds.\n        \"\"\"\n        super().__init__(\n            name,\n            request_timeout=request_timeout,\n        )\n        self.url = url\n        self.http_client = http_client\n        self.terminate_on_close = terminate_on_close or True\n\n    def get_mcp_client(self) -&gt; _AsyncGeneratorContextManager[Any, None]:\n        \"\"\"\n        Get an MCP client transport for streamable http.\n\n        Returns\n        -------\n        _AsyncGeneratorContextManager[Any, None]\n            An async context manager for the streamable HTTP client transport.\n        \"\"\"\n        return streamable_http_client(\n            url=self.url,\n            http_client=self.http_client,\n            terminate_on_close=self.terminate_on_close,\n        )\n</code></pre>"},{"location":"reference/bridgic-protocols-mcp/bridgic/protocols/mcp/#bridgic.protocols.mcp.McpServerConnectionStreamableHttp.url","title":"url  <code>instance-attribute</code>","text":"<pre><code>url: str = url\n</code></pre> <p>The URL of the MCP server.</p>"},{"location":"reference/bridgic-protocols-mcp/bridgic/protocols/mcp/#bridgic.protocols.mcp.McpServerConnectionStreamableHttp.http_client","title":"http_client  <code>instance-attribute</code>","text":"<pre><code>http_client: AsyncClient = http_client\n</code></pre> <p>The HTTP client to use for the connection.</p>"},{"location":"reference/bridgic-protocols-mcp/bridgic/protocols/mcp/#bridgic.protocols.mcp.McpServerConnectionStreamableHttp.terminate_on_close","title":"terminate_on_close  <code>instance-attribute</code>","text":"<pre><code>terminate_on_close: bool = terminate_on_close or True\n</code></pre> <p>Whether to terminate the session when the connection is closed.</p>"},{"location":"reference/bridgic-protocols-mcp/bridgic/protocols/mcp/#bridgic.protocols.mcp.McpServerConnectionStreamableHttp.get_mcp_client","title":"get_mcp_client","text":"<pre><code>get_mcp_client() -&gt; (\n    _AsyncGeneratorContextManager[Any, None]\n)\n</code></pre> <p>Get an MCP client transport for streamable http.</p> <p>Returns:</p> Type Description <code>_AsyncGeneratorContextManager[Any, None]</code> <p>An async context manager for the streamable HTTP client transport.</p> Source code in <code>bridgic/protocols/mcp/_mcp_server_connection.py</code> <pre><code>def get_mcp_client(self) -&gt; _AsyncGeneratorContextManager[Any, None]:\n    \"\"\"\n    Get an MCP client transport for streamable http.\n\n    Returns\n    -------\n    _AsyncGeneratorContextManager[Any, None]\n        An async context manager for the streamable HTTP client transport.\n    \"\"\"\n    return streamable_http_client(\n        url=self.url,\n        http_client=self.http_client,\n        terminate_on_close=self.terminate_on_close,\n    )\n</code></pre>"},{"location":"reference/bridgic-protocols-mcp/bridgic/protocols/mcp/#bridgic.protocols.mcp.McpServerConnectionManager","title":"McpServerConnectionManager","text":"<p>Manages multiple MCP server connections, sharing a single thread and event loop.</p> <p>This manager ensures that all MCP operations run in a dedicated thread with its own event loop, avoiding issues with cross-thread event loop usage.</p> Source code in <code>bridgic/protocols/mcp/_mcp_server_connection_manager.py</code> <pre><code>class McpServerConnectionManager:\n    \"\"\"\n    Manages multiple MCP server connections, sharing a single thread and event loop.\n\n    This manager ensures that all MCP operations run in a dedicated thread with its own\n    event loop, avoiding issues with cross-thread event loop usage.\n    \"\"\"\n\n    _instances_lock: ClassVar[threading.Lock] = threading.Lock()\n    _instances: ClassVar[Dict[str, \"McpServerConnectionManager\"]] = {}\n    _connection_to_manager: ClassVar[Dict[str, str]] = {}\n\n    _name: str\n\n    _connections_lock: threading.Lock\n    _connections: weakref.WeakValueDictionary[str, \"McpServerConnection\"]\n\n    _thread: threading.Thread\n    _loop: asyncio.AbstractEventLoop\n    _shutdown: bool\n\n    def __init__(self, name: str):\n        self._name = name\n        self._connections_lock = threading.Lock()\n        self._connections = weakref.WeakValueDictionary()\n        self._thread = None\n        self._loop = None\n        self._shutdown = False\n\n        cls = type(self)\n\n        # Automatically register this instance in the class-level registry.\n        with cls._instances_lock:\n            if name in cls._instances:\n                raise ValueError(f\"Manager with name '{name}' already exists\")\n            cls._instances[name] = self\n\n    @classmethod\n    def get_instance(cls, manager_name: str = \"default-mcp-manager\") -&gt; \"McpServerConnectionManager\":\n        \"\"\"\n        Get a manager instance by name, creating it if it doesn't exist.\n\n        Parameters\n        ----------\n        manager_name : str\n            The name of the manager instance to retrieve. Defaults to \"default-mcp-manager\".\n\n        Returns\n        -------\n        McpServerConnectionManager\n            The manager instance with the specified name.\n        \"\"\"\n        if manager_name not in cls._instances:\n            cls._instances[manager_name] = cls(name=manager_name)\n        return cls._instances[manager_name]\n\n    @classmethod\n    def get_connection(cls, connection_name: str) -&gt; Optional[\"McpServerConnection\"]:\n        \"\"\"\n        Get a connection by its name across all manager instances.\n\n        It first finds the manager that owns the connection using the class-level \n        connection-to-manager mapping, then retrieves the connection from that manager.\n\n        Parameters\n        ----------\n        connection_name : str\n            The name of the connection to retrieve.\n\n        Returns\n        -------\n        Optional[McpServerConnection]\n            The connection with the specified name, or None if not found, the manager\n            doesn't exist, or the connection has been garbage collected.\n\n        Raises\n        ------\n        KeyError\n            If the connection is not found.\n        \"\"\"\n        manager_name = cls._connection_to_manager.get(connection_name)\n        manager = cls.get_instance(manager_name) if manager_name is not None else None\n\n        if manager is None:\n            raise KeyError(\n                f\"McpServerConnectionManager-[{manager_name}] is not found: \"\n                f\"connection_name={connection_name}\"\n            )\n\n        return manager.get_connection_by_name(connection_name)\n\n    def register_connection(self, connection: \"McpServerConnection\"):\n        \"\"\"\n        Register a connection into the manager.\n\n        Parameters\n        ----------\n        connection : McpServerConnection\n            The connection to register.\n\n        Raises\n        ------\n        McpServerConnectionError\n            If a connection with the same name is already registered.\n        \"\"\"\n        cls = type(self)\n        if connection.name in cls._connection_to_manager:\n            raise McpServerConnectionError(\n                f\"A connection with the name '{connection.name}' is already registered.\"\n            )\n\n        with cls._instances_lock:\n            cls._connection_to_manager[connection.name] = self._name\n\n        with self._connections_lock:\n            self._connections[connection.name] = connection\n\n        connection._manager = self\n\n\n    def unregister_connection(self, connection: \"McpServerConnection\"):\n        \"\"\"\n        Unregister a connection from the manager.\n\n        Parameters\n        ----------\n        connection : McpServerConnection\n            The connection to unregister.\n        \"\"\"\n        with self._connections_lock:\n            self._connections.pop(connection.name, None)\n\n        cls = type(self)\n        with cls._instances_lock:\n            cls._connection_to_manager.pop(connection.name, None)\n\n        connection._manager = None\n\n    def get_connection_by_name(self, name: str) -&gt; Optional[\"McpServerConnection\"]:\n        \"\"\"\n        Get a connection by its name from this manager instance.\n\n        This method looks up a registered connection by its name within this manager.\n        If the connection has been garbage collected, `weakref.WeakValueDictionary` will\n        automatically remove it and None will be returned.\n\n        Parameters\n        ----------\n        name : str\n            The name of the connection to retrieve.\n\n        Returns\n        -------\n        Optional[McpServerConnection]\n            The connection with the specified name, or None if not found or has been\n            garbage collected.\n\n        Raises\n        ------\n        KeyError\n            If the connection is not found.\n        \"\"\"\n        if name not in self._connections:\n            raise KeyError(\n                f\"Connection '{name}' is not found in McpServerConnectionManager-[{self._name}]\"\n            )\n\n        return self._connections.get(name)\n\n    def run_sync(\n        self,\n        coro: Coroutine[Any, Any, Any],\n        timeout: Optional[float] = None,\n    ) -&gt; Any:\n        \"\"\"\n        Submit a coroutine to the manager's event loop and wait for the result synchronously.\n\n        This method blocks until the coroutine completes, suitable for use in synchronous contexts.\n\n        Parameters\n        ----------\n        coro : Coroutine\n            The coroutine to run.\n        timeout : Optional[float]\n            Timeout in seconds. If None, no timeout.\n\n        Returns\n        -------\n        Any\n            The result of the coroutine execution.\n\n        Raises\n        ------\n        RuntimeError\n            If the event loop is not running.\n        TimeoutError\n            If the coroutine execution times out.\n        \"\"\"\n        self._ensure_loop_running()\n\n        if self._loop is None or not self._loop.is_running():\n            raise RuntimeError(f\"Event loop is not running in McpServerConnectionManager-[{self._name}]\")\n\n        future = asyncio.run_coroutine_threadsafe(coro, self._loop)\n        return future.result(timeout=timeout)\n\n    async def run_async(\n        self,\n        coro: Coroutine[Any, Any, Any],\n        timeout: Optional[float] = None,\n    ) -&gt; Any:\n        \"\"\"\n        Submit a coroutine to the manager's event loop and await its result in a non-blocking way.\n\n        This method submits the coroutine to the manager's dedicated event loop, and then \n        waits for its completion in a non-blocking way.\n\n        Parameters\n        ----------\n        coro : Coroutine\n            The coroutine to run.\n        timeout : Optional[float]\n            Timeout in seconds. If None, no timeout.\n\n        Returns\n        -------\n        Any\n            The result of the coroutine execution.\n\n        Raises\n        ------\n        RuntimeError\n            If the event loop is not running.\n        TimeoutError\n            If the coroutine execution times out.\n        \"\"\"\n        self._ensure_loop_running()\n\n        if self._loop is None or not self._loop.is_running():\n            raise RuntimeError(f\"Event loop is not running in McpServerConnectionManager-[{self._name}]\")\n\n        future = asyncio.run_coroutine_threadsafe(coro, self._loop)\n        asyncio_future = asyncio.wrap_future(future)\n\n        if timeout is not None:\n            return await asyncio.wait_for(asyncio_future, timeout=timeout)\n        else:\n            return await asyncio_future\n\n    def shutdown(self):\n        \"\"\"\n        Shutdown the manager and stop the event loop.\n\n        This method also removes the manager from the class-level registry and cleans up\n        all connection-to-manager mappings for connections registered with this manager.\n        \"\"\"\n        # Mark the manager is going to shutdown.\n        self._shutdown = True\n\n        # Stop the event loop as soon as possible.\n        if self._loop is not None and self._loop.is_running():\n            self._loop.call_soon_threadsafe(self._loop.stop)\n\n        # Join the thread to wait for it to finish.\n        if self._thread is not None:\n            self._thread.join(timeout=5)\n\n        self._loop = None\n        self._thread = None\n\n        cls = type(self)\n\n        # Remove the registered item.\n        with cls._instances_lock:\n            # Remove the manager from the class-level registry.\n            cls._instances.pop(self._name, None)\n\n        # Clean up connection-to-manager mappings.\n        conn_names_to_remove = [\n            conn_name for conn_name, mgr_name in cls._connection_to_manager.items() if mgr_name == self._name\n        ]\n        with cls._instances_lock:\n            for conn_name in conn_names_to_remove:\n                cls._connection_to_manager.pop(conn_name, None)\n\n    def _ensure_loop_running(self):\n        \"\"\"\n        Ensure the manager's event loop is running in a dedicated thread.\n        \"\"\"\n        def run_until_shutdown():\n            asyncio.set_event_loop(self._loop)\n            try:\n                self._loop.run_forever()\n            finally:\n                self._loop.close()\n\n        with self._connections_lock:\n            if self._loop is not None and self._loop.is_running():\n                return\n\n            if self._shutdown:\n                raise RuntimeError(f\"McpServerConnectionManager-[{self._name}] has been shut down\")\n\n            self._loop = asyncio.new_event_loop()\n            self._thread = threading.Thread(\n                target=run_until_shutdown,\n                daemon=True,\n            )\n            self._thread.start()\n</code></pre>"},{"location":"reference/bridgic-protocols-mcp/bridgic/protocols/mcp/#bridgic.protocols.mcp.McpServerConnectionManager.get_instance","title":"get_instance","text":"<code>classmethod</code> <pre><code>get_instance(\n    manager_name: str = \"default-mcp-manager\",\n) -&gt; McpServerConnectionManager\n</code></pre> <p>Get a manager instance by name, creating it if it doesn't exist.</p> <p>Parameters:</p> Name Type Description Default <code>manager_name</code> <code>str</code> <p>The name of the manager instance to retrieve. Defaults to \"default-mcp-manager\".</p> <code>'default-mcp-manager'</code> <p>Returns:</p> Type Description <code>McpServerConnectionManager</code> <p>The manager instance with the specified name.</p> Source code in <code>bridgic/protocols/mcp/_mcp_server_connection_manager.py</code> <pre><code>@classmethod\ndef get_instance(cls, manager_name: str = \"default-mcp-manager\") -&gt; \"McpServerConnectionManager\":\n    \"\"\"\n    Get a manager instance by name, creating it if it doesn't exist.\n\n    Parameters\n    ----------\n    manager_name : str\n        The name of the manager instance to retrieve. Defaults to \"default-mcp-manager\".\n\n    Returns\n    -------\n    McpServerConnectionManager\n        The manager instance with the specified name.\n    \"\"\"\n    if manager_name not in cls._instances:\n        cls._instances[manager_name] = cls(name=manager_name)\n    return cls._instances[manager_name]\n</code></pre>"},{"location":"reference/bridgic-protocols-mcp/bridgic/protocols/mcp/#bridgic.protocols.mcp.McpServerConnectionManager.get_connection","title":"get_connection","text":"<code>classmethod</code> <pre><code>get_connection(\n    connection_name: str,\n) -&gt; Optional[McpServerConnection]\n</code></pre> <p>Get a connection by its name across all manager instances.</p> <p>It first finds the manager that owns the connection using the class-level  connection-to-manager mapping, then retrieves the connection from that manager.</p> <p>Parameters:</p> Name Type Description Default <code>connection_name</code> <code>str</code> <p>The name of the connection to retrieve.</p> required <p>Returns:</p> Type Description <code>Optional[McpServerConnection]</code> <p>The connection with the specified name, or None if not found, the manager doesn't exist, or the connection has been garbage collected.</p> <p>Raises:</p> Type Description <code>KeyError</code> <p>If the connection is not found.</p> Source code in <code>bridgic/protocols/mcp/_mcp_server_connection_manager.py</code> <pre><code>@classmethod\ndef get_connection(cls, connection_name: str) -&gt; Optional[\"McpServerConnection\"]:\n    \"\"\"\n    Get a connection by its name across all manager instances.\n\n    It first finds the manager that owns the connection using the class-level \n    connection-to-manager mapping, then retrieves the connection from that manager.\n\n    Parameters\n    ----------\n    connection_name : str\n        The name of the connection to retrieve.\n\n    Returns\n    -------\n    Optional[McpServerConnection]\n        The connection with the specified name, or None if not found, the manager\n        doesn't exist, or the connection has been garbage collected.\n\n    Raises\n    ------\n    KeyError\n        If the connection is not found.\n    \"\"\"\n    manager_name = cls._connection_to_manager.get(connection_name)\n    manager = cls.get_instance(manager_name) if manager_name is not None else None\n\n    if manager is None:\n        raise KeyError(\n            f\"McpServerConnectionManager-[{manager_name}] is not found: \"\n            f\"connection_name={connection_name}\"\n        )\n\n    return manager.get_connection_by_name(connection_name)\n</code></pre>"},{"location":"reference/bridgic-protocols-mcp/bridgic/protocols/mcp/#bridgic.protocols.mcp.McpServerConnectionManager.register_connection","title":"register_connection","text":"<pre><code>register_connection(connection: McpServerConnection)\n</code></pre> <p>Register a connection into the manager.</p> <p>Parameters:</p> Name Type Description Default <code>connection</code> <code>McpServerConnection</code> <p>The connection to register.</p> required <p>Raises:</p> Type Description <code>McpServerConnectionError</code> <p>If a connection with the same name is already registered.</p> Source code in <code>bridgic/protocols/mcp/_mcp_server_connection_manager.py</code> <pre><code>def register_connection(self, connection: \"McpServerConnection\"):\n    \"\"\"\n    Register a connection into the manager.\n\n    Parameters\n    ----------\n    connection : McpServerConnection\n        The connection to register.\n\n    Raises\n    ------\n    McpServerConnectionError\n        If a connection with the same name is already registered.\n    \"\"\"\n    cls = type(self)\n    if connection.name in cls._connection_to_manager:\n        raise McpServerConnectionError(\n            f\"A connection with the name '{connection.name}' is already registered.\"\n        )\n\n    with cls._instances_lock:\n        cls._connection_to_manager[connection.name] = self._name\n\n    with self._connections_lock:\n        self._connections[connection.name] = connection\n\n    connection._manager = self\n</code></pre>"},{"location":"reference/bridgic-protocols-mcp/bridgic/protocols/mcp/#bridgic.protocols.mcp.McpServerConnectionManager.unregister_connection","title":"unregister_connection","text":"<pre><code>unregister_connection(connection: McpServerConnection)\n</code></pre> <p>Unregister a connection from the manager.</p> <p>Parameters:</p> Name Type Description Default <code>connection</code> <code>McpServerConnection</code> <p>The connection to unregister.</p> required Source code in <code>bridgic/protocols/mcp/_mcp_server_connection_manager.py</code> <pre><code>def unregister_connection(self, connection: \"McpServerConnection\"):\n    \"\"\"\n    Unregister a connection from the manager.\n\n    Parameters\n    ----------\n    connection : McpServerConnection\n        The connection to unregister.\n    \"\"\"\n    with self._connections_lock:\n        self._connections.pop(connection.name, None)\n\n    cls = type(self)\n    with cls._instances_lock:\n        cls._connection_to_manager.pop(connection.name, None)\n\n    connection._manager = None\n</code></pre>"},{"location":"reference/bridgic-protocols-mcp/bridgic/protocols/mcp/#bridgic.protocols.mcp.McpServerConnectionManager.get_connection_by_name","title":"get_connection_by_name","text":"<pre><code>get_connection_by_name(\n    name: str,\n) -&gt; Optional[McpServerConnection]\n</code></pre> <p>Get a connection by its name from this manager instance.</p> <p>This method looks up a registered connection by its name within this manager. If the connection has been garbage collected, <code>weakref.WeakValueDictionary</code> will automatically remove it and None will be returned.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name of the connection to retrieve.</p> required <p>Returns:</p> Type Description <code>Optional[McpServerConnection]</code> <p>The connection with the specified name, or None if not found or has been garbage collected.</p> <p>Raises:</p> Type Description <code>KeyError</code> <p>If the connection is not found.</p> Source code in <code>bridgic/protocols/mcp/_mcp_server_connection_manager.py</code> <pre><code>def get_connection_by_name(self, name: str) -&gt; Optional[\"McpServerConnection\"]:\n    \"\"\"\n    Get a connection by its name from this manager instance.\n\n    This method looks up a registered connection by its name within this manager.\n    If the connection has been garbage collected, `weakref.WeakValueDictionary` will\n    automatically remove it and None will be returned.\n\n    Parameters\n    ----------\n    name : str\n        The name of the connection to retrieve.\n\n    Returns\n    -------\n    Optional[McpServerConnection]\n        The connection with the specified name, or None if not found or has been\n        garbage collected.\n\n    Raises\n    ------\n    KeyError\n        If the connection is not found.\n    \"\"\"\n    if name not in self._connections:\n        raise KeyError(\n            f\"Connection '{name}' is not found in McpServerConnectionManager-[{self._name}]\"\n        )\n\n    return self._connections.get(name)\n</code></pre>"},{"location":"reference/bridgic-protocols-mcp/bridgic/protocols/mcp/#bridgic.protocols.mcp.McpServerConnectionManager.run_sync","title":"run_sync","text":"<pre><code>run_sync(\n    coro: Coroutine[Any, Any, Any],\n    timeout: Optional[float] = None,\n) -&gt; Any\n</code></pre> <p>Submit a coroutine to the manager's event loop and wait for the result synchronously.</p> <p>This method blocks until the coroutine completes, suitable for use in synchronous contexts.</p> <p>Parameters:</p> Name Type Description Default <code>coro</code> <code>Coroutine</code> <p>The coroutine to run.</p> required <code>timeout</code> <code>Optional[float]</code> <p>Timeout in seconds. If None, no timeout.</p> <code>None</code> <p>Returns:</p> Type Description <code>Any</code> <p>The result of the coroutine execution.</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If the event loop is not running.</p> <code>TimeoutError</code> <p>If the coroutine execution times out.</p> Source code in <code>bridgic/protocols/mcp/_mcp_server_connection_manager.py</code> <pre><code>def run_sync(\n    self,\n    coro: Coroutine[Any, Any, Any],\n    timeout: Optional[float] = None,\n) -&gt; Any:\n    \"\"\"\n    Submit a coroutine to the manager's event loop and wait for the result synchronously.\n\n    This method blocks until the coroutine completes, suitable for use in synchronous contexts.\n\n    Parameters\n    ----------\n    coro : Coroutine\n        The coroutine to run.\n    timeout : Optional[float]\n        Timeout in seconds. If None, no timeout.\n\n    Returns\n    -------\n    Any\n        The result of the coroutine execution.\n\n    Raises\n    ------\n    RuntimeError\n        If the event loop is not running.\n    TimeoutError\n        If the coroutine execution times out.\n    \"\"\"\n    self._ensure_loop_running()\n\n    if self._loop is None or not self._loop.is_running():\n        raise RuntimeError(f\"Event loop is not running in McpServerConnectionManager-[{self._name}]\")\n\n    future = asyncio.run_coroutine_threadsafe(coro, self._loop)\n    return future.result(timeout=timeout)\n</code></pre>"},{"location":"reference/bridgic-protocols-mcp/bridgic/protocols/mcp/#bridgic.protocols.mcp.McpServerConnectionManager.run_async","title":"run_async","text":"<code>async</code> <pre><code>run_async(\n    coro: Coroutine[Any, Any, Any],\n    timeout: Optional[float] = None,\n) -&gt; Any\n</code></pre> <p>Submit a coroutine to the manager's event loop and await its result in a non-blocking way.</p> <p>This method submits the coroutine to the manager's dedicated event loop, and then  waits for its completion in a non-blocking way.</p> <p>Parameters:</p> Name Type Description Default <code>coro</code> <code>Coroutine</code> <p>The coroutine to run.</p> required <code>timeout</code> <code>Optional[float]</code> <p>Timeout in seconds. If None, no timeout.</p> <code>None</code> <p>Returns:</p> Type Description <code>Any</code> <p>The result of the coroutine execution.</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If the event loop is not running.</p> <code>TimeoutError</code> <p>If the coroutine execution times out.</p> Source code in <code>bridgic/protocols/mcp/_mcp_server_connection_manager.py</code> <pre><code>async def run_async(\n    self,\n    coro: Coroutine[Any, Any, Any],\n    timeout: Optional[float] = None,\n) -&gt; Any:\n    \"\"\"\n    Submit a coroutine to the manager's event loop and await its result in a non-blocking way.\n\n    This method submits the coroutine to the manager's dedicated event loop, and then \n    waits for its completion in a non-blocking way.\n\n    Parameters\n    ----------\n    coro : Coroutine\n        The coroutine to run.\n    timeout : Optional[float]\n        Timeout in seconds. If None, no timeout.\n\n    Returns\n    -------\n    Any\n        The result of the coroutine execution.\n\n    Raises\n    ------\n    RuntimeError\n        If the event loop is not running.\n    TimeoutError\n        If the coroutine execution times out.\n    \"\"\"\n    self._ensure_loop_running()\n\n    if self._loop is None or not self._loop.is_running():\n        raise RuntimeError(f\"Event loop is not running in McpServerConnectionManager-[{self._name}]\")\n\n    future = asyncio.run_coroutine_threadsafe(coro, self._loop)\n    asyncio_future = asyncio.wrap_future(future)\n\n    if timeout is not None:\n        return await asyncio.wait_for(asyncio_future, timeout=timeout)\n    else:\n        return await asyncio_future\n</code></pre>"},{"location":"reference/bridgic-protocols-mcp/bridgic/protocols/mcp/#bridgic.protocols.mcp.McpServerConnectionManager.shutdown","title":"shutdown","text":"<pre><code>shutdown()\n</code></pre> <p>Shutdown the manager and stop the event loop.</p> <p>This method also removes the manager from the class-level registry and cleans up all connection-to-manager mappings for connections registered with this manager.</p> Source code in <code>bridgic/protocols/mcp/_mcp_server_connection_manager.py</code> <pre><code>def shutdown(self):\n    \"\"\"\n    Shutdown the manager and stop the event loop.\n\n    This method also removes the manager from the class-level registry and cleans up\n    all connection-to-manager mappings for connections registered with this manager.\n    \"\"\"\n    # Mark the manager is going to shutdown.\n    self._shutdown = True\n\n    # Stop the event loop as soon as possible.\n    if self._loop is not None and self._loop.is_running():\n        self._loop.call_soon_threadsafe(self._loop.stop)\n\n    # Join the thread to wait for it to finish.\n    if self._thread is not None:\n        self._thread.join(timeout=5)\n\n    self._loop = None\n    self._thread = None\n\n    cls = type(self)\n\n    # Remove the registered item.\n    with cls._instances_lock:\n        # Remove the manager from the class-level registry.\n        cls._instances.pop(self._name, None)\n\n    # Clean up connection-to-manager mappings.\n    conn_names_to_remove = [\n        conn_name for conn_name, mgr_name in cls._connection_to_manager.items() if mgr_name == self._name\n    ]\n    with cls._instances_lock:\n        for conn_name in conn_names_to_remove:\n            cls._connection_to_manager.pop(conn_name, None)\n</code></pre>"},{"location":"reference/bridgic-protocols-mcp/bridgic/protocols/mcp/#bridgic.protocols.mcp.McpToolSpec","title":"McpToolSpec","text":"<p>               Bases: <code>ToolSpec</code></p> <p>A tool specification that represents an MCP tool from a connected MCP server.</p> <p>This class provides a bridge between MCP tools and the Bridgic framework, allowing MCP tools to be used seamlessly within Bridgic agentic systems.</p> Source code in <code>bridgic/protocols/mcp/_mcp_tool_spec.py</code> <pre><code>class McpToolSpec(ToolSpec):\n    \"\"\"\n    A tool specification that represents an MCP tool from a connected MCP server.\n\n    This class provides a bridge between MCP tools and the Bridgic framework,\n    allowing MCP tools to be used seamlessly within Bridgic agentic systems.\n    \"\"\"\n\n    tool_info: McpTool\n    \"\"\"The raw MCP tool definition from the server.\"\"\"\n\n    _server_connection: McpServerConnection\n    \"\"\"The connection to the MCP server that provides this tool.\"\"\"\n\n    _server_connection_name: Optional[str]\n    \"\"\"The name of the server connection, used for lookup after deserialization.\"\"\"\n\n    def __init__(\n        self,\n        tool_name: str,\n        tool_info: McpTool,\n        server_connection: Union[str, McpServerConnection],\n    ):\n        super().__init__(\n            tool_name=tool_name,\n            tool_description=tool_info.description or \"\",\n            tool_parameters=tool_info.inputSchema or {},\n        )\n\n        # Store the corresponding McpTool object.\n        self.tool_info = tool_info\n\n        # Try to associate with the real connection object.\n        if isinstance(server_connection, str):\n            self._server_connection = McpServerConnectionManager.get_connection(server_connection)\n            self._server_connection_name = server_connection\n        elif isinstance(server_connection, McpServerConnection):\n            self._server_connection = server_connection\n            self._server_connection_name = server_connection.name\n        else:\n            raise TypeError(f\"Invalid type for server connection: {type(server_connection)}\")\n\n    @classmethod\n    def from_raw(\n        cls,\n        tool_name: str,\n        server_connection: Union[str, McpServerConnection],\n    ) -&gt; \"McpToolSpec\":\n        \"\"\"\n        Create a McpToolSpec from a specified server connection and tool name.\n        \"\"\"\n        # Try to associate with the real connection object.\n        connection = None\n        if isinstance(server_connection, str):\n            connection = McpServerConnectionManager.get_connection(server_connection)\n        elif isinstance(server_connection, McpServerConnection):\n            connection = server_connection\n        else:\n            raise TypeError(f\"Invalid type for server connection: {type(server_connection)}\")\n\n        # Use the connection to get the McpToolSpec object corresponding to tool_name\n        all_tools = connection.list_tools()\n        tool_spec = next((tool for tool in all_tools if tool.tool_name == tool_name), None)\n        if tool_spec is None:\n            raise ValueError(f\"Cannot find tool '{tool_name}' in the provided server connection.\")\n        return tool_spec\n\n    @property\n    def server_connection(self) -&gt; McpServerConnection:\n        \"\"\"\n        Get the server connection, loading it from the server connection manager if necessary.\n\n        This property implements lazy loading of the server connection. If the connection \n        is not available (e.g., after deserialization), it will be retrieved from the \n        server connection manager by its name.\n\n        Returns\n        -------\n        McpServerConnection\n            The server connection instance.\n\n        Raises\n        ------\n        McpServerConnectionError\n            If the connection cannot be found in the manager.\n        \"\"\"\n        if self._server_connection is None:\n            if self._server_connection_name is None:\n                raise McpServerConnectionError(\n                    f\"Cannot load server connection for McpToolSpec '{self._tool_name}': \"\n                    f\"connection name is not available.\"\n                )\n\n            try:\n                connection = McpServerConnectionManager.get_connection(self._server_connection_name)\n            except KeyError as e:\n                raise McpServerConnectionError(\n                    f\"Failed to load the server connection for McpToolSpec \\\"{self._tool_name}\\\", because the \"\n                    f\"connection named \\\"{self._server_connection_name}\\\" was not found in any connection manager. \"\n                    f\"You must create a McpServerConnection with name \\\"{self._server_connection_name}\\\" and \"\n                    f\"ensure it is properly registered in a connection manager before using this tool spec.\"\n                ) from e\n\n            self._server_connection = connection\n\n        return self._server_connection\n\n    @override\n    def to_tool(self) -&gt; Tool:\n        \"\"\"\n        Transform this McpToolSpec to a `Tool` object used by LLM.\n\n        Returns\n        -------\n        Tool\n            A `Tool` object that can be used by LLM for tool selection.\n        \"\"\"\n        return Tool(\n            name=self._tool_name,\n            description=self._tool_description,\n            parameters=self._tool_parameters\n        )\n\n    @override\n    def create_worker(self) -&gt; Worker:\n        \"\"\"\n        Create a Worker from the information included in this McpToolSpec.\n\n        Returns\n        -------\n        Worker\n            A new `McpToolWorker` object that can be added to an Automa to execute the tool.\n        \"\"\"\n        from bridgic.protocols.mcp._mcp_tool_worker import McpToolWorker\n        return McpToolWorker(\n            tool_name=self._tool_name,\n            server_connection=self.server_connection,\n        )\n\n    @override\n    def dump_to_dict(self) -&gt; Dict[str, Any]:\n        state_dict = super().dump_to_dict()\n        state_dict[\"server_connection_name\"] = self._server_connection_name\n        return state_dict\n\n    @override\n    def load_from_dict(self, state_dict: Dict[str, Any]) -&gt; None:\n        super().load_from_dict(state_dict)\n        connection_name = state_dict.get(\"server_connection_name\", None)\n\n        if connection_name is None:\n            raise McpServerConnectionError(\n                f\"Cannot load server connection for McpToolSpec \\\"{self._tool_name}\\\", because \"\n                f\"its connection name is not available.\"\n            )\n\n        # Try to reload the server connection from the global manager.\n        self._server_connection = McpServerConnectionManager.get_connection(connection_name)\n        self._server_connection_name = connection_name\n\n        # Load tool_info from the real connection.\n        all_tools = self.server_connection.list_tools()\n        reload_tool_spec = next((tool for tool in all_tools if tool.tool_name == self._tool_name), None)\n        if reload_tool_spec is None:\n            raise RuntimeError(\n                f\"Cannot restore McpToolSpec because the tool is not found on server: \"\n                f\"server_connection_name=\\\"{connection_name}\\\", tool_name=\\\"{self._tool_name}\\\".\"\n            )\n        self.tool_info = reload_tool_spec.tool_info\n</code></pre>"},{"location":"reference/bridgic-protocols-mcp/bridgic/protocols/mcp/#bridgic.protocols.mcp.McpToolSpec.tool_info","title":"tool_info  <code>instance-attribute</code>","text":"<pre><code>tool_info: Tool = tool_info\n</code></pre> <p>The raw MCP tool definition from the server.</p>"},{"location":"reference/bridgic-protocols-mcp/bridgic/protocols/mcp/#bridgic.protocols.mcp.McpToolSpec.server_connection","title":"server_connection  <code>property</code>","text":"<pre><code>server_connection: McpServerConnection\n</code></pre> <p>Get the server connection, loading it from the server connection manager if necessary.</p> <p>This property implements lazy loading of the server connection. If the connection  is not available (e.g., after deserialization), it will be retrieved from the  server connection manager by its name.</p> <p>Returns:</p> Type Description <code>McpServerConnection</code> <p>The server connection instance.</p> <p>Raises:</p> Type Description <code>McpServerConnectionError</code> <p>If the connection cannot be found in the manager.</p>"},{"location":"reference/bridgic-protocols-mcp/bridgic/protocols/mcp/#bridgic.protocols.mcp.McpToolSpec.from_raw","title":"from_raw","text":"<code>classmethod</code> <pre><code>from_raw(\n    tool_name: str,\n    server_connection: Union[str, McpServerConnection],\n) -&gt; McpToolSpec\n</code></pre> <p>Create a McpToolSpec from a specified server connection and tool name.</p> Source code in <code>bridgic/protocols/mcp/_mcp_tool_spec.py</code> <pre><code>@classmethod\ndef from_raw(\n    cls,\n    tool_name: str,\n    server_connection: Union[str, McpServerConnection],\n) -&gt; \"McpToolSpec\":\n    \"\"\"\n    Create a McpToolSpec from a specified server connection and tool name.\n    \"\"\"\n    # Try to associate with the real connection object.\n    connection = None\n    if isinstance(server_connection, str):\n        connection = McpServerConnectionManager.get_connection(server_connection)\n    elif isinstance(server_connection, McpServerConnection):\n        connection = server_connection\n    else:\n        raise TypeError(f\"Invalid type for server connection: {type(server_connection)}\")\n\n    # Use the connection to get the McpToolSpec object corresponding to tool_name\n    all_tools = connection.list_tools()\n    tool_spec = next((tool for tool in all_tools if tool.tool_name == tool_name), None)\n    if tool_spec is None:\n        raise ValueError(f\"Cannot find tool '{tool_name}' in the provided server connection.\")\n    return tool_spec\n</code></pre>"},{"location":"reference/bridgic-protocols-mcp/bridgic/protocols/mcp/#bridgic.protocols.mcp.McpToolSpec.to_tool","title":"to_tool","text":"<pre><code>to_tool() -&gt; Tool\n</code></pre> <p>Transform this McpToolSpec to a <code>Tool</code> object used by LLM.</p> <p>Returns:</p> Type Description <code>Tool</code> <p>A <code>Tool</code> object that can be used by LLM for tool selection.</p> Source code in <code>bridgic/protocols/mcp/_mcp_tool_spec.py</code> <pre><code>@override\ndef to_tool(self) -&gt; Tool:\n    \"\"\"\n    Transform this McpToolSpec to a `Tool` object used by LLM.\n\n    Returns\n    -------\n    Tool\n        A `Tool` object that can be used by LLM for tool selection.\n    \"\"\"\n    return Tool(\n        name=self._tool_name,\n        description=self._tool_description,\n        parameters=self._tool_parameters\n    )\n</code></pre>"},{"location":"reference/bridgic-protocols-mcp/bridgic/protocols/mcp/#bridgic.protocols.mcp.McpToolSpec.create_worker","title":"create_worker","text":"<pre><code>create_worker() -&gt; Worker\n</code></pre> <p>Create a Worker from the information included in this McpToolSpec.</p> <p>Returns:</p> Type Description <code>Worker</code> <p>A new <code>McpToolWorker</code> object that can be added to an Automa to execute the tool.</p> Source code in <code>bridgic/protocols/mcp/_mcp_tool_spec.py</code> <pre><code>@override\ndef create_worker(self) -&gt; Worker:\n    \"\"\"\n    Create a Worker from the information included in this McpToolSpec.\n\n    Returns\n    -------\n    Worker\n        A new `McpToolWorker` object that can be added to an Automa to execute the tool.\n    \"\"\"\n    from bridgic.protocols.mcp._mcp_tool_worker import McpToolWorker\n    return McpToolWorker(\n        tool_name=self._tool_name,\n        server_connection=self.server_connection,\n    )\n</code></pre>"},{"location":"reference/bridgic-protocols-mcp/bridgic/protocols/mcp/#bridgic.protocols.mcp.McpToolSetBuilder","title":"McpToolSetBuilder","text":"<p>               Bases: <code>ToolSetBuilder</code></p> <p>A builder for creating exclusive <code>McpToolSpec</code> instances from a newly created MCP server connection.</p> <p>This builder creates a new MCP server connection from scratch, ensuring that the connection is  exclusive and not shared with other instances. This is important for stateful connections that  should not be shared to different owners.</p> <p>Examples:</p> <p>Create a builder for stdio connection and build a tool set:</p> <pre><code>&gt;&gt;&gt; builder = McpToolSetBuilder.stdio(\n...     command=\"npx\",\n...     args=[\"-y\", \"@modelcontextprotocol/server-filesystem\", \"/path/to/dir\"],\n...     tool_names=[\"read_file\", \"write_file\"]  # Optional: filter specific tools\n... )\n&gt;&gt;&gt; tool_set = builder.build()[\"tool_specs\"]\n</code></pre> <p>Create a builder for streamable HTTP connection and build a tool set:</p> <pre><code>&gt;&gt;&gt; builder = McpToolSetBuilder.streamable_http(\n...     url=\"http://localhost:8000\",\n...     tool_names=[\"get_weather\"]  # Optional: filter specific tools\n... )\n&gt;&gt;&gt; tool_set = builder.build()[\"tool_specs\"]\n</code></pre> Source code in <code>bridgic/protocols/mcp/_mcp_tool_set_builder.py</code> <pre><code>class McpToolSetBuilder(ToolSetBuilder):\n    \"\"\"\n    A builder for creating exclusive `McpToolSpec` instances from a newly created MCP server connection.\n\n    This builder creates a new MCP server connection from scratch, ensuring that the connection is \n    exclusive and not shared with other instances. This is important for stateful connections that \n    should not be shared to different owners.\n\n    Examples\n    --------\n    Create a builder for stdio connection and build a tool set:\n    &gt;&gt;&gt; builder = McpToolSetBuilder.stdio(\n    ...     command=\"npx\",\n    ...     args=[\"-y\", \"@modelcontextprotocol/server-filesystem\", \"/path/to/dir\"],\n    ...     tool_names=[\"read_file\", \"write_file\"]  # Optional: filter specific tools\n    ... )\n    &gt;&gt;&gt; tool_set = builder.build()[\"tool_specs\"]\n\n    Create a builder for streamable HTTP connection and build a tool set:\n    &gt;&gt;&gt; builder = McpToolSetBuilder.streamable_http(\n    ...     url=\"http://localhost:8000\",\n    ...     tool_names=[\"get_weather\"]  # Optional: filter specific tools\n    ... )\n    &gt;&gt;&gt; tool_set = builder.build()[\"tool_specs\"]\n    \"\"\"\n\n    _connection_type: Literal[McpServerConnectionType.STDIO, McpServerConnectionType.STREAMABLE_HTTP]\n    \"\"\"The type of connection: 'stdio' or 'streamable_http'.\"\"\"\n\n    _connection_config: Dict[str, Any]\n    \"\"\"Configuration parameters for creating the connection.\"\"\"\n\n    _tool_names: Optional[List[str]]\n    \"\"\"Optional list of tool names to include. If None, all tools will be included.\"\"\"\n\n    def __init__(\n        self,\n        connection_type: str,\n        connection_config: Dict[str, Any],\n        tool_names: Optional[List[str]] = None,\n    ):\n        \"\"\"\n        Initialize the McpToolSetBuilder.\n\n        Parameters\n        ----------\n        connection_type : str\n            The type of connection: 'stdio' or 'streamable_http'.\n        connection_config : Dict[str, Any]\n            Configuration parameters for creating the connection.\n            - For stdio connections, this should include 'command', and optionally 'args', 'env', 'encoding', 'request_timeout'.\n            - For streamable_http connections, this should include 'url', and optionally 'http_client_config', 'terminate_on_close', 'request_timeout'.\n        tool_names : Optional[List[str]]\n            Optional list of tool names to include.\n            If None, all available tools from the connection will be included.\n        \"\"\"\n        if connection_type not in (McpServerConnectionType.STDIO, McpServerConnectionType.STREAMABLE_HTTP):\n            raise ValueError(\n                f\"Invalid connection_type: {connection_type}. \"\n                f\"Expected 'stdio' or 'streamable_http'.\"\n            )\n\n        self._connection_type = connection_type\n        self._connection_config = connection_config\n        self._tool_names = tool_names\n\n    @classmethod\n    def stdio(\n        cls,\n        command: str,\n        *,\n        args: Optional[List[str]] = None,\n        env: Optional[Dict[str, str]] = None,\n        encoding: Optional[str] = None,\n        request_timeout: Optional[int] = None,\n        tool_names: Optional[List[str]] = None,\n    ) -&gt; \"McpToolSetBuilder\":\n        \"\"\"\n        Create a builder for a stdio-based MCP server connection.\n\n        Parameters\n        ----------\n        command : str\n            The command to use for the connection (e.g., \"npx\", \"python\").\n        args : Optional[List[str]]\n            The arguments to pass to the command.\n        env : Optional[Dict[str, str]]\n            Environment variables to set for the process.\n        encoding : Optional[str]\n            The encoding to use for the connection. Defaults to \"utf-8\".\n        request_timeout : Optional[int]\n            The timeout in seconds for MCP requests. Default is 30 seconds.\n        tool_names : Optional[List[str]]\n            Optional list of tool names to include. If None, all available tools will be included.\n\n        Returns\n        -------\n        McpToolSetBuilder\n            A builder instance configured for stdio connection.\n        \"\"\"\n        connection_config = {\n            \"command\": command,\n        }\n        if args is not None:\n            connection_config[\"args\"] = args\n        if env is not None:\n            connection_config[\"env\"] = env\n        if encoding is not None:\n            connection_config[\"encoding\"] = encoding\n        if request_timeout is not None:\n            connection_config[\"request_timeout\"] = request_timeout\n\n        return cls(\n            connection_type=\"stdio\",\n            connection_config=connection_config,\n            tool_names=tool_names,\n        )\n\n    @classmethod\n    def streamable_http(\n        cls,\n        url: str,\n        *,\n        http_client_config: Optional[HttpClientConfig] = None,\n        terminate_on_close: Optional[bool] = None,\n        request_timeout: Optional[int] = None,\n        tool_names: Optional[List[str]] = None,\n    ) -&gt; \"McpToolSetBuilder\":\n        \"\"\"\n        Create a builder for a streamable HTTP-based MCP server connection.\n\n        Parameters\n        ----------\n        url : str\n            The URL of the MCP server.\n        http_client_config : Optional[HttpClientConfig]\n            Optional configuration for creating the HTTP client.\n            If None, a default client will be created with MCP defaults.\n        terminate_on_close : Optional[bool]\n            If True, send a DELETE request to terminate the session when the connection\n            is closed. Defaults to True.\n        request_timeout : Optional[int]\n            The timeout in seconds for MCP requests. Default is 30 seconds.\n        tool_names : Optional[List[str]]\n            Optional list of tool names to include. If None, all available tools will be included.\n\n        Returns\n        -------\n        McpToolSetBuilder\n            A builder instance configured for streamable HTTP connection.\n        \"\"\"\n        connection_config = {\n            \"url\": url,\n        }\n        if http_client_config is not None:\n            connection_config[\"http_client_config\"] = http_client_config\n        if terminate_on_close is not None:\n            connection_config[\"terminate_on_close\"] = terminate_on_close\n        if request_timeout is not None:\n            connection_config[\"request_timeout\"] = request_timeout\n\n        return cls(\n            connection_type=\"streamable_http\",\n            connection_config=connection_config,\n            tool_names=tool_names,\n        )\n\n    @override\n    def build(self) -&gt; ToolSetResponse:\n        \"\"\"\n        Build and return McpToolSpec instances from a newly created server connection.\n\n        This method creates a new connection on each call, ensuring that each call gets its own \n        exclusive connection instance. The builder acts as a factory for creating connections.\n\n        Returns\n        -------\n        ToolSetResponse\n            A response containing the list of McpToolSpec instances with `_from_builder=True`,\n            along with optional extras.\n\n        Raises\n        ------\n        McpServerConnectionError\n            If the connection cannot be created or accessed.\n        RuntimeError\n            If the connection fails to establish.\n        \"\"\"\n        # Create a new connection for this build call.\n        connection = self._create_connection()\n\n\n        # Get all available tools from the connection.\n        all_tool_specs = connection.list_tools()\n\n        # Filter by tool_names if specified.\n        if self._tool_names is not None:\n            tool_specs = [\n                tool_spec for tool_spec in all_tool_specs\n                if tool_spec.tool_name in self._tool_names\n            ]\n\n            # Check if all requested tools were found.\n            found_tool_names = {tool_spec.tool_name for tool_spec in tool_specs}\n            missing_tool_names = set(self._tool_names) - found_tool_names\n            if missing_tool_names:\n                raise ValueError(\n                    f\"The following tools were not found in the MCP server connection \"\n                    f\"'{connection.name}': {sorted(missing_tool_names)}\"\n                )\n        else:\n            tool_specs = all_tool_specs\n\n        # Mark all tool specs as from builder.\n        for tool_spec in tool_specs:\n            tool_spec._from_builder = True\n\n        return ToolSetResponse(\n            tool_specs=tool_specs,\n            extras={\n                \"mcp_server_connection_name\": connection.name,\n            }\n        )\n\n    def _create_connection(self) -&gt; McpServerConnection:\n        \"\"\"\n        Create and connect a new MCP server connection.\n\n        Each call to this method creates a new connection with a unique name, allowing the \n        builder to act as a factory for creating multiple connection instances.\n\n        Returns\n        -------\n        McpServerConnection\n            The created and connected connection instance.\n        \"\"\"\n        # Generate a unique connection name for each connection instance\n        connection_name = f\"mcp-builder-{uuid.uuid4().hex[:8]}\"\n\n        # Create connection based on type\n        if self._connection_type == McpServerConnectionType.STDIO:\n            connection = McpServerConnectionStdio(\n                name=connection_name,\n                **self._connection_config,\n            )\n        elif self._connection_type == McpServerConnectionType.STREAMABLE_HTTP:\n            # Copy connection config to avoid modifying the original config.\n            connection_config = self._connection_config.copy()\n\n            # Pop http_client_config because the real initialization of the connection doesn't need it.\n            http_client_config: HttpClientConfig = connection_config.pop(\"http_client_config\", None)\n\n            if not http_client_config:\n                http_client = None\n            else:\n                if \"timeout\" not in http_client_config or http_client_config[\"timeout\"] is None:\n                    http_client_config[\"timeout\"] = {\n                        \"default\": MCP_DEFAULT_TIMEOUT,\n                        \"read\": MCP_DEFAULT_SSE_READ_TIMEOUT,\n                    }\n                http_client = create_http_client_from_config(http_client_config.copy(), is_async=True)\n\n            connection = McpServerConnectionStreamableHttp(\n                name=connection_name,\n                http_client=http_client,\n                **connection_config,\n            )\n        else:\n            raise ValueError(f\"Unknown connection type: {self._connection_type}\")\n\n        # Register and connect\n        manager = McpServerConnectionManager.get_instance()\n        manager.register_connection(connection)\n        connection.connect()\n\n        return connection\n\n    @override\n    def dump_to_dict(self) -&gt; Dict[str, Any]:\n        state_dict = {\n            \"connection_type\": self._connection_type,\n            \"connection_config\": self._connection_config,\n        }\n        if self._tool_names is not None:\n            state_dict[\"tool_names\"] = self._tool_names\n        return state_dict\n\n    @override\n    def load_from_dict(self, state_dict: Dict[str, Any]) -&gt; None:\n        self._connection_type = state_dict[\"connection_type\"]\n        self._connection_config = state_dict[\"connection_config\"]\n        self._tool_names = state_dict.get(\"tool_names\")\n</code></pre>"},{"location":"reference/bridgic-protocols-mcp/bridgic/protocols/mcp/#bridgic.protocols.mcp.McpToolSetBuilder.stdio","title":"stdio","text":"<code>classmethod</code> <pre><code>stdio(\n    command: str,\n    *,\n    args: Optional[List[str]] = None,\n    env: Optional[Dict[str, str]] = None,\n    encoding: Optional[str] = None,\n    request_timeout: Optional[int] = None,\n    tool_names: Optional[List[str]] = None\n) -&gt; McpToolSetBuilder\n</code></pre> <p>Create a builder for a stdio-based MCP server connection.</p> <p>Parameters:</p> Name Type Description Default <code>command</code> <code>str</code> <p>The command to use for the connection (e.g., \"npx\", \"python\").</p> required <code>args</code> <code>Optional[List[str]]</code> <p>The arguments to pass to the command.</p> <code>None</code> <code>env</code> <code>Optional[Dict[str, str]]</code> <p>Environment variables to set for the process.</p> <code>None</code> <code>encoding</code> <code>Optional[str]</code> <p>The encoding to use for the connection. Defaults to \"utf-8\".</p> <code>None</code> <code>request_timeout</code> <code>Optional[int]</code> <p>The timeout in seconds for MCP requests. Default is 30 seconds.</p> <code>None</code> <code>tool_names</code> <code>Optional[List[str]]</code> <p>Optional list of tool names to include. If None, all available tools will be included.</p> <code>None</code> <p>Returns:</p> Type Description <code>McpToolSetBuilder</code> <p>A builder instance configured for stdio connection.</p> Source code in <code>bridgic/protocols/mcp/_mcp_tool_set_builder.py</code> <pre><code>@classmethod\ndef stdio(\n    cls,\n    command: str,\n    *,\n    args: Optional[List[str]] = None,\n    env: Optional[Dict[str, str]] = None,\n    encoding: Optional[str] = None,\n    request_timeout: Optional[int] = None,\n    tool_names: Optional[List[str]] = None,\n) -&gt; \"McpToolSetBuilder\":\n    \"\"\"\n    Create a builder for a stdio-based MCP server connection.\n\n    Parameters\n    ----------\n    command : str\n        The command to use for the connection (e.g., \"npx\", \"python\").\n    args : Optional[List[str]]\n        The arguments to pass to the command.\n    env : Optional[Dict[str, str]]\n        Environment variables to set for the process.\n    encoding : Optional[str]\n        The encoding to use for the connection. Defaults to \"utf-8\".\n    request_timeout : Optional[int]\n        The timeout in seconds for MCP requests. Default is 30 seconds.\n    tool_names : Optional[List[str]]\n        Optional list of tool names to include. If None, all available tools will be included.\n\n    Returns\n    -------\n    McpToolSetBuilder\n        A builder instance configured for stdio connection.\n    \"\"\"\n    connection_config = {\n        \"command\": command,\n    }\n    if args is not None:\n        connection_config[\"args\"] = args\n    if env is not None:\n        connection_config[\"env\"] = env\n    if encoding is not None:\n        connection_config[\"encoding\"] = encoding\n    if request_timeout is not None:\n        connection_config[\"request_timeout\"] = request_timeout\n\n    return cls(\n        connection_type=\"stdio\",\n        connection_config=connection_config,\n        tool_names=tool_names,\n    )\n</code></pre>"},{"location":"reference/bridgic-protocols-mcp/bridgic/protocols/mcp/#bridgic.protocols.mcp.McpToolSetBuilder.streamable_http","title":"streamable_http","text":"<code>classmethod</code> <pre><code>streamable_http(\n    url: str,\n    *,\n    http_client_config: Optional[HttpClientConfig] = None,\n    terminate_on_close: Optional[bool] = None,\n    request_timeout: Optional[int] = None,\n    tool_names: Optional[List[str]] = None\n) -&gt; McpToolSetBuilder\n</code></pre> <p>Create a builder for a streamable HTTP-based MCP server connection.</p> <p>Parameters:</p> Name Type Description Default <code>url</code> <code>str</code> <p>The URL of the MCP server.</p> required <code>http_client_config</code> <code>Optional[HttpClientConfig]</code> <p>Optional configuration for creating the HTTP client. If None, a default client will be created with MCP defaults.</p> <code>None</code> <code>terminate_on_close</code> <code>Optional[bool]</code> <p>If True, send a DELETE request to terminate the session when the connection is closed. Defaults to True.</p> <code>None</code> <code>request_timeout</code> <code>Optional[int]</code> <p>The timeout in seconds for MCP requests. Default is 30 seconds.</p> <code>None</code> <code>tool_names</code> <code>Optional[List[str]]</code> <p>Optional list of tool names to include. If None, all available tools will be included.</p> <code>None</code> <p>Returns:</p> Type Description <code>McpToolSetBuilder</code> <p>A builder instance configured for streamable HTTP connection.</p> Source code in <code>bridgic/protocols/mcp/_mcp_tool_set_builder.py</code> <pre><code>@classmethod\ndef streamable_http(\n    cls,\n    url: str,\n    *,\n    http_client_config: Optional[HttpClientConfig] = None,\n    terminate_on_close: Optional[bool] = None,\n    request_timeout: Optional[int] = None,\n    tool_names: Optional[List[str]] = None,\n) -&gt; \"McpToolSetBuilder\":\n    \"\"\"\n    Create a builder for a streamable HTTP-based MCP server connection.\n\n    Parameters\n    ----------\n    url : str\n        The URL of the MCP server.\n    http_client_config : Optional[HttpClientConfig]\n        Optional configuration for creating the HTTP client.\n        If None, a default client will be created with MCP defaults.\n    terminate_on_close : Optional[bool]\n        If True, send a DELETE request to terminate the session when the connection\n        is closed. Defaults to True.\n    request_timeout : Optional[int]\n        The timeout in seconds for MCP requests. Default is 30 seconds.\n    tool_names : Optional[List[str]]\n        Optional list of tool names to include. If None, all available tools will be included.\n\n    Returns\n    -------\n    McpToolSetBuilder\n        A builder instance configured for streamable HTTP connection.\n    \"\"\"\n    connection_config = {\n        \"url\": url,\n    }\n    if http_client_config is not None:\n        connection_config[\"http_client_config\"] = http_client_config\n    if terminate_on_close is not None:\n        connection_config[\"terminate_on_close\"] = terminate_on_close\n    if request_timeout is not None:\n        connection_config[\"request_timeout\"] = request_timeout\n\n    return cls(\n        connection_type=\"streamable_http\",\n        connection_config=connection_config,\n        tool_names=tool_names,\n    )\n</code></pre>"},{"location":"reference/bridgic-protocols-mcp/bridgic/protocols/mcp/#bridgic.protocols.mcp.McpToolSetBuilder.build","title":"build","text":"<pre><code>build() -&gt; ToolSetResponse\n</code></pre> <p>Build and return McpToolSpec instances from a newly created server connection.</p> <p>This method creates a new connection on each call, ensuring that each call gets its own  exclusive connection instance. The builder acts as a factory for creating connections.</p> <p>Returns:</p> Type Description <code>ToolSetResponse</code> <p>A response containing the list of McpToolSpec instances with <code>_from_builder=True</code>, along with optional extras.</p> <p>Raises:</p> Type Description <code>McpServerConnectionError</code> <p>If the connection cannot be created or accessed.</p> <code>RuntimeError</code> <p>If the connection fails to establish.</p> Source code in <code>bridgic/protocols/mcp/_mcp_tool_set_builder.py</code> <pre><code>@override\ndef build(self) -&gt; ToolSetResponse:\n    \"\"\"\n    Build and return McpToolSpec instances from a newly created server connection.\n\n    This method creates a new connection on each call, ensuring that each call gets its own \n    exclusive connection instance. The builder acts as a factory for creating connections.\n\n    Returns\n    -------\n    ToolSetResponse\n        A response containing the list of McpToolSpec instances with `_from_builder=True`,\n        along with optional extras.\n\n    Raises\n    ------\n    McpServerConnectionError\n        If the connection cannot be created or accessed.\n    RuntimeError\n        If the connection fails to establish.\n    \"\"\"\n    # Create a new connection for this build call.\n    connection = self._create_connection()\n\n\n    # Get all available tools from the connection.\n    all_tool_specs = connection.list_tools()\n\n    # Filter by tool_names if specified.\n    if self._tool_names is not None:\n        tool_specs = [\n            tool_spec for tool_spec in all_tool_specs\n            if tool_spec.tool_name in self._tool_names\n        ]\n\n        # Check if all requested tools were found.\n        found_tool_names = {tool_spec.tool_name for tool_spec in tool_specs}\n        missing_tool_names = set(self._tool_names) - found_tool_names\n        if missing_tool_names:\n            raise ValueError(\n                f\"The following tools were not found in the MCP server connection \"\n                f\"'{connection.name}': {sorted(missing_tool_names)}\"\n            )\n    else:\n        tool_specs = all_tool_specs\n\n    # Mark all tool specs as from builder.\n    for tool_spec in tool_specs:\n        tool_spec._from_builder = True\n\n    return ToolSetResponse(\n        tool_specs=tool_specs,\n        extras={\n            \"mcp_server_connection_name\": connection.name,\n        }\n    )\n</code></pre>"},{"location":"reference/bridgic-protocols-mcp/bridgic/protocols/mcp/#bridgic.protocols.mcp.McpToolWorker","title":"McpToolWorker","text":"<p>               Bases: <code>Worker</code></p> <p>A worker that executes an MCP tool on a connected MCP server.</p> <p>This worker receives tool arguments as keyword arguments and calls the corresponding tool on the MCP server, returning the result.</p> Source code in <code>bridgic/protocols/mcp/_mcp_tool_worker.py</code> <pre><code>class McpToolWorker(Worker):\n    \"\"\"\n    A worker that executes an MCP tool on a connected MCP server.\n\n    This worker receives tool arguments as keyword arguments and calls the\n    corresponding tool on the MCP server, returning the result.\n    \"\"\"\n\n    _tool_name: str\n    \"\"\"The name of the MCP tool to call.\"\"\"\n\n    _server_connection: Optional[McpServerConnection]\n    \"\"\"The connection to the MCP server that provides this tool.\"\"\"\n\n    _server_connection_name: Optional[str]\n    \"\"\"The name of the server connection, used for lookup after deserialization.\"\"\"\n\n    def __init__(\n        self,\n        tool_name: str,\n        server_connection: Union[str, McpServerConnection],\n    ):\n        super().__init__()\n        self._tool_name = tool_name\n\n        # Try to associate with the real connection object.\n        if isinstance(server_connection, str):\n            self._server_connection = McpServerConnectionManager.get_connection(server_connection)\n            self._server_connection_name = server_connection\n        elif isinstance(server_connection, McpServerConnection):\n            self._server_connection = server_connection\n            self._server_connection_name = server_connection.name\n        else:\n            raise TypeError(f\"Invalid type for server connection: {type(server_connection)}\")\n\n    @property\n    def tool_name(self) -&gt; str:\n        \"\"\"Get the name of the tool.\"\"\"\n        return self._tool_name\n\n    @property\n    def server_connection(self) -&gt; McpServerConnection:\n        \"\"\"\n        Get the server connection, loading it from the server connection manager if necessary.\n\n        This property implements lazy loading of the server connection. If the connection \n        is not available (e.g., after deserialization), it will be retrieved from the \n        server connection manager by its name.\n\n        Returns\n        -------\n        McpServerConnection\n            The server connection instance.\n\n        Raises\n        ------\n        McpServerConnectionError\n            If the connection cannot be found in the manager.\n        \"\"\"\n        if self._server_connection is None:\n            if self._server_connection_name is None:\n                raise McpServerConnectionError(\n                    f\"Cannot load server connection for McpToolWorker '{self._tool_name}': \"\n                    f\"connection name is not available.\"\n                )\n\n            try:\n                connection = McpServerConnectionManager.get_connection(self._server_connection_name)\n            except KeyError as e:\n                raise McpServerConnectionError(\n                    f\"Failed to load the server connection for McpToolWorker \\\"{self._tool_name}\\\", because the \"\n                    f\"connection named \\\"{self._server_connection_name}\\\" was not found in any connection manager. \"\n                    f\"You must create a McpServerConnection with name \\\"{self._server_connection_name}\\\" and \"\n                    f\"ensure it is properly registered in a connection manager before using this worker.\"\n                ) from e\n\n            self._server_connection = connection\n\n        return self._server_connection\n\n    async def arun(self, **kwargs: Dict[str, Any]) -&gt; CallToolResult:\n        \"\"\"\n        Asynchronously execute the MCP tool with the provided arguments.\n\n        Parameters\n        ----------\n        **kwargs : Dict[str, Any]\n            The arguments to pass to the tool. These are typically provided\n            from the tool call arguments generated by the LLM.\n\n        Returns\n        -------\n        CallToolResult\n            The result of the tool call from the MCP server, containing content\n            and optionally structured content.\n\n        Raises\n        ------\n        RuntimeError\n            If the connection is not established and cannot be established.\n        \"\"\"\n        try:\n            result = await self.server_connection.acall_tool(\n                tool_name=self._tool_name,\n                arguments=kwargs if kwargs else None,\n            )\n        except Exception as e:\n            result = CallToolResult(\n                content=[TextContent(text=f\"[MCP Tool Error] {type(e).__name__}: {str(e)}\")],\n                structuredContent=None,\n                isError=True,\n            )\n        return result\n\n    @override\n    def dump_to_dict(self) -&gt; Dict[str, Any]:\n        state_dict = super().dump_to_dict()\n        state_dict[\"tool_name\"] = self._tool_name\n        state_dict[\"server_connection_name\"] = self._server_connection_name\n        return state_dict\n\n    @override\n    def load_from_dict(self, state_dict: Dict[str, Any]) -&gt; None:\n        super().load_from_dict(state_dict)\n        self._tool_name = state_dict.get(\"tool_name\")\n        connection_name = state_dict.get(\"server_connection_name\", None)\n\n        if connection_name is None:\n            raise McpServerConnectionError(\n                f\"Cannot load server connection for McpToolWorker \\\"{self._tool_name}\\\", because \"\n                f\"its connection name is not available.\"\n            )\n\n        # Try to reload the server connection from the global manager.\n        try:\n            self._server_connection = McpServerConnectionManager.get_connection(connection_name)\n            self._server_connection_name = connection_name\n        except KeyError as e:\n            raise McpServerConnectionError(\n                f\"Failed to load the server connection for McpToolWorker \\\"{self._tool_name}\\\", because the \"\n                f\"connection named \\\"{connection_name}\\\" was not found in any connection manager. \"\n                f\"You must create a McpServerConnection with name \\\"{connection_name}\\\" and \"\n                f\"ensure it is properly registered in a connection manager before using this worker.\"\n            ) from e\n</code></pre>"},{"location":"reference/bridgic-protocols-mcp/bridgic/protocols/mcp/#bridgic.protocols.mcp.McpToolWorker.tool_name","title":"tool_name  <code>property</code>","text":"<pre><code>tool_name: str\n</code></pre> <p>Get the name of the tool.</p>"},{"location":"reference/bridgic-protocols-mcp/bridgic/protocols/mcp/#bridgic.protocols.mcp.McpToolWorker.server_connection","title":"server_connection  <code>property</code>","text":"<pre><code>server_connection: McpServerConnection\n</code></pre> <p>Get the server connection, loading it from the server connection manager if necessary.</p> <p>This property implements lazy loading of the server connection. If the connection  is not available (e.g., after deserialization), it will be retrieved from the  server connection manager by its name.</p> <p>Returns:</p> Type Description <code>McpServerConnection</code> <p>The server connection instance.</p> <p>Raises:</p> Type Description <code>McpServerConnectionError</code> <p>If the connection cannot be found in the manager.</p>"},{"location":"reference/bridgic-protocols-mcp/bridgic/protocols/mcp/#bridgic.protocols.mcp.McpToolWorker.arun","title":"arun","text":"<code>async</code> <pre><code>arun(**kwargs: Dict[str, Any]) -&gt; CallToolResult\n</code></pre> <p>Asynchronously execute the MCP tool with the provided arguments.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <code>Dict[str, Any]</code> <p>The arguments to pass to the tool. These are typically provided from the tool call arguments generated by the LLM.</p> <code>{}</code> <p>Returns:</p> Type Description <code>CallToolResult</code> <p>The result of the tool call from the MCP server, containing content and optionally structured content.</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If the connection is not established and cannot be established.</p> Source code in <code>bridgic/protocols/mcp/_mcp_tool_worker.py</code> <pre><code>async def arun(self, **kwargs: Dict[str, Any]) -&gt; CallToolResult:\n    \"\"\"\n    Asynchronously execute the MCP tool with the provided arguments.\n\n    Parameters\n    ----------\n    **kwargs : Dict[str, Any]\n        The arguments to pass to the tool. These are typically provided\n        from the tool call arguments generated by the LLM.\n\n    Returns\n    -------\n    CallToolResult\n        The result of the tool call from the MCP server, containing content\n        and optionally structured content.\n\n    Raises\n    ------\n    RuntimeError\n        If the connection is not established and cannot be established.\n    \"\"\"\n    try:\n        result = await self.server_connection.acall_tool(\n            tool_name=self._tool_name,\n            arguments=kwargs if kwargs else None,\n        )\n    except Exception as e:\n        result = CallToolResult(\n            content=[TextContent(text=f\"[MCP Tool Error] {type(e).__name__}: {str(e)}\")],\n            structuredContent=None,\n            isError=True,\n        )\n    return result\n</code></pre>"},{"location":"reference/bridgic-protocols-mcp/bridgic/protocols/mcp/#bridgic.protocols.mcp.McpPromptTemplate","title":"McpPromptTemplate","text":"<p>               Bases: <code>BasePromptTemplate</code></p> <p>This template implementation is used to generate a prompt from a connected MCP server.</p> Source code in <code>bridgic/protocols/mcp/_mcp_template.py</code> <pre><code>class McpPromptTemplate(BasePromptTemplate):\n    \"\"\"\n    This template implementation is used to generate a prompt from a connected MCP server.\n    \"\"\"\n\n    prompt_name: str\n    \"\"\"The name of the prompt template.\"\"\"\n\n    prompt_info: Prompt\n    \"\"\"The raw information of the prompt.\"\"\"\n\n    _server_connection: McpServerConnection\n    \"\"\"The connection to the MCP server.\"\"\"\n\n    def __init__(\n        self,\n        prompt_name: str,\n        prompt_info: Prompt,\n        server_connection: Union[str, McpServerConnection],\n    ):\n        super().__init__()\n        self.prompt_name = prompt_name\n        self.prompt_info = prompt_info\n\n        # Try to associate with the real connection object.\n        if isinstance(server_connection, str):\n            self._server_connection = McpServerConnectionManager.get_connection(server_connection)\n        elif isinstance(server_connection, McpServerConnection):\n            self._server_connection = server_connection\n        else:\n            raise TypeError(f\"Invalid type for server connection: {type(server_connection)}\")\n\n    def format_messages(self, **kwargs) -&gt; List[Message]:\n        \"\"\"\n        Format the prompt template from a connected MCP server into messages.\n\n        Parameters\n        ----------\n        **kwargs : Any\n            The keyword arguments to pass to the prompt template.\n\n        Returns\n        -------\n        List[Message]\n            The list of messages.\n        \"\"\"\n        if not self._server_connection or not self._server_connection.is_connected:\n            raise RuntimeError(\"MCP session is not connected, unable to render prompt.\")\n\n        mcp_result = self._server_connection.get_prompt(\n            prompt_name=self.prompt_name,\n            arguments=kwargs,\n        )\n\n        mcp_messages: List[PromptMessage] = mcp_result.messages\n\n        messages: List[Message] = []\n        if mcp_messages:\n            for msg in mcp_messages:\n                messages.append(Message.from_text(text=msg.content.text, role=msg.role))\n\n        return messages\n</code></pre>"},{"location":"reference/bridgic-protocols-mcp/bridgic/protocols/mcp/#bridgic.protocols.mcp.McpPromptTemplate.prompt_name","title":"prompt_name  <code>instance-attribute</code>","text":"<pre><code>prompt_name: str = prompt_name\n</code></pre> <p>The name of the prompt template.</p>"},{"location":"reference/bridgic-protocols-mcp/bridgic/protocols/mcp/#bridgic.protocols.mcp.McpPromptTemplate.prompt_info","title":"prompt_info  <code>instance-attribute</code>","text":"<pre><code>prompt_info: Prompt = prompt_info\n</code></pre> <p>The raw information of the prompt.</p>"},{"location":"reference/bridgic-protocols-mcp/bridgic/protocols/mcp/#bridgic.protocols.mcp.McpPromptTemplate.format_messages","title":"format_messages","text":"<pre><code>format_messages(**kwargs) -&gt; List[Message]\n</code></pre> <p>Format the prompt template from a connected MCP server into messages.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <code>Any</code> <p>The keyword arguments to pass to the prompt template.</p> <code>{}</code> <p>Returns:</p> Type Description <code>List[Message]</code> <p>The list of messages.</p> Source code in <code>bridgic/protocols/mcp/_mcp_template.py</code> <pre><code>def format_messages(self, **kwargs) -&gt; List[Message]:\n    \"\"\"\n    Format the prompt template from a connected MCP server into messages.\n\n    Parameters\n    ----------\n    **kwargs : Any\n        The keyword arguments to pass to the prompt template.\n\n    Returns\n    -------\n    List[Message]\n        The list of messages.\n    \"\"\"\n    if not self._server_connection or not self._server_connection.is_connected:\n        raise RuntimeError(\"MCP session is not connected, unable to render prompt.\")\n\n    mcp_result = self._server_connection.get_prompt(\n        prompt_name=self.prompt_name,\n        arguments=kwargs,\n    )\n\n    mcp_messages: List[PromptMessage] = mcp_result.messages\n\n    messages: List[Message] = []\n    if mcp_messages:\n        for msg in mcp_messages:\n            messages.append(Message.from_text(text=msg.content.text, role=msg.role))\n\n    return messages\n</code></pre>"},{"location":"reference/bridgic-protocols-mcp/bridgic/protocols/mcp/#bridgic.protocols.mcp.McpServerConnectionError","title":"McpServerConnectionError","text":"<p>               Bases: <code>Exception</code></p> <p>Raised when the connection to an MCP server fails.</p> Source code in <code>bridgic/protocols/mcp/_error.py</code> <pre><code>class McpServerConnectionError(Exception):\n    \"\"\"\n    Raised when the connection to an MCP server fails.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/bridgic-traces-langwatch/bridgic/traces/langwatch/","title":"bridgic.traces.langwatch","text":""},{"location":"reference/bridgic-traces-langwatch/bridgic/traces/langwatch/#langwatch","title":"langwatch","text":""},{"location":"reference/bridgic-traces-langwatch/bridgic/traces/langwatch/#bridgic.traces.langwatch.LangWatchTraceCallback","title":"LangWatchTraceCallback","text":"<p>               Bases: <code>WorkerCallback</code></p> <p>LangWatch tracing callback handler for Bridgic.</p> <p>This callback handler integrates LangWatch tracing with Bridgic framework, providing step-level tracing for worker execution and automa orchestration. It tracks worker execution, creates spans for each worker, and manages trace lifecycle for top-level automa instances.</p> <p>Parameters:</p> Name Type Description Default <code>api_key</code> <code>Optional[str]</code> <p>The API key for the LangWatch tracing service, if none is provided,  the <code>LANGWATCH_API_KEY</code> environment variable will be used.</p> <code>None</code> <code>endpoint_url</code> <code>Optional[str]</code> <p>The URL of the LangWatch tracing service, if none is provided,  the <code>LANGWATCH_ENDPOINT</code> environment variable will be used. If that is not provided, the default value will be https://app.langwatch.ai.</p> <code>None</code> <code>base_attributes</code> <code>Optional[BaseAttributes]</code> <p>The base attributes to use for the LangWatch tracing client.</p> <code>None</code> Notes <p>Since tracing requires the execution within an automa to establish the corresponding record root, only global configurations (via <code>GlobalSetting</code>) and automa-level configurations (via <code>RunningOptions</code>) will take effect.  In other words, if you set the callback by using <code>@worker</code> or <code>add_worker</code>, it will not work.</p> Source code in <code>bridgic/traces/langwatch/_langwatch_trace_callback.py</code> <pre><code>class LangWatchTraceCallback(WorkerCallback):\n    \"\"\"\n    LangWatch tracing callback handler for Bridgic.\n\n    This callback handler integrates LangWatch tracing with Bridgic framework,\n    providing step-level tracing for worker execution and automa orchestration.\n    It tracks worker execution, creates spans for each worker, and manages\n    trace lifecycle for top-level automa instances.\n\n    Parameters\n    ----------\n    api_key : Optional[str], default=None\n        The API key for the LangWatch tracing service, if none is provided, \n        the `LANGWATCH_API_KEY` environment variable will be used.\n    endpoint_url : Optional[str], default=None\n        The URL of the LangWatch tracing service, if none is provided, \n        the `LANGWATCH_ENDPOINT` environment variable will be used. If that is not provided, the default value will be https://app.langwatch.ai.\n    base_attributes : Optional[BaseAttributes], default=None\n        The base attributes to use for the LangWatch tracing client.\n\n    Notes\n    ------\n    Since tracing requires the execution within an automa to establish the corresponding record root,\n    only global configurations (via `GlobalSetting`) and automa-level configurations (via `RunningOptions`) will take effect. \n    In other words, if you set the callback by using `@worker` or `add_worker`, it will not work.\n    \"\"\"\n\n    _api_key: Optional[str]\n    _endpoint_url: Optional[str]\n    _base_attributes: BaseAttributes\n    _is_ready: bool\n    _current_trace: ContextVar[Optional[LangWatchTrace]]\n    _current_span_stack: ContextVar[Tuple[LangWatchSpan, ...]]\n\n    def __init__(\n        self,\n        api_key: Optional[str] = None,\n        endpoint_url: Optional[str] = None,\n        base_attributes: Optional[BaseAttributes] = None,\n    ):\n        super().__init__()\n        self._is_ready = False\n        self._api_key = api_key\n        self._endpoint_url = endpoint_url\n        self._base_attributes = base_attributes\n        self._current_trace = ContextVar(\n            \"langwatch_current_trace\", default=None\n        )\n        self._current_span_stack = ContextVar(\n            \"langwatch_current_span_stack\", default=()\n        )\n        self._setup_langwatch()\n\n    def _setup_langwatch(self) -&gt; None:\n        \"\"\"\n        Initialize LangWatch and mark the callback as ready if configuration succeeds.\n        \"\"\"\n        try:\n            if get_instance() is None or self._api_key != get_instance().api_key:\n                langwatch.setup(api_key=self._api_key, endpoint_url=self._endpoint_url, base_attributes=self._base_attributes)\n        except Exception as exc:\n            self._is_ready = False\n            warnings.warn(f\"LangWatch setup failed, callback disabled: {exc}\")\n        else:\n            self._is_ready = True\n\n    def _stringify_value(self, value: Any) -&gt; str:\n        \"\"\"Serialize a value into a JSON string, falling back to str() when needed.\"\"\"\n        try:\n            return json.dumps(value, default=str)\n        except TypeError:\n            return str(value)\n\n    def _normalize_attribute_value(self, value: Any) -&gt; Any:\n        \"\"\"\n        Normalize arbitrary attribute values into LangWatch-safe primitives.\n\n        Attempts serialization through `serialize_data` and gracefully falls back to\n        JSON strings when complex structures remain.\n        \"\"\"\n        primitive_types = (str, bool, int, float, bytes)\n\n        if isinstance(value, primitive_types) or value is None:\n            return value\n\n        if isinstance(value, Sequence) and not isinstance(\n            value, (str, bytes, bytearray)\n        ):\n            normalized = []\n            for item in value:\n                normalized.append(self._normalize_attribute_value(item))\n            return normalized\n\n        serialized = serialize_data(value)\n        if isinstance(serialized, primitive_types) or serialized is None:\n            return serialized\n        if isinstance(serialized, Sequence) and not isinstance(\n            serialized, (str, bytes, bytearray)\n        ):\n            normalized = []\n            for item in serialized:\n                normalized.append(self._normalize_attribute_value(item))\n            return normalized\n\n        return self._stringify_value(serialized)\n\n    async def _complete_trace(\n        self,\n        output: Dict[str, Any],\n        error: Optional[Exception] = None,\n    ) -&gt; None:\n        \"\"\"Finalize the active trace context with output and optional error.\"\"\"\n        trace_data = self._current_trace.get()\n        if trace_data is None:\n            return\n        self._current_trace.set(None)\n        try:\n            trace_data.update(output=output, error=error)\n        finally:\n            await trace_data.__aexit__(\n                type(error) if error else None,\n                error,\n                error.__traceback__ if error else None,\n            )\n\n    async def _finish_worker_span(\n        self,\n        output: Dict[str, Any],\n        error: Optional[Exception] = None,\n    ) -&gt; None:\n        \"\"\"Finalize the current worker span and propagate outputs/errors.\"\"\"\n        stack = self._current_span_stack.get()\n        if not stack:\n            warnings.warn(\n                \"No active LangWatch span context found when finishing worker span\"\n            )\n            return\n        span_data = stack[-1]\n        self._current_span_stack.set(stack[:-1])\n\n        try:\n            span_data.update(output=output, error=error)\n        except Exception:\n            warnings.warn(\"Failed to update LangWatch span when finishing worker span\")\n        await span_data.__aexit__(\n            type(error) if error else None,\n            error,\n            error.__traceback__ if error else None,\n        )\n\n    def _build_output_payload(\n        self,\n        result: Any = None,\n        error: Optional[Exception] = None,\n    ) -&gt; Dict[str, Any]:\n        \"\"\"Create a normalized payload for either successful results or errors.\"\"\"\n        if error:\n            return {\"error_type\": type(error).__name__, \"error_message\": str(error)}\n        return {\n            \"result_type\": type(result).__name__ if result is not None else None,\n            \"result\": serialize_data(result),\n        }\n\n    async def _start_worker_span(\n        self,\n        key: str,\n        worker: \"Worker\",\n        parent: Automa,\n        arguments: Optional[Dict[str, Any]],\n    ) -&gt; None:\n        \"\"\"\n        Start a LangWatch span for the worker execution using normalized metadata.\n        \"\"\"\n        step_name = get_worker_tracing_step_name(key, worker)\n        worker_tracing_dict = build_worker_tracing_dict(worker, parent)\n        normalized_worker_tracing = {\n            key: self._normalize_attribute_value(value)\n            for key, value in worker_tracing_dict.items()\n        }\n        serialized_args = serialize_data(arguments)\n\n        # LangWatch refers to span metadata as \"attributes\".\n        span = langwatch.span(\n            name=step_name,\n            input=serialized_args,\n            type=\"span\",\n            attributes={\n                **normalized_worker_tracing,\n                # TODO: Investigate why LangWatch coerces integers into dict form; keep string for now.\n                \"nesting_level\": str(worker_tracing_dict[\"nesting_level\"]),\n            },\n        )\n        await span.__aenter__()\n        stack = self._current_span_stack.get()\n        self._current_span_stack.set((*stack, span))\n\n    async def _start_top_level_trace(self, key: str, arguments: Optional[Dict[str, Any]]) -&gt; None:\n        serialized_args = serialize_data(arguments)\n        trace_metadata = {\n            \"created_from\": \"bridgic\", \n            \"key\": key, \n            \"nesting_level\": \"0\",\n        }\n        trace_data = langwatch.trace(\n            name=key or \"top_level_automa\",\n            input=serialized_args,\n            metadata=trace_metadata,\n            type=\"span\",\n        )\n        await trace_data.__aenter__()\n        self._current_trace.set(trace_data)\n\n    def _get_worker_instance(self, key: str, parent: Optional[Automa]) -&gt; Worker:\n        \"\"\"\n        Get worker instance from parent automa.\n\n        Returns\n        -------\n        Worker\n            The worker instance.\n        \"\"\"\n        if parent is None:\n            raise ValueError(\"Parent automa is required to get worker instance\")\n        return parent._get_worker_instance(key)\n\n    async def on_worker_start(\n        self,\n        key: str,\n        is_top_level: bool = False,\n        parent: Optional[Automa] = None,\n        arguments: Optional[Dict[str, Any]] = None,\n    ) -&gt; None:\n        \"\"\"\n        Hook invoked before worker execution.\n\n        For top-level automa, initializes a new trace. For workers, creates\n        a new span. Handles nested automa as workers by checking if the\n        decorated worker is an automa instance.\n\n        Parameters\n        ----------\n        key : str\n            Worker identifier.\n        is_top_level : bool, default=False\n            Whether the worker is the top-level automa. When True, parent will be the automa itself (parent is self).\n        parent : Optional[Automa], default=None\n            Parent automa instance containing this worker. For top-level automa, parent is the automa itself.\n        arguments : Optional[Dict[str, Any]], default=None\n            Execution arguments with keys \"args\" and \"kwargs\".\n        \"\"\"\n        if not self._is_ready:\n            return\n\n        if is_top_level:\n            await self._start_top_level_trace(key, arguments)\n            return\n\n        try:\n            worker = self._get_worker_instance(key, parent)\n        except (KeyError, ValueError) as e:\n            warnings.warn(f\"Failed to get worker instance for key '{key}': {e}\")\n            return\n\n        await self._start_worker_span(key, worker, parent, arguments)\n\n    async def _complete_worker_execution(\n        self,\n        output: Dict[str, Any],\n        is_top_level: bool,\n        error: Optional[Exception] = None,\n    ) -&gt; None:\n        if is_top_level:\n            await self._complete_trace(output, error)\n        else:\n            await self._finish_worker_span(output, error)\n\n    async def on_worker_end(\n        self,\n        key: str,\n        is_top_level: bool = False,\n        parent: Optional[Automa] = None,\n        arguments: Optional[Dict[str, Any]] = None,\n        result: Any = None,\n    ) -&gt; None:\n        \"\"\"\n        Hook invoked after worker execution.\n\n        For top-level automa, ends the trace. For workers, ends the span\n        with execution results.\n\n        Parameters\n        ----------\n        key : str\n            Worker identifier.\n        is_top_level : bool, default=False\n            Whether the worker is the top-level automa. When True, parent will be the automa itself (parent is self).\n        parent : Optional[Automa], default=None\n            Parent automa instance containing this worker. For top-level automa, parent is the automa itself.\n        arguments : Optional[Dict[str, Any]], default=None\n            Execution arguments with keys \"args\" and \"kwargs\".\n        result : Any, default=None\n            Worker execution result.\n        \"\"\"\n        if not self._is_ready:\n            return\n        output = self._build_output_payload(result=result)\n        await self._complete_worker_execution(output, is_top_level)\n\n    async def on_worker_error(\n        self,\n        key: str,\n        is_top_level: bool = False,\n        parent: Optional[Automa] = None,\n        arguments: Optional[Dict[str, Any]] = None,\n        error: Exception = None,\n    ) -&gt; bool:\n        \"\"\"\n        Hook invoked when worker execution raises an exception.\n\n        For top-level automa, ends the trace with error information.\n        For workers, ends the span with error information.\n\n        Parameters\n        ----------\n        key : str\n            Worker identifier.\n        is_top_level : bool, default=False\n            Whether the worker is the top-level automa. When True, parent will be the automa itself (parent is self).\n        parent : Optional[Automa], default=None\n            Parent automa instance containing this worker. For top-level automa, parent is the automa itself.\n        arguments : Optional[Dict[str, Any]], default=None\n            Execution arguments with keys \"args\" and \"kwargs\".\n        error : Exception, default=None\n            The exception raised during worker execution.\n\n        Returns\n        -------\n        bool\n            Always returns False, indicating the exception should not be suppressed.\n        \"\"\"\n        if not self._is_ready:\n            return False\n        output = self._build_output_payload(error=error)\n        await self._complete_worker_execution(output, is_top_level, error=error)\n        return False\n\n    def dump_to_dict(self) -&gt; Dict[str, Any]:\n        state_dict = super().dump_to_dict()\n        state_dict[\"api_key\"] = self._api_key\n        state_dict[\"endpoint_url\"] = self._endpoint_url\n        state_dict[\"base_attributes\"] = self._base_attributes\n        return state_dict\n\n    def load_from_dict(self, state_dict: Dict[str, Any]) -&gt; None:\n        super().load_from_dict(state_dict)\n        self._api_key = state_dict.get(\"api_key\")\n        self._endpoint_url = state_dict.get(\"endpoint_url\")\n        self._base_attributes = state_dict.get(\"base_attributes\")\n        self._current_trace = ContextVar(\n            \"langwatch_current_trace\", default=None\n        )\n        self._current_span_stack = ContextVar(\n            \"langwatch_current_span_stack\", default=()\n        )\n        self._setup_langwatch()\n</code></pre>"},{"location":"reference/bridgic-traces-langwatch/bridgic/traces/langwatch/#bridgic.traces.langwatch.LangWatchTraceCallback.on_worker_start","title":"on_worker_start","text":"<code>async</code> <pre><code>on_worker_start(\n    key: str,\n    is_top_level: bool = False,\n    parent: Optional[Automa] = None,\n    arguments: Optional[Dict[str, Any]] = None,\n) -&gt; None\n</code></pre> <p>Hook invoked before worker execution.</p> <p>For top-level automa, initializes a new trace. For workers, creates a new span. Handles nested automa as workers by checking if the decorated worker is an automa instance.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>Worker identifier.</p> required <code>is_top_level</code> <code>bool</code> <p>Whether the worker is the top-level automa. When True, parent will be the automa itself (parent is self).</p> <code>False</code> <code>parent</code> <code>Optional[Automa]</code> <p>Parent automa instance containing this worker. For top-level automa, parent is the automa itself.</p> <code>None</code> <code>arguments</code> <code>Optional[Dict[str, Any]]</code> <p>Execution arguments with keys \"args\" and \"kwargs\".</p> <code>None</code> Source code in <code>bridgic/traces/langwatch/_langwatch_trace_callback.py</code> <pre><code>async def on_worker_start(\n    self,\n    key: str,\n    is_top_level: bool = False,\n    parent: Optional[Automa] = None,\n    arguments: Optional[Dict[str, Any]] = None,\n) -&gt; None:\n    \"\"\"\n    Hook invoked before worker execution.\n\n    For top-level automa, initializes a new trace. For workers, creates\n    a new span. Handles nested automa as workers by checking if the\n    decorated worker is an automa instance.\n\n    Parameters\n    ----------\n    key : str\n        Worker identifier.\n    is_top_level : bool, default=False\n        Whether the worker is the top-level automa. When True, parent will be the automa itself (parent is self).\n    parent : Optional[Automa], default=None\n        Parent automa instance containing this worker. For top-level automa, parent is the automa itself.\n    arguments : Optional[Dict[str, Any]], default=None\n        Execution arguments with keys \"args\" and \"kwargs\".\n    \"\"\"\n    if not self._is_ready:\n        return\n\n    if is_top_level:\n        await self._start_top_level_trace(key, arguments)\n        return\n\n    try:\n        worker = self._get_worker_instance(key, parent)\n    except (KeyError, ValueError) as e:\n        warnings.warn(f\"Failed to get worker instance for key '{key}': {e}\")\n        return\n\n    await self._start_worker_span(key, worker, parent, arguments)\n</code></pre>"},{"location":"reference/bridgic-traces-langwatch/bridgic/traces/langwatch/#bridgic.traces.langwatch.LangWatchTraceCallback.on_worker_end","title":"on_worker_end","text":"<code>async</code> <pre><code>on_worker_end(\n    key: str,\n    is_top_level: bool = False,\n    parent: Optional[Automa] = None,\n    arguments: Optional[Dict[str, Any]] = None,\n    result: Any = None,\n) -&gt; None\n</code></pre> <p>Hook invoked after worker execution.</p> <p>For top-level automa, ends the trace. For workers, ends the span with execution results.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>Worker identifier.</p> required <code>is_top_level</code> <code>bool</code> <p>Whether the worker is the top-level automa. When True, parent will be the automa itself (parent is self).</p> <code>False</code> <code>parent</code> <code>Optional[Automa]</code> <p>Parent automa instance containing this worker. For top-level automa, parent is the automa itself.</p> <code>None</code> <code>arguments</code> <code>Optional[Dict[str, Any]]</code> <p>Execution arguments with keys \"args\" and \"kwargs\".</p> <code>None</code> <code>result</code> <code>Any</code> <p>Worker execution result.</p> <code>None</code> Source code in <code>bridgic/traces/langwatch/_langwatch_trace_callback.py</code> <pre><code>async def on_worker_end(\n    self,\n    key: str,\n    is_top_level: bool = False,\n    parent: Optional[Automa] = None,\n    arguments: Optional[Dict[str, Any]] = None,\n    result: Any = None,\n) -&gt; None:\n    \"\"\"\n    Hook invoked after worker execution.\n\n    For top-level automa, ends the trace. For workers, ends the span\n    with execution results.\n\n    Parameters\n    ----------\n    key : str\n        Worker identifier.\n    is_top_level : bool, default=False\n        Whether the worker is the top-level automa. When True, parent will be the automa itself (parent is self).\n    parent : Optional[Automa], default=None\n        Parent automa instance containing this worker. For top-level automa, parent is the automa itself.\n    arguments : Optional[Dict[str, Any]], default=None\n        Execution arguments with keys \"args\" and \"kwargs\".\n    result : Any, default=None\n        Worker execution result.\n    \"\"\"\n    if not self._is_ready:\n        return\n    output = self._build_output_payload(result=result)\n    await self._complete_worker_execution(output, is_top_level)\n</code></pre>"},{"location":"reference/bridgic-traces-langwatch/bridgic/traces/langwatch/#bridgic.traces.langwatch.LangWatchTraceCallback.on_worker_error","title":"on_worker_error","text":"<code>async</code> <pre><code>on_worker_error(\n    key: str,\n    is_top_level: bool = False,\n    parent: Optional[Automa] = None,\n    arguments: Optional[Dict[str, Any]] = None,\n    error: Exception = None,\n) -&gt; bool\n</code></pre> <p>Hook invoked when worker execution raises an exception.</p> <p>For top-level automa, ends the trace with error information. For workers, ends the span with error information.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>Worker identifier.</p> required <code>is_top_level</code> <code>bool</code> <p>Whether the worker is the top-level automa. When True, parent will be the automa itself (parent is self).</p> <code>False</code> <code>parent</code> <code>Optional[Automa]</code> <p>Parent automa instance containing this worker. For top-level automa, parent is the automa itself.</p> <code>None</code> <code>arguments</code> <code>Optional[Dict[str, Any]]</code> <p>Execution arguments with keys \"args\" and \"kwargs\".</p> <code>None</code> <code>error</code> <code>Exception</code> <p>The exception raised during worker execution.</p> <code>None</code> <p>Returns:</p> Type Description <code>bool</code> <p>Always returns False, indicating the exception should not be suppressed.</p> Source code in <code>bridgic/traces/langwatch/_langwatch_trace_callback.py</code> <pre><code>async def on_worker_error(\n    self,\n    key: str,\n    is_top_level: bool = False,\n    parent: Optional[Automa] = None,\n    arguments: Optional[Dict[str, Any]] = None,\n    error: Exception = None,\n) -&gt; bool:\n    \"\"\"\n    Hook invoked when worker execution raises an exception.\n\n    For top-level automa, ends the trace with error information.\n    For workers, ends the span with error information.\n\n    Parameters\n    ----------\n    key : str\n        Worker identifier.\n    is_top_level : bool, default=False\n        Whether the worker is the top-level automa. When True, parent will be the automa itself (parent is self).\n    parent : Optional[Automa], default=None\n        Parent automa instance containing this worker. For top-level automa, parent is the automa itself.\n    arguments : Optional[Dict[str, Any]], default=None\n        Execution arguments with keys \"args\" and \"kwargs\".\n    error : Exception, default=None\n        The exception raised during worker execution.\n\n    Returns\n    -------\n    bool\n        Always returns False, indicating the exception should not be suppressed.\n    \"\"\"\n    if not self._is_ready:\n        return False\n    output = self._build_output_payload(error=error)\n    await self._complete_worker_execution(output, is_top_level, error=error)\n    return False\n</code></pre>"},{"location":"reference/bridgic-traces-langwatch/bridgic/traces/langwatch/#bridgic.traces.langwatch.start_langwatch_trace","title":"start_langwatch_trace","text":"<pre><code>start_langwatch_trace(\n    api_key: Optional[str] = None,\n    endpoint_url: Optional[str] = None,\n    base_attributes: Optional[BaseAttributes] = None,\n) -&gt; None\n</code></pre> <p>Start a LangWatch trace for a given project and service.</p> <p>Parameters:</p> Name Type Description Default <code>api_key</code> <code>Optional[str]</code> <p>The API key for the LangWatch tracing service, if none is provided, the <code>LANGWATCH_API_KEY</code> environment variable will be used.</p> <code>None</code> <code>endpoint_url</code> <code>Optional[str]</code> <p>The URL of the LangWatch tracing service, if none is provided, the <code>LANGWATCH_ENDPOINT</code> environment variable will be used. If that is not provided, the default value will be <code>https://app.langwatch.ai</code>.</p> <code>None</code> <code>base_attributes</code> <code>Optional[BaseAttributes]</code> <p>The base attributes to use for the LangWatch tracing client.</p> <code>None</code> <p>Returns:</p> Type Description <code>None</code> Source code in <code>bridgic/traces/langwatch/_utils.py</code> <pre><code>def start_langwatch_trace(\n    api_key: Optional[str] = None,\n    endpoint_url: Optional[str] = None,\n    base_attributes: Optional[BaseAttributes] = None,\n) -&gt; None:\n    \"\"\"Start a LangWatch trace for a given project and service.\n\n    Parameters\n    ----------\n    api_key : Optional[str], default=None\n        The API key for the LangWatch tracing service, if none is provided, the `LANGWATCH_API_KEY` environment variable will be used.\n    endpoint_url : Optional[str], default=None\n        The URL of the LangWatch tracing service, if none is provided, the `LANGWATCH_ENDPOINT` environment variable will be used. If that is not provided, the default value will be `https://app.langwatch.ai`.\n    base_attributes : Optional[BaseAttributes], default=None\n        The base attributes to use for the LangWatch tracing client.\n\n    Returns\n    -------\n    None\n    \"\"\"\n    from bridgic.core.config import GlobalSetting\n    builder = WorkerCallbackBuilder(\n        LangWatchTraceCallback, \n        init_kwargs={\"api_key\": api_key, \"endpoint_url\": endpoint_url, \"base_attributes\": base_attributes}\n    )\n    GlobalSetting.add(callback_builder=builder)\n</code></pre>"},{"location":"reference/bridgic-traces-opik/bridgic/traces/opik/","title":"bridgic.traces.opik","text":""},{"location":"reference/bridgic-traces-opik/bridgic/traces/opik/#opik","title":"opik","text":""},{"location":"reference/bridgic-traces-opik/bridgic/traces/opik/#bridgic.traces.opik.OpikTraceCallback","title":"OpikTraceCallback","text":"<p>               Bases: <code>WorkerCallback</code></p> <p>Opik tracing callback handler for Bridgic.</p> <p>This callback handler integrates Opik tracing with Bridgic framework, providing step-level tracing for worker execution and automa orchestration. It tracks worker execution, creates spans for each worker, and manages trace lifecycle for top-level automa instances.</p> <p>Parameters:</p> Name Type Description Default <code>project_name</code> <code>Optional[str]</code> <p>The name of the project. If None, uses <code>Default Project</code> project name.</p> <code>None</code> <code>workspace</code> <code>Optional[str]</code> <p>The name of the workspace. If None, uses <code>default</code> workspace name.</p> <code>None</code> <code>host</code> <code>Optional[str]</code> <p>The host URL for the Opik server. If None, it will default to <code>https://www.comet.com/opik/api</code>.</p> <code>None</code> <code>api_key</code> <code>Optional[str]</code> <p>The API key for Opik. This parameter is ignored for local installations.</p> <code>None</code> <code>use_local</code> <code>bool</code> <p>Whether to use local Opik server.</p> <code>False</code> Notes <p>Since tracing requires the execution within an automa to establish the corresponding record root, only global configurations (via <code>GlobalSetting</code>) and automa-level configurations (via <code>RunningOptions</code>) will take effect.  In other words, if you set the callback by using <code>@worker</code> or <code>add_worker</code>, it will not work.</p> <p>Examples:</p> <p>If you want to report tracking information to the self-hosted Opik service, you can initialize the callback instance like this: </p><pre><code>OpikTraceCallback(project_name=\"my-project\", use_local=True)\n</code></pre> <p>If you want to report tracking information to the Opik Cloud service, you can initialize the callback instance like this: </p><pre><code>OpikTraceCallback(project_name=\"my-project\", api_key=\"my-api-key\")\n</code></pre> Source code in <code>bridgic/traces/opik/_opik_trace_callback.py</code> <pre><code>class OpikTraceCallback(WorkerCallback):\n    \"\"\"\n    Opik tracing callback handler for Bridgic.\n\n    This callback handler integrates Opik tracing with Bridgic framework,\n    providing step-level tracing for worker execution and automa orchestration.\n    It tracks worker execution, creates spans for each worker, and manages\n    trace lifecycle for top-level automa instances.\n\n    Parameters\n    ----------\n    project_name : Optional[str], default=None\n        The name of the project. If None, uses `Default Project` project name.\n    workspace : Optional[str], default=None\n        The name of the workspace. If None, uses `default` workspace name.\n    host : Optional[str], default=None\n        The host URL for the Opik server. If None, it will default to `https://www.comet.com/opik/api`.\n    api_key : Optional[str], default=None\n        The API key for Opik. This parameter is ignored for local installations.\n    use_local : bool, default=False\n        Whether to use local Opik server.\n\n    Notes\n    ------\n    Since tracing requires the execution within an automa to establish the corresponding record root,\n    only global configurations (via `GlobalSetting`) and automa-level configurations (via `RunningOptions`) will take effect. \n    In other words, if you set the callback by using `@worker` or `add_worker`, it will not work.\n\n    Examples\n    ------\n    If you want to report tracking information to the self-hosted Opik service, you can initialize the callback instance like this:\n    ```python\n    OpikTraceCallback(project_name=\"my-project\", use_local=True)\n    ```\n\n    If you want to report tracking information to the Opik Cloud service, you can initialize the callback instance like this:\n    ```python\n    OpikTraceCallback(project_name=\"my-project\", api_key=\"my-api-key\")\n    ```\n    \"\"\"\n\n    _project_name: Optional[str]\n    _workspace: Optional[str]\n    _is_ready: bool\n    _api_key: Optional[str]\n    _host: Optional[str]\n    _use_local: bool\n    _opik_client: opik_client.Opik\n\n    def __init__(\n        self,\n        project_name: Optional[str] = None,\n        workspace: Optional[str] = None,\n        host: Optional[str] = None,\n        api_key: Optional[str] = None,\n        use_local: bool = False,\n    ):\n        super().__init__()\n        self._project_name = project_name\n        self._workspace = workspace\n        self._api_key = api_key\n        self._host = host\n        self._use_local = use_local\n        self._is_ready = False\n        self._setup_opik()\n\n    def _setup_opik(self) -&gt; None:\n        if self._use_local:\n            opik.configure(use_local=True)\n        self._opik_client = opik_client.Opik(_use_batching=True, project_name=self._project_name, workspace=self._workspace, api_key=self._api_key, host=self._host)\n        missing_configuration, _ = self._opik_client._config.get_misconfiguration_detection_results()\n        if missing_configuration:\n            self._is_ready = False # for serialization compatibility\n            return\n        self._check_opik_auth()\n\n    def _check_opik_auth(self) -&gt; None:\n        try:\n            self._opik_client.auth_check()\n        except Exception as e:\n            self._is_ready = False # for serialization compatibility\n            warnings.warn(f\"Opik auth check failed, OpikTracer will be disabled: {e}\")\n        else:\n            self._is_ready = True\n\n    def _get_worker_instance(self, key: str, parent: Optional[Automa]) -&gt; Worker:\n        \"\"\"\n        Get worker instance from parent automa.\n\n        Returns\n        -------\n        Worker\n            The worker instance.\n        \"\"\"\n        if parent is None:\n            raise ValueError(\"Parent automa is required to get worker instance\")\n        return parent._get_worker_instance(key)\n\n    def _create_trace_data(self, trace_name: Optional[str] = None) -&gt; trace.TraceData:\n        return trace.TraceData(\n            name=trace_name, \n            metadata={\"created_from\": \"bridgic\"}, \n            project_name=self._project_name\n        )\n\n    def _get_or_create_trace_data(self, trace_name: Optional[str] = None) -&gt; trace.TraceData:\n        \"\"\"Initialize or reuse existing trace.\"\"\"\n        existing_trace = opik_context_storage.get_trace_data()\n        if existing_trace:\n            return existing_trace\n\n        # Create new trace and set in context\n        trace_data = self._create_trace_data(trace_name)\n        opik_context_storage.set_trace_data(trace_data)\n\n        if self._opik_client.config.log_start_trace_span:\n            self._opik_client.trace(**trace_data.as_start_parameters)\n        return trace_data\n\n    def _complete_trace(self, output: Optional[Dict[str, Any]], error_info: Optional[ErrorInfoDict]) -&gt; None:\n        \"\"\"Finalize and log trace we own.\"\"\"\n        trace_data = opik_context_storage.get_trace_data()\n        if trace_data is None:\n            return\n\n        trace_data.init_end_time()\n\n        # Compute execution duration from trace start_time\n        if trace_data.start_time:\n            end_time = trace_data.end_time.timestamp() if trace_data.end_time else time.time()\n            start_time = trace_data.start_time.timestamp()\n            trace_data.metadata = merge_optional_dicts(\n                trace_data.metadata,\n                {\"execution_duration\": end_time - start_time, \"end_time\": end_time}\n            )\n\n        if output:\n            trace_data.update(output=output)\n\n        if error_info:\n            trace_data.update(error_info=error_info)\n\n        self._opik_client.trace(**trace_data.as_parameters)\n        opik_context_storage.pop_trace_data(ensure_id=trace_data.id)\n        self._flush()\n\n    def _start_span(\n        self,\n        step_name: str,\n        inputs: Optional[Dict[str, Any]] = None,\n        metadata: Optional[Dict[str, Any]] = None,\n    ) -&gt; None:\n        \"\"\"Start a span for a worker execution step and push it to context.\"\"\"\n        trace_data = opik_context_storage.get_trace_data()\n\n        parent_span = opik_context_storage.top_span_data()\n\n        project_name = helpers.resolve_child_span_project_name(\n            parent_project_name=trace_data.project_name,\n            child_project_name=self._project_name,\n            show_warning=True,\n        )\n\n        span_data = span.SpanData(\n            trace_id=trace_data.id,\n            name=step_name,\n            parent_span_id=parent_span.id if parent_span else None,\n            input=inputs,\n            metadata=metadata,\n            project_name=project_name,\n        )\n        # Store start_time in metadata for later duration calculation\n        if span_data.start_time and metadata is not None:\n            metadata[\"start_time\"] = span_data.start_time.timestamp()\n            span_data.update(metadata=metadata)\n        # Add span to context stack\n        opik_context_storage.add_span_data(span_data)\n\n        if self._opik_client.config.log_start_trace_span:\n            self._opik_client.span(**span_data.as_start_parameters)\n\n    def _finish_span(self, span_data: span.SpanData, worker_metadata: Optional[Dict[str, Any]] = None) -&gt; None:\n        \"\"\"Finish a worker span with metadata and output, then pop from context.\"\"\"\n        if worker_metadata:\n            output = worker_metadata.get(\"output\")\n            # Merge all metadata except 'output' into span metadata\n            current_metadata = span_data.metadata or {}\n            current_metadata.update({k: v for k, v in worker_metadata.items() if k != \"output\"})\n            span_data.update(metadata=current_metadata)\n\n            if output is not None:\n                span_data.update(output=output)\n\n        span_data.init_end_time()\n        self._opik_client.span(**span_data.as_parameters)\n\n        # Pop span from context stack\n        opik_context_storage.pop_span_data(ensure_id=span_data.id)\n\n    def _start_top_level_trace(self, key: str, arguments: Optional[Dict[str, Any]]) -&gt; None:\n        \"\"\"Start trace initialization for top-level automa.\"\"\"\n        trace_data = self._get_or_create_trace_data(trace_name=key or \"top_level_automa\")\n\n        serialized_args = serialize_data(arguments)\n        metadata_updates = {\"key\": key, \"nesting_level\": 0}\n        if trace_data.start_time:\n            metadata_updates[\"start_time\"] = trace_data.start_time.timestamp()\n\n        trace_data.metadata = merge_optional_dicts(trace_data.metadata, metadata_updates)\n\n        if serialized_args:\n            trace_data.input = serialized_args\n\n    def _start_worker_span(self, key: str, worker: Worker, parent: Automa, arguments: Optional[Dict[str, Any]]) -&gt; None:\n        \"\"\"Start a span for worker execution.\"\"\"\n        step_name = get_worker_tracing_step_name(key, worker)\n        worker_tracing_dict = build_worker_tracing_dict(worker, parent)\n        self._start_span(\n            step_name=step_name,\n            inputs=serialize_data(arguments),\n            metadata=worker_tracing_dict,\n        )\n\n    async def on_worker_start(\n        self,\n        key: str,\n        is_top_level: bool = False,\n        parent: Optional[Automa] = None,\n        arguments: Optional[Dict[str, Any]] = None,\n    ) -&gt; None:\n        \"\"\"\n        Hook invoked before worker execution.\n\n        For top-level automa, initializes a new trace. For workers, creates\n        a new span. Handles nested automa as workers by checking if the\n        decorated worker is an automa instance.\n\n        Parameters\n        ----------\n        key : str\n            Worker identifier.\n        is_top_level : bool, default=False\n            Whether the worker is the top-level automa. When True, parent will be the automa itself (parent is self).\n        parent : Optional[Automa], default=None\n            Parent automa instance containing this worker. For top-level automa, parent is the automa itself.\n        arguments : Optional[Dict[str, Any]], default=None\n            Execution arguments with keys \"args\" and \"kwargs\".\n        \"\"\"\n        if not self._is_ready:\n            return\n        if is_top_level:\n            self._start_top_level_trace(key, arguments)\n            return\n\n        try:\n            worker = self._get_worker_instance(key, parent)\n        except (KeyError, ValueError) as e:\n            warnings.warn(f\"Failed to get worker instance for key '{key}': {e}\")\n            return\n\n        self._start_worker_span(key, worker, parent, arguments)\n\n    def _finish_current_span(self, output: Dict[str, Any], error: Optional[Exception] = None) -&gt; None:\n        \"\"\"Finish the current span and pop it from context.\"\"\"\n        current_span = opik_context_storage.top_span_data()\n        if not current_span:\n            warnings.warn(\"No span found in context when finishing worker span\")\n            return\n\n        # Calculate execution timing\n        end_time = time.time()\n        start_time = current_span.start_time.timestamp() if current_span.start_time else end_time\n\n        # Build worker metadata with timing and output\n        worker_metadata = {\n            \"end_time\": end_time,\n            \"execution_duration\": end_time - start_time,\n            \"output\": serialize_data(output),\n        }\n\n        # Handle error if present\n        if error:\n            error_info = error_info_collector.collect(error)\n            if error_info:\n                current_span.update(error_info=error_info)\n\n        # Finish the span (this will merge metadata and pop from context)\n        self._finish_span(current_span, worker_metadata=worker_metadata)\n\n    def _build_output_payload(self, result: Any = None, error: Optional[Exception] = None) -&gt; Dict[str, Any]:\n        \"\"\"Build a standardized output dictionary for results or errors.\"\"\"\n        if error:\n            return {\"error_type\": type(error).__name__, \"error_message\": str(error)}\n        return {\n            \"result_type\": type(result).__name__ if result is not None else None,\n            \"result\": serialize_data(result),\n        }\n\n    def _complete_worker_execution(self, output: Dict[str, Any], is_top_level: bool, error: Optional[Exception] = None) -&gt; None:\n        \"\"\"Complete worker or trace execution.\"\"\"\n        if is_top_level:\n            trace_data = opik_context_storage.get_trace_data()\n            if trace_data:\n                execution_status = \"failed\" if error else \"completed\"\n                trace_data.metadata = merge_optional_dicts(\n                    trace_data.metadata, {\"execution_status\": execution_status}\n                )\n\n            error_info = error_info_collector.collect(error) if error else None\n            self._complete_trace(output, error_info)\n        else:\n            self._finish_current_span(output=output, error=error)\n\n    async def on_worker_end(\n        self,\n        key: str,\n        is_top_level: bool = False,\n        parent: Optional[Automa] = None,\n        arguments: Optional[Dict[str, Any]] = None,\n        result: Any = None,\n    ) -&gt; None:\n        \"\"\"\n        Hook invoked after worker execution.\n\n        For top-level automa, ends the trace. For workers, ends the span\n        with execution results.\n\n        Parameters\n        ----------\n        key : str\n            Worker identifier.\n        is_top_level : bool, default=False\n            Whether the worker is the top-level automa. When True, parent will be the automa itself (parent is self).\n        parent : Optional[Automa], default=None\n            Parent automa instance containing this worker. For top-level automa, parent is the automa itself.\n        arguments : Optional[Dict[str, Any]], default=None\n            Execution arguments with keys \"args\" and \"kwargs\".\n        result : Any, default=None\n            Worker execution result.\n        \"\"\"\n        if not self._is_ready:\n            return\n        output = self._build_output_payload(result=result)\n        self._complete_worker_execution(output, is_top_level)\n\n    async def on_worker_error(\n        self,\n        key: str,\n        is_top_level: bool = False,\n        parent: Optional[Automa] = None,\n        arguments: Optional[Dict[str, Any]] = None,\n        error: Exception = None,\n    ) -&gt; bool:\n        \"\"\"\n        Hook invoked when worker execution raises an exception.\n\n        For top-level automa, ends the trace with error information.\n        For workers, ends the span with error information.\n\n        Parameters\n        ----------\n        key : str\n            Worker identifier.\n        is_top_level : bool, default=False\n            Whether the worker is the top-level automa. When True, parent will be the automa itself (parent is self).\n        parent : Optional[Automa], default=None\n            Parent automa instance containing this worker. For top-level automa, parent is the automa itself.\n        arguments : Optional[Dict[str, Any]], default=None\n            Execution arguments with keys \"args\" and \"kwargs\".\n        error : Exception, default=None\n            The exception raised during worker execution.\n\n        Returns\n        -------\n        bool\n            Always returns False, indicating the exception should not be suppressed.\n        \"\"\"\n        if not self._is_ready:\n            return False\n        if not is_top_level and parent:\n            try:\n                self._get_worker_instance(key, parent)\n            except (KeyError, ValueError) as e:\n                warnings.warn(f\"Failed to get worker instance for key '{key}': {e}\")\n                return False\n\n        output = self._build_output_payload(error=error)\n        self._complete_worker_execution(output, is_top_level, error=error)\n        return False\n\n    def _flush(self) -&gt; None:\n        self._opik_client.flush()\n\n    @override\n    def dump_to_dict(self) -&gt; Dict[str, Any]:\n        state_dict = super().dump_to_dict()\n        state_dict[\"project_name\"] = self._project_name\n        state_dict[\"workspace\"] = self._workspace\n        state_dict[\"api_key\"] = self._api_key\n        state_dict[\"host\"] = self._host\n        state_dict[\"use_local\"] = self._use_local\n        return state_dict\n\n    @override\n    def load_from_dict(self, state_dict: Dict[str, Any]) -&gt; None:\n        super().load_from_dict(state_dict)\n        self._project_name = state_dict[\"project_name\"]\n        self._workspace = state_dict[\"workspace\"]\n        self._api_key = state_dict[\"api_key\"]\n        self._host = state_dict[\"host\"]\n        self._use_local = state_dict[\"use_local\"]\n        self._setup_opik() # if opik is not ready, it will be set to False\n</code></pre>"},{"location":"reference/bridgic-traces-opik/bridgic/traces/opik/#bridgic.traces.opik.OpikTraceCallback.on_worker_start","title":"on_worker_start","text":"<code>async</code> <pre><code>on_worker_start(\n    key: str,\n    is_top_level: bool = False,\n    parent: Optional[Automa] = None,\n    arguments: Optional[Dict[str, Any]] = None,\n) -&gt; None\n</code></pre> <p>Hook invoked before worker execution.</p> <p>For top-level automa, initializes a new trace. For workers, creates a new span. Handles nested automa as workers by checking if the decorated worker is an automa instance.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>Worker identifier.</p> required <code>is_top_level</code> <code>bool</code> <p>Whether the worker is the top-level automa. When True, parent will be the automa itself (parent is self).</p> <code>False</code> <code>parent</code> <code>Optional[Automa]</code> <p>Parent automa instance containing this worker. For top-level automa, parent is the automa itself.</p> <code>None</code> <code>arguments</code> <code>Optional[Dict[str, Any]]</code> <p>Execution arguments with keys \"args\" and \"kwargs\".</p> <code>None</code> Source code in <code>bridgic/traces/opik/_opik_trace_callback.py</code> <pre><code>async def on_worker_start(\n    self,\n    key: str,\n    is_top_level: bool = False,\n    parent: Optional[Automa] = None,\n    arguments: Optional[Dict[str, Any]] = None,\n) -&gt; None:\n    \"\"\"\n    Hook invoked before worker execution.\n\n    For top-level automa, initializes a new trace. For workers, creates\n    a new span. Handles nested automa as workers by checking if the\n    decorated worker is an automa instance.\n\n    Parameters\n    ----------\n    key : str\n        Worker identifier.\n    is_top_level : bool, default=False\n        Whether the worker is the top-level automa. When True, parent will be the automa itself (parent is self).\n    parent : Optional[Automa], default=None\n        Parent automa instance containing this worker. For top-level automa, parent is the automa itself.\n    arguments : Optional[Dict[str, Any]], default=None\n        Execution arguments with keys \"args\" and \"kwargs\".\n    \"\"\"\n    if not self._is_ready:\n        return\n    if is_top_level:\n        self._start_top_level_trace(key, arguments)\n        return\n\n    try:\n        worker = self._get_worker_instance(key, parent)\n    except (KeyError, ValueError) as e:\n        warnings.warn(f\"Failed to get worker instance for key '{key}': {e}\")\n        return\n\n    self._start_worker_span(key, worker, parent, arguments)\n</code></pre>"},{"location":"reference/bridgic-traces-opik/bridgic/traces/opik/#bridgic.traces.opik.OpikTraceCallback.on_worker_end","title":"on_worker_end","text":"<code>async</code> <pre><code>on_worker_end(\n    key: str,\n    is_top_level: bool = False,\n    parent: Optional[Automa] = None,\n    arguments: Optional[Dict[str, Any]] = None,\n    result: Any = None,\n) -&gt; None\n</code></pre> <p>Hook invoked after worker execution.</p> <p>For top-level automa, ends the trace. For workers, ends the span with execution results.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>Worker identifier.</p> required <code>is_top_level</code> <code>bool</code> <p>Whether the worker is the top-level automa. When True, parent will be the automa itself (parent is self).</p> <code>False</code> <code>parent</code> <code>Optional[Automa]</code> <p>Parent automa instance containing this worker. For top-level automa, parent is the automa itself.</p> <code>None</code> <code>arguments</code> <code>Optional[Dict[str, Any]]</code> <p>Execution arguments with keys \"args\" and \"kwargs\".</p> <code>None</code> <code>result</code> <code>Any</code> <p>Worker execution result.</p> <code>None</code> Source code in <code>bridgic/traces/opik/_opik_trace_callback.py</code> <pre><code>async def on_worker_end(\n    self,\n    key: str,\n    is_top_level: bool = False,\n    parent: Optional[Automa] = None,\n    arguments: Optional[Dict[str, Any]] = None,\n    result: Any = None,\n) -&gt; None:\n    \"\"\"\n    Hook invoked after worker execution.\n\n    For top-level automa, ends the trace. For workers, ends the span\n    with execution results.\n\n    Parameters\n    ----------\n    key : str\n        Worker identifier.\n    is_top_level : bool, default=False\n        Whether the worker is the top-level automa. When True, parent will be the automa itself (parent is self).\n    parent : Optional[Automa], default=None\n        Parent automa instance containing this worker. For top-level automa, parent is the automa itself.\n    arguments : Optional[Dict[str, Any]], default=None\n        Execution arguments with keys \"args\" and \"kwargs\".\n    result : Any, default=None\n        Worker execution result.\n    \"\"\"\n    if not self._is_ready:\n        return\n    output = self._build_output_payload(result=result)\n    self._complete_worker_execution(output, is_top_level)\n</code></pre>"},{"location":"reference/bridgic-traces-opik/bridgic/traces/opik/#bridgic.traces.opik.OpikTraceCallback.on_worker_error","title":"on_worker_error","text":"<code>async</code> <pre><code>on_worker_error(\n    key: str,\n    is_top_level: bool = False,\n    parent: Optional[Automa] = None,\n    arguments: Optional[Dict[str, Any]] = None,\n    error: Exception = None,\n) -&gt; bool\n</code></pre> <p>Hook invoked when worker execution raises an exception.</p> <p>For top-level automa, ends the trace with error information. For workers, ends the span with error information.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>Worker identifier.</p> required <code>is_top_level</code> <code>bool</code> <p>Whether the worker is the top-level automa. When True, parent will be the automa itself (parent is self).</p> <code>False</code> <code>parent</code> <code>Optional[Automa]</code> <p>Parent automa instance containing this worker. For top-level automa, parent is the automa itself.</p> <code>None</code> <code>arguments</code> <code>Optional[Dict[str, Any]]</code> <p>Execution arguments with keys \"args\" and \"kwargs\".</p> <code>None</code> <code>error</code> <code>Exception</code> <p>The exception raised during worker execution.</p> <code>None</code> <p>Returns:</p> Type Description <code>bool</code> <p>Always returns False, indicating the exception should not be suppressed.</p> Source code in <code>bridgic/traces/opik/_opik_trace_callback.py</code> <pre><code>async def on_worker_error(\n    self,\n    key: str,\n    is_top_level: bool = False,\n    parent: Optional[Automa] = None,\n    arguments: Optional[Dict[str, Any]] = None,\n    error: Exception = None,\n) -&gt; bool:\n    \"\"\"\n    Hook invoked when worker execution raises an exception.\n\n    For top-level automa, ends the trace with error information.\n    For workers, ends the span with error information.\n\n    Parameters\n    ----------\n    key : str\n        Worker identifier.\n    is_top_level : bool, default=False\n        Whether the worker is the top-level automa. When True, parent will be the automa itself (parent is self).\n    parent : Optional[Automa], default=None\n        Parent automa instance containing this worker. For top-level automa, parent is the automa itself.\n    arguments : Optional[Dict[str, Any]], default=None\n        Execution arguments with keys \"args\" and \"kwargs\".\n    error : Exception, default=None\n        The exception raised during worker execution.\n\n    Returns\n    -------\n    bool\n        Always returns False, indicating the exception should not be suppressed.\n    \"\"\"\n    if not self._is_ready:\n        return False\n    if not is_top_level and parent:\n        try:\n            self._get_worker_instance(key, parent)\n        except (KeyError, ValueError) as e:\n            warnings.warn(f\"Failed to get worker instance for key '{key}': {e}\")\n            return False\n\n    output = self._build_output_payload(error=error)\n    self._complete_worker_execution(output, is_top_level, error=error)\n    return False\n</code></pre>"},{"location":"reference/bridgic-traces-opik/bridgic/traces/opik/#bridgic.traces.opik.start_opik_trace","title":"start_opik_trace","text":"<pre><code>start_opik_trace(\n    project_name: Optional[str] = None,\n    workspace: Optional[str] = None,\n    host: Optional[str] = None,\n    api_key: Optional[str] = None,\n    use_local: bool = False,\n) -&gt; None\n</code></pre> <p>Start a Opik trace for a given project and service.</p> <p>Parameters:</p> Name Type Description Default <code>project_name</code> <code>Optional[str]</code> <p>The name of the project. If None, uses <code>Default Project</code> project name.</p> <code>None</code> <code>workspace</code> <code>Optional[str]</code> <p>The name of the workspace. If None, uses <code>default</code> workspace name.</p> <code>None</code> <code>host</code> <code>Optional[str]</code> <p>The host URL for the Opik server. If None, it will default to <code>https://www.comet.com/opik/api</code>.</p> <code>None</code> <code>api_key</code> <code>Optional[str]</code> <p>The API key for Opik. This parameter is ignored for local installations.</p> <code>None</code> <code>use_local</code> <code>bool</code> <p>Whether to use local Opik server.</p> <code>False</code> <p>Returns:</p> Type Description <code>None</code> Source code in <code>bridgic/traces/opik/_utils.py</code> <pre><code>def start_opik_trace(\n    project_name: Optional[str] = None,\n    workspace: Optional[str] = None,\n    host: Optional[str] = None,\n    api_key: Optional[str] = None,\n    use_local: bool = False,\n) -&gt; None:\n    \"\"\"Start a Opik trace for a given project and service.\n\n    Parameters\n    ----------\n    project_name : Optional[str], default=None\n        The name of the project. If None, uses `Default Project` project name.\n    workspace : Optional[str], default=None\n        The name of the workspace. If None, uses `default` workspace name.\n    host : Optional[str], default=None\n        The host URL for the Opik server. If None, it will default to `https://www.comet.com/opik/api`.\n    api_key : Optional[str], default=None\n        The API key for Opik. This parameter is ignored for local installations.\n    use_local : bool, default=False\n        Whether to use local Opik server.\n\n    Returns\n    -------\n    None\n    \"\"\"\n    from bridgic.core.config import GlobalSetting\n    builder = WorkerCallbackBuilder(\n        OpikTraceCallback, \n        init_kwargs={\"project_name\": project_name, \"workspace\": workspace, \"host\": host, \"api_key\": api_key, \"use_local\": use_local}\n    )\n    GlobalSetting.add(callback_builder=builder)\n</code></pre>"},{"location":"tutorials/","title":"Tutorials","text":""},{"location":"tutorials/#tutorials","title":"Tutorials","text":"<p>Welcome to the Bridgic tutorial!</p> <p>Here, we demonstrate the foundational concepts for developing agentic systems using the Bridgic framework, helping you quickly grasp the key ideas and best practices. For each core feature, we provide a concise example inspired by real-world development scenarios to make the concepts easier to understand and apply.</p> <p>This tutorial will walk you through these tasks:</p> <ol> <li>Quick Start: Write your first Bridgic-style program.</li> <li>Core Mechanism: Learn about the core mechanism and how Bridgic organizes program into modular building blocks.</li> <li>Model Integration: Discover how to incorporate AI models into your workflows for creating more autonomous and intelligent systems.</li> </ol> <p>Start your Bridgic journey! \ud83c\udf89</p>"},{"location":"tutorials/items/agentic_module/","title":"Agentic","text":""},{"location":"tutorials/items/agentic_module/#agentic-module","title":"Agentic Module","text":""},{"location":"tutorials/items/agentic_module/#introduction","title":"Introduction","text":"<p>Agentic Module is important in the Bridgic framework for building agentic systems. It provides a comprehensive set of agentic classes in a systematic and modular way, derived from <code>GraphAutoma</code>. Whether you're building simple task orchestration or complex autonomous agent workflows, developers can flexibly combine various automas according to their business scenarios to build their own agentic systems. It also support full-process observability, helping you easily watch and debug every step of task execution, providing comprehensive support from task flow to state monitoring.</p>"},{"location":"tutorials/items/agentic_module/#provided-agentic-modules","title":"Provided Agentic Modules","text":""},{"location":"tutorials/items/agentic_module/#concurrentautoma","title":"ConcurrentAutoma","text":"<p>ConcurrentAutoma provides concurrent execution capabilities for multiple workers. All workers start simultaneously and execute concurrently, with results aggregated into a list upon completion.</p> <p>Features: - Concurrent Execution: All workers execute simultaneously, independent of each other - Result Aggregation: Automatically aggregates all worker results into a list - Execution Modes: Supports both async mode (Async Mode) and parallel mode (Parallel Mode) - Use Cases: Scenarios requiring parallel execution of multiple independent tasks, such as calling multiple APIs simultaneously or processing multiple data sources in parallel</p>"},{"location":"tutorials/items/agentic_module/#sequentialautoma","title":"SequentialAutoma","text":"<p>SequentialAutoma provides strict sequential execution capabilities. Workers execute in the order they are added, with each worker's output serving as the input for the next worker.</p> <p>Features: - Strict Ordering: Workers execute sequentially in the order they are registered - Data Passing: The output of the previous worker is automatically passed to the next worker - Linear Workflow: Ensures an ordered, step-by-step processing flow - Use Cases: Scenarios requiring strict sequential dependencies, such as data processing pipelines or multi-step transformation processes</p>"},{"location":"tutorials/items/agentic_module/#recentautoma","title":"ReCentAutoma","text":"<p>ReCentAutoma provides autonomous agent capabilities, enabling systems to autonomously make decisions and plan based on specific goals and dynamic environments at runtime. It is driven by Large Language Models (LLMs) at its core, allowing it to reason about the current state in stages based on high-level goals, dynamically select appropriate tools to invoke, and determine whether goals are achieved or actions should continue based on results.</p> <p>Core Features: - Autonomous Reasoning and Planning: LLMs can understand goals and context, automatically deciding the next action or tool combination - Intermediate Decision-Making and Looping: Supports multi-round reasoning, with each decision based on the latest context results and historical trajectory - Goal-Driven Execution: The entire process is anchored by \"goal completion,\" automatically converging and ending; supports interruption and recovery - Memory Mechanism: Built-in short-term and long-term memory capabilities, performing information compression and review for long-chain tasks - Adaptation to Complex Environments: Capable of handling complex scenarios where task flows are uncertain and execution paths change dynamically</p> <p>Typical Use Cases: - Highly autonomous intelligent assistants and Q&amp;A systems - Complex tool and plugin orchestration (e.g., internet search + data analysis) - Exploratory tasks with unknown requirements and processes</p>"},{"location":"tutorials/items/agentic_module/#tutorials-and-resources","title":"Tutorials and Resources","text":"<ul> <li>Core Mechanisms: Understand Bridgic's fundamental concepts, like <code>Worker</code>, <code>Automa</code>, etc.</li> <li>About ConcurrentAutoma: Learn how to use <code>ConcurrentAutoma</code> through practical examples</li> <li>About SequentialAutoma: Learn how to use <code>SequentialAutoma</code> through practical examples</li> <li>About ReCentAutoma: Learn how to use <code>ReCentAutoma</code> to build a more autonomous system</li> </ul>"},{"location":"tutorials/items/agentic_module/about_concurrent_automa/","title":"ConcurrentAutoma","text":"In\u00a0[5]: Copied! <pre>import asyncio\nimport time\nimport json\n\nfrom bridgic.core.automa import worker\nfrom bridgic.core.agentic import ConcurrentAutoma\n\n# Define the data fetcher with concurrent workers\nclass DataFetcher(ConcurrentAutoma):\n    @worker()\n    async def fetch_from_api_1(self, query: str) -&gt; str:\n        \"\"\"Fetch data from API-1\"\"\"\n        await asyncio.sleep(0.2)\n        return f\"API-1 result: {query}\"\n    \n    @worker()\n    async def fetch_from_api_2(self, query: str) -&gt; str:\n        \"\"\"Fetch data from API-2\"\"\"\n        await asyncio.sleep(0.2)\n        return f\"API-2 result: {query}\"\n    \n    @worker()\n    def fetch_from_api_3(self, query: str) -&gt; str:\n        \"\"\"Fetch data from API-3 (synchronous function)\"\"\"\n        time.sleep(0.2)\n        return f\"API-3 result: {query}\"\n\n\n# Execute all workers concurrently\nasync def main():\n    fetcher = DataFetcher()\n    \n    start_time = time.time()\n    results = await fetcher.arun(query=\"search keyword\")\n    elapsed_time = time.time() - start_time\n    \n    print(f\"Time Cost: {elapsed_time:.2f} seconds\")\n    print(f\"Results: {json.dumps(results, indent=4)}\")\n\nawait main()\n</pre> import asyncio import time import json  from bridgic.core.automa import worker from bridgic.core.agentic import ConcurrentAutoma  # Define the data fetcher with concurrent workers class DataFetcher(ConcurrentAutoma):     @worker()     async def fetch_from_api_1(self, query: str) -&gt; str:         \"\"\"Fetch data from API-1\"\"\"         await asyncio.sleep(0.2)         return f\"API-1 result: {query}\"          @worker()     async def fetch_from_api_2(self, query: str) -&gt; str:         \"\"\"Fetch data from API-2\"\"\"         await asyncio.sleep(0.2)         return f\"API-2 result: {query}\"          @worker()     def fetch_from_api_3(self, query: str) -&gt; str:         \"\"\"Fetch data from API-3 (synchronous function)\"\"\"         time.sleep(0.2)         return f\"API-3 result: {query}\"   # Execute all workers concurrently async def main():     fetcher = DataFetcher()          start_time = time.time()     results = await fetcher.arun(query=\"search keyword\")     elapsed_time = time.time() - start_time          print(f\"Time Cost: {elapsed_time:.2f} seconds\")     print(f\"Results: {json.dumps(results, indent=4)}\")  await main() <pre>Time Cost: 0.20 seconds\nResults: [\n    \"API-1 result: search keyword\",\n    \"API-2 result: search keyword\",\n    \"API-3 result: search keyword\"\n]\n</pre>"},{"location":"tutorials/items/agentic_module/about_concurrent_automa/#concurrentautoma","title":"ConcurrentAutoma\u00b6","text":""},{"location":"tutorials/items/agentic_module/about_concurrent_automa/#introduction","title":"Introduction\u00b6","text":"<p><code>ConcurrentAutoma</code> is an orchestration Automa in the Bridgic framework designed for concurrent execution of multiple workers. It allows you to execute multiple independent tasks simultaneously and aggregates all results into a list.</p> <ul> <li>Simplified Interface: Provides a simpler concurrent orchestration interface compared to <code>GraphAutoma</code></li> <li>Concurrent Execution: All workers execute simultaneously, independent of each other</li> <li>Result Aggregation: Automatically aggregates all worker results into a list</li> </ul>"},{"location":"tutorials/items/agentic_module/about_concurrent_automa/#example","title":"Example\u00b6","text":"<p>Let's learn how to use ConcurrentAutoma through a practical example. In the following example, we'll create a data fetcher that concurrently fetches data from multiple APIs.</p>"},{"location":"tutorials/items/agentic_module/about_recent_automa/","title":"ReCentAutoma","text":"In\u00a0[1]: Copied! <pre>import dotenv\nimport os\n\nfrom bridgic.llms.openai import OpenAILlm, OpenAIConfiguration\n\ndotenv.load_dotenv()\n\n_api_key = os.environ.get(\"OPENAI_API_KEY\")\n_api_base = os.environ.get(\"OPENAI_API_BASE\", \"https://api.openai.com/v1\")\n_model_name = os.environ.get(\"OPENAI_MODEL_NAME\", \"gpt-4o-mini\")\n\n# Initialize LLM (you can replace this with your own LLM instance)\nllm = OpenAILlm(\n    api_base=_api_base,\n    api_key=_api_key,\n    configuration=OpenAIConfiguration(model=_model_name),\n    timeout=60,\n)\n</pre> import dotenv import os  from bridgic.llms.openai import OpenAILlm, OpenAIConfiguration  dotenv.load_dotenv()  _api_key = os.environ.get(\"OPENAI_API_KEY\") _api_base = os.environ.get(\"OPENAI_API_BASE\", \"https://api.openai.com/v1\") _model_name = os.environ.get(\"OPENAI_MODEL_NAME\", \"gpt-4o-mini\")  # Initialize LLM (you can replace this with your own LLM instance) llm = OpenAILlm(     api_base=_api_base,     api_key=_api_key,     configuration=OpenAIConfiguration(model=_model_name),     timeout=60, ) In\u00a0[2]: Copied! <pre>from bridgic.core.automa import RunningOptions\nfrom bridgic.core.agentic.recent import ReCentAutoma, StopCondition\n\n# Define custom tools for weight management\nasync def calculate_bmi(weight_kg: float, height_m: float) -&gt; str:\n    \"\"\"\n    Calculate Body Mass Index (BMI) from weight and height.\n\n    Parameters\n    ----------\n    weight_kg : float\n        Weight in kilograms. Must be a positive number.\n    height_m : float\n        Height in meters. Must be a positive number.\n\n    Returns\n    -------\n    str\n        A formatted string containing the BMI value and its interpretation. BMI categories:\n        - Underweight: &lt; 18.5\n        - Normal: 18.5 - 24.9\n        - Overweight: 25 - 29.9\n        - Obese: &gt;= 30\n    \"\"\"\n    if weight_kg &lt;= 0 or height_m &lt;= 0:\n        return \"Error: Weight and height must be positive numbers.\"\n\n    bmi = weight_kg / (height_m ** 2)\n\n    if bmi &lt; 18.5:\n        category = \"Underweight\"\n    elif bmi &lt; 25:\n        category = \"Normal\"\n    elif bmi &lt; 30:\n        category = \"Overweight\"\n    else:\n        category = \"Obese\"\n\n    return f\"BMI: {bmi:.2f} ({category}). Weight: {weight_kg} kg, Height: {height_m} m.\"\n\n# Create an agent with the custom tools (setting debug=True to see more details)\nweight_assistant = ReCentAutoma(\n    llm=llm,\n    tools=[calculate_bmi],\n    stop_condition=StopCondition(max_iteration=5, max_consecutive_no_tool_selected=1),\n    running_options=RunningOptions(debug=True),\n)\n\nperson_weight = 82\nperson_height = 1.70\nperson_name = \"John Smith\"\nperson_gender = \"male\"\n\nresult = await weight_assistant.arun(\n    goal=(\n        f\"Calculate a person's BMI and provide personalized suggestions for effective weight management.\"\n        f\"\\n- Name: {person_name}\"\n        f\"\\n- Gender: {person_gender}\"\n        f\"\\n- Weight: {person_weight} kg\"\n        f\"\\n- Height: {person_height} m\"\n    ),\n    guidance=(\n        \"First calculate the BMI of the person and then give out a suggestion about the weight management.\"\n    ),\n)\nprint(result)\n</pre> from bridgic.core.automa import RunningOptions from bridgic.core.agentic.recent import ReCentAutoma, StopCondition  # Define custom tools for weight management async def calculate_bmi(weight_kg: float, height_m: float) -&gt; str:     \"\"\"     Calculate Body Mass Index (BMI) from weight and height.      Parameters     ----------     weight_kg : float         Weight in kilograms. Must be a positive number.     height_m : float         Height in meters. Must be a positive number.      Returns     -------     str         A formatted string containing the BMI value and its interpretation. BMI categories:         - Underweight: &lt; 18.5         - Normal: 18.5 - 24.9         - Overweight: 25 - 29.9         - Obese: &gt;= 30     \"\"\"     if weight_kg &lt;= 0 or height_m &lt;= 0:         return \"Error: Weight and height must be positive numbers.\"      bmi = weight_kg / (height_m ** 2)      if bmi &lt; 18.5:         category = \"Underweight\"     elif bmi &lt; 25:         category = \"Normal\"     elif bmi &lt; 30:         category = \"Overweight\"     else:         category = \"Obese\"      return f\"BMI: {bmi:.2f} ({category}). Weight: {weight_kg} kg, Height: {height_m} m.\"  # Create an agent with the custom tools (setting debug=True to see more details) weight_assistant = ReCentAutoma(     llm=llm,     tools=[calculate_bmi],     stop_condition=StopCondition(max_iteration=5, max_consecutive_no_tool_selected=1),     running_options=RunningOptions(debug=True), )  person_weight = 82 person_height = 1.70 person_name = \"John Smith\" person_gender = \"male\"  result = await weight_assistant.arun(     goal=(         f\"Calculate a person's BMI and provide personalized suggestions for effective weight management.\"         f\"\\n- Name: {person_name}\"         f\"\\n- Gender: {person_gender}\"         f\"\\n- Weight: {person_weight} kg\"         f\"\\n- Height: {person_height} m\"     ),     guidance=(         \"First calculate the BMI of the person and then give out a suggestion about the weight management.\"     ), ) print(result) <pre>\n[ReCentAutoma]-[ReCentAutoma-751ecd6b] is started.\n[ReCentAutoma]-[ReCentAutoma-751ecd6b] [__dynamic_step__] driving [initialize_task_goal]\n[ReCentAutoma]-[ReCentAutoma-751ecd6b] [__automa__] triggers [initialize_task_goal]\n[ReCentAutoma]-[ReCentAutoma-751ecd6b] \ud83c\udfaf Task Goal\nCalculate a person's BMI and provide personalized suggestions for effective weight management.\n- Name: John Smith\n- Gender: male\n- Weight: 82 kg\n- Height: 1.7 m\n\nFirst calculate the BMI of the person and then give out a suggestion about the weight management.\n[ReCentAutoma]-[ReCentAutoma-751ecd6b] [__dynamic_step__] driving [observe]\n[ReCentAutoma]-[ReCentAutoma-751ecd6b] [initialize_task_goal] triggers [observe]\n[ReCentAutoma]-[ReCentAutoma-751ecd6b] \ud83d\udc40 Observation\n    Iteration: 1\n    Achieved: False\n    Thinking: The BMI has not yet been calculated for John Smith, and no personalized weight management suggestions have been provided. Consequently, there are significant gaps between the current state and the task goal, as both required outputs are incomplete.\n[ReCentAutoma]-[ReCentAutoma-751ecd6b] [__dynamic_step__] driving [select_tools, compress_memory]\n[ReCentAutoma]-[ReCentAutoma-751ecd6b] [observe] triggers [select_tools]\n[ReCentAutoma]-[ReCentAutoma-751ecd6b] [observe] triggers [compress_memory]\n[ReCentAutoma]-[ReCentAutoma-751ecd6b] \ud83e\udded Memory Check\n    Compression Needed: False\n[ReCentAutoma]-[ReCentAutoma-751ecd6b] \ud83d\udd27 Tool Selection\n    Tool 1: calculate_bmi\n      id: tool_ae193be845224084986f1e56b\n      arguments: {'weight_kg': 82, 'height_m': 1.7}\n[ReCentAutoma]-[ReCentAutoma-751ecd6b] [__dynamic_step__] driving [tool-&lt;calculate_bmi&gt;-&lt;tool_ae193be845224084986f1e56b&gt;]\n[ReCentAutoma]-[ReCentAutoma-751ecd6b] [select_tools] triggers [tool-&lt;calculate_bmi&gt;-&lt;tool_ae193be845224084986f1e56b&gt;]\n[ReCentAutoma]-[ReCentAutoma-751ecd6b] [__dynamic_step__] driving [collect_results-&lt;abe2a140&gt;]\n[ReCentAutoma]-[ReCentAutoma-751ecd6b] [tool-&lt;calculate_bmi&gt;-&lt;tool_ae193be845224084986f1e56b&gt;] triggers [collect_results-&lt;abe2a140&gt;]\n[ReCentAutoma]-[ReCentAutoma-751ecd6b] \ud83d\udea9 Tool Results\n    Tool 1: calculate_bmi\n      id: tool_ae193be845224084986f1e56b\n      result: BMI: 28.37 (Overweight). Weight: 82 kg, Height: 1.7 m.\n[ReCentAutoma]-[ReCentAutoma-751ecd6b] [__dynamic_step__] driving [observe]\n[ReCentAutoma]-[ReCentAutoma-751ecd6b] [collect_results-&lt;abe2a140&gt;] triggers [observe]\n[ReCentAutoma]-[ReCentAutoma-751ecd6b] \ud83d\udc40 Observation\n    Iteration: 2\n    Achieved: False\n    Thinking: The BMI has been calculated as 28.37, indicating that John is overweight. However, personalized suggestions for effective weight management still need to be provided. Therefore, gaps remain in the completion of the task goal, specifically in offering actionable weight management advice.\n[ReCentAutoma]-[ReCentAutoma-751ecd6b] [__dynamic_step__] driving [select_tools, compress_memory]\n[ReCentAutoma]-[ReCentAutoma-751ecd6b] [observe] triggers [select_tools]\n[ReCentAutoma]-[ReCentAutoma-751ecd6b] [observe] triggers [compress_memory]\n[ReCentAutoma]-[ReCentAutoma-751ecd6b] \ud83e\udded Memory Check\n    Compression Needed: False\n[ReCentAutoma]-[ReCentAutoma-751ecd6b] \ud83d\udd27 Tool Selection\n    (No tools selected)\n\n    LLM Response: Now that the BMI has been calculated as 28.37, which classifies John as overweight, the next step is to provide personalized weight management suggestions based on this BMI classification. Since there are no specific tools listed for providing suggestions, I will proceed to formulate some general weight management advice based on his BMI.\n\n### Suggested Weight Management Tips for John Smith:\n1. **Healthy Eating**: Focus on a balanced diet rich in fruits, vegetables, lean proteins, and whole grains. Limit processed foods, sugars, and saturated fats.\n\n2. **Regular Exercise**: Aim for at least 150 minutes of moderate-intensity aerobic exercise each week, along with strength training exercises on two or more days a week.\n\n3. **Monitor Portion Sizes**: Being mindful of portion sizes can help manage calorie intake.\n\n4. **Stay Hydrated**: Drink plenty of water throughout the day and limit sugary beverages.\n\n5. **Set Realistic Goals**: Aim for gradual weight loss (0.5 to 1 kg per week) for sustainable results.\n\n6. **Consult a Professional**: If possible, consult with a dietitian or a fitness expert for personalized guidance and support.\n\nWith this information, the task goal is now complete.\n[ReCentAutoma]-[ReCentAutoma-751ecd6b] [__dynamic_step__] driving [finalize_answer]\n[ReCentAutoma]-[ReCentAutoma-751ecd6b] [select_tools] triggers [finalize_answer]\n[ReCentAutoma]-[ReCentAutoma-751ecd6b] is finished.\n### Comprehensive Final Answer\n\n**Personal Information:**\n- **Name:** John Smith\n- **Gender:** Male\n- **Weight:** 82 kg\n- **Height:** 1.7 m\n\n**BMI Calculation:**\n- **BMI Result:** 28.37 \n- **Weight Status:** Overweight \n\n**Personalized Weight Management Suggestions:**\n\n1. **Dietary Changes:**\n   - Aim for a balanced diet rich in fruits, vegetables, whole grains, and lean proteins.\n   - Reduce intake of processed foods, sugary beverages, and high-fat snacks.\n   - Consider portion control to prevent overeating.\n\n2. **Regular Physical Activity:**\n   - Incorporate at least 150 minutes of moderate aerobic activity each week, such as brisk walking, swimming, or cycling.\n   - Include strength training exercises at least two days a week to build muscle mass and boost metabolism.\n\n3. **Monitor Progress:**\n   - Keep a food diary to track daily food intake and physical activity.\n   - Weigh yourself regularly (e.g., once a week) to monitor progress without becoming discouraged by day-to-day fluctuations.\n\n4. **Consult a Professional:**\n   - Consider speaking with a healthcare provider or a registered dietitian for a personalized plan tailored to John\u2019s specific needs and goals.\n\n5. **Set Realistic Goals:**\n   - Aim for gradual weight loss (about 0.5-1 kg per week) to achieve long-term success without feeling deprived.\n\nBy following these suggestions, John Smith can work towards achieving a healthier weight and improving overall well-being.\n</pre> In\u00a0[2]: Copied! <pre>import os\nimport tempfile\n\nfrom bridgic.core.automa import RunningOptions\nfrom bridgic.core.agentic.recent import ReCentAutoma, ReCentMemoryConfig, StopCondition\nfrom bridgic.protocols.mcp import (\n    McpServerConnectionStdio,\n    McpServerConnectionManager,\n)\n\ntemp_dir = os.path.realpath(tempfile.mkdtemp())\nprint(f\"\u2713 Using temporary directory: {temp_dir}\")\n\n# Connect to Playwright MCP server\n# Note: This requires Node.js and npx to be installed\nplaywright_connection = McpServerConnectionStdio(\n    name=\"connection-playwright-stdio\",\n    command=\"npx\",\n    args=[\n        f\"@playwright/mcp@latest\",\n        f\"--output-dir={temp_dir}\",\n        f\"--viewport-size=1920x1080\",\n        f\"--save-video=1920x1080\",\n    ],\n    request_timeout=60,\n)\n\n# Register the connection with a dedicated manager\nMcpServerConnectionManager.get_instance(\"browser-use\").register_connection(playwright_connection)\n\n# Establish the connection and verify connection and list available tools\nplaywright_connection.connect()\nprint(f\"\u2713 Connected to Playwright MCP server: {playwright_connection.name}\")\nprint(f\"  Connection status: {playwright_connection.is_connected}\")\n\n# List tools\ntools = playwright_connection.list_tools()\nprint(f\"\u2713 Found {len(tools)} available browser tools\")\n\n# Create a browser automation agent with Playwright MCP tools\nbrowser_agent = ReCentAutoma(\n    llm=llm,\n    tools=tools,\n    memory_config=ReCentMemoryConfig(\n        llm=llm,\n        max_node_size=8,\n        max_token_size=1024*32,\n    ),\n    stop_condition=StopCondition(max_iteration=20, max_consecutive_no_tool_selected=1),\n    running_options=RunningOptions(debug=True),\n)\n\n# Use the agent to search for information on Google\nsearch_query = \"The latest price of gold\"\n\nresult = await browser_agent.arun(\n    goal=(\n        f\"Find the recent gold prices on Hong Kong Gold Exchange website.\"\n    ),\n    guidance=(\n        \"Do the following steps one by one:\\n\"\n        \"1. Navigate to https://hkgx.com.hk/en\\n\"\n        \"2. Hover on the 'Market &amp; Data' button to show more button options\\n\"\n        \"3. Click the 'History Price' button to access the historical price page\\n\"\n        \"4. Since the current date was selected, only need to select the option of RMB-denominated kilo gold\\n\"\n        \"5. Click the search button and have a look at the recent gold price trends\\n\"\n        \"6. Close the browser and give out a summary of recent gold price trends\\n\"\n    ),\n)\n\nprint(\"Final Result:\\n\\n\")\nprint(result)\n\n# Close the connection when done\nplaywright_connection.close()\nprint(f\"\\n\u2713 Connection closed: {not playwright_connection.is_connected}\")\n</pre> import os import tempfile  from bridgic.core.automa import RunningOptions from bridgic.core.agentic.recent import ReCentAutoma, ReCentMemoryConfig, StopCondition from bridgic.protocols.mcp import (     McpServerConnectionStdio,     McpServerConnectionManager, )  temp_dir = os.path.realpath(tempfile.mkdtemp()) print(f\"\u2713 Using temporary directory: {temp_dir}\")  # Connect to Playwright MCP server # Note: This requires Node.js and npx to be installed playwright_connection = McpServerConnectionStdio(     name=\"connection-playwright-stdio\",     command=\"npx\",     args=[         f\"@playwright/mcp@latest\",         f\"--output-dir={temp_dir}\",         f\"--viewport-size=1920x1080\",         f\"--save-video=1920x1080\",     ],     request_timeout=60, )  # Register the connection with a dedicated manager McpServerConnectionManager.get_instance(\"browser-use\").register_connection(playwright_connection)  # Establish the connection and verify connection and list available tools playwright_connection.connect() print(f\"\u2713 Connected to Playwright MCP server: {playwright_connection.name}\") print(f\"  Connection status: {playwright_connection.is_connected}\")  # List tools tools = playwright_connection.list_tools() print(f\"\u2713 Found {len(tools)} available browser tools\")  # Create a browser automation agent with Playwright MCP tools browser_agent = ReCentAutoma(     llm=llm,     tools=tools,     memory_config=ReCentMemoryConfig(         llm=llm,         max_node_size=8,         max_token_size=1024*32,     ),     stop_condition=StopCondition(max_iteration=20, max_consecutive_no_tool_selected=1),     running_options=RunningOptions(debug=True), )  # Use the agent to search for information on Google search_query = \"The latest price of gold\"  result = await browser_agent.arun(     goal=(         f\"Find the recent gold prices on Hong Kong Gold Exchange website.\"     ),     guidance=(         \"Do the following steps one by one:\\n\"         \"1. Navigate to https://hkgx.com.hk/en\\n\"         \"2. Hover on the 'Market &amp; Data' button to show more button options\\n\"         \"3. Click the 'History Price' button to access the historical price page\\n\"         \"4. Since the current date was selected, only need to select the option of RMB-denominated kilo gold\\n\"         \"5. Click the search button and have a look at the recent gold price trends\\n\"         \"6. Close the browser and give out a summary of recent gold price trends\\n\"     ), )  print(\"Final Result:\\n\\n\") print(result)  # Close the connection when done playwright_connection.close() print(f\"\\n\u2713 Connection closed: {not playwright_connection.is_connected}\") <pre>\u2713 Using temporary directory: /private/var/folders/9t/5r9fms9s5q33p6xty_0_k1mw0000gn/T/tmpspv3qdhb\n\u2713 Connected to Playwright MCP server: connection-playwright-stdio\n  Connection status: True\n\u2713 Found 22 available browser tools\n\n[ReCentAutoma]-[ReCentAutoma-7708d76f] is started.\n[ReCentAutoma]-[ReCentAutoma-7708d76f] [__dynamic_step__] driving [initialize_task_goal]\n[ReCentAutoma]-[ReCentAutoma-7708d76f] [__automa__] triggers [initialize_task_goal]\n[ReCentAutoma]-[ReCentAutoma-7708d76f] \ud83c\udfaf Task Goal\nFind the recent gold prices on Hong Kong Gold Exchange website.\n\nDo the following steps one by one:\n1. Navigate to https://hkgx.com.hk/en\n2. Hover on the 'Market &amp; Data' button to show more button options\n3. Click the 'History Price' button to access the historical price page\n4. Since the current date was selected, only need to select the option of RMB-denominated kilo gold\n5. Click the search button and have a look at the recent gold price trends\n6. Close the browser and give out a summary of recent gold price trends\n\n[ReCentAutoma]-[ReCentAutoma-7708d76f] [__dynamic_step__] driving [observe]\n[ReCentAutoma]-[ReCentAutoma-7708d76f] [initialize_task_goal] triggers [observe]\n[ReCentAutoma]-[ReCentAutoma-7708d76f] \ud83d\udc40 Observation\n    Iteration: 1\n    Achieved: False\n    Thinking: No observation history or stage summaries are provided in the conversation. Therefore, I have no information on whether the steps to navigate the Hong Kong Gold Exchange website, access the historical prices page, select RMB-denominated kilo gold options, and review recent gold prices have been performed. Without this information, I cannot confirm if the recent gold prices were found or if the browser was closed and a summary given.\n[ReCentAutoma]-[ReCentAutoma-7708d76f] [__dynamic_step__] driving [select_tools, compress_memory]\n[ReCentAutoma]-[ReCentAutoma-7708d76f] [observe] triggers [select_tools]\n[ReCentAutoma]-[ReCentAutoma-7708d76f] [observe] triggers [compress_memory]\n[ReCentAutoma]-[ReCentAutoma-7708d76f] \ud83e\udded Memory Check\n    Compression Needed: False\n[ReCentAutoma]-[ReCentAutoma-7708d76f] \ud83d\udd27 Tool Selection\n    Tool 1: browser_navigate\n      id: tool_c9910ae1b63e44fab7722d7d7\n      arguments: {'url': 'https://hkgx.com.hk/en'}\n[ReCentAutoma]-[ReCentAutoma-7708d76f] [__dynamic_step__] driving [tool-&lt;browser_navigate&gt;-&lt;tool_c9910ae1b63e44fab7722d7d7&gt;]\n[ReCentAutoma]-[ReCentAutoma-7708d76f] [select_tools] triggers [tool-&lt;browser_navigate&gt;-&lt;tool_c9910ae1b63e44fab7722d7d7&gt;]\n[ReCentAutoma]-[ReCentAutoma-7708d76f] [__dynamic_step__] driving [collect_results-&lt;61f4034c&gt;]\n[ReCentAutoma]-[ReCentAutoma-7708d76f] [tool-&lt;browser_navigate&gt;-&lt;tool_c9910ae1b63e44fab7722d7d7&gt;] triggers [collect_results-&lt;61f4034c&gt;]\n[ReCentAutoma]-[ReCentAutoma-7708d76f] \ud83d\udea9 Tool Results\n    Tool 1: browser_navigate\n      id: tool_c9910ae1b63e44fab7722d7d7\n      result: meta=None content=[TextContent(type='text', text='### Ran Playwright code\\nawait page.goto(\\'https://hkgx.com.hk/en\\');\\n### Page\\n- Page URL: https://hkgx.com.hk/en\\n- Page Title: HKGX\\n### Snapshot\\n```yaml\\n- generic [active] [ref=e1]:\\n  - banner:\\n    - banner [ref=e2]:\\n      - generic [ref=e3]:\\n        - link \"logo\" [ref=e4] [cursor=pointer]:\\n          - /url: /en;jsessionid=-61AZx-98mAc68CPzVmJWQDoLCFCndodIRgoH6qe\\n          - img \"logo\" [ref=e5]\\n        - generic [ref=e6]:\\n          - generic [ref=e7] [cursor=pointer]:\\n            - link \"Contact Us\" [ref=e9]:\\n              - /url: /en/contact;jsessionid=-61AZx-98mAc68CPzVmJWQDoLCFCndodIRgoH6qe\\n            - link \"downloadZone\" [ref=e11]:\\n              - /url: /en/download/downloadtheform;jsessionid=-61AZx-98mAc68CPzVmJWQDoLCFCndodIRgoH6qe\\n            - generic [ref=e12]: \u7b80 | \u7e41 | Eng\\n            - generic [ref=e13]:\\n              - img \"user\" [ref=e14]\\n              - generic [ref=e15]: Member Login\\n          - li...\n[ReCentAutoma]-[ReCentAutoma-7708d76f] [__dynamic_step__] driving [observe]\n[ReCentAutoma]-[ReCentAutoma-7708d76f] [collect_results-&lt;61f4034c&gt;] triggers [observe]\n[ReCentAutoma]-[ReCentAutoma-7708d76f] \ud83d\udc40 Observation\n    Iteration: 2\n    Achieved: False\n    Thinking: The initial step of the task, namely navigating to the Hong Kong Gold Exchange website (https://hkgx.com.hk/en), has been successfully completed. However, no further actions have been performed such as hovering on 'Market &amp; Data', clicking on 'History Price', selecting RMB-denominated kilo gold, searching, or reviewing the recent gold price trends. Thus, the recent gold prices have not been gathered or summarized yet.\n[ReCentAutoma]-[ReCentAutoma-7708d76f] [__dynamic_step__] driving [select_tools, compress_memory]\n[ReCentAutoma]-[ReCentAutoma-7708d76f] [observe] triggers [select_tools]\n[ReCentAutoma]-[ReCentAutoma-7708d76f] [observe] triggers [compress_memory]\n[ReCentAutoma]-[ReCentAutoma-7708d76f] \ud83e\udded Memory Check\n    Compression Needed: False\n[ReCentAutoma]-[ReCentAutoma-7708d76f] \ud83d\udd27 Tool Selection\n    Tool 1: browser_hover\n      id: tool_3352bb794f5a439a954784486\n      arguments: {'ref': 'e23'}\n[ReCentAutoma]-[ReCentAutoma-7708d76f] [__dynamic_step__] driving [tool-&lt;browser_hover&gt;-&lt;tool_3352bb794f5a439a954784486&gt;]\n[ReCentAutoma]-[ReCentAutoma-7708d76f] [select_tools] triggers [tool-&lt;browser_hover&gt;-&lt;tool_3352bb794f5a439a954784486&gt;]\n[ReCentAutoma]-[ReCentAutoma-7708d76f] [__dynamic_step__] driving [collect_results-&lt;54f40f16&gt;]\n[ReCentAutoma]-[ReCentAutoma-7708d76f] [tool-&lt;browser_hover&gt;-&lt;tool_3352bb794f5a439a954784486&gt;] triggers [collect_results-&lt;54f40f16&gt;]\n[ReCentAutoma]-[ReCentAutoma-7708d76f] \ud83d\udea9 Tool Results\n    Tool 1: browser_hover\n      id: tool_3352bb794f5a439a954784486\n      result: meta=None content=[TextContent(type='text', text='### Ran Playwright code\\nawait page.getByRole(\\'link\\', { name: \\'Market &amp; Data\\' }).hover();\\n### Page\\n- Page URL: https://hkgx.com.hk/en\\n- Page Title: HKGX\\n### Snapshot\\n```yaml\\n- &lt;changed&gt; generic [active] [ref=e1]:\\n  - banner:\\n    - banner [ref=e2]:\\n      - generic [ref=e3]:\\n        - ref=e4 [unchanged]\\n        - generic [ref=e6]:\\n          - ref=e7 [unchanged]\\n          - list [ref=e16]:\\n            - ref=e17 [unchanged]\\n            - listitem [ref=e21] [cursor=pointer]:\\n              - ref=e23 [unchanged]\\n              - generic [ref=e134]:\\n                - link \"Open outcry\" [ref=e135]:\\n                  - /url: /en/marketdata/openoutcry;jsessionid=-61AZx-98mAc68CPzVmJWQDoLCFCndodIRgoH6qe\\n                  - generic [ref=e136]: Open outcry\\n                - link \"GTP\" [ref=e138]:\\n                  - /url: /en/marketdata/gtp;jsessionid=-61AZx-98mAc68CPzVmJWQDoLCFCndodIRgoH6qe\\n                  - generic [ref=...\n[ReCentAutoma]-[ReCentAutoma-7708d76f] [__dynamic_step__] driving [observe]\n[ReCentAutoma]-[ReCentAutoma-7708d76f] [collect_results-&lt;54f40f16&gt;] triggers [observe]\n[ReCentAutoma]-[ReCentAutoma-7708d76f] \ud83d\udc40 Observation\n    Iteration: 3\n    Achieved: False\n    Thinking: So far, I have navigated to the Hong Kong Gold Exchange website and hovered over the 'Market &amp; Data' button, revealing additional options including 'Price History'. However, I have not yet clicked the 'Price History' button, selected the RMB-denominated kilo gold option, clicked the search button, or reviewed the recent gold prices and trends. No summary of recent gold price trends has been provided yet. Therefore, the recent gold prices on the site have not been accessed or summarized to fulfill the task goal.\n[ReCentAutoma]-[ReCentAutoma-7708d76f] [__dynamic_step__] driving [select_tools, compress_memory]\n[ReCentAutoma]-[ReCentAutoma-7708d76f] [observe] triggers [select_tools]\n[ReCentAutoma]-[ReCentAutoma-7708d76f] [observe] triggers [compress_memory]\n[ReCentAutoma]-[ReCentAutoma-7708d76f] \ud83e\udded Memory Check\n    Compression Needed: False\n[ReCentAutoma]-[ReCentAutoma-7708d76f] \ud83d\udd27 Tool Selection\n    Tool 1: browser_click\n      id: tool_1fd8b5486d85407293ac916d8\n      arguments: {'ref': 'e148'}\n[ReCentAutoma]-[ReCentAutoma-7708d76f] [__dynamic_step__] driving [tool-&lt;browser_click&gt;-&lt;tool_1fd8b5486d85407293ac916d8&gt;]\n[ReCentAutoma]-[ReCentAutoma-7708d76f] [select_tools] triggers [tool-&lt;browser_click&gt;-&lt;tool_1fd8b5486d85407293ac916d8&gt;]\n[ReCentAutoma]-[ReCentAutoma-7708d76f] [__dynamic_step__] driving [collect_results-&lt;2953a6f5&gt;]\n[ReCentAutoma]-[ReCentAutoma-7708d76f] [tool-&lt;browser_click&gt;-&lt;tool_1fd8b5486d85407293ac916d8&gt;] triggers [collect_results-&lt;2953a6f5&gt;]\n[ReCentAutoma]-[ReCentAutoma-7708d76f] \ud83d\udea9 Tool Results\n    Tool 1: browser_click\n      id: tool_1fd8b5486d85407293ac916d8\n      result: meta=None content=[TextContent(type='text', text='### Ran Playwright code\\nawait page.getByRole(\\'link\\', { name: \\'Price History\\' }).click();\\n### Page\\n- Page URL: https://hkgx.com.hk/en/marketdata/pricehistory;jsessionid=-61AZx-98mAc68CPzVmJWQDoLCFCndodIRgoH6qe\\n- Page Title: HKGX\\n### Snapshot\\n```yaml\\n- generic [active] [ref=e1]:\\n  - banner:\\n    - banner [ref=e2]:\\n      - generic [ref=e3]:\\n        - link \"logo\" [ref=e4] [cursor=pointer]:\\n          - /url: /en\\n          - img \"logo\" [ref=e5]\\n        - generic [ref=e6]:\\n          - generic [ref=e7] [cursor=pointer]:\\n            - link \"Contact Us\" [ref=e9]:\\n              - /url: /en/contact\\n            - link \"downloadZone\" [ref=e11]:\\n              - /url: /en/download/downloadtheform\\n            - generic [ref=e12]: \u7b80 | \u7e41 | Eng\\n            - generic [ref=e13]:\\n              - img \"user\" [ref=e14]\\n              - generic [ref=e15]: Member Login\\n          - list [ref=e16]:\\n            - listitem [ref=e17] [cursor=...\n[ReCentAutoma]-[ReCentAutoma-7708d76f] [__dynamic_step__] driving [observe]\n[ReCentAutoma]-[ReCentAutoma-7708d76f] [collect_results-&lt;2953a6f5&gt;] triggers [observe]\n[ReCentAutoma]-[ReCentAutoma-7708d76f] \ud83d\udc40 Observation\n    Iteration: 4\n    Achieved: False\n    Thinking: I have navigated to the Hong Kong Gold Exchange website, hovered over 'Market &amp; Data', and clicked the 'Price History' button to go to the historical price page. On the Price History page, I see options to select the product and date, including the option for 'RMB Kilo Gold'. However, I have not yet selected this option or clicked the search button to retrieve recent gold prices. I also have not reviewed the price trends or closed the browser and summarized the results. Therefore, some steps remain to fully achieve the task goal.\n[ReCentAutoma]-[ReCentAutoma-7708d76f] [__dynamic_step__] driving [select_tools, compress_memory]\n[ReCentAutoma]-[ReCentAutoma-7708d76f] [observe] triggers [select_tools]\n[ReCentAutoma]-[ReCentAutoma-7708d76f] [observe] triggers [compress_memory]\n[ReCentAutoma]-[ReCentAutoma-7708d76f] \ud83e\udded Memory Check\n    Compression Needed: False\n[ReCentAutoma]-[ReCentAutoma-7708d76f] \ud83d\udd27 Tool Selection\n    Tool 1: browser_select_option\n      id: tool_f907e9c341ba43938db2c604d\n      arguments: {'ref': 'e65', 'values': ['RMB Kilo Gold']}\n    Tool 2: browser_click\n      id: tool_74afc176c10f4dd79037b1302\n      arguments: {'ref': 'e66'}\n[ReCentAutoma]-[ReCentAutoma-7708d76f] [__dynamic_step__] driving [tool-&lt;browser_select_option&gt;-&lt;tool_f907e9c341ba43938db2c604d&gt;, tool-&lt;browser_click&gt;-&lt;tool_74afc176c10f4dd79037b1302&gt;]\n[ReCentAutoma]-[ReCentAutoma-7708d76f] [select_tools] triggers [tool-&lt;browser_select_option&gt;-&lt;tool_f907e9c341ba43938db2c604d&gt;]\n[ReCentAutoma]-[ReCentAutoma-7708d76f] [select_tools] triggers [tool-&lt;browser_click&gt;-&lt;tool_74afc176c10f4dd79037b1302&gt;]\n[ReCentAutoma]-[ReCentAutoma-7708d76f] [__dynamic_step__] driving [collect_results-&lt;6adb8285&gt;]\n[ReCentAutoma]-[ReCentAutoma-7708d76f] [tool-&lt;browser_select_option&gt;-&lt;tool_f907e9c341ba43938db2c604d&gt;] triggers [collect_results-&lt;6adb8285&gt;]\n[ReCentAutoma]-[ReCentAutoma-7708d76f] \ud83d\udea9 Tool Results\n    Tool 1: browser_select_option\n      id: tool_f907e9c341ba43938db2c604d\n      result: meta=None content=[TextContent(type='text', text='### Ran Playwright code\\nawait page.locator(\\'#selectCategory\\').selectOption([\\'RMB Kilo Gold\\']);\\n### Page\\n- Page URL: https://hkgx.com.hk/en/marketdata/pricehistory;jsessionid=-61AZx-98mAc68CPzVmJWQDoLCFCndodIRgoH6qe\\n- Page Title: HKGX\\n### Snapshot\\n```yaml\\n- &lt;changed&gt; generic [ref=e52]:\\n  - ref=e53 [unchanged]\\n  - generic [ref=e56]:\\n    - generic [ref=e57]:\\n      - ref=e60 [unchanged]\\n      - combobox [ref=e65]:\\n        - option \"99 Tael Gold\"\\n        - option \"HK Kilo Gold\"\\n        - option \"RMB Kilo Gold\" [selected]\\n        - option \"999.9 Gold contract\"\\n    - ref=e66 [unchanged]\\n  - generic [ref=e67]:\\n    - generic [ref=e96]: RMB Kilo Gold Please review the following table\\n    - table [ref=e98]:\\n      - rowgroup [ref=e99]:\\n        - row \"Date Open High Low Close Benchmark Price\" [ref=e100]:\\n          - columnheader \"Date\" [ref=e101]\\n          - columnheader \"Open\" [ref=e102]\\n          - columnheader \"High\" ...\n    Tool 2: browser_click\n      id: tool_74afc176c10f4dd79037b1302\n      result: meta=None content=[TextContent(type='text', text=\"### Ran Playwright code\\nawait page.getByRole('button', { name: 'Search' }).click();\\n### Page\\n- Page URL: https://hkgx.com.hk/en/marketdata/pricehistory;jsessionid=-61AZx-98mAc68CPzVmJWQDoLCFCndodIRgoH6qe\\n- Page Title: HKGX\\n### Snapshot\\n```yaml\\n\\n```\", annotations=None, meta=None)] structuredContent=None isError=False\n[ReCentAutoma]-[ReCentAutoma-7708d76f] [__dynamic_step__] driving [observe]\n[ReCentAutoma]-[ReCentAutoma-7708d76f] [collect_results-&lt;6adb8285&gt;] triggers [observe]\n[ReCentAutoma]-[ReCentAutoma-7708d76f] \ud83d\udc40 Observation\n    Iteration: 5\n    Achieved: False\n    Thinking: I have navigated to the Hong Kong Gold Exchange website, hovered over 'Market &amp; Data', clicked the 'Price History' link, selected 'RMB Kilo Gold', and clicked the search button. The recent gold price data for RMB-denominated kilo gold is visible and includes values for dates around January 2026. However, I have not yet provided a summary of the recent gold price trends or closed the browser. Therefore, the final task steps remain to be completed.\n[ReCentAutoma]-[ReCentAutoma-7708d76f] [__dynamic_step__] driving [select_tools, compress_memory]\n[ReCentAutoma]-[ReCentAutoma-7708d76f] [observe] triggers [select_tools]\n[ReCentAutoma]-[ReCentAutoma-7708d76f] [observe] triggers [compress_memory]\n[ReCentAutoma]-[ReCentAutoma-7708d76f] \ud83e\udded Memory Check\n    Compression Needed: False\n[ReCentAutoma]-[ReCentAutoma-7708d76f] \ud83d\udd27 Tool Selection\n    Tool 1: browser_evaluate\n      id: tool_95d81e5be7b54796be84866b4\n      arguments: {'function': \"() =&gt; {\\n  const rows = Array.from(document.querySelectorAll('table tr'));\\n  const prices = rows.slice(1, 6).map(row =&gt; {\\n    const cells = row.querySelectorAll('td');\\n    return {\\n      date: cells[0]?.textContent.trim(),\\n      open: cells[1]?.textContent.trim(),\\n      high: cells[2]?.textContent.trim(),\\n      low: cells[3]?.textContent.trim(),\\n      close: cells[4]?.textContent.trim(),\\n      benchmarkPrice: cells[5]?.textContent.trim(),\\n    };\\n  });\\n  return prices;\\n}\"}\n[ReCentAutoma]-[ReCentAutoma-7708d76f] [__dynamic_step__] driving [tool-&lt;browser_evaluate&gt;-&lt;tool_95d81e5be7b54796be84866b4&gt;]\n[ReCentAutoma]-[ReCentAutoma-7708d76f] [select_tools] triggers [tool-&lt;browser_evaluate&gt;-&lt;tool_95d81e5be7b54796be84866b4&gt;]\n[ReCentAutoma]-[ReCentAutoma-7708d76f] [__dynamic_step__] driving [collect_results-&lt;3a1c1d46&gt;]\n[ReCentAutoma]-[ReCentAutoma-7708d76f] [tool-&lt;browser_evaluate&gt;-&lt;tool_95d81e5be7b54796be84866b4&gt;] triggers [collect_results-&lt;3a1c1d46&gt;]\n[ReCentAutoma]-[ReCentAutoma-7708d76f] \ud83d\udea9 Tool Results\n    Tool 1: browser_evaluate\n      id: tool_95d81e5be7b54796be84866b4\n      result: meta=None content=[TextContent(type='text', text='### Result\\n[\\n  {\\n    \"date\": \"2026-01-26\",\\n    \"open\": \"1121.43\",\\n    \"high\": \"1142.14\",\\n    \"low\": \"1120.03\",\\n    \"close\": \"-\",\\n    \"benchmarkPrice\": \"1134.15\"\\n  },\\n  {\\n    \"date\": \"2026-01-23\",\\n    \"open\": \"-\",\\n    \"high\": \"0.00\",\\n    \"low\": \"0.00\",\\n    \"close\": \"-\",\\n    \"benchmarkPrice\": \"-\"\\n  },\\n  {\\n    \"date\": \"2026-01-22\",\\n    \"open\": \"1077.35\",\\n    \"high\": \"1105.64\",\\n    \"low\": \"1068.54\",\\n    \"close\": \"1105.24\",\\n    \"benchmarkPrice\": \"1073.00\"\\n  },\\n  {\\n    \"date\": \"2026-01-21\",\\n    \"open\": \"1065.21\",\\n    \"high\": \"1093.60\",\\n    \"low\": \"1063.93\",\\n    \"close\": \"1080.88\",\\n    \"benchmarkPrice\": \"1084.50\"\\n  }\\n]\\n### Ran Playwright code\\nawait page.evaluate(\\'() =&gt; {\\\\n  const rows = Array.from(document.querySelectorAll(\\\\\\'table tr\\\\\\'));\\\\n  const prices = rows.slice(1, 6).map(row =&gt; {\\\\n    const cells = row.querySelectorAll(\\\\\\'td\\\\\\');\\\\n    return {\\\\n      date: cells[0]?.textContent.trim(),\\\\n  ...\n[ReCentAutoma]-[ReCentAutoma-7708d76f] [__dynamic_step__] driving [observe]\n[ReCentAutoma]-[ReCentAutoma-7708d76f] [collect_results-&lt;3a1c1d46&gt;] triggers [observe]\n[ReCentAutoma]-[ReCentAutoma-7708d76f] \ud83d\udc40 Observation\n    Iteration: 6\n    Achieved: False\n    Thinking: I have successfully navigated to the Hong Kong Gold Exchange website, accessed the Price History page, selected RMB-denominated kilo gold, and retrieved recent gold price data for several dates in January 2026. I extracted key details like open, high, low, and benchmark prices. However, I have not yet closed the browser or provided a summary of the recent gold price trends as required. Therefore, the primary data has been gathered but the final summarization and closure steps remain.\n[ReCentAutoma]-[ReCentAutoma-7708d76f] [__dynamic_step__] driving [select_tools, compress_memory]\n[ReCentAutoma]-[ReCentAutoma-7708d76f] [observe] triggers [select_tools]\n[ReCentAutoma]-[ReCentAutoma-7708d76f] [observe] triggers [compress_memory]\n[ReCentAutoma]-[ReCentAutoma-7708d76f] \ud83e\udded Memory Check\n    Compression Needed: False\n[ReCentAutoma]-[ReCentAutoma-7708d76f] \ud83d\udd27 Tool Selection\n    Tool 1: browser_close\n      id: tool_34e03cbac741453eb6ff33e7a\n      arguments: {}\n\n    LLM Response: I have obtained the recent gold price data for RMB-denominated kilo gold from the Hong Kong Gold Exchange website. The next logical step is to close the browser and provide a summary of the recent gold price trends as per the task goal.\n\nTools to use next:\n1. Close the browser using functions.browser_close.\n2. Provide a summary of the recent gold price trends based on the extracted data.\n\nI will proceed with closing the browser first.\n[ReCentAutoma]-[ReCentAutoma-7708d76f] [__dynamic_step__] driving [tool-&lt;browser_close&gt;-&lt;tool_34e03cbac741453eb6ff33e7a&gt;]\n[ReCentAutoma]-[ReCentAutoma-7708d76f] [select_tools] triggers [tool-&lt;browser_close&gt;-&lt;tool_34e03cbac741453eb6ff33e7a&gt;]\n[ReCentAutoma]-[ReCentAutoma-7708d76f] [__dynamic_step__] driving [collect_results-&lt;ed448c42&gt;]\n[ReCentAutoma]-[ReCentAutoma-7708d76f] [tool-&lt;browser_close&gt;-&lt;tool_34e03cbac741453eb6ff33e7a&gt;] triggers [collect_results-&lt;ed448c42&gt;]\n[ReCentAutoma]-[ReCentAutoma-7708d76f] \ud83d\udea9 Tool Results\n    Tool 1: browser_close\n      id: tool_34e03cbac741453eb6ff33e7a\n      result: meta=None content=[TextContent(type='text', text='### Result\\nNo open tabs. Navigate to a URL to create one.\\n### Ran Playwright code\\nawait page.close()', annotations=None, meta=None)] structuredContent=None isError=False\n[ReCentAutoma]-[ReCentAutoma-7708d76f] [__dynamic_step__] driving [observe]\n[ReCentAutoma]-[ReCentAutoma-7708d76f] [collect_results-&lt;ed448c42&gt;] triggers [observe]\n[ReCentAutoma]-[ReCentAutoma-7708d76f] \ud83d\udc40 Observation\n    Iteration: 7\n    Achieved: False\n    Thinking: The assistant successfully navigated to the Hong Kong Gold Exchange website, accessed the Price History page under the 'Market &amp; Data' menu, selected the RMB-denominated kilo gold option, clicked search, and retrieved recent gold price data including open, high, low, and benchmark prices for several recent dates. The browser was then closed. However, the assistant did not explicitly provide a summarized overview of the recent gold price trends before closing the browser. Therefore, although the core data retrieval steps were completed, the final summarization step is missing.\n[ReCentAutoma]-[ReCentAutoma-7708d76f] [__dynamic_step__] driving [select_tools, compress_memory]\n[ReCentAutoma]-[ReCentAutoma-7708d76f] [observe] triggers [select_tools]\n[ReCentAutoma]-[ReCentAutoma-7708d76f] [observe] triggers [compress_memory]\n[ReCentAutoma]-[ReCentAutoma-7708d76f] \ud83e\udded Memory Check\n    Compression Needed: False\n[ReCentAutoma]-[ReCentAutoma-7708d76f] \ud83d\udd27 Tool Selection\n    (No tools selected)\n\n    LLM Response: The browser has been successfully closed, and I have the recent gold price data extracted. No further browsing actions are needed.\n\nNext, I will provide a summary of the recent gold price trends for RMB-denominated kilo gold based on the extracted data. This step does not require a tool and can be done directly in the response.\n\nSummary of recent gold price trends:\n- On 2026-01-26, the benchmark price was 1134.15 with an open price of 1121.43 and a high of 1142.14.\n- On 2026-01-22, the benchmark price was 1073.00 with an open price of 1077.35, high of 1105.64, low of 1068.54, and close of 1105.24.\n- On 2026-01-21, the benchmark price was 1084.50 with an open price of 1065.21, high of 1093.60, low of 1063.93, and close of 1080.88.\n- Data for 2026-01-23 was incomplete or missing.\n\nOverall, the recent gold prices showed some fluctuation but a general upward trend between January 21 and January 26, 2026.\n[ReCentAutoma]-[ReCentAutoma-7708d76f] [__dynamic_step__] driving [finalize_answer]\n[ReCentAutoma]-[ReCentAutoma-7708d76f] [select_tools] triggers [finalize_answer]\n[ReCentAutoma]-[ReCentAutoma-7708d76f] is finished.\nFinal Result:\n\n\nBased on the information gathered from the Hong Kong Gold Exchange (HKGX) website (https://hkgx.com.hk/en), here is a summary of the recent gold price trends for RMB-denominated kilo gold:\n\n1. Navigational Steps:\n- Accessed the HKGX homepage.\n- Hovered over the 'Market &amp; Data' menu to reveal additional options.\n- Selected and clicked on the 'Price History' option to open the historical prices page.\n- Chose the \"RMB Kilo Gold\" product option for the current date range.\n- Clicked the search button to retrieve recent historical price data.\n\n2. Recent Gold Price Data for RMB-denominated Kilo Gold (Selected Dates in January 2026):\n| Date       | Open     | High     | Low      | Close    | Benchmark Price |\n|------------|----------|----------|----------|----------|-----------------|\n| 2026-01-26 | 1121.43  | 1142.14  | 1120.03  | -        | 1134.15         |\n| 2026-01-23 | -        | 0.00     | 0.00     | -        | -               |\n| 2026-01-22 | 1077.35  | 1105.64  | 1068.54  | 1105.24  | 1073.00         |\n| 2026-01-21 | 1065.21  | 1093.60  | 1063.93  | 1080.88  | 1084.50         |\n\n3. Observations and Trends:\n- The benchmark price on January 26, 2026, was 1134.15 RMB per kilo, reflecting an increase compared to previous dates.\n- From January 21 to January 26, prices opened around 1065 to 1121 RMB and showed upward movement with highs reaching above 1142 RMB.\n- Some missing or zero values around January 23 possibly indicate non-trading days or unavailable data.\n- Overall, there is a general upward trend in the benchmark prices and daily highs over the few recorded days in late January 2026.\n\nIn conclusion, the RMB-denominated kilo gold prices on the Hong Kong Gold Exchange have shown an upward trend in the past few days of January 2026, with the latest benchmark price around 1134.15 RMB per kilo as of January 26, 2026. This reflects strengthening price momentum within this timeframe.\n\nIf you need gold price data for other denominations, dates, or require ongoing updates, you can use the HKGX Price History page to explore as needed.\n\n\u2713 Connection closed: True\n</pre> <p>In this example, the agent autonomously uses a browser to navigate to the Hong Kong Gold Exchange website and search for the gold price. The entire execution process is fully recorded during the execution of <code>ReCentAutoma</code> (as shown above).</p> <p>Because we launched the Playwright MCP Server with the <code>--save-video</code> option, a recording of the session is saved after execution. The entire browser operation can be viewed in the following video:</p> <p></p>"},{"location":"tutorials/items/agentic_module/about_recent_automa/#recentautoma","title":"ReCentAutoma\u00b6","text":"<p><code>ReCentAutoma</code> is an agentic module implementation in Bridgic that provides a goal-oriented, guidance-enabled long-running autonomous agent. It enables developers to build intelligent systems that can autonomously plan and execute multi-step tasks using tools, with built-in memory management and goal tracking.</p>"},{"location":"tutorials/items/agentic_module/about_recent_automa/#introduction","title":"Introduction\u00b6","text":"<p><code>ReCentAutoma</code> serves as a specialized agentic module in Bridgic, designed to provide a goal-oriented, guidance-enabled implementation for long-running autonomous agents. It addresses the fundamental challenges of building intelligent systems that can autonomously plan, execute, and adapt over extended interactions.</p>"},{"location":"tutorials/items/agentic_module/about_recent_automa/#when-to-use-recentautoma","title":"When to Use ReCentAutoma\u00b6","text":"<p><code>ReCentAutoma</code> is suitable when you need to develop an application with autonomous capabilities and dynamic tool usage. In many cases, developers also need:</p> <ul> <li>Efficient memory management: As tasks span multiple steps and tool calls, conversation history grows. ReCentAutoma's built-in ReCENT memory managemnt algorithm automatically compresses history when needed, preventing context explosion.</li> <li>Natural language goal definition: ReCentAutoma accepts <code>goal</code> as a first-class input, so you can state your objectives directly in natural language.</li> <li>Customized task guidance: You can provide an optional <code>guidance</code> input to further constrain or direct the agent's behavior, enabling more control across the execution.</li> </ul> <p>These capabilities are well-supported in <code>ReCentAutoma</code>, making it ideal for building autonomous agents that need to:</p> <ul> <li>Dynamically make decisions at runtime to select tools for each step</li> <li>Autonomously execute multiple steps during a long-time task</li> <li>Automatically maintain alignment with the initial goal throughout the entire execution process</li> </ul>"},{"location":"tutorials/items/agentic_module/about_recent_automa/#how-does-recentautoma-work","title":"How Does ReCentAutoma Work\u00b6","text":"<p><code>ReCentAutoma</code> leverages <code>GraphAutoma</code>'s core features\u2014dynamic topology and dynamic routing\u2014to implement an enhanced ReAct (Resoning and Acting) loop. It incorporates the innovative ReCENT (Recursive Compressed Episodic Node Tree) memory management algorithm, which triggers memory compression when necessary, effectively addressing two long-standing challenges in autonomous agents:</p> <ul> <li>Context Explosion: As conversation history grows linearly, the context window eventually overflows. ReCENT automatically compresses episodic memory when thresholds are reached, maintaining a compact yet informative context.</li> <li>Goal Drift: Without explicit goal management, agents tend to deviate from original objectives during extended execution. ReCentAutoma maintains goal and guidance as persistent, non-compressible nodes, ensuring the agent stays focused on the original task.</li> </ul>"},{"location":"tutorials/items/agentic_module/about_recent_automa/#quick-start","title":"Quick Start\u00b6","text":"<p>Let's explore <code>ReCentAutoma</code> through practical examples. We'll start with a simple weight management assistant using custom tools, then move to a more advanced browser automation agent using MCP integration.</p> <p>First of all we have to initialize an LLM instance:</p>"},{"location":"tutorials/items/agentic_module/about_recent_automa/#example-1-building-a-weight-management-assistant-with-custom-tools","title":"Example 1: Building a Weight Management Assistant with Custom Tools\u00b6","text":"<p>In this example, we'll create custom tools for BMI calculation and ideal weight calculation, then use ReCentAutoma to build a weight management assistant that can help users understand their health metrics.</p>"},{"location":"tutorials/items/agentic_module/about_recent_automa/#example-2-building-a-browser-automation-agent-with-playwright-mcp-server","title":"Example 2: Building a Browser Automation Agent with Playwright MCP Server\u00b6","text":"<p>This example demonstrates how to integrate <code>ReCentAutoma</code> with MCP (Model Context Protocol) to create a browser automation agent. We'll use the Playwright MCP server to enable the agent to interact with browsers like Chrome autonomously to achieve the user-defined goal.</p> <p>This example demonstrates how to combine <code>ReCentAutoma</code> with MCP (Model Context Protocol) Integration to build a browser automation agent for retrieving the latest gold prices from the official website of the Hong Kong Gold Exchange.</p> <p>Steps overview:</p> <ol> <li>Connect to the Playwright MCP server via stdio transport for browser automation capabilities.</li> <li>Register the connection using a dedicated Connection Manager to manage browser operations in isolation.</li> <li>Create a <code>ReCentAutoma</code> agent loaded with browser automation tools.</li> <li>Let the agent autonomously navigate to the Hong Kong Gold Exchange website and extract the needed information.</li> </ol> <p>If you\u2019re new to MCP, please check the MCP Quick Start for setup and usage instructions.</p>"},{"location":"tutorials/items/agentic_module/about_recent_automa/#build-a-multi-agent-system-with-recentautoma","title":"Build a Multi-Agent System with ReCentAutoma\u00b6","text":"<p>Bridgic\u2019s component-based architecture makes it a natural choice for building Multi-Agent systems.</p> <p>Because <code>ReCentAutoma</code> itself is an <code>Automa</code> (and also a <code>Worker</code>), it can be flexibly nested and combined with other automas. This allows you to create hierarchical agentic systems, where each agent manages its own goals, tools, and memory independently. There are two common methods for building a Multi-Agent system:</p> <ul> <li>Utilizing a <code>ReCentAutoma</code> instance as a nested worker, allowing for flexible and customized dynamic routing</li> <li>Integrating <code>ReCentAutoma</code> as a kind of tool, enabling intelligent and context-aware parameter filling</li> </ul> <p>More articles on this topic will be published in the near future.</p>"},{"location":"tutorials/items/agentic_module/about_sequential_automa/","title":"SequentialAutoma","text":"In\u00a0[5]: Copied! <pre>from bridgic.core.automa import worker\nfrom bridgic.core.agentic import SequentialAutoma\n\n# Define the data processing pipeline\nclass DataProcessingPipeline(SequentialAutoma):\n    @worker()\n    def filter(self, data: list) -&gt; list:\n        \"\"\"Step 1: Filter data (receives output from previous step)\"\"\"\n        print(f\"Filtering data: {data}\")\n        return [x for x in data if x % 2 == 0]\n\n    @worker()\n    def transform(self, data: list) -&gt; list:\n        \"\"\"Step 2: Transform data (receives output from previous step)\"\"\"\n        print(f\"Transforming data: {data}\")\n        return [x * 2 for x in data]\n\n    @worker()\n    async def save(self, data: list) -&gt; dict:\n        \"\"\"Step 3: Save data (receives output from previous step)\"\"\"\n        print(f\"Saving data: {data}\")\n        return {\"saved\": True, \"count\": len(data), \"data\": data}\n\n\n# Execute all workers sequentially\nasync def main():\n    # Create SequentialAutoma and add workers\n    pipeline = DataProcessingPipeline()\n\n    result = await pipeline.arun(data=[1, 2, 3, 4, 5, 6, 7, 8, 9, 10])\n    print(f\"\\nFinal result: {result}\")\n\nawait main()\n</pre> from bridgic.core.automa import worker from bridgic.core.agentic import SequentialAutoma  # Define the data processing pipeline class DataProcessingPipeline(SequentialAutoma):     @worker()     def filter(self, data: list) -&gt; list:         \"\"\"Step 1: Filter data (receives output from previous step)\"\"\"         print(f\"Filtering data: {data}\")         return [x for x in data if x % 2 == 0]      @worker()     def transform(self, data: list) -&gt; list:         \"\"\"Step 2: Transform data (receives output from previous step)\"\"\"         print(f\"Transforming data: {data}\")         return [x * 2 for x in data]      @worker()     async def save(self, data: list) -&gt; dict:         \"\"\"Step 3: Save data (receives output from previous step)\"\"\"         print(f\"Saving data: {data}\")         return {\"saved\": True, \"count\": len(data), \"data\": data}   # Execute all workers sequentially async def main():     # Create SequentialAutoma and add workers     pipeline = DataProcessingPipeline()      result = await pipeline.arun(data=[1, 2, 3, 4, 5, 6, 7, 8, 9, 10])     print(f\"\\nFinal result: {result}\")  await main() <pre>Filtering data: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\nTransforming data: [2, 4, 6, 8, 10]\nSaving data: [4, 8, 12, 16, 20]\n\nFinal result: {'saved': True, 'count': 5, 'data': [4, 8, 12, 16, 20]}\n</pre>"},{"location":"tutorials/items/agentic_module/about_sequential_automa/#sequentialautoma","title":"SequentialAutoma\u00b6","text":""},{"location":"tutorials/items/agentic_module/about_sequential_automa/#introduction","title":"Introduction\u00b6","text":"<p><code>SequentialAutoma</code> is an orchestration Automa in the Bridgic framework designed for sequential execution of multiple workers. It ensures that workers execute in the order they are added, with each worker's output serving as the input for the next worker.</p> <ul> <li>Simplified Interface: Provides a simpler sequential orchestration interface compared to <code>GraphAutoma</code></li> <li>Linear Workflow: Ensures an ordered, step-by-step processing flow</li> <li>Data Passing: The output of the previous worker is automatically passed to the next worker</li> </ul>"},{"location":"tutorials/items/agentic_module/about_sequential_automa/#example","title":"Example\u00b6","text":"<p>Let's learn how to use SequentialAutoma through practical examples. In the following example, we'll create a simple data processing pipeline where each step depends on the output of the previous step.</p>"},{"location":"tutorials/items/asl/quick_start/","title":"Agent Structure Language","text":"In\u00a0[\u00a0]: Copied! <pre># Get the environment variables.\nimport os\n\n_api_key = os.environ.get(\"OPENAI_API_KEY\")\n_api_base = os.environ.get(\"OPENAI_API_BASE\")\n_model_name = os.environ.get(\"OPENAI_MODEL_NAME\")\n\n# Import the necessary packages.\nfrom typing import List, Dict\nfrom bridgic.core.model.types import Message, Role\nfrom bridgic.asl import ASLAutoma, graph\nfrom bridgic.llms.openai import OpenAILlm, OpenAIConfiguration\n\nllm = OpenAILlm(  # the llm instance\n    api_base=_api_base,\n    api_key=_api_key,\n    timeout=5,\n    configuration=OpenAIConfiguration(model=_model_name),\n)\n</pre> # Get the environment variables. import os  _api_key = os.environ.get(\"OPENAI_API_KEY\") _api_base = os.environ.get(\"OPENAI_API_BASE\") _model_name = os.environ.get(\"OPENAI_MODEL_NAME\")  # Import the necessary packages. from typing import List, Dict from bridgic.core.model.types import Message, Role from bridgic.asl import ASLAutoma, graph from bridgic.llms.openai import OpenAILlm, OpenAIConfiguration  llm = OpenAILlm(  # the llm instance     api_base=_api_base,     api_key=_api_key,     timeout=5,     configuration=OpenAIConfiguration(model=_model_name), ) <p>Now, let's implement the core functions that will be used in our agent workflow.</p> In\u00a0[\u00a0]: Copied! <pre># Break down the query into a list of sub-queries.\nasync def break_down_query(user_input: str) -&gt; List[str]:\n    llm_response = await llm.achat(\n        messages=[\n            Message.from_text(text=f\"Break down the query into multiple sub-queries and only return the sub-queries\", role=Role.SYSTEM),\n            Message.from_text(text=user_input, role=Role.USER,),\n        ]\n    )\n    return [item.strip() for item in llm_response.message.content.split(\"\\n\") if item.strip()]\n\n# Generate answers for each sub-query.\nasync def query_answer(queries: List[str]) -&gt; Dict[str, str]:\n    answers = []\n    for query in queries:\n        response = await llm.achat(\n            messages=[\n                Message.from_text(text=f\"Answer the given query briefly\", role=Role.SYSTEM),\n                Message.from_text(text=query, role=Role.USER,),\n            ]\n        )\n        answers.append(response.message.content)\n    \n    res = {\n        query: answer\n        for query, answer in zip(queries, answers)\n    }\n    return res\n</pre> # Break down the query into a list of sub-queries. async def break_down_query(user_input: str) -&gt; List[str]:     llm_response = await llm.achat(         messages=[             Message.from_text(text=f\"Break down the query into multiple sub-queries and only return the sub-queries\", role=Role.SYSTEM),             Message.from_text(text=user_input, role=Role.USER,),         ]     )     return [item.strip() for item in llm_response.message.content.split(\"\\n\") if item.strip()]  # Generate answers for each sub-query. async def query_answer(queries: List[str]) -&gt; Dict[str, str]:     answers = []     for query in queries:         response = await llm.achat(             messages=[                 Message.from_text(text=f\"Answer the given query briefly\", role=Role.SYSTEM),                 Message.from_text(text=query, role=Role.USER,),             ]         )         answers.append(response.message.content)          res = {         query: answer         for query, answer in zip(queries, answers)     }     return res <p>Now, let's use ASL to orchestrate this workflow. The core idea of ASL is: first declare all workers under a corresponding graph, assigning a unique key to both the graph and each worker, and then define the dependencies between them.</p> In\u00a0[4]: Copied! <pre>class SplitSolveAgent(ASLAutoma):\n    with graph as g:\n        a = break_down_query\n        b = query_answer\n\n        +a &gt;&gt; ~b\n</pre> class SplitSolveAgent(ASLAutoma):     with graph as g:         a = break_down_query         b = query_answer          +a &gt;&gt; ~b <p>The <code>SplitSolveAgent</code> implementation uses ASL syntax to orchestrate the workflow. Let's break down the core ASL syntax:</p> <p>Key points:</p> <ul> <li><code>with graph as g:</code> - Opens a graph context. Within this block, you can declare workers and define dependencies between them.</li> <li><code>a = break_down_query</code> - Declares a worker named <code>a</code> that corresponds to the <code>break_down_query</code> function</li> <li><code>a &gt;&gt; b</code> - Defines a dependency: <code>b</code> depends on <code>a</code>, meaning <code>b</code> will execute only after <code>a</code> completes</li> <li><code>+a</code> - Marks <code>a</code> as a start worker (entry point of the workflow)</li> <li><code>~b</code> - Marks <code>b</code> as an output worker (exit point of the workflow)</li> </ul> <p>Syntax Reference</p> <p>In the syntax specification below, angle brackets like <code>&lt;GRAPH&gt;</code>, <code>&lt;KEY&gt;</code>, <code>&lt;WORKER&gt;</code>, etc., are metasyntax variables (placeholders) that represent entities of specific types. Replace them with actual values when writing your code.</p> Syntax Meaning Example <code>with &lt;GRAPH&gt; as &lt;KEY&gt;:</code> Defines a graph container with a given key.Notes:<code>&lt;GRAPH&gt;</code> can be either <code>graph</code> or <code>concurrent</code>:1. <code>graph</code>: Allows arbitrary orchestration of worker execution order  2. <code>concurrent</code>: All workers added to it will execute concurrently<code>&lt;KEY&gt;</code> is required and must be unique. Without it, the program cannot identify this as an executable graph structure <code>with graph as g:</code> declares a graph named <code>g</code>, under which you can start declaring workers and orchestrating the workflow <code>&lt;KEY&gt; = &lt;WORKER&gt;</code> Declares and registers a worker with a unique key.Notes:<code>&lt;WORKER&gt;</code> can be one of three types: 1. Python functions (defined with <code>def</code> or <code>async def</code>)  2. Bridgic <code>Worker</code> objects (e.g., <code>GraphAutoma</code>, custom <code>Worker</code> classes) 3. Another agent implemented using <code>ASLAutoma</code> <code>&lt;KEY&gt;</code> is required and must be unique within the graph scope. Without it, the worker cannot be identified and scheduled All worker declarations must be written inside a <code>with &lt;GRAPH&gt; as &lt;KEY&gt;:</code> block, otherwise workers cannot be registered to the correct execution graph <code>a = break_down_query</code> registers <code>break_down_query</code> as a worker with key <code>a</code> <code>&lt;KEY1&gt; &gt;&gt; &lt;KEY2&gt;</code> Defines a dependency: The worker named <code>&lt;KEY2&gt;</code> will be triggered after the worker named <code>&lt;WORKER1&gt;</code> completes execution <code>a &gt;&gt; b</code> means <code>b</code> will execute after <code>a</code> finishes <code>+&lt;KEY&gt;</code> Marks a worker as a start worker (entry point of the execution graph) <code>+a</code> marks <code>a</code> as the starting worker of the execution graph <code>~&lt;KEY&gt;</code> Marks a worker as an output worker (exit point of the execution graph) <code>~a</code> marks <code>a</code> as an output worker of the execution graph <p>Now let's run this agent!</p> In\u00a0[5]: Copied! <pre>text_generation_agent = SplitSolveAgent()\nawait text_generation_agent.arun(\"When and where was the Einstein born?\")\n</pre> text_generation_agent = SplitSolveAgent() await text_generation_agent.arun(\"When and where was the Einstein born?\") Out[5]: <pre>{'1. When was Einstein born?': 'Albert Einstein was born in 1879.',\n '2. Where was Einstein born?': 'Albert Einstein was born in Ulm, Kingdom of W\u00fcrttemberg, German Empire.'}</pre> <p>Excellent! We successfully obtained the results. From the ASL code, we can clearly see that <code>SplitSolveAgent</code> has only two workers, with <code>b</code> depending on <code>a</code>, and no other redundant information. ASL elevates worker declaration and dependency management to first-class language constructs.</p> In\u00a0[\u00a0]: Copied! <pre>async def merge_answers(qa_pairs: Dict[str, str], user_input: str) -&gt; str:\n    answers = \"\\n\".join([v for v in qa_pairs.values()])\n    llm_response = await llm.achat(\n        messages=[\n            Message.from_text(text=f\"Merge the given answers into a unified response to the original question\", role=Role.SYSTEM),\n            Message.from_text(text=f\"Query: {user_input}\\nAnswers: {answers}\", role=Role.USER,),\n        ]\n    )\n    return llm_response.message.content\n\n# Define the Chatbot agent, reuse `SplitSolveAgent` in a component-oriented parttern.\nclass Chatbot(ASLAutoma):\n    with graph as g:\n        a = SplitSolveAgent()\n        b = merge_answers\n\n        +a &gt;&gt; ~b\n</pre> async def merge_answers(qa_pairs: Dict[str, str], user_input: str) -&gt; str:     answers = \"\\n\".join([v for v in qa_pairs.values()])     llm_response = await llm.achat(         messages=[             Message.from_text(text=f\"Merge the given answers into a unified response to the original question\", role=Role.SYSTEM),             Message.from_text(text=f\"Query: {user_input}\\nAnswers: {answers}\", role=Role.USER,),         ]     )     return llm_response.message.content  # Define the Chatbot agent, reuse `SplitSolveAgent` in a component-oriented parttern. class Chatbot(ASLAutoma):     with graph as g:         a = SplitSolveAgent()         b = merge_answers          +a &gt;&gt; ~b <p>Let's run the chatbot:</p> In\u00a0[7]: Copied! <pre>chatbot = Chatbot()\nawait chatbot.arun(user_input=\"When and where was the Einstein born?\")\n</pre> chatbot = Chatbot() await chatbot.arun(user_input=\"When and where was the Einstein born?\") Out[7]: <pre>'Albert Einstein was born in 1879 in Ulm, Kingdom of W\u00fcrttemberg, German Empire.'</pre> <p>Perfect! Our chatbot successfully answered the question. Notice that in the implementation, we directly declared <code>SplitSolveAgent</code> within the <code>with graph</code> block. We didn't need to write a wrapper function or use any API to manually add it to the graph. Everything was naturally declared and composed.</p> In\u00a0[\u00a0]: Copied! <pre>class ChatbotNested(ASLAutoma):\n    with graph as g:  # The chatbot agent defines its graph\n\n        with graph as split_solve:  # The split_solve agent defines its graph\n            a = break_down_query\n            b = query_answer    \n            +a &gt;&gt; ~b\n\n        end = merge_answers\n        +split_solve &gt;&gt; ~end\n</pre> class ChatbotNested(ASLAutoma):     with graph as g:  # The chatbot agent defines its graph          with graph as split_solve:  # The split_solve agent defines its graph             a = break_down_query             b = query_answer                 +a &gt;&gt; ~b          end = merge_answers         +split_solve &gt;&gt; ~end <p>Let's run it!</p> In\u00a0[9]: Copied! <pre>chatbot_nested = ChatbotNested()\nawait chatbot_nested.arun(user_input=\"When and where was the Einstein born?\")\n</pre> chatbot_nested = ChatbotNested() await chatbot_nested.arun(user_input=\"When and where was the Einstein born?\") Out[9]: <pre>'Albert Einstein was born in 1879 in Ulm, Kingdom of W\u00fcrttemberg, German Empire.'</pre> <p>We successfully achieved the expected result! We can clearly see that <code>ChatbotNested</code> has two layers of graphs, and the arrangement structure of each graph is visible at a glance.</p> <p>Important: workers in one graph cannot reference workers from another graph. In the above example, for graph <code>g</code>, workers <code>a</code> and <code>b</code> are invisible\u2014it can only use <code>split_solve</code> as a whole. Similarly, <code>split_solve</code> cannot see workers within other graphs.</p> <p>Note: If you reference a worker that doesn't exist in the current graph scope, an exception will be raised.</p> <p>These are the basic usages of ASL. Next, let's explore more advanced features.</p> In\u00a0[10]: Copied! <pre>async def add1(x: int) -&gt; int:\n    return x + 1\n\nasync def add2(x: int) -&gt; int:\n    return x + 2\n\nasync def multiply(x: int) -&gt; int:\n    return x * 2\n\nasync def division(x: int) -&gt; int:\n    return x / 2\n\nasync def merge_result(x: int, y: int) -&gt; int:\n    return x + y\n\nclass MyAutoma(ASLAutoma):\n    with graph as g:\n        a = add1\n        b = add2\n        add_process = +a &gt;&gt; b\n\n        c = multiply\n        d = division\n        add_multiply = add_process &gt;&gt; c\n        add_division = add_process &gt;&gt; d\n\n        merge = merge_result\n        (add_multiply &amp; add_division) &gt;&gt; ~merge\n</pre> async def add1(x: int) -&gt; int:     return x + 1  async def add2(x: int) -&gt; int:     return x + 2  async def multiply(x: int) -&gt; int:     return x * 2  async def division(x: int) -&gt; int:     return x / 2  async def merge_result(x: int, y: int) -&gt; int:     return x + y  class MyAutoma(ASLAutoma):     with graph as g:         a = add1         b = add2         add_process = +a &gt;&gt; b          c = multiply         d = division         add_multiply = add_process &gt;&gt; c         add_division = add_process &gt;&gt; d          merge = merge_result         (add_multiply &amp; add_division) &gt;&gt; ~merge <p>In this example, we have two logical sequences: one performs two additions followed by multiplication, and the other performs two additions followed by division. We can name the common <code>a &gt;&gt; b</code> sequence as <code>add_process</code> and reuse it, avoiding repetition. We also named each complete logical fragment (<code>add_multiply</code> and <code>add_division</code>) and then arranged them together.</p> <p>Key points:</p> <ul> <li><code>add_process = +a &gt;&gt; b</code>: Declares the orchestration logic <code>+a &gt;&gt; b</code> as a fragment named <code>add_process</code>. This fragment can be reused later to compose new execution logic without repeating the same sequence.</li> <li><code>(add_multiply &amp; add_division)</code>: Groups <code>add_multiply</code> and <code>add_division</code> together. They will be treated as a single unit during orchestration.</li> </ul> <p>Syntax Reference</p> Syntax Meaning Example <code>&lt;KEY&gt; = &lt;ORCHESTRATION_EXPRESSION&gt;</code> Defines and registers a fragment of a workflow.Note: The <code>+</code> and <code>~</code> operators cannot be applied to fragments. If you declare <code>flow = a &gt;&gt; b</code> and then use <code>+flow</code>, this is not equivalent to <code>+a &gt;&gt; b</code>. <code>flow = a &gt;&gt; b</code> registers the execution logic <code>a &gt;&gt; b</code> as a fragment named <code>flow</code>. You can reuse this fragment later without rewriting <code>a &gt;&gt; b</code> <code>&lt;KEY1&gt; &amp; &lt;KEY2&gt;</code> Defines a union unit (parallel group).When an operator is applied to this unit, it operates on all elements within it, following the distributive property. <code>(a &amp; b) &gt;&gt; c</code> means <code>c</code> will execute after both <code>a</code> and <code>b</code> finish; <code>a &gt;&gt; (b &amp; c)</code> means both <code>b</code> and <code>c</code> depend on <code>a</code> and will execute concurrently after <code>a</code> completes In\u00a0[11]: Copied! <pre>my_automa = MyAutoma()\nawait my_automa.arun(x=1)\n</pre> my_automa = MyAutoma() await my_automa.arun(x=1) Out[11]: <pre>10.0</pre> In\u00a0[12]: Copied! <pre>from bridgic.core.automa.args import ArgsMappingRule\nfrom bridgic.asl import Settings\n\n\nasync def start1(user_input: int) -&gt; int:\n    return user_input + 1\n\nasync def start2(user_input: int) -&gt; int:\n    return user_input + 2\n\nasync def worker1(x: List[int], user_input: int) -&gt; int:\n    return sum(x) + user_input\n\n\nclass MyAutoma(ASLAutoma):\n    with graph as g:\n        a = start1\n        b = start2\n        c = worker1 *Settings(args_mapping_rule=ArgsMappingRule.MERGE)\n\n        +(a &amp; b) &gt;&gt; ~c\n</pre> from bridgic.core.automa.args import ArgsMappingRule from bridgic.asl import Settings   async def start1(user_input: int) -&gt; int:     return user_input + 1  async def start2(user_input: int) -&gt; int:     return user_input + 2  async def worker1(x: List[int], user_input: int) -&gt; int:     return sum(x) + user_input   class MyAutoma(ASLAutoma):     with graph as g:         a = start1         b = start2         c = worker1 *Settings(args_mapping_rule=ArgsMappingRule.MERGE)          +(a &amp; b) &gt;&gt; ~c <p>The above code defines this structure:</p> <p>Key points:</p> <ul> <li><code>add_process = +a &gt;&gt; b</code>: Declares the orchestration logic <code>+a &gt;&gt; b</code> as a fragment named <code>add_process</code>. This fragment can be reused later to compose new execution logic without repeating the same sequence.</li> <li><code>(add_multiply &amp; add_division)</code>: Groups <code>add_multiply</code> and <code>add_division</code> together. They will be treated as a single unit during orchestration.</li> </ul> <p>Syntax Reference</p> Syntax Meaning Example <code>&lt;WORKER&gt; *Settings(...)</code> Attaches configuration settings to a worker using the <code>*</code> operator.Settings supports three configuration fields:\u2022 <code>key</code>: The unique identifier for the worker at runtime, defaults to the <code>&lt;KEY&gt;</code> from <code>&lt;KEY&gt; = &lt;WORKER&gt;</code>\u2022 <code>args_mapping_rule</code>: Defines how the worker receives results from preceding workers, defaults to <code>ArgsMappingRule.AS_IS</code>\u2022 <code>result_dispatching_rule</code>: Defines how the worker dispatches its results to subsequent workers, defaults to <code>ResultDispatchingRule.AS_IS</code> <code>worker1 *Settings(args_mapping_rule=ArgsMappingRule.MERGE)</code> configures <code>worker1</code> to merge results from multiple dependencies into a list. Other fields remain at their default values <p>Note: The <code>key</code> defined in <code>Settings</code> is only used internally for scheduling during the graph execution. Therefore, when orchestrating the workflow in ASL, you still need to use the <code>&lt;KEY&gt;</code> in <code>&lt;KEY&gt; = &lt;WORKER&gt;</code>.</p> <p>Let's run it:</p> In\u00a0[13]: Copied! <pre>my_automa = MyAutoma()\nawait my_automa.arun(user_input=1)\n</pre> my_automa = MyAutoma() await my_automa.arun(user_input=1) Out[13]: <pre>6</pre> In\u00a0[14]: Copied! <pre>from bridgic.core.automa.args import From\nfrom bridgic.asl import Data\n\n\nasync def worker1(user_input: int) -&gt; int:\n    return user_input + 1\n\nasync def worker2(x: int) -&gt; int:\n    return x + 2\n\nasync def worker3(x: int, y: int) -&gt; int:\n    return x + y\n\n\nclass MyAutoma(ASLAutoma):\n    with graph as g:\n        a = worker1\n        b = worker2\n        c = worker3 *Data(y=From('a'))\n\n        +a &gt;&gt; b &gt;&gt; ~c\n</pre> from bridgic.core.automa.args import From from bridgic.asl import Data   async def worker1(user_input: int) -&gt; int:     return user_input + 1  async def worker2(x: int) -&gt; int:     return x + 2  async def worker3(x: int, y: int) -&gt; int:     return x + y   class MyAutoma(ASLAutoma):     with graph as g:         a = worker1         b = worker2         c = worker3 *Data(y=From('a'))          +a &gt;&gt; b &gt;&gt; ~c <p>Key points:</p> <ul> <li><code>*Data(y=From('a'))</code>: The <code>*</code> operator attaches data configuration to a worker. <code>From('a')</code> specifies that the parameter <code>y</code> of <code>worker3</code> should be injected from the result of worker <code>a</code> at runtime.</li> </ul> <p>Syntax Reference</p> Syntax Meaning Example <code>&lt;WORKER&gt; *Data(...)</code> Attaches data configuration to a worker using the <code>*</code> operator. Important Note: Data injection is equivalent to assigning a default value to a function parameter, which is dynamically injected at runtime. For example, <code>c = worker3 *Data(y=From('a'))</code> is conceptually equivalent to <code>async def worker3(x: int, y: int = From('a'))</code>. This means you cannot write <code>c = worker3 *Data(x=From('a'))</code>, because it would be equivalent to <code>async def worker3(x: int = From('a'), y: int)</code>, which violates Python\u2019s rule that parameters with default values cannot precede parameters without default values in a function signature. <code>worker3 *Data(y=From('a'))</code> injects the result of worker <code>a</code> into parameter <code>y</code> of <code>worker3</code> In\u00a0[15]: Copied! <pre>my_automa = MyAutoma()\nawait my_automa.arun(user_input=1)\n</pre> my_automa = MyAutoma() await my_automa.arun(user_input=1) Out[15]: <pre>6</pre> In\u00a0[\u00a0]: Copied! <pre>from bridgic.asl import ASLField, concurrent\nfrom bridgic.core.automa.args import ResultDispatchingRule\n\n\nasync def produce_task(user_input: int) -&gt; List[int]:\n    tasks = [i for i in range(user_input)]\n    return tasks\n\nasync def task_handler(sub_task: int) -&gt; int:\n    res = sub_task + 1\n    return res\n\n\nclass DynamicGraph(ASLAutoma):\n    with graph(user_input=ASLField(type=int)) as g:\n        producer = produce_task\n\n        with concurrent(tasks = ASLField(type=list, dispatching_rule=ResultDispatchingRule.IN_ORDER)) as c:\n            dynamic_handler = lambda tasks: (\n                task_handler *Settings(key=f\"handler_{task}\")\n                for task in tasks\n            )\n\n        +producer &gt;&gt; ~c\n</pre> from bridgic.asl import ASLField, concurrent from bridgic.core.automa.args import ResultDispatchingRule   async def produce_task(user_input: int) -&gt; List[int]:     tasks = [i for i in range(user_input)]     return tasks  async def task_handler(sub_task: int) -&gt; int:     res = sub_task + 1     return res   class DynamicGraph(ASLAutoma):     with graph(user_input=ASLField(type=int)) as g:         producer = produce_task          with concurrent(tasks = ASLField(type=list, dispatching_rule=ResultDispatchingRule.IN_ORDER)) as c:             dynamic_handler = lambda tasks: (                 task_handler *Settings(key=f\"handler_{task}\")                 for task in tasks             )          +producer &gt;&gt; ~c <p>In this example, the <code>producer</code> worker generates a list based on <code>user_input</code>. Each element in this list needs to be processed by a <code>task_handler</code> worker. However, we don't know how many <code>task_handler</code> workers we'll need until runtime.</p> <p>This example uses the <code>concurrent</code> container, which represents a graph structure where all internal workers run concurrently.</p> <p>Key points:</p> <ul> <li><code>with graph(&lt;PARAM&gt;=ASLField(...))</code>: Declares input parameters for a graph using <code>ASLField</code> to specify type and behavior. Parameter names must match those used by workers within the graph.</li> <li><code>with concurrent(...)</code>: Creates a concurrent execution container where all internal workers execute concurrently.</li> <li><code>dynamic_handler = lambda ...</code>: A lambda function that generates worker instances dynamically at runtime.</li> </ul> <p>Syntax Reference</p> Syntax Meaning Example <code>with graph(&lt;PARAM&gt;=ASLField(...)) as &lt;KEY&gt;</code> Declares input parameters for a graph. <code>ASLField</code> is a field class that extends Pydantic's <code>FieldInfo</code>, used to specify parameter type, default values, and behavior. ASLField parameters: <code>type</code> (the parameter type), <code>default</code> (optional default value), and <code>dispatching_rule</code> (for concurrent containers, controls how data is distributed). Parameter names must match those used by workers within the graph. <code>with graph(user_input=ASLField(type=int)) as g:</code> declares a graph parameter <code>user_input</code> of type <code>int</code> <code>&lt;KEY&gt; = lambda &lt;PARAM&gt;: (...)</code> Defines a lambda function that generates worker instances dynamically. It receives parameters from the container and returns a generator of worker instances. ASL creates and executes these workers at runtime. <code>dynamic_handler = lambda tasks: (task_handler *Settings(key=f\"task_{i}\") for i, task in enumerate(tasks))</code> generates worker instances based on the <code>tasks</code> parameter <p>Note: Lambda functions for dynamic workers must be declared within a <code>concurrent</code> or other container(such as <code>sequential</code> in the future), not within a regular <code>graph</code>.</p> In\u00a0[19]: Copied! <pre>dynamic_graph = DynamicGraph()\nawait dynamic_graph.arun(user_input=3)\n</pre> dynamic_graph = DynamicGraph() await dynamic_graph.arun(user_input=3) Out[19]: <pre>[1, 2, 3]</pre>"},{"location":"tutorials/items/asl/quick_start/#agent-structure-language","title":"Agent Structure Language\u00b6","text":"<p>ASL (Agent Structure Language) is a Python internal DSL (Domain Specific Language) for building agents. This tutorial will walk you through the core concepts and the most commonly used features of ASL.</p>"},{"location":"tutorials/items/asl/quick_start/#overview","title":"Overview\u00b6","text":"<p>ASL is a declarative language for agent construction that follows a \"what-you-see-is-what-you-get\" philosophy. Once you've implemented your basic functions, ASL allows you to clearly express and define the orchestration process and hierarchical structure of your agent at a glance.</p> <p>In this tutorial, you will learn:</p> <ol> <li>How to build your first agent</li> <li>How to reuse existing agents in a componentized pattern</li> <li>How to build agents with nested graph structures</li> <li>How to reuse fragments of control flow</li> <li>How to control data transmission between workers</li> <li>How to achieve dynamic topology during runtime</li> </ol>"},{"location":"tutorials/items/asl/quick_start/#building-your-first-agent","title":"Building Your First Agent\u00b6","text":"<p>Let's start with a simple example: a text generation agent that breaks down a user query into sub-queries and generates answers for each one.</p> <p>First, let's set up the necessary environment and imports.</p>"},{"location":"tutorials/items/asl/quick_start/#componentized-agent-reuse","title":"Componentized Agent Reuse\u00b6","text":"<p>In the example above, we created an agent that splits queries and answers them separately. This is a reusable module. Now, let's build a chatbot that merges these individual answers into a unified response. We can directly reuse <code>SplitSolveAgent</code> just to write: <code>a = SplitSolveAgent()</code></p>"},{"location":"tutorials/items/asl/quick_start/#building-agents-with-nested-structures","title":"Building Agents with Nested Structures\u00b6","text":"<p>If you have all the functions ready but haven't created <code>SplitSolveAgent</code> as a separate class, you don't need to implement it separately just to reuse it. Instead, you can define everything directly in one agent using nested graphs:</p>"},{"location":"tutorials/items/asl/quick_start/#reusing-control-flow-fragments","title":"Reusing Control Flow Fragments\u00b6","text":"<p>In a workflow, certain parts of the process might be common across multiple paths. For example, if you have <code>a &gt;&gt; b &gt;&gt; c</code> and <code>a &gt;&gt; b &gt;&gt; d</code>, the <code>a &gt;&gt; b</code> sequence is shared. ASL allows you to name and reuse such fragments.</p> <p>Here's an example:</p>"},{"location":"tutorials/items/asl/quick_start/#controlling-data-transmission","title":"Controlling Data Transmission\u00b6","text":"<p>When ASL code is executed, it's first translated into corresponding Bridgic objects. For instance, a worker declared in ASL becomes a <code>Worker</code> during execution. Therefore, ASL inherits all the underlying capabilities of Bridgic.</p> <p>Bridgic provides rich parameter resolving mechanisms. In ASL, you can utilize these by configuring the <code>Settings</code> and <code>Data</code> attributes of a worker.</p>"},{"location":"tutorials/items/asl/quick_start/#using-settings-for-argument-mapping","title":"Using Settings for Argument Mapping\u00b6","text":"<p>The <code>Settings</code> class allows you to configure how arguments are mapped to workers. For example:</p>"},{"location":"tutorials/items/asl/quick_start/#using-data-for-argument-injection","title":"Using Data for Argument Injection\u00b6","text":"<p>Bridgic also provides an argument injection mechanism that can be used in ASL through the <code>Data</code> class. This allows you to inject values from other workers' results into function parameters:</p>"},{"location":"tutorials/items/asl/quick_start/#dynamic-topology-at-runtime","title":"Dynamic Topology at Runtime\u00b6","text":"<p>Sometimes you need to dynamically adjust the graph structure based on intermediate execution results. ASL provides the ability to declare such dynamic behaviors using lambda functions.</p> <p>A typical use case is creating workers dynamically based on the number of items in a list returned by a previous task. Each item needs its own handler worker, but you don't know how many handlers you'll need until runtime.</p> <p>Here's an example:</p>"},{"location":"tutorials/items/asl/quick_start/#summary","title":"Summary\u00b6","text":"<p>Congratulations! You've learned the features of ASL:</p> <ul> <li>\u2705 Basic syntax: Declaring workers, defining dependencies with <code>&gt;&gt;</code>, marking start/output workers with <code>+</code>/<code>~</code></li> <li>\u2705 Component reuse: Using existing agents as building blocks</li> <li>\u2705 Nested structures: Creating hierarchical graph organizations</li> <li>\u2705 Fragment reuse: Naming and reusing common control flow patterns</li> <li>\u2705 Data control: Using <code>Settings</code> for argument mapping and <code>Data</code> for argument injection</li> <li>\u2705 Dynamic topology: Creating workers dynamically at runtime using lambda functions</li> </ul> <p>Syntax of ASL</p> Syntax Meaning Example <code>with &lt;GRAPH&gt; as &lt;KEY&gt;:</code> Defines a graph container with a given name.Notes:<code>&lt;GRAPH&gt;</code> can be either <code>graph</code> or <code>concurrent</code>:1. <code>graph</code>: Allows arbitrary orchestration of worker execution order  2. <code>concurrent</code>: All workers added to it will execute concurrently<code>&lt;KEY&gt;</code> is required and must be unique. Without it, the program cannot identify this as an executable graph structure <code>with graph as g:</code> declares a graph named <code>g</code>, under which you can start declaring workers and orchestrating the workflow <code>with &lt;GRAPH&gt;(&lt;PARAM&gt;=ASLField(...)) as &lt;KEY&gt;</code> Declares input parameters for a graph. <code>ASLField</code> is a field class that extends Pydantic's <code>FieldInfo</code>, used to specify parameter type, default values, and behavior. ASLField parameters: <code>type</code> (the parameter type), <code>default</code> (optional default value), and <code>dispatching_rule</code> (for concurrent containers, controls how data is distributed). Parameter names must match those used by workers within the graph. <code>with graph(user_input=ASLField(type=int)) as g:</code> declares a graph parameter <code>user_input</code> of type <code>int</code> <code>&lt;KEY&gt; = &lt;WORKER&gt;</code> Declares and registers a worker with a unique key.Notes:<code>&lt;WORKER&gt;</code> can be one of three types: 1. Python functions (defined with <code>def</code> or <code>async def</code>)  2. Bridgic <code>Worker</code> objects (e.g., <code>GraphAutoma</code>, custom <code>Worker</code> classes) 3. Another agent implemented using <code>ASLAutoma</code> <code>&lt;KEY&gt;</code> is required and must be unique within the graph scope. Without it, the worker cannot be identified and scheduled All worker declarations must be written inside a <code>with &lt;GRAPH&gt; as &lt;KEY&gt;:</code> block, otherwise workers cannot be registered to the correct execution graph <code>a = break_down_query</code> registers <code>break_down_query</code> as a worker with key <code>a</code> <code>&lt;KEY&gt; = lambda &lt;PARAM&gt;: (...)</code> Defines a lambda function that generates worker instances dynamically. It receives parameters from the container and returns a generator of worker instances. ASL creates and executes these workers at runtime. <code>dynamic_handler = lambda tasks: (task_handler *Settings(key=f\"task_{i}\") for i, task in enumerate(tasks))</code> generates worker instances based on the <code>tasks</code> parameter <code>&lt;KEY1&gt; &gt;&gt; &lt;KEY2&gt;</code> Defines a dependency: The worker named <code>&lt;KEY2&gt;</code> will be triggered after the worker named <code>&lt;WORKER1&gt;</code> completes execution <code>a &gt;&gt; b</code> means <code>b</code> will execute after <code>a</code> finishes <code>+&lt;KEY&gt;</code> Marks a worker as a start worker (entry point of the execution graph) <code>+a</code> marks <code>a</code> as the starting worker of the execution graph <code>~&lt;KEY&gt;</code> Marks a worker as an output worker (exit point of the execution graph) <code>~a</code> marks <code>a</code> as an output worker of the execution graph <code>&lt;KEY1&gt; &amp; &lt;KEY2&gt;</code> Defines a union unit (parallel group).When an operator is applied to this unit, it operates on all elements within it, following the distributive property. <code>(a &amp; b) &gt;&gt; c</code> means <code>c</code> will execute after both <code>a</code> and <code>b</code> finish; <code>a &gt;&gt; (b &amp; c)</code> means both <code>b</code> and <code>c</code> depend on <code>a</code> and will execute concurrently after <code>a</code> completes <code>&lt;KEY&gt; = &lt;ORCHESTRATION_EXPRESSION&gt;</code> Defines and registers a fragment of a workflow.Note: The <code>+</code> and <code>~</code> operators cannot be applied to fragments. If you declare <code>flow = a &gt;&gt; b</code> and then use <code>+flow</code>, this is not equivalent to <code>+a &gt;&gt; b</code>. <code>flow = a &gt;&gt; b</code> registers the execution logic <code>a &gt;&gt; b</code> as a fragment named <code>flow</code>. You can reuse this fragment later without rewriting <code>a &gt;&gt; b</code> <code>&lt;WORKER&gt; *Settings(...)</code> Attaches configuration settings to a worker using the <code>*</code> operator.Settings supports three configuration fields:\u2022 <code>key</code>: The unique identifier for the worker at runtime, defaults to the <code>&lt;KEY&gt;</code> from <code>&lt;KEY&gt; = &lt;WORKER&gt;</code>\u2022 <code>args_mapping_rule</code>: Defines how the worker receives results from preceding workers, defaults to <code>ArgsMappingRule.AS_IS</code>\u2022 <code>result_dispatching_rule</code>: Defines how the worker dispatches its results to subsequent workers, defaults to <code>ResultDispatchingRule.AS_IS</code> <code>worker1 *Settings(args_mapping_rule=ArgsMappingRule.MERGE)</code> configures <code>worker1</code> to merge results from multiple dependencies into a list. Other fields remain at their default values <code>&lt;WORKER&gt; *Data(...)</code> Attaches data configuration to a worker using the <code>*</code> operator. Important Note: Data injection is equivalent to assigning a default value to a function parameter, which is dynamically injected at runtime. For example, <code>c = worker3 *Data(y=From('a'))</code> is conceptually equivalent to <code>async def worker3(x: int, y: int = From('a'))</code>. This means you cannot write <code>c = worker3 *Data(x=From('a'))</code>, because it would be equivalent to <code>async def worker3(x: int = From('a'), y: int)</code>, which violates Python\u2019s rule that parameters with default values cannot precede parameters without default values in a function signature. <code>worker3 *Data(y=From('a'))</code> injects the result of worker <code>a</code> into parameter <code>y</code> of <code>worker3</code>"},{"location":"tutorials/items/asl/quick_start/#next-steps","title":"Next Steps\u00b6","text":"<p>Now that you understand the basics, you're ready to:</p> <ul> <li>Explore more advanced features in the ASL reference documentation</li> <li>Build your own agents using ASL</li> <li>Learn about Bridgic's underlying mechanisms for parameter resolving and worker execution</li> </ul> <p>Happy building! \ud83d\ude80</p>"},{"location":"tutorials/items/core_mechanism/","title":"Core Mechanism","text":""},{"location":"tutorials/items/core_mechanism/#core-mechanism","title":"Core Mechanism","text":"<p>Bridgic lets you build agentic systems by breaking down your workflows into modular building blocks called worker. Each worker represents a specific task or behavior, making it easy to organize complex processes.</p> <p>Bridgic introduces clear abstractions for structuring flows, passing data between execution units, handling concurrency, and enabling dynamic control logic (such as conditional branching and routing). This design allows users to build systerm that scale efficiently from simple workflows to sophisticated agentic systems.</p> <p>Key features include:</p> <ol> <li>Concurrency Mode: Organize your concurrent execution units systematically and conveniently.</li> <li>Parameter Resolving: Explore three ways for passing data and two ways for dispatching data between execution units, including Arguments Mapping, Arguments Injection, and Inputs Propagation about parameter binding; including Input Dispatching and Result Dispatching.</li> <li>Dynamic Routing: Enable conditional branching and decision-making through an easy-to-use <code>ferry_to()</code> API.</li> <li>Dynamic Topology: Change the underlying  graph topology at runtime to support highly autonomous AI applications..</li> <li>Human-in-the-loop: Enable human interaction or external input during workflow / agent execution.</li> <li>Worker Callback: Bring non-intrusive and multi-scope callback mechanisms that allows you to observe, customize, or extend execution at different levels with minimal impact on core logic.</li> <li>Modularity: Reuse and compose Automata by embedding one inside another for scalable agentic systems.</li> <li>Model Integration: Incorporate model to building a program with more autonomous capabilities.</li> </ol> <p>This architectural foundation makes Bridgic a powerful platform for building agentic systems that are robust, adaptive, and easy to reason about, enabling you to bridge the precision of logic with the creativity potential of AI.</p>"},{"location":"tutorials/items/core_mechanism/concurrency_mode/","title":"Concurrency Mode","text":"In\u00a0[\u00a0]: Copied! <pre>url=\"http://books.toscrape.com/catalogue/a-light-in-the-attic_1000/index.html\"\n</pre> url=\"http://books.toscrape.com/catalogue/a-light-in-the-attic_1000/index.html\" <p>The page looks like this:</p> <p>Note: We use Books to Scrape, a demo website created specifically for practicing web scraping, to introduce in this tutorial. Please note that the purpose of writing a crawler here is not to build a real scraper, but to provide a simple and safe example to demonstrate how Bridgic handles both synchronous and asynchronous execution models.</p> <p>We use <code>requests</code> to obtain the web content of the given url. First, install the <code>requests</code> package, then crawl the page like this:</p> In\u00a0[\u00a0]: Copied! <pre>pip install requests\n</pre> pip install requests In\u00a0[\u00a0]: Copied! <pre>import requests\n\ndef get_web_content(url):  # will return the web content of the given url\n    response = requests.get(url)\n    return response.text\n</pre> import requests  def get_web_content(url):  # will return the web content of the given url     response = requests.get(url)     return response.text In\u00a0[\u00a0]: Copied! <pre>import os\n\n# Set the API base and key.\n_api_key = os.environ.get(\"OPENAI_API_KEY\")\n_api_base = os.environ.get(\"OPENAI_API_BASE\")\n_model_name = os.environ.get(\"OPENAI_MODEL_NAME\")\n\n# Import the necessary modules.\nfrom bridgic.core.automa import GraphAutoma, worker\nfrom bridgic.core.model.types import Message, Role\nfrom bridgic.llms.openai_like import OpenAILikeLlm\n\nllm = OpenAILikeLlm(api_base=_api_base, api_key=_api_key, timeout=30)\n</pre> import os  # Set the API base and key. _api_key = os.environ.get(\"OPENAI_API_KEY\") _api_base = os.environ.get(\"OPENAI_API_BASE\") _model_name = os.environ.get(\"OPENAI_MODEL_NAME\")  # Import the necessary modules. from bridgic.core.automa import GraphAutoma, worker from bridgic.core.model.types import Message, Role from bridgic.llms.openai_like import OpenAILikeLlm  llm = OpenAILikeLlm(api_base=_api_base, api_key=_api_key, timeout=30) <p>Let's write web content analysis assistant.</p> In\u00a0[\u00a0]: Copied! <pre>class WebContentAnalysisAgent(GraphAutoma):\n    @worker(is_start=True)\n    def crawl_web_content(self, url: str) -&gt; str:\n        response = requests.get(url)\n        return response.text\n\n    @worker(dependencies=[\"crawl_web_content\"], is_output=True)\n    async def analyze_web_content(self, content: str) -&gt; str:\n        response = await llm.achat(\n            model=_model_name,\n            messages=[\n                Message.from_text(text=\"You are a web content analysis assistant. Your task is to analyze the given web content and summarize the main content.\", role=Role.SYSTEM),\n                Message.from_text(text=content, role=Role.USER),\n            ]\n        )\n        return response.message.content\n</pre> class WebContentAnalysisAgent(GraphAutoma):     @worker(is_start=True)     def crawl_web_content(self, url: str) -&gt; str:         response = requests.get(url)         return response.text      @worker(dependencies=[\"crawl_web_content\"], is_output=True)     async def analyze_web_content(self, content: str) -&gt; str:         response = await llm.achat(             model=_model_name,             messages=[                 Message.from_text(text=\"You are a web content analysis assistant. Your task is to analyze the given web content and summarize the main content.\", role=Role.SYSTEM),                 Message.from_text(text=content, role=Role.USER),             ]         )         return response.message.content <p>Now, let's use it to help us analyze the content.</p> In\u00a0[22]: Copied! <pre>web_content_analysis_agent = WebContentAnalysisAgent()\n\n# Input the url of the web page to be analyzed.\nurl = \"http://books.toscrape.com/catalogue/a-light-in-the-attic_1000/index.html\"\n\n# Call the agent to analyze the web content.\nres = await web_content_analysis_agent.arun(url)\n\n# Print the result.\nprint(f'- - - - - result - - - - -')\nprint(res)\nprint(f'- - - - - end - - - - -')\n</pre> web_content_analysis_agent = WebContentAnalysisAgent()  # Input the url of the web page to be analyzed. url = \"http://books.toscrape.com/catalogue/a-light-in-the-attic_1000/index.html\"  # Call the agent to analyze the web content. res = await web_content_analysis_agent.arun(url)  # Print the result. print(f'- - - - - result - - - - -') print(res) print(f'- - - - - end - - - - -')  <pre>- - - - - result - - - - -\nThe provided HTML content is from a product page on **Books to Scrape**, a demo website designed for web scraping education. Here's a clear summary of the main content:\n\n---\n\n### **Main Content Summary: A Light in the Attic**\n\n- **Product Title**: *A Light in the Attic*  \n- **Author**: Shel Silverstein  \n- **Category**: Poetry  \n- **Product Type**: Book (Poetry with illustrations)  \n- **Price**: \u00a351.77 (excl. and incl. tax; tax is \u00a30.00)  \n- **Availability**: In stock (22 units available)  \n- **Rating**: 5 stars (all full stars)  \n- **Number of Reviews**: 0  \n\n---\n\n### **Product Description Highlights**\n- Celebrates its 20th anniversary with a special edition.\n- Known for humorous, creative, and rhythmic poetry that appeals to both children and adults.\n- Features classic verses such as *\"Rockabye Baby\"*:\n  &gt; *\"Rockabye baby, in the treetop / Don't you know a treetop / Is no safe place to rock?\"*\n- Described as a timeless classic that brings joy and laughter to readers of all ages.\n\n---\n\n### **Important Note**\n&gt; \u26a0\ufe0f **This is a demo website** for web scraping training.  \n&gt; - Prices and ratings are **randomly assigned** and **do not reflect real-world data**.  \n&gt; - The site is not a real marketplace and should not be used for actual purchases.\n\n---\n\n### **Navigation Path**\nHome \u2192 Books \u2192 Poetry \u2192 *A Light in the Attic*\n\n---\n\n### **Visual Elements**\n- A single image of the book displayed in a carousel.\n- Clean, responsive layout with a header, product gallery, pricing, and description sections.\n\n---\n\n\u2705 **Purpose of the Page**: To demonstrate how to extract product details (title, price, description, availability, etc.) from e-commerce-style web pages \u2014 useful for teaching web scraping techniques.  \n\n\u274c **Not for real shopping** \u2014 all data is fictional.  \n\n--- \n\nIn short: This page showcases a fictional version of a beloved children's poetry book, presented in a realistic e-commerce format, but with no real pricing or user reviews.\n- - - - - end - - - - -\n</pre>"},{"location":"tutorials/items/core_mechanism/concurrency_mode/#concurrency-mode","title":"Concurrency Mode\u00b6","text":"<p>Bridgic runs primarily on an asynchronous event loop, while seamlessly supporting I/O-bound tasks through threads. This design ensures high concurrency across diverse workloads.</p>"},{"location":"tutorials/items/core_mechanism/concurrency_mode/#web-content-analysis-assistant","title":"Web content analysis assistant\u00b6","text":"<p>To explore Bridgic\u2019s support for concurrency, let\u2019s build a web content analysis assistant to summarize and introduce the main content of a given web page. The steps are as follows:</p> <ol> <li>Crawl relevant content of the input url.</li> <li>Summarize and introduce main content</li> </ol>"},{"location":"tutorials/items/core_mechanism/concurrency_mode/#1-crawl-relevant-content","title":"1. Crawl relevant content\u00b6","text":"<p>Taking the Books to Scrape website as an example, we are given the url of a book page on the website. Like this:</p>"},{"location":"tutorials/items/core_mechanism/concurrency_mode/#2-summarize-and-introduce-main-content","title":"2. Summarize and introduce main content\u00b6","text":"<p>We create an agent, input a url and crawl the corresponding page, and then let the model summarize the main content of the web page.</p> <p>Initialize the runtime environment.</p>"},{"location":"tutorials/items/core_mechanism/concurrency_mode/#what-have-we-learnt","title":"What have we learnt?\u00b6","text":"<p>It can be seen from the example that Bridgic can seamlessly schedule both asynchronous and synchronous workers within the same automa. Although <code>crawl_web_content</code> performs a blocking network request, Bridgic automatically dispatches it to a thread so that the event loop remains unblocked. Meanwhile, <code>analyze_web_content</code> runs asynchronously within the event loop. Refer to <code>Worker</code> for details.</p> <p>In this way, Bridgic keeps an asynchronous-first design, but also provides built-in support for I/O-bound operations through its thread pool, ensuring smooth execution across different types of workloads.</p>"},{"location":"tutorials/items/core_mechanism/dynamic_routing/","title":"Dynamic Routing","text":"In\u00a0[\u00a0]: Copied! <pre># Get the environment variables.\nimport os\nfrom typing import Literal\n\n_api_key = os.environ.get(\"OPENAI_API_KEY\")\n_api_base = os.environ.get(\"OPENAI_API_BASE\")\n_model_name = os.environ.get(\"OPENAI_MODEL_NAME\")\n\n# Import the necessary packages.\nfrom pydantic import BaseModel\nfrom bridgic.core.model.protocols import PydanticModel\nfrom bridgic.core.automa import GraphAutoma, worker\nfrom bridgic.core.model.protocols import Choice\nfrom bridgic.core.model.types import Message, Role\nfrom bridgic.llms.openai import OpenAILlm, OpenAIConfiguration\n\nllm = OpenAILlm(  # the llm instance\n    api_base=_api_base,\n    api_key=_api_key,\n    configuration=OpenAIConfiguration(model=_model_name),\n    timeout=20,\n)\n</pre> # Get the environment variables. import os from typing import Literal  _api_key = os.environ.get(\"OPENAI_API_KEY\") _api_base = os.environ.get(\"OPENAI_API_BASE\") _model_name = os.environ.get(\"OPENAI_MODEL_NAME\")  # Import the necessary packages. from pydantic import BaseModel from bridgic.core.model.protocols import PydanticModel from bridgic.core.automa import GraphAutoma, worker from bridgic.core.model.protocols import Choice from bridgic.core.model.types import Message, Role from bridgic.llms.openai import OpenAILlm, OpenAIConfiguration  llm = OpenAILlm(  # the llm instance     api_base=_api_base,     api_key=_api_key,     configuration=OpenAIConfiguration(model=_model_name),     timeout=20, ) In\u00a0[\u00a0]: Copied! <pre>class Domain(BaseModel):\n    choices: Literal[\"medical\", \"legal\"]\n</pre> class Domain(BaseModel):     choices: Literal[\"medical\", \"legal\"] In\u00a0[\u00a0]: Copied! <pre>class DomainChatbot(GraphAutoma):\n    @worker(is_start=True)\n    async def pre_query(self, query: str):  # receive user input and preprocess it\n        return query\n\n    @worker(dependencies=[\"pre_query\"])\n    async def domain_recognition(self, query: str):  # domain recognition and dynamic routing\n        response: Domain = await llm.astructured_output(\n            constraint=PydanticModel(model=Domain),\n            messages=[\n                Message.from_text(text=\"Please identify the domain of the following query\", role=Role.SYSTEM),\n                Message.from_text(text=query, role=Role.USER),\n            ]\n        )\n        print(f'The query domain is: {response}')  # print the response\n        \n        # dynamic routing\n        if response.choices == \"medical\":\n            return self.ferry_to('answer_medical_query', query)\n        elif response.choices == \"legal\":\n            return self.ferry_to('answer_legal_query', query)\n    \n    @worker(is_output=True)\n    async def answer_medical_query(self, query: str):  # answer medical-related query\n        response = await llm.achat(\n            messages=[\n                Message.from_text(text=\"You are a medical expert. Answer the following medical-related query\", role=Role.SYSTEM),\n                Message.from_text(text=query, role=Role.USER),\n            ]\n        )\n        return response.message.content\n\n    @worker(is_output=True)\n    async def answer_legal_query(self, query: str):  # answer legal-related query\n        response = await llm.achat(\n            messages=[\n                Message.from_text(text=\"You are a legal expert. Answer the following legal-related query\", role=Role.SYSTEM),\n                Message.from_text(text=query, role=Role.USER),\n            ]\n        )\n        return response.message.content\n</pre> class DomainChatbot(GraphAutoma):     @worker(is_start=True)     async def pre_query(self, query: str):  # receive user input and preprocess it         return query      @worker(dependencies=[\"pre_query\"])     async def domain_recognition(self, query: str):  # domain recognition and dynamic routing         response: Domain = await llm.astructured_output(             constraint=PydanticModel(model=Domain),             messages=[                 Message.from_text(text=\"Please identify the domain of the following query\", role=Role.SYSTEM),                 Message.from_text(text=query, role=Role.USER),             ]         )         print(f'The query domain is: {response}')  # print the response                  # dynamic routing         if response.choices == \"medical\":             return self.ferry_to('answer_medical_query', query)         elif response.choices == \"legal\":             return self.ferry_to('answer_legal_query', query)          @worker(is_output=True)     async def answer_medical_query(self, query: str):  # answer medical-related query         response = await llm.achat(             messages=[                 Message.from_text(text=\"You are a medical expert. Answer the following medical-related query\", role=Role.SYSTEM),                 Message.from_text(text=query, role=Role.USER),             ]         )         return response.message.content      @worker(is_output=True)     async def answer_legal_query(self, query: str):  # answer legal-related query         response = await llm.achat(             messages=[                 Message.from_text(text=\"You are a legal expert. Answer the following legal-related query\", role=Role.SYSTEM),                 Message.from_text(text=query, role=Role.USER),             ]         )         return response.message.content  <p>Let's run it!</p> In\u00a0[5]: Copied! <pre>query = \"What is the aspirin?\"\ndomain_chatbot = DomainChatbot()\nawait domain_chatbot.arun(query)\n</pre> query = \"What is the aspirin?\" domain_chatbot = DomainChatbot() await domain_chatbot.arun(query) <pre>The query domain is: medical\n</pre> Out[5]: <pre>'Aspirin, also known as acetylsalicylic acid, is a widely used medication with several important medical properties. It is one of the oldest and most well-known over-the-counter (OTC) drugs, originally derived from willow bark.\\n\\n### Key Uses of Aspirin:\\n\\n1. **Pain Relief (Analgesic)**  \\n   Aspirin helps relieve mild to moderate pain such as headaches, toothaches, muscle aches, and menstrual cramps.\\n\\n2. **Reducing Fever (Antipyretic)**  \\n   It lowers body temperature in cases of fever, such as in infections.\\n\\n3. **Reducing Inflammation (Anti-inflammatory)**  \\n   Aspirin helps reduce inflammation, which can be beneficial in conditions like rheumatoid arthritis or other inflammatory disorders.\\n\\n4. **Preventing Blood Clots (Antiplatelet Effect)**  \\n   This is one of its most important therapeutic uses. Aspirin inhibits the formation of blood clots by blocking the action of a substance called cyclooxygenase (COX), which reduces the production of thromboxane\u2014a molecule that promotes platelet aggregation.  \\n   - **Low-dose aspirin (typically 81 mg)** is commonly prescribed for long-term use to prevent heart attacks and strokes in individuals at risk due to cardiovascular disease (e.g., history of heart attack, angina, or stroke).\\n\\n5. **Cardiovascular Protection**  \\n   Regular low-dose aspirin is recommended for secondary prevention in patients with a history of cardiovascular events to reduce the risk of recurrence.\\n\\n---\\n\\n### How It Works:\\nAspirin irreversibly inhibits the enzyme COX-1 and COX-2, which are involved in producing prostaglandins. Prostaglandins play roles in inflammation, pain, fever, and platelet aggregation. By blocking COX-1, aspirin reduces platelet activation and clot formation.\\n\\n---\\n\\n### Safety and Precautions:\\n- **Gastrointestinal Side Effects**: Aspirin can irritate the stomach lining and increase the risk of ulcers or bleeding, especially with long-term or high-dose use.\\n- **Bleeding Risk**: Due to its antiplatelet effect, it increases the risk of bleeding (e.g., gastrointestinal bleeding, bruising).\\n- **Allergic Reactions**: Some people may have a severe allergic reaction, including asthma exacerbation (especially in those with a history of aspirin sensitivity).\\n- **Reproductive Health**: Aspirin is not recommended during pregnancy, especially in the third trimester, due to risks of fetal complications.\\n- **Children and Fever**: Aspirin should be avoided in children and adolescents with viral infections (like chickenpox or flu) due to the risk of Reye\u2019s syndrome, a rare but serious condition.\\n\\n---\\n\\n### Summary:\\n**Aspirin is a versatile medication used for pain, fever, inflammation, and cardiovascular protection. Its most significant medical role today is in preventing heart attacks and strokes through long-term low-dose use. However, it should be used cautiously and only as directed by a healthcare provider.**\\n\\nAlways consult a doctor before starting or stopping aspirin therapy, especially if you have underlying health conditions or are taking other medications.'</pre> <p>If you encounter a Readtimeout error during the execution of the above process, it might be because the timeout period set during the initialization of the llm is too short, causing the model to time out before it finishes responding.</p> <p>Great! We have successfully completed the domain chatbot.</p>"},{"location":"tutorials/items/core_mechanism/dynamic_routing/#dynamic-routing","title":"Dynamic Routing\u00b6","text":"<p>Bridgic supports dynamic routing to the corresponding worker based on runtime conditions. Now let's understand it with a sample example.</p>"},{"location":"tutorials/items/core_mechanism/dynamic_routing/#domain-chatbot","title":"Domain chatbot\u00b6","text":"<p>Dynamic routing is a common requirement in chatbots. For instance, it involves analyzing the input query to determine the main domain or field it pertains to, and then dynamically routing it to the corresponding processing logic.</p> <p>The user inputs the original query, we identify whether it is a medical-related or legal-related issue, and then we answer it with specialized logic. There are three steps:</p> <ol> <li>Receive user input</li> <li>Domain recognition and dynamic routing to correct worker</li> <li>Answer query</li> </ol>"},{"location":"tutorials/items/core_mechanism/dynamic_routing/#1-initialize","title":"1. Initialize\u00b6","text":"<p>Before we start, let's prepare the running environment.</p>"},{"location":"tutorials/items/core_mechanism/dynamic_routing/#2-complete","title":"2. Complete\u00b6","text":"<p>We assume that the user query we receive is a string. Let's implement the domain chatbot to answer the user query.</p>"},{"location":"tutorials/items/core_mechanism/dynamic_routing/#what-have-we-learnt","title":"What have we learnt?\u00b6","text":"<p>We have implemented the routing mechanism using Bridgic.</p>"},{"location":"tutorials/items/core_mechanism/dynamic_routing/#ferry-to","title":"Ferry To\u00b6","text":"<p>In Bridgic, we use the <code>ferry_to</code> mechanism to achieve dynamic routing. Its first parameter is the <code>key</code> of the worker to which the route will be directed, and the second parameter is the input parameter passed to the target worker.</p>"},{"location":"tutorials/items/core_mechanism/dynamic_topology/","title":"Dynamic Topology","text":"<p>Run the following <code>pip</code> command to make sure the 'openai' integration is installed.</p> In\u00a0[\u00a0]: Copied! <pre>pip install -U bridgic\npip install -U bridgic-llms-openai\n</pre> pip install -U bridgic pip install -U bridgic-llms-openai In\u00a0[3]: Copied! <pre>import os\n\n# Get the API base, API key and model name.\n_api_key = os.environ.get(\"OPENAI_API_KEY\")\n_api_base = os.environ.get(\"OPENAI_API_BASE\")\n_model_name = os.environ.get(\"OPENAI_MODEL_NAME\")\n\nfrom bridgic.llms.openai import OpenAILlm, OpenAIConfiguration\n\nllm = OpenAILlm(  # the llm instance\n    api_base=_api_base,\n    api_key=_api_key,\n    configuration=OpenAIConfiguration(model=_model_name),\n    timeout=20,\n)\n</pre> import os  # Get the API base, API key and model name. _api_key = os.environ.get(\"OPENAI_API_KEY\") _api_base = os.environ.get(\"OPENAI_API_BASE\") _model_name = os.environ.get(\"OPENAI_MODEL_NAME\")  from bridgic.llms.openai import OpenAILlm, OpenAIConfiguration  llm = OpenAILlm(  # the llm instance     api_base=_api_base,     api_key=_api_key,     configuration=OpenAIConfiguration(model=_model_name),     timeout=20, ) In\u00a0[5]: Copied! <pre># Three mock tools defined as async functions.\n\nasync def get_weather(city: str, days: int):\n    \"\"\"\n    Get the weather forecast for the next few days in a specified city.\n\n    Parameters\n    ----------\n    city : str\n        The city to get the weather of, e.g. New York.\n    days : int\n        The number of days to get the weather forecast for.\n    \n    Returns\n    -------\n    str\n        The weather forecast for the next few days in the specified city.\n    \"\"\"\n    return f\"The weather in {city} will be mostly sunny for the next {days} days.\"\n\nasync def get_flight_price(origin_city: str, destination_city: str):\n    \"\"\"\n    Get the average round-trip flight price from one city to another.\n\n    Parameters\n    ----------\n    origin_city : str\n        The origin city of the flight.\n    destination_city : str\n        The destination city of the flight.\n    \n    Returns\n    -------\n    str\n        The average round-trip flight price from the origin city to the destination city.\n    \"\"\"\n    return f\"The average round-trip flight from {origin_city} to {destination_city} is about $850.\"\n\nasync def get_hotel_price(city: str, nights: int):\n    \"\"\"\n    Get the average price of a hotel stay in a specified city for a given number of nights.\n\n    Parameters\n    ----------\n    city : str\n        The city to get the hotel price of, e.g. New York.\n    nights : int\n        The number of nights to get the hotel price for.\n    \n    Returns\n    -------\n    str\n        The average price of a hotel stay in the specified city for the given number of nights.\n    \"\"\"\n    return f\"A 3-star hotel in {city} costs about $120 per night for {nights} nights.\"\n\nfrom bridgic.core.agentic.tool_specs import FunctionToolSpec\n\nfuncs = [get_weather, get_flight_price, get_hotel_price]\ntool_list = [FunctionToolSpec.from_raw(func) for func in funcs]\n</pre> # Three mock tools defined as async functions.  async def get_weather(city: str, days: int):     \"\"\"     Get the weather forecast for the next few days in a specified city.      Parameters     ----------     city : str         The city to get the weather of, e.g. New York.     days : int         The number of days to get the weather forecast for.          Returns     -------     str         The weather forecast for the next few days in the specified city.     \"\"\"     return f\"The weather in {city} will be mostly sunny for the next {days} days.\"  async def get_flight_price(origin_city: str, destination_city: str):     \"\"\"     Get the average round-trip flight price from one city to another.      Parameters     ----------     origin_city : str         The origin city of the flight.     destination_city : str         The destination city of the flight.          Returns     -------     str         The average round-trip flight price from the origin city to the destination city.     \"\"\"     return f\"The average round-trip flight from {origin_city} to {destination_city} is about $850.\"  async def get_hotel_price(city: str, nights: int):     \"\"\"     Get the average price of a hotel stay in a specified city for a given number of nights.      Parameters     ----------     city : str         The city to get the hotel price of, e.g. New York.     nights : int         The number of nights to get the hotel price for.          Returns     -------     str         The average price of a hotel stay in the specified city for the given number of nights.     \"\"\"     return f\"A 3-star hotel in {city} costs about $120 per night for {nights} nights.\"  from bridgic.core.agentic.tool_specs import FunctionToolSpec  funcs = [get_weather, get_flight_price, get_hotel_price] tool_list = [FunctionToolSpec.from_raw(func) for func in funcs] <p>In the code above, three tools are defined. The docstring of each tool provides important information, which will serve as the tool descriptions presented to the LLM. Each tool is transformed to a <code>FunctionToolSpec</code> instance, and these three tools are stored in the <code>tool_list</code> variable for later use.</p> In\u00a0[8]: Copied! <pre>from typing import List, Tuple, Any\nfrom bridgic.core.automa import GraphAutoma, worker\nfrom bridgic.core.agentic.tool_specs import ToolSpec\nfrom bridgic.core.model.types import Message, Role, ToolCall\nfrom bridgic.core.automa.args import From, ArgsMappingRule\nfrom bridgic.core.agentic.types import ToolMessage\n\nclass TravelPlanner(GraphAutoma):\n    @worker(is_start=True)\n    async def invoke_llm(self, user_input: str, tool_list: List[ToolSpec]):\n        tool_calls, _ = await llm.aselect_tool(\n            messages=[\n                Message.from_text(text=\"You are an intelligent AI assistant that can perform tasks by calling available tools.\", role=Role.SYSTEM),\n                Message.from_text(text=user_input, role=Role.USER),\n            ], \n            tools=[tool.to_tool() for tool in tool_list], \n        )\n        print(f\"[invoke_llm] - LLM returns tool_calls: {tool_calls}\")\n        return tool_calls\n    \n    @worker(dependencies=[\"invoke_llm\"])\n    async def process_tool_calls(\n        self,\n        tool_calls: List[ToolCall],\n        tool_list: List[ToolSpec],\n    ):\n        matched_list = self._match_tool_calls_and_tool_specs(tool_calls, tool_list)\n        matched_tool_calls = []\n        tool_worker_keys = []\n        for tool_call, tool_spec in matched_list:\n            matched_tool_calls.append(tool_call)\n            tool_worker = tool_spec.create_worker()\n            worker_key = f\"tool_{tool_call.name}_{tool_call.id}\"\n            print(f\"[process_tool_calls] - add worker: {worker_key}\")\n            self.add_worker(\n                key=worker_key,\n                worker=tool_worker,\n            )\n            self.ferry_to(worker_key, **tool_call.arguments)\n            tool_worker_keys.append(worker_key)\n        self.add_func_as_worker(\n            key=\"aggregate_results\",\n            func=self.aggregate_results,\n            dependencies=tool_worker_keys,\n            args_mapping_rule=ArgsMappingRule.MERGE,\n        )\n        return matched_tool_calls\n\n    async def aggregate_results(\n        self, \n        tool_results: List[Any],\n        tool_calls: List[ToolCall] = From(\"process_tool_calls\"),\n    ) -&gt; List[ToolMessage]:\n        print(f\"[aggregate_results] - tool execution results: {tool_results}\")\n        tool_messages = []\n        for tool_result, tool_call in zip(tool_results, tool_calls):\n            tool_messages.append(ToolMessage(\n                role=\"tool\", \n                content=str(tool_result), \n                tool_call_id=tool_call.id\n            ))\n        # `tool_messages` may be used as the inputs of the next LLM call...\n        print(f\"[aggregate_results] - assembled ToolMessage list: {tool_messages}\")\n        return tool_messages\n\n    def _match_tool_calls_and_tool_specs(\n        self,\n        tool_calls: List[ToolCall],\n        tool_list: List[ToolSpec],\n    ) -&gt; List[Tuple[ToolCall, ToolSpec]]:\n        matched_list: List[Tuple[ToolCall, ToolSpec]] = []\n        for tool_call in tool_calls:\n            for tool_spec in tool_list:\n                if tool_call.name == tool_spec.tool_name:\n                    matched_list.append((tool_call, tool_spec))\n        return matched_list\n</pre> from typing import List, Tuple, Any from bridgic.core.automa import GraphAutoma, worker from bridgic.core.agentic.tool_specs import ToolSpec from bridgic.core.model.types import Message, Role, ToolCall from bridgic.core.automa.args import From, ArgsMappingRule from bridgic.core.agentic.types import ToolMessage  class TravelPlanner(GraphAutoma):     @worker(is_start=True)     async def invoke_llm(self, user_input: str, tool_list: List[ToolSpec]):         tool_calls, _ = await llm.aselect_tool(             messages=[                 Message.from_text(text=\"You are an intelligent AI assistant that can perform tasks by calling available tools.\", role=Role.SYSTEM),                 Message.from_text(text=user_input, role=Role.USER),             ],              tools=[tool.to_tool() for tool in tool_list],          )         print(f\"[invoke_llm] - LLM returns tool_calls: {tool_calls}\")         return tool_calls          @worker(dependencies=[\"invoke_llm\"])     async def process_tool_calls(         self,         tool_calls: List[ToolCall],         tool_list: List[ToolSpec],     ):         matched_list = self._match_tool_calls_and_tool_specs(tool_calls, tool_list)         matched_tool_calls = []         tool_worker_keys = []         for tool_call, tool_spec in matched_list:             matched_tool_calls.append(tool_call)             tool_worker = tool_spec.create_worker()             worker_key = f\"tool_{tool_call.name}_{tool_call.id}\"             print(f\"[process_tool_calls] - add worker: {worker_key}\")             self.add_worker(                 key=worker_key,                 worker=tool_worker,             )             self.ferry_to(worker_key, **tool_call.arguments)             tool_worker_keys.append(worker_key)         self.add_func_as_worker(             key=\"aggregate_results\",             func=self.aggregate_results,             dependencies=tool_worker_keys,             args_mapping_rule=ArgsMappingRule.MERGE,         )         return matched_tool_calls      async def aggregate_results(         self,          tool_results: List[Any],         tool_calls: List[ToolCall] = From(\"process_tool_calls\"),     ) -&gt; List[ToolMessage]:         print(f\"[aggregate_results] - tool execution results: {tool_results}\")         tool_messages = []         for tool_result, tool_call in zip(tool_results, tool_calls):             tool_messages.append(ToolMessage(                 role=\"tool\",                  content=str(tool_result),                  tool_call_id=tool_call.id             ))         # `tool_messages` may be used as the inputs of the next LLM call...         print(f\"[aggregate_results] - assembled ToolMessage list: {tool_messages}\")         return tool_messages      def _match_tool_calls_and_tool_specs(         self,         tool_calls: List[ToolCall],         tool_list: List[ToolSpec],     ) -&gt; List[Tuple[ToolCall, ToolSpec]]:         matched_list: List[Tuple[ToolCall, ToolSpec]] = []         for tool_call in tool_calls:             for tool_spec in tool_list:                 if tool_call.name == tool_spec.tool_name:                     matched_list.append((tool_call, tool_spec))         return matched_list <p>In the start worker <code>invoke_llm</code>, the LLM is invoked to return a list of <code>ToolCalls</code>. Therefore, the information about the tool calls contained in this list is dynamic.</p> <p>In the second worker <code>process_tool_calls</code>, based on the dynamic list of <code>tool_calls</code>, a worker is created (through <code>tool_spec.create_worker()</code>) for each tool to be invoked and added to the DDG. Then, the <code>aggregate_results</code> worker is also dynamically added to the DDG via the <code>add_func_as_worker()</code> API, responsible for aggregating the execution results from all the tool workers.</p> <p>It is worth noting that invoking multiple tools as workers can fully leverage certain features of the Bridgic framework, such as Concurrency Mode. Here, these tools are able to execute concurrently.</p> In\u00a0[9]: Copied! <pre>agent = TravelPlanner()\nawait agent.arun(\n    user_input=\"Plan a 3-day trip to Tokyo. Check the weather forecast, estimate the flight price from San Francisco, and the hotel cost for 3 nights.\",\n    tool_list=tool_list,\n)\n</pre> agent = TravelPlanner() await agent.arun(     user_input=\"Plan a 3-day trip to Tokyo. Check the weather forecast, estimate the flight price from San Francisco, and the hotel cost for 3 nights.\",     tool_list=tool_list, ) <pre>[invoke_llm] - LLM returns tool_calls: [ToolCall(id='call_cLERxyz110tylRxgE4XQjaRQ', name='get_weather', arguments={'city': 'Tokyo', 'days': 3}), ToolCall(id='call_CqicPm6yZoyNksEl9HGVJEOQ', name='get_flight_price', arguments={'origin_city': 'San Francisco', 'destination_city': 'Tokyo'}), ToolCall(id='call_GscwR3pvHtzR2wTki1ndpHZp', name='get_hotel_price', arguments={'city': 'Tokyo', 'nights': 3})]\n[process_tool_calls] - add worker: tool_get_weather_call_cLERxyz110tylRxgE4XQjaRQ\n[process_tool_calls] - add worker: tool_get_flight_price_call_CqicPm6yZoyNksEl9HGVJEOQ\n[process_tool_calls] - add worker: tool_get_hotel_price_call_GscwR3pvHtzR2wTki1ndpHZp\n[aggregate_results] - tool execution results: ['The weather in Tokyo will be mostly sunny for the next 3 days.', 'The average round-trip flight from San Francisco to Tokyo is about $850.', 'A 3-star hotel in Tokyo costs about $120 per night for 3 nights.']\n[aggregate_results] - assembled ToolMessage list: [{'role': 'tool', 'content': 'The weather in Tokyo will be mostly sunny for the next 3 days.', 'tool_call_id': 'call_cLERxyz110tylRxgE4XQjaRQ'}, {'role': 'tool', 'content': 'The average round-trip flight from San Francisco to Tokyo is about $850.', 'tool_call_id': 'call_CqicPm6yZoyNksEl9HGVJEOQ'}, {'role': 'tool', 'content': 'A 3-star hotel in Tokyo costs about $120 per night for 3 nights.', 'tool_call_id': 'call_GscwR3pvHtzR2wTki1ndpHZp'}]\n</pre>"},{"location":"tutorials/items/core_mechanism/dynamic_topology/#dynamic-topology","title":"Dynamic Topology\u00b6","text":"<p>In the previous section of this tutorial, we learned how to use the <code>ferry_to()</code> API to implement dynamic routing. This capability allows us to create branching and looping logic, forming the foundation for handling dynamic behavior driven by runtime inputs. However, when we take into account the highly autonomous planning capabilities of LLMs, the dynamic features provided by <code>ferry_to()</code> alone are no longer sufficient.</p> <p>In order to support highly autonomous AI applications, the orchestration of workers in Bridgic is built on a Dynamic Directed Graph (DDG), whose topology can change at runtime. This DDG-based architecture is especially useful in scenarios where the execution path planned by an LLM cannot be predetermined at coding time. It provides a greater degree of flexibility than the routing mechanism described earlier.</p>"},{"location":"tutorials/items/core_mechanism/dynamic_topology/#example-tool-selection","title":"Example: Tool Selection\u00b6","text":"<p>Most LLMs support tool selection and invocation \u2014 a crucial step of a typical agent loop. In the following example, we\u2019ll demonstrate the key process of tool selection through a Travel Planning Agent, and use Bridgic\u2019s dynamic topology to implement tool calling.</p> <p>Note:</p> <p>This code example is for demonstration purposes only. It represents part of the overall execution flow within a complete agent loop. If you intend to use tool calling and the agent loop in production, please use the <code>ReCentAutoma</code> class provided by the Bridgic framework.</p>"},{"location":"tutorials/items/core_mechanism/dynamic_topology/#1-initialization","title":"1. Initialization\u00b6","text":"<p>Before we start, let's initialize the OpenAI LLM instance and the running environment.</p>"},{"location":"tutorials/items/core_mechanism/dynamic_topology/#2-preparing-tools","title":"2. Preparing Tools\u00b6","text":"<p>In the travel-planning example, we need to provide several tools for the LLM to call. The following code defines these tools as functions.</p>"},{"location":"tutorials/items/core_mechanism/dynamic_topology/#3-orchestration","title":"3. Orchestration\u00b6","text":"<p>This demo consists of four steps:</p> <ol> <li>Invoke the LLM: Pass the list of available tools to the LLM and obtain its <code>tool_calls</code> output.</li> <li>Create workers dynamically: Dynamically create workers based on the <code>tool_calls</code> results.</li> <li>Invoke tools: Let the Bridgic framework automatically schedule and execute those workers that represent the tools.</li> <li>Aggregate results: Combine the execution results into a list of <code>ToolMessage</code> objects, which may later be fed into the LLM for further processing.</li> </ol> <p>We implement these steps by subclassing <code>GraphAutoma</code>:</p>"},{"location":"tutorials/items/core_mechanism/dynamic_topology/#4-lets-run-it","title":"4. Let's run it\u00b6","text":"<p>Let's create a instance of <code>TravelPlanner</code> and run it.</p>"},{"location":"tutorials/items/core_mechanism/dynamic_topology/#what-have-we-learnt","title":"What have we learnt?\u00b6","text":"<p>In this Travel Planning Agent example, we have demonstrated how to use Bridgic\u2019s dynamic topology mechanism to create workers for tools. The <code>GraphAutoma</code> class is implemented as a Dynamic Directed Graph (DDG) in Bridgic, to support topology change at runtime. The APIs that support dynamic change of topology include: <code>add_worker</code>, <code>add_func_as_worker</code>, <code>remove_worker</code>, and <code>add_dependency</code>.</p> <p>You might notice that interspersing these API calls within the worker implementation code can look a bit untidy. We plan to address this issue with a new feature in the near future.</p>"},{"location":"tutorials/items/core_mechanism/human_in_the_loop/","title":"Human-in-the-loop","text":"<p>Run the following <code>pip</code> command to make sure the 'openai' integration is installed.</p> In\u00a0[\u00a0]: Copied! <pre>pip install -U bridgic\npip install -U bridgic-llms-openai\n</pre> pip install -U bridgic pip install -U bridgic-llms-openai In\u00a0[\u00a0]: Copied! <pre>import os\n\n# Get the API base, API key and model name.\n_api_key = os.environ.get(\"OPENAI_API_KEY\")\n_api_base = os.environ.get(\"OPENAI_API_BASE\")\n_model_name = os.environ.get(\"OPENAI_MODEL_NAME\")\n\nfrom pydantic import BaseModel, Field\nfrom bridgic.core.automa import GraphAutoma, worker, Snapshot\nfrom bridgic.core.automa.args import From\nfrom bridgic.core.automa.interaction import Event, Feedback, FeedbackSender, InteractionFeedback, InteractionException\nfrom bridgic.core.model.types import Message, Role\nfrom bridgic.core.model.protocols import PydanticModel\nfrom bridgic.llms.openai import OpenAILlm\n</pre> import os  # Get the API base, API key and model name. _api_key = os.environ.get(\"OPENAI_API_KEY\") _api_base = os.environ.get(\"OPENAI_API_BASE\") _model_name = os.environ.get(\"OPENAI_MODEL_NAME\")  from pydantic import BaseModel, Field from bridgic.core.automa import GraphAutoma, worker, Snapshot from bridgic.core.automa.args import From from bridgic.core.automa.interaction import Event, Feedback, FeedbackSender, InteractionFeedback, InteractionException from bridgic.core.model.types import Message, Role from bridgic.core.model.protocols import PydanticModel from bridgic.llms.openai import OpenAILlm In\u00a0[25]: Copied! <pre># Set the LLM\nllm = OpenAILlm(api_base=_api_base, api_key=_api_key, timeout=10)\n\nclass CodeBlock(BaseModel):\n    code: str = Field(description=\"The code to be executed.\")\n\nclass CodeAssistant(GraphAutoma):\n    @worker(is_start=True)\n    async def generate_code(self, user_requirement: str):\n        response = await llm.astructured_output(\n            model=_model_name,\n            messages=[\n                Message.from_text(text=f\"You are a programming assistant. Please generate code according to the user's requirements.\", role=Role.SYSTEM),\n                Message.from_text(text=user_requirement, role=Role.USER),\n            ],\n            constraint=PydanticModel(model=CodeBlock)\n        )\n        return response.code\n\n    @worker(dependencies=[\"generate_code\"])\n    async def ask_to_run_code(self, code: str):\n        event = Event(event_type=\"can_run_code\", data=code)\n        feedback = await self.request_feedback_async(event)\n        return feedback.data\n        \n    @worker(dependencies=[\"ask_to_run_code\"])\n    async def output_result(self, feedback: str, code: str = From(\"generate_code\")):\n        code = code.strip(\"```python\").strip(\"```\")\n        if feedback == \"yes\":\n            print(f\"- - - - - - Result - - - - - -\")\n            exec(code)\n            print(f\"- - - - - - End - - - - - -\")\n        else:\n            print(f\"This code was rejected for execution. In response to the requirements, I have generated the following code:\\n```python\\n{code}\\n```\")\n</pre> # Set the LLM llm = OpenAILlm(api_base=_api_base, api_key=_api_key, timeout=10)  class CodeBlock(BaseModel):     code: str = Field(description=\"The code to be executed.\")  class CodeAssistant(GraphAutoma):     @worker(is_start=True)     async def generate_code(self, user_requirement: str):         response = await llm.astructured_output(             model=_model_name,             messages=[                 Message.from_text(text=f\"You are a programming assistant. Please generate code according to the user's requirements.\", role=Role.SYSTEM),                 Message.from_text(text=user_requirement, role=Role.USER),             ],             constraint=PydanticModel(model=CodeBlock)         )         return response.code      @worker(dependencies=[\"generate_code\"])     async def ask_to_run_code(self, code: str):         event = Event(event_type=\"can_run_code\", data=code)         feedback = await self.request_feedback_async(event)         return feedback.data              @worker(dependencies=[\"ask_to_run_code\"])     async def output_result(self, feedback: str, code: str = From(\"generate_code\")):         code = code.strip(\"```python\").strip(\"```\")         if feedback == \"yes\":             print(f\"- - - - - - Result - - - - - -\")             exec(code)             print(f\"- - - - - - End - - - - - -\")         else:             print(f\"This code was rejected for execution. In response to the requirements, I have generated the following code:\\n```python\\n{code}\\n```\") <p>In the <code>ask_to_run_code()</code> method of <code>CodeAssistant</code>, we use <code>request_feedback_async()</code> to send an Event to the human user and expect to receive a feedback. To handle this Event, the corresponding logic needs to be registered with the automa, like this:</p> In\u00a0[26]: Copied! <pre># Handle can_run_code event\ndef can_run_code_handler(event: Event, feedback_sender: FeedbackSender):\n    print(f\"Can I run this code now to verify if it's correct?\")\n    print(f\"```python\\n{event.data}\\n```\")\n    res = input(\"Please input your answer (yes/no): \")\n    if res in [\"yes\", \"no\"]:\n        feedback_sender.send(Feedback(data=res))\n    else:\n        print(\"Invalid input. Please input yes or no.\")\n        feedback_sender.send(Feedback(data=\"no\"))\n\n# register can_run_code event handler to `CodeAssistant` automa\ncode_assistant = CodeAssistant()\ncode_assistant.register_event_handler(\"can_run_code\", can_run_code_handler)\n</pre> # Handle can_run_code event def can_run_code_handler(event: Event, feedback_sender: FeedbackSender):     print(f\"Can I run this code now to verify if it's correct?\")     print(f\"```python\\n{event.data}\\n```\")     res = input(\"Please input your answer (yes/no): \")     if res in [\"yes\", \"no\"]:         feedback_sender.send(Feedback(data=res))     else:         print(\"Invalid input. Please input yes or no.\")         feedback_sender.send(Feedback(data=\"no\"))  # register can_run_code event handler to `CodeAssistant` automa code_assistant = CodeAssistant() code_assistant.register_event_handler(\"can_run_code\", can_run_code_handler) <p>Now let's use it!</p> In\u00a0[27]: Copied! <pre>await code_assistant.arun(user_requirement=\"Please write a function to print 'Hello, World!' and run it.\")\n</pre> await code_assistant.arun(user_requirement=\"Please write a function to print 'Hello, World!' and run it.\") <pre>Can I run this code now to verify if it's correct?\n```python\ndef greet():\n    print('Hello, World!')\n\ngreet()\n```\n- - - - - - Result - - - - - -\nHello, World!\n- - - - - - End - - - - - -\n</pre> <p>In the above example, Bridgic wrap the message sent to the human user in an <code>Event</code> and he message received from the user in a <code>FeedBack</code>.</p> <ul> <li><code>Event</code> contains three fields:<ul> <li><code>event_type</code>: A string. The event type is used to identify the registered event handler.</li> <li><code>timestamp</code>: A Python datetime object. The timestamp of the event. The default is <code>datetime.now()</code>.</li> <li><code>data</code>: The data attached to the event.</li> </ul> </li> <li><code>FeedBack</code> contains one field:<ul> <li><code>data</code>: The data attached to the feedback.</li> </ul> </li> </ul> <p><code>request_feedback_async()</code> send an event to the user and request for a feedback. This method call will block the caller until the feedback is received. However, thanks to Python\u2019s asynchronous event loop mechanism, other automas running on the same main thread will not be blocked.</p> <p>The registered event handler must be defined as type of <code>EventHandlerType</code>.  Here it should be a function that takes an <code>Event</code> and a <code>FeedbackSender</code> as arguments.</p> In\u00a0[\u00a0]: Copied! <pre>class MessageNotifier(GraphAutoma):\n    @worker(is_start=True)\n    async def notify(self, user_input: int, notify_int: int):\n        print(f\"Loop from 1 to {user_input}\")\n        for i in range(1, user_input + 1):\n            if i == notify_int:\n                event = Event(event_type=\"message_notification\", data=f\"Loop {i} times\")\n                self.post_event(event)\n\ndef message_notification_handler(event: Event):\n    print(f'!! Now count to {event.data}. !!')\n\nmessage_notifier = MessageNotifier()\nmessage_notifier.register_event_handler(\"message_notification\", message_notification_handler)\nawait message_notifier.arun(user_input=10, notify_int=5)\n        \n</pre> class MessageNotifier(GraphAutoma):     @worker(is_start=True)     async def notify(self, user_input: int, notify_int: int):         print(f\"Loop from 1 to {user_input}\")         for i in range(1, user_input + 1):             if i == notify_int:                 event = Event(event_type=\"message_notification\", data=f\"Loop {i} times\")                 self.post_event(event)  def message_notification_handler(event: Event):     print(f'!! Now count to {event.data}. !!')  message_notifier = MessageNotifier() message_notifier.register_event_handler(\"message_notification\", message_notification_handler) await message_notifier.arun(user_input=10, notify_int=5)          <pre>Loop from 1 to 10\n!! Now count to Loop 5 times. !!\n</pre> In\u00a0[28]: Copied! <pre>import os\nimport tempfile\nfrom httpx import delete\nfrom pydantic import BaseModel\nfrom datetime import datetime\nfrom bridgic.core.automa import GraphAutoma, worker, Snapshot\nfrom bridgic.core.automa.args import From\nfrom bridgic.core.automa.interaction import Event, InteractionFeedback, InteractionException\n\nclass ReimbursementRecord(BaseModel):\n    request_id: int\n    employee_id: int\n    employee_name: str\n    reimbursement_month: str\n    reimbursement_amount: float\n    description: str\n    created_at: datetime\n    updated_at: datetime\n\nclass AuditResult(BaseModel):\n    request_id: int\n    passed: bool\n    audit_reason: str\n\nclass ReimbursementWorkflow(GraphAutoma):\n    @worker(is_start=True)\n    async def load_record(self, request_id: int):\n        \"\"\"\n        The reimbursement workflow can be triggered by the OA system \u2014 for instance, when an employee submits a new reimbursement request. Each request is uniquely identified by a `request_id`, which is then used to retrieve the corresponding reimbursement record from the database. \n        \"\"\"\n        # Load the data from database.\n        return await self.load_record_from_database(request_id)\n    \n    @worker(dependencies=[\"load_record\"])\n    async def audit_by_rules(self, record: ReimbursementRecord):\n        \"\"\"\n        This method simulates the logic for automatically determining whether a reimbursement request complies with business rules.  \n\n        Typical reasons for a reimbursement request failing the audit include:\n\n        - Unusually large individual amounts\n        - Excessive total amounts within a month\n        - Expenses that do not meet reimbursement policies\n        - Missing or invalid supporting documents\n        - Duplicate submissions\n        - Other non-compliant cases\n        \"\"\"\n        if record.reimbursement_amount &gt; 2500:\n            return AuditResult(\n                request_id=record.request_id,\n                passed=False,\n                audit_reason=\"The reimbursement amount {record.reimbursement_amount} exceeds the limit of 2500.\"\n            )\n        # TODO: Add more audit rules here.\n        ...\n\n        return AuditResult(\n            request_id=record.request_id,\n            passed=True,\n            audit_reason=\"The reimbursement request passed the audit.\"\n        )\n    \n    @worker(dependencies=[\"audit_by_rules\"])\n    async def execute_payment(self, result: AuditResult, record: ReimbursementRecord = From(\"load_record\")):\n        if not result.passed:\n            print(f\"The reimbursement request {record.request_id} failed the audit. Reason: {result.audit_reason}\")\n            return False\n        \n        # The reimbursement request {record.request_id} has passed the audit rules. Requesting approval from the manager...\n        # human-in-the-loop: request approval from the manager.\n        event = Event(\n            event_type=\"request_approval\",\n            data={\n                \"reimbursement_record\": record,\n                \"audit_result\": result\n            }        \n        )\n        feedback: InteractionFeedback = self.interact_with_human(event)\n        if feedback.data == \"yes\":\n            await self.lanuch_payment_transaction(record.request_id)\n            print(f\"The reimbursement request {record.request_id} has been approved. Payment transaction launched.\")\n            return True\n\n        print(f\"!!!The reimbursement request {record.request_id} has been rejected. Payment transaction not launched.\\nRejection info:\\n {feedback.data}\")\n        return False\n\n    async def load_record_from_database(self, request_id: int):\n        # Simulate a database query...\n        return ReimbursementRecord(\n            request_id=request_id,\n            employee_id=888888,\n            employee_name=\"John Doe\",\n            reimbursement_month=\"2025-10\",\n            reimbursement_amount=1024.00,\n            description=\"Hotel expenses for a business trip\",\n            created_at=datetime(2025, 10, 11, 10, 0, 0),\n            updated_at=datetime(2025, 10, 11, 10, 0, 0)\n        )\n    async def lanuch_payment_transaction(self, request_id: int):\n        # Simulate a payment execution...\n        ...\n</pre> import os import tempfile from httpx import delete from pydantic import BaseModel from datetime import datetime from bridgic.core.automa import GraphAutoma, worker, Snapshot from bridgic.core.automa.args import From from bridgic.core.automa.interaction import Event, InteractionFeedback, InteractionException  class ReimbursementRecord(BaseModel):     request_id: int     employee_id: int     employee_name: str     reimbursement_month: str     reimbursement_amount: float     description: str     created_at: datetime     updated_at: datetime  class AuditResult(BaseModel):     request_id: int     passed: bool     audit_reason: str  class ReimbursementWorkflow(GraphAutoma):     @worker(is_start=True)     async def load_record(self, request_id: int):         \"\"\"         The reimbursement workflow can be triggered by the OA system \u2014 for instance, when an employee submits a new reimbursement request. Each request is uniquely identified by a `request_id`, which is then used to retrieve the corresponding reimbursement record from the database.          \"\"\"         # Load the data from database.         return await self.load_record_from_database(request_id)          @worker(dependencies=[\"load_record\"])     async def audit_by_rules(self, record: ReimbursementRecord):         \"\"\"         This method simulates the logic for automatically determining whether a reimbursement request complies with business rules.            Typical reasons for a reimbursement request failing the audit include:          - Unusually large individual amounts         - Excessive total amounts within a month         - Expenses that do not meet reimbursement policies         - Missing or invalid supporting documents         - Duplicate submissions         - Other non-compliant cases         \"\"\"         if record.reimbursement_amount &gt; 2500:             return AuditResult(                 request_id=record.request_id,                 passed=False,                 audit_reason=\"The reimbursement amount {record.reimbursement_amount} exceeds the limit of 2500.\"             )         # TODO: Add more audit rules here.         ...          return AuditResult(             request_id=record.request_id,             passed=True,             audit_reason=\"The reimbursement request passed the audit.\"         )          @worker(dependencies=[\"audit_by_rules\"])     async def execute_payment(self, result: AuditResult, record: ReimbursementRecord = From(\"load_record\")):         if not result.passed:             print(f\"The reimbursement request {record.request_id} failed the audit. Reason: {result.audit_reason}\")             return False                  # The reimbursement request {record.request_id} has passed the audit rules. Requesting approval from the manager...         # human-in-the-loop: request approval from the manager.         event = Event(             event_type=\"request_approval\",             data={                 \"reimbursement_record\": record,                 \"audit_result\": result             }                 )         feedback: InteractionFeedback = self.interact_with_human(event)         if feedback.data == \"yes\":             await self.lanuch_payment_transaction(record.request_id)             print(f\"The reimbursement request {record.request_id} has been approved. Payment transaction launched.\")             return True          print(f\"!!!The reimbursement request {record.request_id} has been rejected. Payment transaction not launched.\\nRejection info:\\n {feedback.data}\")         return False      async def load_record_from_database(self, request_id: int):         # Simulate a database query...         return ReimbursementRecord(             request_id=request_id,             employee_id=888888,             employee_name=\"John Doe\",             reimbursement_month=\"2025-10\",             reimbursement_amount=1024.00,             description=\"Hotel expenses for a business trip\",             created_at=datetime(2025, 10, 11, 10, 0, 0),             updated_at=datetime(2025, 10, 11, 10, 0, 0)         )     async def lanuch_payment_transaction(self, request_id: int):         # Simulate a payment execution...         ... <p>This workflow, <code>ReimbursementWorkflow</code>, consists of three steps:</p> <ul> <li><code>load_record</code>: Loading the reimbursement record identified by a <code>request_id</code> from database.</li> <li><code>audit_by_rules</code>: Automatically audit the reimbursement request by predefined rules.</li> <li><code>execute_payment</code>: Requesting approval from the manager (a human user) after the reimbursement request has passed the audit rules.</li> </ul> <p>In the third step (<code>execute_payment</code>), calling <code>interact_with_human()</code> posts an event and pauses the workflow execution.</p> In\u00a0[29]: Copied! <pre>async def save_snapshot_to_database(snapshot: Snapshot):\n    # Simulate a database storage using temporary files.\n    temp_dir = tempfile.TemporaryDirectory()\n    bytes_file = os.path.join(temp_dir.name, \"reimbursement_workflow.bytes\")\n    version_file = os.path.join(temp_dir.name, \"reimbursement_workflow.version\")\n    with open(bytes_file, \"wb\") as f:\n        f.write(snapshot.serialized_bytes)\n    with open(version_file, \"w\") as f:\n        f.write(snapshot.serialization_version)\n\n    return {\n        \"bytes_file\": bytes_file,\n        \"version_file\": version_file,\n        \"temp_dir\": temp_dir,\n    }\n\n\nreimbursement_workflow = ReimbursementWorkflow()\ntry:\n    await reimbursement_workflow.arun(request_id=123456)\nexcept InteractionException as e:\n    # The `ReimbursementWorkflow` instance has been paused and serialized to a snapshot.\n    interaction_id = e.interactions[0].interaction_id\n    record = e.interactions[0].event.data[\"reimbursement_record\"]\n    # Save the snapshot to the database.\n    db_context = await save_snapshot_to_database(e.snapshot)\n    print(\"The `ReimbursementWorkflow` instance has been paused and serialized to a snapshot.\")\n    print(\"The snapshot has been persisted to database.\")\n</pre> async def save_snapshot_to_database(snapshot: Snapshot):     # Simulate a database storage using temporary files.     temp_dir = tempfile.TemporaryDirectory()     bytes_file = os.path.join(temp_dir.name, \"reimbursement_workflow.bytes\")     version_file = os.path.join(temp_dir.name, \"reimbursement_workflow.version\")     with open(bytes_file, \"wb\") as f:         f.write(snapshot.serialized_bytes)     with open(version_file, \"w\") as f:         f.write(snapshot.serialization_version)      return {         \"bytes_file\": bytes_file,         \"version_file\": version_file,         \"temp_dir\": temp_dir,     }   reimbursement_workflow = ReimbursementWorkflow() try:     await reimbursement_workflow.arun(request_id=123456) except InteractionException as e:     # The `ReimbursementWorkflow` instance has been paused and serialized to a snapshot.     interaction_id = e.interactions[0].interaction_id     record = e.interactions[0].event.data[\"reimbursement_record\"]     # Save the snapshot to the database.     db_context = await save_snapshot_to_database(e.snapshot)     print(\"The `ReimbursementWorkflow` instance has been paused and serialized to a snapshot.\")     print(\"The snapshot has been persisted to database.\") <pre>The `ReimbursementWorkflow` instance has been paused and serialized to a snapshot.\nThe snapshot has been persisted to database.\n</pre> <p>When the <code>arun</code> method of the automa instance is called, an <code>InteractionException</code> will be raised as a result of invoking <code>interact_with_human()</code>.</p> <p>An <code>InteractionException</code> contains two fields:</p> <ul> <li><code>interactions</code>: A list of <code>Interaction</code>s, each <code>Interaction</code> containing an <code>interaction_id</code> and an <code>event</code>.</li> <li><code>snapshot</code>: a <code>Snapshot</code> instance, representing the Automa's current state serialized in bytes.</li> </ul> <p>Then the workflow, <code>ReimbursementWorkflow</code>, pauses, and the snapshot corresponding to the interaction is persisted in the database for later recovery.</p> In\u00a0[\u00a0]: Copied! <pre>async def load_snapshot_from_database(db_context):\n    # Simulate a database query using temporary files.\n    bytes_file = db_context[\"bytes_file\"]\n    version_file = db_context[\"version_file\"]\n    temp_dir = db_context[\"temp_dir\"]\n\n    with open(bytes_file, \"rb\") as f:\n        serialized_bytes = f.read()\n    with open(version_file, \"r\") as f:\n        serialization_version = f.read()\n    snapshot = Snapshot(\n        serialized_bytes=serialized_bytes, \n        serialization_version=serialization_version\n    )\n    return snapshot\n\nprint(\"Waiting for the manager's approval (It may take long time) ...\")\nhuman_feedback = input(\n    \"\\n\"\n    \"---------- Message to User ------------\\n\"\n    \"A reimbursement request has been submitted and audited by the system.\\n\"\n    \"Please check the details and give your approval or rejection.\\n\"\n\n    \"Reimbursement Request Details:\\n\"\n    f\"\\n{record.model_dump_json(indent=4)}\\n\"\n    \"If you approve the request, please input 'yes'.\\n\"\n    \"Otherwise, please input 'no' or the reason for rejection.\\n\"\n    \"Your input: \"\n    )\n\n# Load the snapshot from the database.\nsnapshot = await load_snapshot_from_database(db_context)\n# Deserialize the `ReimbursementWorkflow` instance from the snapshot.\nreimbursement_workflow = ReimbursementWorkflow.load_from_snapshot(snapshot)\nprint(\"-------------------------------------\\n\")\nprint(\"The `ReimbursementWorkflow` instance has been deserialized and loaded from the snapshot. It will resume to run immediately...\")\nfeedback = InteractionFeedback(\n    interaction_id=interaction_id,\n    data=human_feedback\n)\nawait reimbursement_workflow.arun(feedback_data=feedback)\n</pre> async def load_snapshot_from_database(db_context):     # Simulate a database query using temporary files.     bytes_file = db_context[\"bytes_file\"]     version_file = db_context[\"version_file\"]     temp_dir = db_context[\"temp_dir\"]      with open(bytes_file, \"rb\") as f:         serialized_bytes = f.read()     with open(version_file, \"r\") as f:         serialization_version = f.read()     snapshot = Snapshot(         serialized_bytes=serialized_bytes,          serialization_version=serialization_version     )     return snapshot  print(\"Waiting for the manager's approval (It may take long time) ...\") human_feedback = input(     \"\\n\"     \"---------- Message to User ------------\\n\"     \"A reimbursement request has been submitted and audited by the system.\\n\"     \"Please check the details and give your approval or rejection.\\n\"      \"Reimbursement Request Details:\\n\"     f\"\\n{record.model_dump_json(indent=4)}\\n\"     \"If you approve the request, please input 'yes'.\\n\"     \"Otherwise, please input 'no' or the reason for rejection.\\n\"     \"Your input: \"     )  # Load the snapshot from the database. snapshot = await load_snapshot_from_database(db_context) # Deserialize the `ReimbursementWorkflow` instance from the snapshot. reimbursement_workflow = ReimbursementWorkflow.load_from_snapshot(snapshot) print(\"-------------------------------------\\n\") print(\"The `ReimbursementWorkflow` instance has been deserialized and loaded from the snapshot. It will resume to run immediately...\") feedback = InteractionFeedback(     interaction_id=interaction_id,     data=human_feedback ) await reimbursement_workflow.arun(feedback_data=feedback) <pre>Waiting for the manager's approval (It may take long time) ...\n-------------------------------------\n\nThe `ReimbursementWorkflow` instance has been deserialized and loaded from the snapshot. It will resume to run immediately...\nThe reimbursement request 123456 has been approved. Payment transaction launched.\n</pre> <p>After an extended period, the user may complete the approval interaction for the reimbursement workflow. The system then retrieves the serialized snapshot from the database and deserializes it into an instance of the <code>ReimbursementWorkflow</code> class. Subsequently, the user\u2019s decision \u2014 either approval or rejection \u2014 is wrapped into an <code>InteractionFeedback</code> object, and the arun method of the automa is invoked again to resume the workflow execution from the previously paused state.</p> <p>When facing a situation that requires feedback but the waiting time is uncertain, this mechanism saves the current state and re-enters when the right moment comes in the future. This not only enables the system to release resources that would otherwise be occupied for a long time, but also allows it to be awakened at an appropriate time.</p>"},{"location":"tutorials/items/core_mechanism/human_in_the_loop/#human-in-the-loop","title":"Human-in-the-loop\u00b6","text":"<p>When building workflows or agents with Bridgic, developers can seamlessly integrate human-in-the-loop interactions into the execution flow. At any point, the system can pause its automated process to request human input \u2014 such as approval, verification, or additional instructions \u2014 and wait for a response. Once the human feedback is provided, the workflow or agent resumes execution from the point of interruption, adapting its behavior based on the new input. Bridgic ensures that the entire process, including paused and resumed states, can be reliably serialized and deserialized for persistence and recovery.</p>"},{"location":"tutorials/items/core_mechanism/human_in_the_loop/#interaction-scenarios","title":"Interaction Scenarios\u00b6","text":"<p>Let's go through a few simple examples to understand this process. Before that, let's set up the running environment.</p>"},{"location":"tutorials/items/core_mechanism/human_in_the_loop/#programming-assistant","title":"Programming assistant\u00b6","text":"<p>During the development of a programming assistant, it can be designed to automatically execute and verify the code it generates. However, since program execution consumes system resources, the user must decide whether to grant permission for the assistant to run the code.</p> <p>Let's achieve it with Bridgic. The source code can be downloaded in GitHub. The steps are as follows:</p> <ol> <li>Generate code based on user requirements.</li> <li>Ask the user if it is allowed to execute the generated code.</li> <li>Output result.</li> </ol>"},{"location":"tutorials/items/core_mechanism/human_in_the_loop/#counting-notifier","title":"Counting notifier\u00b6","text":"<p>Sometimes, it may be necessary to post an event without expecting any feedback, for example, message notifications or progress updates. At this point, we call the <code>post_event()</code> method and register a event handler of type <code>EventHandlerType</code> to process the event. Here the event handler should be a function that takes only an <code>Event</code> as an argument</p> <p>For example, a counting notifier is implemented to count from 1 up to the number specified by the <code>user_input</code> argument. The user can also specify which number (<code>notify_int</code>) should trigger a reminder.</p>"},{"location":"tutorials/items/core_mechanism/human_in_the_loop/#reimbursement-workflow","title":"Reimbursement Workflow\u00b6","text":"<p>In certain scenarios, it may be necessary to wait for feedback for a long time after triggering an event. However, keeping the system in a waiting state would result in unnecessary resource consumption.</p> <p>Bridgic provides a powerful <code>interact_with_human</code> mechanism for interruption recovery in this situation. This allows the workflow or agent to pause and persist its current execution state when such an event occur, wait for feedback, and then resume execution.</p> <p>Let's implement a reimbursement workflow that is automatically triggered by the enterprise's OA system and requires approval before the reimbursement can be completed. The source code can be downloaded in GitHub.</p>"},{"location":"tutorials/items/core_mechanism/human_in_the_loop/#what-have-we-learnt","title":"What have we learnt?\u00b6","text":"<p>Bridgic provides flexible support for any form of human-in-the-loop interaction:</p> <ul> <li><code>request_feedback_async</code>: Used when the event must return feedback before the program can proceed. The program remains blocked until feedback is received.</li> <li><code>post_event</code>: Used when you just want to notify or trigger an event without expecting any feedback. The main program never blocks.</li> <li><code>interact_with_human</code>: Used when feedback is required but may arrive much later. The program is suspended and persisted, and resumes immediately when feedback becomes available.</li> </ul>"},{"location":"tutorials/items/core_mechanism/modularity/","title":"Modularity","text":"In\u00a0[\u00a0]: Copied! <pre>import os\n\n# Set the API base and key.\n_api_key = os.environ.get(\"OPENAI_API_KEY\")\n_api_base = os.environ.get(\"OPENAI_API_BASE\")\n_model_name = os.environ.get(\"OPENAI_MODEL_NAME\")\n\n# Import the necessary modules.\nfrom typing import List\nfrom pydantic import BaseModel, Field\nfrom bridgic.core.automa import GraphAutoma, worker\nfrom bridgic.core.model.types import Message, Role\nfrom bridgic.core.model.protocols import PydanticModel\nfrom bridgic.llms.openai import OpenAILlm\n\nllm = OpenAILlm(api_base=_api_base, api_key=_api_key, timeout=30)\n</pre> import os  # Set the API base and key. _api_key = os.environ.get(\"OPENAI_API_KEY\") _api_base = os.environ.get(\"OPENAI_API_BASE\") _model_name = os.environ.get(\"OPENAI_MODEL_NAME\")  # Import the necessary modules. from typing import List from pydantic import BaseModel, Field from bridgic.core.automa import GraphAutoma, worker from bridgic.core.model.types import Message, Role from bridgic.core.model.protocols import PydanticModel from bridgic.llms.openai import OpenAILlm  llm = OpenAILlm(api_base=_api_base, api_key=_api_key, timeout=30) In\u00a0[\u00a0]: Copied! <pre>class Outline(BaseModel):\n    outline: List[str] = Field(description=\"The outline of the use input to write report\")\n\nclass OutlineWriter(GraphAutoma):\n    @worker(is_start=True)\n    async def pre_query(self, user_input: str):  # Receive the user's input and preprocess query\n        return user_input\n\n    @worker(dependencies=[\"pre_query\"], is_output=True)\n    async def topic_split(self, query: str):\n        response: Outline = await llm.astructured_output(\n            model=_model_name,\n            messages=[\n                Message.from_text(text=\"Write a sample report outline within 3 topics based on the user's input\", role=Role.SYSTEM),\n                Message.from_text(text=query, role=Role.USER,),\n            ],\n            constraint=PydanticModel(model=Outline)\n        )\n        return response\n</pre> class Outline(BaseModel):     outline: List[str] = Field(description=\"The outline of the use input to write report\")  class OutlineWriter(GraphAutoma):     @worker(is_start=True)     async def pre_query(self, user_input: str):  # Receive the user's input and preprocess query         return user_input      @worker(dependencies=[\"pre_query\"], is_output=True)     async def topic_split(self, query: str):         response: Outline = await llm.astructured_output(             model=_model_name,             messages=[                 Message.from_text(text=\"Write a sample report outline within 3 topics based on the user's input\", role=Role.SYSTEM),                 Message.from_text(text=query, role=Role.USER,),             ],             constraint=PydanticModel(model=Outline)         )         return response In\u00a0[\u00a0]: Copied! <pre>class ReportWriter(GraphAutoma):\n    @worker(dependencies=[\"outline_writer\"])\n    async def write_report(self, outline: Outline):\n        outline_str = \"\\n\".join(outline.outline)\n        print(f'- - - - - Outline - - - - -')\n        print(outline_str)\n        print(f'- - - - - End - - - - -\\n')\n        \n        response = await llm.achat(\n            model=_model_name,\n            messages=[\n                Message.from_text(text=\"write a sample report based on the user's input and strictly follow the outline\", role=Role.SYSTEM),\n                Message.from_text(text=f\"{outline_str}.\", role=Role.USER),\n            ],\n        )\n        print(f'- - - - - Report - - - - -')\n        print(response.message.content)\n        print(f'- - - - - End - - - - -\\n')\n        return response.message.content\n\nreport_writer = ReportWriter()\nreport_writer.add_worker(\n    key=\"outline_writer\",\n    worker=OutlineWriter(),\n    is_start=True\n)\n</pre> class ReportWriter(GraphAutoma):     @worker(dependencies=[\"outline_writer\"])     async def write_report(self, outline: Outline):         outline_str = \"\\n\".join(outline.outline)         print(f'- - - - - Outline - - - - -')         print(outline_str)         print(f'- - - - - End - - - - -\\n')                  response = await llm.achat(             model=_model_name,             messages=[                 Message.from_text(text=\"write a sample report based on the user's input and strictly follow the outline\", role=Role.SYSTEM),                 Message.from_text(text=f\"{outline_str}.\", role=Role.USER),             ],         )         print(f'- - - - - Report - - - - -')         print(response.message.content)         print(f'- - - - - End - - - - -\\n')         return response.message.content  report_writer = ReportWriter() report_writer.add_worker(     key=\"outline_writer\",     worker=OutlineWriter(),     is_start=True ) <p>Now, let's run it!</p> In\u00a0[19]: Copied! <pre>await report_writer.arun(user_input=\"What is an agent?\")\n</pre> await report_writer.arun(user_input=\"What is an agent?\") <pre>- - - - - Outline - - - - -\nDefinition of an Agent\nTypes of Agents in Different Contexts\nRole and Function of Agents in AI and Automation\n- - - - - End - - - - -\n\n- - - - - Report - - - - -\n**Report: Definition of an Agent, Types of Agents in Different Contexts, and Role and Function of Agents in AI and Automation**\n\n---\n\n**1. Definition of an Agent**\n\nAn *agent* is an autonomous entity that perceives its environment through sensors and acts upon that environment using actuators to achieve specific goals. In a broad sense, an agent is capable of making decisions based on its internal state and external inputs, adapting its behavior over time in response to changing conditions. The concept of an agent is foundational in artificial intelligence (AI), robotics, and automated systems. It operates independently or in coordination with other agents, following predefined rules, learning from experience, or using reasoning mechanisms to perform tasks efficiently.\n\nIn computational terms, an agent typically consists of:\n- **Perception**: Sensing and interpreting environmental inputs.\n- **Reasoning/Decision-making**: Processing information to determine the best course of action.\n- **Action**: Executing decisions through physical or digital means.\n\nAgents can be simple (e.g., a thermostat adjusting temperature) or complex (e.g., AI-driven trading bots in financial markets).\n\n---\n\n**2. Types of Agents in Different Contexts**\n\nAgents vary significantly depending on the domain or context in which they operate. Below are key types categorized by application areas:\n\n**a. In Artificial Intelligence (AI):**\n- **Simple Reflex Agents**: Make decisions based solely on current percept. Example: A thermostat that turns on the heater when the room temperature drops below a threshold.\n- **Model-Based Reflex Agents**: Use an internal model of the world to make decisions. Example: A self-driving car that predicts traffic patterns based on historical data.\n- **Goal-Based Agents**: Act to achieve specific goals. Example: A virtual assistant like Siri or Alexa that performs tasks such as setting reminders or sending messages.\n- **Utility-Based Agents**: Choose actions that maximize expected utility. Example: An AI recommending a product based on user preferences and potential profit.\n- **Learning Agents**: Improve performance over time through experience. Example: A recommendation engine that learns user behavior and adjusts suggestions.\n\n**b. In Robotics:**\n- **Autonomous Robots**: Operate independently in dynamic environments (e.g., drones, warehouse robots).\n- **Human-Robot Collaborative Agents**: Work alongside humans (e.g., robotic arms in manufacturing).\n- **Mobile Agents**: Move through physical environments to perform tasks (e.g., delivery robots).\n\n**c. In Software and Business Automation:**\n- **Business Process Agents**: Automate routine tasks like invoice processing or customer service responses.\n- **Workflow Agents**: Coordinate multiple tasks within a system (e.g., workflow management in CRM software).\n- **Customer Service Agents**: Chatbots that respond to user queries in real time.\n\n**d. In Distributed Systems:**\n- **Multi-Agent Systems (MAS)**: Groups of agents that interact to solve complex problems (e.g., traffic management, supply chain optimization).\n- **Swarm Agents**: Simulate collective behavior (e.g., ant colonies, drone swarms).\n\n---\n\n**3. Role and Function of Agents in AI and Automation**\n\nAgents play a pivotal role in advancing AI and automation by enabling intelligent, adaptive, and scalable systems. Their key functions include:\n\n- **Autonomy and Decision-Making**: Agents operate independently, making real-time decisions without constant human intervention, which is essential in dynamic environments such as autonomous vehicles or smart homes.\n\n- **Adaptability and Learning**: Through machine learning and reinforcement learning, agents evolve their strategies based on feedback, improving accuracy and efficiency over time.\n\n- **Efficiency and Scalability**: Agents can perform repetitive or time-consuming tasks quickly and consistently, reducing human workload and operational costs. For example, AI agents in customer service reduce response times and increase service availability.\n\n- **Interoperability and Integration**: Agents can communicate and collaborate across different platforms and systems, enabling seamless integration in enterprise environments (e.g., connecting ERP, CRM, and supply chain systems).\n\n- **Problem Solving and Optimization**: In complex domains like logistics or energy management, agents use algorithms to find optimal solutions, such as route planning or load balancing.\n\n- **Human-AI Collaboration**: Agents serve as intelligent interfaces between humans and machines, enhancing user experience by providing personalized, context-aware assistance.\n\nIn summary, agents are central to the development of intelligent systems that can perceive, reason, act, and learn\u2014making them indispensable in modern AI and automation ecosystems.\n\n---\n\n**Conclusion**\n\nThe concept of an agent bridges the gap between passive systems and intelligent, responsive automation. From simple rule-based tools to sophisticated AI-driven entities, agents are transforming how systems interact with their environments. As AI and automation continue to evolve, the design, deployment, and coordination of agents will remain a critical focus area across industries, driving innovation, efficiency, and human-machine collaboration.\n\n---  \n*End of Report*\n- - - - - End - - - - -\n\n</pre> <p>Great! We successfully reused <code>OutlineWriter</code> in <code>ReportWriter</code>.</p>"},{"location":"tutorials/items/core_mechanism/modularity/#modularity","title":"Modularity\u00b6","text":"<p>During the development process, we may have already had some individual automas. Now, we want to combine them to create a more powerful automa. An automa can also be added as a Worker to another Automa to achieve reuse.</p> <p>Let's understand it with a sample case.</p>"},{"location":"tutorials/items/core_mechanism/modularity/#report-writer","title":"Report Writer\u00b6","text":"<p>The typical process of report writing usually involves drafting an outline based on user input and then writing the report according to the outline. Now, let's first write an Automa for generating an outline, and then nest it within the Automa for writing the report.</p>"},{"location":"tutorials/items/core_mechanism/modularity/#1-initialize","title":"1. Initialize\u00b6","text":"<p>Initialize the runtime environment.</p>"},{"location":"tutorials/items/core_mechanism/modularity/#2-outline-automa","title":"2. Outline Automa\u00b6","text":"<p>Outline Automa drafts an outline based on user input. Output an <code>Outline</code> object for subsequent processing.</p>"},{"location":"tutorials/items/core_mechanism/modularity/#3-write-automa","title":"3. Write Automa\u00b6","text":"<p>We can reuse the <code>OutlineWriter</code> in <code>ReportWriter</code>.</p>"},{"location":"tutorials/items/core_mechanism/modularity/#what-have-we-learnt","title":"What have we learnt?\u00b6","text":"<p>we added an automa as a <code>Worker</code> to another <code>Automa</code> to achieve nested reuse. There are several points that need special attention during the reuse process:</p> <ol> <li>Worker Dependency: A <code>Worker</code> in an <code>Automa</code> can only depend on other workers within the same <code>Automa</code>, and cannot depend across different <code>Automa</code>.</li> <li>Routing: A <code>Worker</code> in an <code>Automa</code> can only route to other workers within the same <code>Automa</code>, and cannot route across different <code>Automa</code>.</li> <li>Human-in-the-loop: When a worker inside throws an event, it hands over this event to the automa agent for handling. At this point, the event handling functions of nested automa need to be registered to the outermost Automa.</li> </ol>"},{"location":"tutorials/items/core_mechanism/parameter_resolving/","title":"Parameter Resolving","text":"In\u00a0[\u00a0]: Copied! <pre>from typing import Tuple\nfrom bridgic.core.automa import GraphAutoma, worker\nfrom bridgic.core.automa.args import InOrder, ResultDispatchingRule\n</pre> from typing import Tuple from bridgic.core.automa import GraphAutoma, worker from bridgic.core.automa.args import InOrder, ResultDispatchingRule In\u00a0[2]: Copied! <pre>class ParallelProcessing(GraphAutoma):\n    @worker(is_start=True)\n    async def process_query_1(self, user_input: int) -&gt; int:\n        print(f\"Processing query 1 with input: {user_input}\")\n        return user_input * 2  # 2\n    \n    @worker(is_start=True)\n    async def process_query_2(self, user_input: int) -&gt; int:\n        print(f\"Processing query 2 with input: {user_input}\")\n        return user_input * 3  # 6\n    \n    @worker(dependencies=[\"process_query_1\", \"process_query_2\"], is_output=True)\n    async def aggregate_results(self, result1: int, result2: int) -&gt; int:\n        print(f\"Aggregating: {result1} + {result2}\")\n        return result1 + result2  # 8\n</pre> class ParallelProcessing(GraphAutoma):     @worker(is_start=True)     async def process_query_1(self, user_input: int) -&gt; int:         print(f\"Processing query 1 with input: {user_input}\")         return user_input * 2  # 2          @worker(is_start=True)     async def process_query_2(self, user_input: int) -&gt; int:         print(f\"Processing query 2 with input: {user_input}\")         return user_input * 3  # 6          @worker(dependencies=[\"process_query_1\", \"process_query_2\"], is_output=True)     async def aggregate_results(self, result1: int, result2: int) -&gt; int:         print(f\"Aggregating: {result1} + {result2}\")         return result1 + result2  # 8  <p>Now let's run it with <code>InOrder</code> to distribute different values to the two start workers:</p> In\u00a0[\u00a0]: Copied! <pre>automa = ParallelProcessing()\nres = await automa.arun(user_input=InOrder([1, 2]))\nprint(res)\n</pre> automa = ParallelProcessing() res = await automa.arun(user_input=InOrder([1, 2])) print(res) <pre>Processing query 1 with input: 1\nProcessing query 2 with input: 2\nAggregating: 2 + 6\n8\n</pre> <p>Great! As you can see, <code>InOrder([1, 2])</code> distributed the values <code>1</code> and <code>2</code> to <code>process_query_1</code> and <code>process_query_2</code> respectively, based on the order of the declaration of start workers.</p> <p>Note: The length of the list/tuple in <code>InOrder()</code> must match the number of start workers that accept the corresponding parameter. Otherwise, an error will be raised.</p> In\u00a0[8]: Copied! <pre>class ResultDispatchingExample(GraphAutoma):\n    @worker(is_start=True, result_dispatching_rule=ResultDispatchingRule.AS_IS)\n    async def generate_data(self, user_input: int) -&gt; int:\n        return user_input\n    \n    @worker(dependencies=[\"generate_data\"], result_dispatching_rule=ResultDispatchingRule.IN_ORDER)\n    async def process_and_split(self, data: int) -&gt; Tuple[int, int]:\n        print(f\"Processing data: {data}\")\n        # Return a tuple that will be distributed to downstream workers\n        return data + 1, data + 2  # (2, 3)\n    \n    @worker(dependencies=[\"process_and_split\"])\n    async def worker_a(self, value: int) -&gt; int:\n        print(f\"Worker A received: {value}\")\n        return value * 10  # 20\n    \n    @worker(dependencies=[\"process_and_split\"])\n    async def worker_b(self, value: int) -&gt; int:\n        print(f\"Worker B received: {value}\")\n        return value * 20  # 60\n    \n    @worker(dependencies=[\"worker_a\", \"worker_b\"], is_output=True)\n    async def combine_results(self, result_a: int, result_b: int) -&gt; int:\n        print(f\"Combining: {result_a} + {result_b}\")\n        return result_a + result_b  # 80\n</pre> class ResultDispatchingExample(GraphAutoma):     @worker(is_start=True, result_dispatching_rule=ResultDispatchingRule.AS_IS)     async def generate_data(self, user_input: int) -&gt; int:         return user_input          @worker(dependencies=[\"generate_data\"], result_dispatching_rule=ResultDispatchingRule.IN_ORDER)     async def process_and_split(self, data: int) -&gt; Tuple[int, int]:         print(f\"Processing data: {data}\")         # Return a tuple that will be distributed to downstream workers         return data + 1, data + 2  # (2, 3)          @worker(dependencies=[\"process_and_split\"])     async def worker_a(self, value: int) -&gt; int:         print(f\"Worker A received: {value}\")         return value * 10  # 20          @worker(dependencies=[\"process_and_split\"])     async def worker_b(self, value: int) -&gt; int:         print(f\"Worker B received: {value}\")         return value * 20  # 60          @worker(dependencies=[\"worker_a\", \"worker_b\"], is_output=True)     async def combine_results(self, result_a: int, result_b: int) -&gt; int:         print(f\"Combining: {result_a} + {result_b}\")         return result_a + result_b  # 80  <p>When <code>process_and_split</code> returns <code>(2, 3)</code>, we want to distribute these elements to <code>worker_a</code> and <code>worker_b</code> respectively. Note that <code>result_dispatching_rule</code> should be set on the worker that produces the results, not on the workers that receive them.</p> <p>Now let's run it:</p> In\u00a0[9]: Copied! <pre>automa = ResultDispatchingExample()\nres = await automa.arun(user_input=1)\nprint(res)\n</pre> automa = ResultDispatchingExample() res = await automa.arun(user_input=1) print(res) <pre>Processing data: 1\nWorker A received: 2\nWorker B received: 3\nCombining: 20 + 60\n80\n</pre> <p>Perfect! As you can see:</p> <ol> <li><code>process_and_split</code> returned <code>(2, 3)</code> and had <code>result_dispatching_rule=ResultDispatchingRule.IN_ORDER</code>.</li> <li>The first value <code>2</code> was distributed to <code>worker_a</code> (the first downstream worker).</li> <li>The second value <code>3</code> was distributed to <code>worker_b</code> (the second downstream worker).</li> <li>Both workers processed their values and the results were combined.</li> </ol> <p>Important Notes:</p> <ul> <li>The worker that sets <code>result_dispatching_rule=ResultDispatchingRule.IN_ORDER</code> must return an iterable (tuple or list).</li> <li>The length of the returned iterable must match the number of downstream workers that directly depend on this worker.</li> <li>The distribution order follows the order in which the downstream workers are declared in the graph.</li> </ul> <p>There are two mode of <code>ResultDispatchingRule</code>:</p> <ul> <li>AS_IS: The result of the worker will be sent as a whole to each downstream worker that depends on it. This is the default behavior.</li> <li>IN_ORDER: The workers will distribute the current results in sequence to the corresponding workers one by one according to the order in which the downstream workers are declared or added.</li> </ul> In\u00a0[\u00a0]: Copied! <pre>pip install bridgic-llms-openai\n</pre> pip install bridgic-llms-openai In\u00a0[\u00a0]: Copied! <pre># Get the environment variables.\nimport os\n\n_api_key = os.environ.get(\"OPENAI_API_KEY\")\n_api_base = os.environ.get(\"OPENAI_API_BASE\")\n_model_name = os.environ.get(\"OPENAI_MODEL_NAME\")\n\n# Import the necessary packages.\nfrom pydantic import BaseModel, Field\nfrom typing import List, Dict, Tuple\nfrom bridgic.core.automa import GraphAutoma, worker\nfrom bridgic.core.automa.args import ArgsMappingRule\nfrom bridgic.core.model.types import Message, Role\nfrom bridgic.core.model.protocols import PydanticModel\nfrom bridgic.llms.openai import OpenAILlm, OpenAIConfiguration\n\nllm = OpenAILlm(  # the llm instance\n    api_base=_api_base,\n    api_key=_api_key,\n    timeout=5,\n    configuration=OpenAIConfiguration(model=_model_name),\n)\n</pre> # Get the environment variables. import os  _api_key = os.environ.get(\"OPENAI_API_KEY\") _api_base = os.environ.get(\"OPENAI_API_BASE\") _model_name = os.environ.get(\"OPENAI_MODEL_NAME\")  # Import the necessary packages. from pydantic import BaseModel, Field from typing import List, Dict, Tuple from bridgic.core.automa import GraphAutoma, worker from bridgic.core.automa.args import ArgsMappingRule from bridgic.core.model.types import Message, Role from bridgic.core.model.protocols import PydanticModel from bridgic.llms.openai import OpenAILlm, OpenAIConfiguration  llm = OpenAILlm(  # the llm instance     api_base=_api_base,     api_key=_api_key,     timeout=5,     configuration=OpenAIConfiguration(model=_model_name), ) <p>Now let's implement this query expansion. We assume that the user query we receive is in JSON format. It contains three keys:</p> <ol> <li><code>id</code>: A string that indicates who inputs the query.</li> <li><code>query</code>: A string in the form of <code>Q: user_query</code> representing the question input by the user.</li> <li><code>date</code>: The time when the user entered the query.</li> </ol> In\u00a0[\u00a0]: Copied! <pre>query_obj = {\n    \"id\": \"user_1\",\n    \"query\": \"Q: What new developments have there been in RAG in the past year?\",\n    \"date\": \"2025-09-30\"\n}\n</pre> query_obj = {     \"id\": \"user_1\",     \"query\": \"Q: What new developments have there been in RAG in the past year?\",     \"date\": \"2025-09-30\" } <p>Furthermore, we define that when the model completes entity extraction and query expansion, it returns the result in a Pydantic data structure.</p> In\u00a0[5]: Copied! <pre>class EntityList(BaseModel):  # The expected format of the model output in the extract_entity worker\n    entities: List[str] = Field(description=\"All entities in the input.\")\n\nclass QueryList(BaseModel):  # The expected format of the model output in the expand_query worker\n    queries: List[str] = Field(description=\"All queries in the input.\")\n</pre> class EntityList(BaseModel):  # The expected format of the model output in the extract_entity worker     entities: List[str] = Field(description=\"All entities in the input.\")  class QueryList(BaseModel):  # The expected format of the model output in the expand_query worker     queries: List[str] = Field(description=\"All queries in the input.\") <p>Next, let's complete the three steps of query expansion to achieve our goal:</p> In\u00a0[\u00a0]: Copied! <pre>class QueryExpansion(GraphAutoma):\n    @worker(is_start=True)\n    async def pre_query(self, query_obj: Dict):  # Receive the user's input and preprocess query\n        query = query_obj[\"query\"].split(\":\")[1].strip()  \n        return query\n\n    @worker(is_start=True)\n    async def pre_date(self, query_obj: Dict):  # Receive the user's input and preprocess date\n        date = query_obj[\"date\"]\n        return date\n\n    @worker(dependencies=[\"pre_query\", \"pre_date\"], args_mapping_rule=ArgsMappingRule.AS_IS)\n    async def extract_entity(self, query: str, date: str):  # Extract the entity information from the question, get entity information.\n        response: EntityList = await llm.astructured_output(  \n            constraint=PydanticModel(model=EntityList),\n            messages=[\n                Message.from_text(text=\"extract the entity information from the given query\", role=Role.SYSTEM),\n                Message.from_text(text=query, role=Role.USER,),\n            ]\n        )\n        return query, response.entities, date\n\n    @worker(dependencies=[\"extract_entity\"], is_output=True, args_mapping_rule=ArgsMappingRule.AS_IS)\n    async def expand_query(self, query_meta: Tuple[str, List[str]]):  # Expand and obtain multiple queries.\n        query, entities, date = query_meta\n        task_input = f\"Query: {query}\\nEntities: {entities}\"\n        response: QueryList = await llm.astructured_output(\n            constraint=PydanticModel(model=QueryList),\n            messages=[\n                Message.from_text(text=f\"Centered around the given entities and given date {date}, expand the query to obtain multiple queries\", role=Role.SYSTEM),\n                Message.from_text(text=task_input, role=Role.USER,),\n            ]\n        )\n        return response.queries\n</pre> class QueryExpansion(GraphAutoma):     @worker(is_start=True)     async def pre_query(self, query_obj: Dict):  # Receive the user's input and preprocess query         query = query_obj[\"query\"].split(\":\")[1].strip()           return query      @worker(is_start=True)     async def pre_date(self, query_obj: Dict):  # Receive the user's input and preprocess date         date = query_obj[\"date\"]         return date      @worker(dependencies=[\"pre_query\", \"pre_date\"], args_mapping_rule=ArgsMappingRule.AS_IS)     async def extract_entity(self, query: str, date: str):  # Extract the entity information from the question, get entity information.         response: EntityList = await llm.astructured_output(               constraint=PydanticModel(model=EntityList),             messages=[                 Message.from_text(text=\"extract the entity information from the given query\", role=Role.SYSTEM),                 Message.from_text(text=query, role=Role.USER,),             ]         )         return query, response.entities, date      @worker(dependencies=[\"extract_entity\"], is_output=True, args_mapping_rule=ArgsMappingRule.AS_IS)     async def expand_query(self, query_meta: Tuple[str, List[str]]):  # Expand and obtain multiple queries.         query, entities, date = query_meta         task_input = f\"Query: {query}\\nEntities: {entities}\"         response: QueryList = await llm.astructured_output(             constraint=PydanticModel(model=QueryList),             messages=[                 Message.from_text(text=f\"Centered around the given entities and given date {date}, expand the query to obtain multiple queries\", role=Role.SYSTEM),                 Message.from_text(text=task_input, role=Role.USER,),             ]         )         return response.queries <p>Let's run it!</p> In\u00a0[7]: Copied! <pre>query_expansion = QueryExpansion()\nawait query_expansion.arun(query_obj)\n</pre> query_expansion = QueryExpansion() await query_expansion.arun(query_obj) Out[7]: <pre>['What are the latest advancements in Retrieval-Augmented Generation (RAG) technology as of 2025?',\n 'What new developments have emerged in RAG systems over the past 12 months?',\n 'How have RAG implementations evolved in the last year?',\n 'What innovations in RAG have been introduced between 2024 and 2025?',\n 'What are the key breakthroughs in RAG technology in 2025?',\n 'What new features or improvements have been added to RAG models in the past year?',\n 'How has the performance of RAG systems improved in the last 12 months?',\n 'What are the most recent trends and developments in RAG research and deployment?',\n 'What new techniques have been introduced in RAG to improve accuracy and efficiency in 2025?',\n 'What are the major updates in RAG frameworks and tools from 2024 to 2025?']</pre> <p>Great! We have successfully completed the small module for query expansion.</p> In\u00a0[\u00a0]: Copied! <pre>@worker(dependencies=[\"extract_entity\"], is_output=True, args_mapping_rule=ArgsMappingRule.AS_IS)\nasync def expand_query(self, query_meta: Tuple[str, List[str]]):  # Expand and obtain multiple queries.\n    query, entities, date = query_meta\n    ...\n</pre> @worker(dependencies=[\"extract_entity\"], is_output=True, args_mapping_rule=ArgsMappingRule.AS_IS) async def expand_query(self, query_meta: Tuple[str, List[str]]):  # Expand and obtain multiple queries.     query, entities, date = query_meta     ... <p>This operation requires knowing what the parameters of <code>query_meta</code> as a whole contain, which might seem inconvenient. Could we complete the unpacking operation and fill in the corresponding parameters when returning? At this point, the <code>UNPACK</code> mode comes in handy.</p> <p>Let's modify the <code>expand_query</code> in the above example and add some print messages.</p> In\u00a0[\u00a0]: Copied! <pre>class QueryExpansion(GraphAutoma):\n    @worker(is_start=True)\n    async def pre_query(self, query_obj: Dict):  # Receive the user's input and preprocess query\n        query = query_obj[\"query\"].split(\":\")[1].strip()  \n        return query\n\n    @worker(is_start=True)\n    async def pre_date(self, query_obj: Dict):  # Receive the user's input and preprocess date\n        date = query_obj[\"date\"]\n        return date\n\n    @worker(dependencies=[\"pre_query\", \"pre_date\"], args_mapping_rule=ArgsMappingRule.AS_IS)\n    async def extract_entity(self, query: str, date: str):  # Extract the entity information from the question, get entity information.\n        response: EntityList = await llm.astructured_output(  \n            constraint=PydanticModel(model=EntityList),\n            messages=[\n                Message.from_text(text=\"extract the entity information from the given query\", role=Role.SYSTEM),\n                Message.from_text(text=query, role=Role.USER,),\n            ]\n        )\n        return query, response.entities, date\n\n    @worker(dependencies=[\"extract_entity\"], is_output=True, args_mapping_rule=ArgsMappingRule.UNPACK)\n    async def expand_query(self, query: str, entities: List[str], date: str):  # Expand and obtain multiple queries.\n        print(f\"query: {query}, entities: {entities}, date: {date}\")\n        task_input = f\"Query: {query}\\nEntities: {entities}\"\n        response: QueryList = await llm.astructured_output(\n            constraint=PydanticModel(model=QueryList),\n            messages=[\n                Message.from_text(text=f\"Centered around the given entities and given date {date}, expand the query to obtain multiple queries\", role=Role.SYSTEM),\n                Message.from_text(text=task_input, role=Role.USER,),\n            ]\n        )\n        return response.queries\n</pre> class QueryExpansion(GraphAutoma):     @worker(is_start=True)     async def pre_query(self, query_obj: Dict):  # Receive the user's input and preprocess query         query = query_obj[\"query\"].split(\":\")[1].strip()           return query      @worker(is_start=True)     async def pre_date(self, query_obj: Dict):  # Receive the user's input and preprocess date         date = query_obj[\"date\"]         return date      @worker(dependencies=[\"pre_query\", \"pre_date\"], args_mapping_rule=ArgsMappingRule.AS_IS)     async def extract_entity(self, query: str, date: str):  # Extract the entity information from the question, get entity information.         response: EntityList = await llm.astructured_output(               constraint=PydanticModel(model=EntityList),             messages=[                 Message.from_text(text=\"extract the entity information from the given query\", role=Role.SYSTEM),                 Message.from_text(text=query, role=Role.USER,),             ]         )         return query, response.entities, date      @worker(dependencies=[\"extract_entity\"], is_output=True, args_mapping_rule=ArgsMappingRule.UNPACK)     async def expand_query(self, query: str, entities: List[str], date: str):  # Expand and obtain multiple queries.         print(f\"query: {query}, entities: {entities}, date: {date}\")         task_input = f\"Query: {query}\\nEntities: {entities}\"         response: QueryList = await llm.astructured_output(             constraint=PydanticModel(model=QueryList),             messages=[                 Message.from_text(text=f\"Centered around the given entities and given date {date}, expand the query to obtain multiple queries\", role=Role.SYSTEM),                 Message.from_text(text=task_input, role=Role.USER,),             ]         )         return response.queries <p>Let's run it!</p> In\u00a0[9]: Copied! <pre>query_expansion = QueryExpansion()\nawait query_expansion.arun(query_obj)\n</pre> query_expansion = QueryExpansion() await query_expansion.arun(query_obj) <pre>query: What new developments have there been in RAG in the past year?, entities: ['RAG', 'new developments', 'past year'], date: 2025-09-30\n</pre> Out[9]: <pre>['What are the latest advancements in Retrieval-Augmented Generation (RAG) technologies as of 2025?',\n 'What new developments have emerged in RAG systems over the past 12 months?',\n 'How has RAG evolved in the last year with regard to accuracy, efficiency, and scalability?',\n 'What are the key innovations in RAG that have been introduced between 2024 and 2025?',\n 'What new tools and frameworks have been launched for RAG implementation in the past year?',\n 'What recent breakthroughs in RAG have improved context handling and retrieval precision?',\n 'How have large language models integrated with RAG in the past year to enhance performance?',\n 'What are the most significant updates in RAG-based applications from 2024 to 2025?',\n 'What new techniques in RAG have been proposed to reduce hallucinations and improve factual consistency?',\n 'How have RAG solutions adapted to real-time data retrieval and dynamic content updates in the past year?']</pre> <p>Great! All the parameters were unpacked and accepted. It can be seen that the <code>unpack</code> mode makes our task flow clearer!</p> <p>However, it should be noted that the UNPACK mechanism requires that the current worker can only directly depend on one worker; otherwise, the results of multiple workers will be confused when unpacking!</p> In\u00a0[\u00a0]: Copied! <pre>class QueryExpansion(GraphAutoma):\n    @worker(is_start=True)\n    async def pre_query(self, query_obj: Dict):  # Receive the user's input and preprocess query\n        query = query_obj[\"query\"].split(\":\")[1].strip()  \n        return query\n\n    @worker(is_start=True)\n    async def pre_date(self, query_obj: Dict):  # Receive the user's input and preprocess date\n        date = query_obj[\"date\"]\n        return date\n\n    @worker(dependencies=[\"pre_query\", \"pre_date\"], args_mapping_rule=ArgsMappingRule.MERGE)\n    async def extract_entity(self, query_meta: Tuple[str, str]):  # Extract the entity information from the question, get entity information.\n        print(f\"query_meta: {query_meta}\")\n        query, date = query_meta\n        response: EntityList = await llm.astructured_output(  \n            constraint=PydanticModel(model=EntityList),\n            messages=[\n                Message.from_text(text=\"extract the entity information from the given query\", role=Role.SYSTEM),\n                Message.from_text(text=query, role=Role.USER,),\n            ]\n        )\n        return query, response.entities, date\n\n    @worker(dependencies=[\"extract_entity\"], is_output=True, args_mapping_rule=ArgsMappingRule.UNPACK)\n    async def expand_query(self, query: str, entities: List[str], date: str):  # Expand and obtain multiple queries.\n        task_input = f\"Query: {query}\\nEntities: {entities}\"\n        response: QueryList = await llm.astructured_output(\n            constraint=PydanticModel(model=QueryList),\n            messages=[\n                Message.from_text(text=f\"Centered around the given entities and given date {date}, expand the query to obtain multiple queries\", role=Role.SYSTEM),\n                Message.from_text(text=task_input, role=Role.USER,),\n            ]\n        )\n        return response.queries\n</pre> class QueryExpansion(GraphAutoma):     @worker(is_start=True)     async def pre_query(self, query_obj: Dict):  # Receive the user's input and preprocess query         query = query_obj[\"query\"].split(\":\")[1].strip()           return query      @worker(is_start=True)     async def pre_date(self, query_obj: Dict):  # Receive the user's input and preprocess date         date = query_obj[\"date\"]         return date      @worker(dependencies=[\"pre_query\", \"pre_date\"], args_mapping_rule=ArgsMappingRule.MERGE)     async def extract_entity(self, query_meta: Tuple[str, str]):  # Extract the entity information from the question, get entity information.         print(f\"query_meta: {query_meta}\")         query, date = query_meta         response: EntityList = await llm.astructured_output(               constraint=PydanticModel(model=EntityList),             messages=[                 Message.from_text(text=\"extract the entity information from the given query\", role=Role.SYSTEM),                 Message.from_text(text=query, role=Role.USER,),             ]         )         return query, response.entities, date      @worker(dependencies=[\"extract_entity\"], is_output=True, args_mapping_rule=ArgsMappingRule.UNPACK)     async def expand_query(self, query: str, entities: List[str], date: str):  # Expand and obtain multiple queries.         task_input = f\"Query: {query}\\nEntities: {entities}\"         response: QueryList = await llm.astructured_output(             constraint=PydanticModel(model=QueryList),             messages=[                 Message.from_text(text=f\"Centered around the given entities and given date {date}, expand the query to obtain multiple queries\", role=Role.SYSTEM),                 Message.from_text(text=task_input, role=Role.USER,),             ]         )         return response.queries <p>Let's run it!</p> In\u00a0[11]: Copied! <pre>query_expansion = QueryExpansion()\nawait query_expansion.arun(query_obj)\n</pre> query_expansion = QueryExpansion() await query_expansion.arun(query_obj) <pre>query_meta: ['What new developments have there been in RAG in the past year?', '2025-09-30']\n</pre> Out[11]: <pre>['What are the latest advancements in Retrieval-Augmented Generation (RAG) technology as of 2025?',\n 'What new developments have emerged in RAG systems over the past 12 months?',\n 'How has RAG evolved in the last year with recent innovations in AI and NLP?',\n 'What are the key updates and breakthroughs in RAG models from 2024 to 2025?',\n 'What new features or improvements have been introduced in RAG implementations in the past year?',\n 'What are the most significant RAG developments reported in 2025?',\n 'How have retrieval and generation components in RAG been improved in the last year?',\n 'What are the recent trends and new developments in RAG applications from 2024 to 2025?',\n 'What innovations in RAG have been introduced by leading AI companies in the past year?',\n 'What new challenges and solutions have emerged in RAG research over the last 12 months?']</pre> <p>Great! The results that <code>extract_entity</code> depends on from the workers have all been collected into a list and passed to its parameters.</p> In\u00a0[\u00a0]: Copied! <pre># import the From marker\nfrom bridgic.core.automa import From\n\n@worker(dependencies=[\"extract_entity\"], is_output=True, args_mapping_rule=ArgsMappingRule.UNPACK)\nasync def expand_query(self, query_meta: Tuple[str, str], date: str = From(\"pre_date\", \"2025-01-01\")):  # Expand and obtain multiple queries.\n    ...\n</pre> # import the From marker from bridgic.core.automa import From  @worker(dependencies=[\"extract_entity\"], is_output=True, args_mapping_rule=ArgsMappingRule.UNPACK) async def expand_query(self, query_meta: Tuple[str, str], date: str = From(\"pre_date\", \"2025-01-01\")):  # Expand and obtain multiple queries.     ... <p><code>date: str = From(\"pre_date\", \"2025-01-01\")</code> indicates that the value of <code>date</code> will be assigned based on the result of the <code>pre_date</code> worker. If the result from this worker has not yet been produced, the default value will be used instead.</p> <p>If the pre_date worker does not exist, or if the pre_date worker has not yet produced a result, and there is no default value, an error will be reported: AutomaDataInjectionError.</p> <p>Let's modify the above example and add some print messages.</p> In\u00a0[\u00a0]: Copied! <pre>class QueryExpansion(GraphAutoma):\n    @worker(is_start=True)\n    async def pre_query(self, query_obj: Dict):  # Receive the user's input and preprocess query\n        query = query_obj[\"query\"].split(\":\")[1].strip()  \n        return query\n\n    @worker(is_start=True)\n    async def pre_date(self, query_obj: Dict):  # Receive the user's input and preprocess date\n        date = query_obj[\"date\"]\n        return date\n\n    @worker(dependencies=[\"pre_query\"], args_mapping_rule=ArgsMappingRule.AS_IS)\n    async def extract_entity(self, query: str):  # Extract the entity information from the question, get entity information.\n        response: EntityList = await llm.astructured_output(  \n            constraint=PydanticModel(model=EntityList),\n            messages=[\n                Message.from_text(text=\"extract the entity information from the given query\", role=Role.SYSTEM),\n                Message.from_text(text=query, role=Role.USER,),\n            ]\n        )\n        return query, response.entities\n\n    @worker(dependencies=[\"extract_entity\"], is_output=True, args_mapping_rule=ArgsMappingRule.UNPACK)\n    async def expand_query(self, query: str, entities: List[str], date: str = From(\"pre_date\", \"2025-01-01\")):  # Expand and obtain multiple queries.\n        print(f\"query: {query}, entities: {entities}, date: {date}\")\n        task_input = f\"Query: {query}\\nEntities: {entities}\"\n        response: QueryList = await llm.astructured_output(\n            constraint=PydanticModel(model=QueryList),\n            messages=[\n                Message.from_text(text=f\"Centered around the given entities and given date {date}, expand the query to obtain multiple queries\", role=Role.SYSTEM),\n                Message.from_text(text=task_input, role=Role.USER,),\n            ]\n        )\n        return response.queries\n</pre> class QueryExpansion(GraphAutoma):     @worker(is_start=True)     async def pre_query(self, query_obj: Dict):  # Receive the user's input and preprocess query         query = query_obj[\"query\"].split(\":\")[1].strip()           return query      @worker(is_start=True)     async def pre_date(self, query_obj: Dict):  # Receive the user's input and preprocess date         date = query_obj[\"date\"]         return date      @worker(dependencies=[\"pre_query\"], args_mapping_rule=ArgsMappingRule.AS_IS)     async def extract_entity(self, query: str):  # Extract the entity information from the question, get entity information.         response: EntityList = await llm.astructured_output(               constraint=PydanticModel(model=EntityList),             messages=[                 Message.from_text(text=\"extract the entity information from the given query\", role=Role.SYSTEM),                 Message.from_text(text=query, role=Role.USER,),             ]         )         return query, response.entities      @worker(dependencies=[\"extract_entity\"], is_output=True, args_mapping_rule=ArgsMappingRule.UNPACK)     async def expand_query(self, query: str, entities: List[str], date: str = From(\"pre_date\", \"2025-01-01\")):  # Expand and obtain multiple queries.         print(f\"query: {query}, entities: {entities}, date: {date}\")         task_input = f\"Query: {query}\\nEntities: {entities}\"         response: QueryList = await llm.astructured_output(             constraint=PydanticModel(model=QueryList),             messages=[                 Message.from_text(text=f\"Centered around the given entities and given date {date}, expand the query to obtain multiple queries\", role=Role.SYSTEM),                 Message.from_text(text=task_input, role=Role.USER,),             ]         )         return response.queries <p>Let's run it!</p> In\u00a0[14]: Copied! <pre>query_expansion = QueryExpansion()\nawait query_expansion.arun(query_obj)\n</pre> query_expansion = QueryExpansion() await query_expansion.arun(query_obj) <pre>query: What new developments have there been in RAG in the past year?, entities: ['RAG', 'new developments', 'past year'], date: 2025-09-30\n</pre> Out[14]: <pre>['What are the latest advancements in Retrieval-Augmented Generation (RAG) technology as of 2025?',\n 'What new developments have emerged in RAG systems over the past 12 months?',\n 'How have RAG implementations evolved in the last year in terms of performance and scalability?',\n 'What innovations in RAG have been introduced in 2025 that improve accuracy and context handling?',\n 'What are the key breakthroughs in RAG research and deployment from 2024 to 2025?',\n 'What new tools and frameworks have been released for RAG in the past year?',\n 'How have privacy and security features improved in RAG systems over the last year?',\n 'What are the most notable RAG developments in enterprise AI applications from 2024 to 2025?',\n \"What recent improvements have been made to RAG's ability to handle long-context inputs?\",\n 'How has the integration of RAG with large language models evolved in the past year?']</pre> <p>I have modified <code>extract_entity</code>, and now it only accepts <code>query</code>, making its functionality more pure. Also, in <code>expand_query</code>, I have correctly obtained the <code>date</code>.</p> <pre>@worker(dependencies=[\"extract_entity\"], is_output=True args_mapping_rule=ArgsMappingRule.UNPACK)\nasync def expand_query(\n    self, \n    query: str, \n    entities: List[str], \n+    query_obj: Dict,  # The input of the entire Automa\n    date: str = From(\"pre_date\", \"2025-01-01\"), \n):  # Expand and obtain multiple queries.\n    ...\n</pre> <p>Let's modify the above example and add some print messages.</p> In\u00a0[\u00a0]: Copied! <pre>class QueryExpansion(GraphAutoma):\n    @worker(is_start=True)\n    async def pre_query(self, query_obj: Dict):  # Receive the user's input and preprocess query\n        query = query_obj[\"query\"].split(\":\")[1].strip()  \n        return query\n\n    @worker(is_start=True)\n    async def pre_date(self, query_obj: Dict):  # Receive the user's input and preprocess date\n        date = query_obj[\"date\"]\n        return date\n\n    @worker(dependencies=[\"pre_query\"], args_mapping_rule=ArgsMappingRule.AS_IS)\n    async def extract_entity(self, query: str):  # Extract the entity information from the question, get entity information.\n        response: EntityList = await llm.astructured_output(  \n            constraint=PydanticModel(model=EntityList),\n            messages=[\n                Message.from_text(text=\"extract the entity information from the given query\", role=Role.SYSTEM),\n                Message.from_text(text=query, role=Role.USER,),\n            ]\n        )\n        return query, response.entities\n\n    @worker(dependencies=[\"extract_entity\"], is_output=True, args_mapping_rule=ArgsMappingRule.UNPACK)\n    async def expand_query(self, query: str, entities: List[str], query_obj: Dict, date: str = From(\"pre_date\", \"2025-01-01\")):  # Expand and obtain multiple queries.\n        print(f\"query: {query}, entities: {entities}, date: {date}, query_obj: {query_obj}\")\n        task_input = f\"Query: {query}\\nEntities: {entities}\"\n        response: QueryList = await llm.astructured_output(\n            constraint=PydanticModel(model=QueryList),\n            messages=[\n                Message.from_text(text=f\"Centered around the given entities and given date {date}, expand the query to obtain multiple queries\", role=Role.SYSTEM),\n                Message.from_text(text=task_input, role=Role.USER,),\n            ]\n        )\n        return {\"id\": query_obj[\"id\"], \"queries\": response.queries}\n</pre> class QueryExpansion(GraphAutoma):     @worker(is_start=True)     async def pre_query(self, query_obj: Dict):  # Receive the user's input and preprocess query         query = query_obj[\"query\"].split(\":\")[1].strip()           return query      @worker(is_start=True)     async def pre_date(self, query_obj: Dict):  # Receive the user's input and preprocess date         date = query_obj[\"date\"]         return date      @worker(dependencies=[\"pre_query\"], args_mapping_rule=ArgsMappingRule.AS_IS)     async def extract_entity(self, query: str):  # Extract the entity information from the question, get entity information.         response: EntityList = await llm.astructured_output(               constraint=PydanticModel(model=EntityList),             messages=[                 Message.from_text(text=\"extract the entity information from the given query\", role=Role.SYSTEM),                 Message.from_text(text=query, role=Role.USER,),             ]         )         return query, response.entities      @worker(dependencies=[\"extract_entity\"], is_output=True, args_mapping_rule=ArgsMappingRule.UNPACK)     async def expand_query(self, query: str, entities: List[str], query_obj: Dict, date: str = From(\"pre_date\", \"2025-01-01\")):  # Expand and obtain multiple queries.         print(f\"query: {query}, entities: {entities}, date: {date}, query_obj: {query_obj}\")         task_input = f\"Query: {query}\\nEntities: {entities}\"         response: QueryList = await llm.astructured_output(             constraint=PydanticModel(model=QueryList),             messages=[                 Message.from_text(text=f\"Centered around the given entities and given date {date}, expand the query to obtain multiple queries\", role=Role.SYSTEM),                 Message.from_text(text=task_input, role=Role.USER,),             ]         )         return {\"id\": query_obj[\"id\"], \"queries\": response.queries} <p>Let's run it! When using the Inputs Propagation, the startup parameters must be passed in the form of keywords at startup.</p> In\u00a0[\u00a0]: Copied! <pre>query_expansion = QueryExpansion()\nawait query_expansion.arun(query_obj=query_obj)  # using keyword parameter passing\n</pre> query_expansion = QueryExpansion() await query_expansion.arun(query_obj=query_obj)  # using keyword parameter passing <pre>query: What new developments have there been in RAG in the past year?, entities: ['RAG', 'new developments', 'past year'], date: 2025-09-30, query_obj: {'id': 'user_1', 'query': 'Q: What new developments have there been in RAG in the past year?', 'date': '2025-09-30'}\n</pre> Out[\u00a0]: <pre>{'id': 'user_1',\n 'queries': ['What are the latest advancements in Retrieval-Augmented Generation (RAG) technologies as of 2025?',\n  'What new developments have emerged in RAG systems over the past 12 months?',\n  'How have RAG implementations evolved in the last year in terms of performance and scalability?',\n  'What are the key innovations in RAG models reported between 2024 and 2025?',\n  'What new techniques have been introduced in RAG to improve accuracy and context retention in the past year?',\n  'What recent breakthroughs in RAG have been highlighted in 2025?',\n  'How have industry leaders advanced RAG technology in the last year?',\n  'What are the most significant updates in RAG frameworks and tools from 2024 to 2025?',\n  'What new challenges and solutions have been proposed in RAG research over the past year?',\n  'What developments in RAG have improved real-time retrieval and generation performance in 2025?']}</pre> <p>Among all the ways of parameter passing mentioned above, the priority order is: arguments mapping positional parameters &gt; arguments injection &gt; propagation &gt; arguments mapping keyword parameters.</p>"},{"location":"tutorials/items/core_mechanism/parameter_resolving/#parameter-resolving","title":"Parameter Resolving\u00b6","text":"<p>In Bridgic, an execution unit (worker), can not only control how its results are passed to the workers that directly depend on it through the Result Dispatching mechanism, but also control how it receives the results from its preceding workers through the Parameter Binding mechanism. In addition, these mechanisms also determine how an automa\u2019s input arguments are distributed to its internal workers. These two mechanisms are collectively referred to in Bridgic as Parameter Resolving.</p>"},{"location":"tutorials/items/core_mechanism/parameter_resolving/#result-dispatching","title":"Result Dispatching\u00b6","text":"<p>Result Dispatching is a mechanism used for distributing data from a worker to multiple workers that directly depend on it, or from an automa\u2019s inputs to its start workers. Let's understand this feature through a practical example.</p> <p>Suppose we need to process multiple user queries in parallel. Each query needs to go through preprocessing, analysis, and then be aggregated. We can use the Result Dispatching mechanism to efficiently handle this scenario.</p>"},{"location":"tutorials/items/core_mechanism/parameter_resolving/#1-initialize","title":"1. Initialize\u00b6","text":"<p>Let's start by importing the necessary packages.</p>"},{"location":"tutorials/items/core_mechanism/parameter_resolving/#2-process-every-inputs","title":"2. Process Every Inputs\u00b6","text":"<p>First, let's understand how to use <code>InOrder</code> when calling <code>arun()</code>. When you have multiple start workers and want to distribute different input values to each of them, you can wrap the input data in <code>InOrder()</code>.</p>"},{"location":"tutorials/items/core_mechanism/parameter_resolving/#3-process-every-item-in-result-dispatching","title":"3. Process Every Item in Result Dispatching\u00b6","text":"<p>Now let's understand how to use <code>result_dispatching_rule=ResultDispatchingRule.IN_ORDER</code> to distribute a worker's output to multiple downstream workers.</p> <p>When a worker sets <code>result_dispatching_rule=ResultDispatchingRule.IN_ORDER</code>, its return value must be an iterable (tuple or list). Each element in the return value will be distributed to the downstream workers that directly depend on this worker, in the order they are declared or added.</p>"},{"location":"tutorials/items/core_mechanism/parameter_resolving/#parameter-binding","title":"Parameter Binding\u00b6","text":"<p>Parameter Binding is a mechanism that specifies how a worker receives arguments from its direct predecessors, its non-direct predecessors, or from the input arguments of the automa it belongs to. There are three ways to accomplish the Parameter Binding process, including Arguments Mapping, Arguments Injection, and Inputs Propagation. Now let's understand them through a sample example.</p>"},{"location":"tutorials/items/core_mechanism/parameter_resolving/#query-expansion","title":"Query expansion\u00b6","text":"<p>Query expansion is a common step in RAG and can enhance the quality of RAG. To enhance the quality of query expansion, developers often first extract the entity information from the query and use it to assist the model in expanding the original query.</p> <p>Now let's implement this. The user inputs the original query, and then we expand the query to obtain more queries. There are three steps to complete the query expansion:</p> <ol> <li>Receive the user's input and perform preprocessing to get the original query.</li> <li>Extract the entity information from the query to get the entity information.</li> <li>Expand and obtain multiple queries.</li> </ol>"},{"location":"tutorials/items/core_mechanism/parameter_resolving/#1-initialize","title":"1. Initialize\u00b6","text":"<p>Before we start, let's prepare the running environment. In this tutorial, we will use the OpenAI model integration (not OpenAI-like) that supports the <code>StructuredOutput</code> feature. Run the following <code>pip</code> command to make sure this integration is available.</p>"},{"location":"tutorials/items/core_mechanism/parameter_resolving/#2-complete-query-expansion","title":"2. Complete Query Expansion\u00b6","text":""},{"location":"tutorials/items/core_mechanism/parameter_resolving/#what-have-we-learnt","title":"What have we learnt?\u00b6","text":""},{"location":"tutorials/items/core_mechanism/parameter_resolving/#result-dispatching","title":"Result Dispatching\u00b6","text":"<p>Result Dispatching provides powerful mechanisms for parallel / concurrent data processing:</p> <ul> <li><code>Distribute</code> in <code>arun()</code>: Distributes input values to multiple start workers element-wise</li> <li><code>ResultDispatchRule</code>: Allows a worker to distribute its output (must be iterable) to multiple downstream workers<ul> <li>AS_IS: Default mode to send the result of current worker as a whole</li> <li>Distribute: Set to send the result of current worker according to the order in which the downstream workers are declared or added</li> </ul> </li> </ul> <p>These features enable efficient parallel / concurrent processing and data flow management in complex workflows.</p>"},{"location":"tutorials/items/core_mechanism/parameter_resolving/#arguments-mapping","title":"Arguments Mapping\u00b6","text":"<p>Reviewing the code, we find that each <code>@worker</code> decorator has an <code>args_mapping_rule</code> parameter.</p> <p>The <code>args_mapping_rule</code> defines the way data is passed between directly dependent workers, that is, how the result of the previous worker is mapped to the parameter of the next worker. Its value can only be specified through the properties of <code>ArgsMappingRule</code>.</p> <p>Note: When <code>ArgsMapping</code> is used for a worker with <code>is_start = True</code>, it will only take effect on <code>*args</code> in <code>automa.arun(*args, **kwargs)</code>. This is because all <code>**kwargs</code> will be passed as parameters through a Inputs Propagation mechanism.</p>"},{"location":"tutorials/items/core_mechanism/parameter_resolving/#as_is-mode-default","title":"AS_IS mode (default)\u00b6","text":"<p>In the AS_IS mode, a worker will receive the output of all its directly dependent workers as input parameters in the order declared by the dependencies.</p> <p>In the above example, <code>extract_entity</code> declares dependencies: <code>dependencies=[\"pre_query\", \"pre_date\"]</code>, so the results of the two preceding workers will be mapped to the first and second parameters of <code>extract_entity</code> in the order specified by the dependencies declaration, the result of <code>pre_query</code> is mapped to <code>query</code> parameter and the result of <code>pre_date</code> is mapped to <code>date</code> parameter.</p> <p>Note: The declaration order in dependencies only affects the order of parameter mapping, but does not influence the execution order of the dependent workers.</p> <p>Additionally, if the previous worker returns a result with multiple values, such as <code>return x, y</code>, then all the results will be passed as a tuple result. So in the above example, the parameter <code>query_meta</code> of <code>expand_query</code> received all the result values from <code>extract_entity</code>.</p>"},{"location":"tutorials/items/core_mechanism/parameter_resolving/#unpack-mode","title":"UNPACK mode\u00b6","text":"<p>Let's go back to the previous example. In the <code>expand_query</code>, we receive the parameters from the previous worker in the <code>AS_IS</code> mode and manually unpack them as a whole, like this:</p>"},{"location":"tutorials/items/core_mechanism/parameter_resolving/#merge-mode","title":"MERGE mode\u00b6","text":"<p>At the same time, conversely, since there is an UNPACK mechanism, is there also a mechanism that can aggregate multiple results for receiving? This is particularly useful when a worker collects the results of multiple dependent workers. At this point, the <code>MERGE</code> mode comes in handy.</p> <p>Still referring to the example above, <code>extract_entity</code> actually received the results from two workers. Now let's try to make <code>extract_entity</code> receive all these results in a single parameter for use, instead of receiving two parameters.</p> <p>Let's modify the <code>extract_entity</code> in the above example and add some print messages.</p>"},{"location":"tutorials/items/core_mechanism/parameter_resolving/#arguments-injection","title":"Arguments Injection\u00b6","text":"<p>Looking back at the example above, we actually find that the <code>date</code> information is passed through <code>pre_date</code>, <code>extract_entity</code>, and finally reaches <code>expand_query</code>. However, in reality, <code>extract_entity</code> doesn't use this information at all. Thus, passing <code>date</code> here seems redundant. And The use of <code>date</code> in <code>expand_query</code> essentially only means that the data depends on it, but whether it is executed or not, this control dependency does not directly rely on it.</p> <p>Bridgic emphasizes the separation of data dependency and control dependency. This is beneficial for the future construction of complex graphs, as it allows for decoupling and avoids the need to adjust the entire graph due to changes in data dependency.</p> <p>In Bridgic, we can use Arguments Injection to make it. We can indicate which worker's result to take by using the <code>From</code> marker when declaring parameters, and at the same time set the default value if no result is obtained. For example:</p>"},{"location":"tutorials/items/core_mechanism/parameter_resolving/#inputs-propagation","title":"Inputs Propagation\u00b6","text":"<p>Looking back at the example above again, our program did not process the <code>id</code> field in the input at all. Eventually, we only returned a list of generalized problems, which might cause the external call to be unable to associate which \"id\" corresponds to the result. However, this ID neither requires preprocessing nor is it needed for entity extraction.</p> <p>We can use Inputs Propagation to resolve it. This can be achieved by adding the name of the startup parameter to the worker when declaring the parameters.</p>"},{"location":"tutorials/items/core_mechanism/worker_callback/","title":"Worker Callback","text":"In\u00a0[1]: Copied! <pre>from typing import Any, Dict, Optional\nfrom bridgic.core.automa import Automa\nfrom bridgic.core.automa.worker import WorkerCallback\n\nclass LoggingCallback(WorkerCallback):\n    \"\"\"Log worker lifecycle events.\"\"\"\n\n    def __init__(self, tag: str = None):\n        self._tag = tag or \"\"\n\n    async def on_worker_start(\n        self,\n        key: str,\n        is_top_level: bool = False,\n        parent: Optional[Automa] = None,\n        arguments: Dict[str, Any] = None,\n    ) -&gt; None:\n        print(self._tag + f\"[START] {key} args={arguments}\")\n\n    async def on_worker_end(\n        self,\n        key: str,\n        is_top_level: bool = False,\n        parent: Optional[Automa] = None,\n        arguments: Dict[str, Any] = None,\n        result: Any = None,\n    ) -&gt; None:\n        print(self._tag + f\"[END] {key} result={result}\")\n\n    async def on_worker_error(\n        self,\n        key: str,\n        is_top_level: bool = False,\n        parent: Optional[Automa] = None,\n        arguments: Dict[str, Any] = None,\n        error: Exception = None,\n    ) -&gt; bool:\n        print(self._tag + f\"[ERROR] {key} -&gt; {error}\")\n        return False  # Returning False means don't suppress the exception.\n</pre> from typing import Any, Dict, Optional from bridgic.core.automa import Automa from bridgic.core.automa.worker import WorkerCallback  class LoggingCallback(WorkerCallback):     \"\"\"Log worker lifecycle events.\"\"\"      def __init__(self, tag: str = None):         self._tag = tag or \"\"      async def on_worker_start(         self,         key: str,         is_top_level: bool = False,         parent: Optional[Automa] = None,         arguments: Dict[str, Any] = None,     ) -&gt; None:         print(self._tag + f\"[START] {key} args={arguments}\")      async def on_worker_end(         self,         key: str,         is_top_level: bool = False,         parent: Optional[Automa] = None,         arguments: Dict[str, Any] = None,         result: Any = None,     ) -&gt; None:         print(self._tag + f\"[END] {key} result={result}\")      async def on_worker_error(         self,         key: str,         is_top_level: bool = False,         parent: Optional[Automa] = None,         arguments: Dict[str, Any] = None,         error: Exception = None,     ) -&gt; bool:         print(self._tag + f\"[ERROR] {key} -&gt; {error}\")         return False  # Returning False means don't suppress the exception. In\u00a0[2]: Copied! <pre>from bridgic.core.automa.worker import WorkerCallbackBuilder\n\n# Build in shared instance mode.\nshared_builder = WorkerCallbackBuilder(LoggingCallback)\nisolated_builder = WorkerCallbackBuilder(LoggingCallback, is_shared=False)\n\n# Build in independent instance mode.\nshared_builder_with_args = WorkerCallbackBuilder(LoggingCallback, init_kwargs={\"tag\": \"-\"})\nisolated_builder_with_args = WorkerCallbackBuilder(LoggingCallback, init_kwargs={\"tag\": \"-\"}, is_shared=False)\n</pre> from bridgic.core.automa.worker import WorkerCallbackBuilder  # Build in shared instance mode. shared_builder = WorkerCallbackBuilder(LoggingCallback) isolated_builder = WorkerCallbackBuilder(LoggingCallback, is_shared=False)  # Build in independent instance mode. shared_builder_with_args = WorkerCallbackBuilder(LoggingCallback, init_kwargs={\"tag\": \"-\"}) isolated_builder_with_args = WorkerCallbackBuilder(LoggingCallback, init_kwargs={\"tag\": \"-\"}, is_shared=False) <p>Global-Level Configuration</p> <p>The following example shows how to configure callbacks at the global level using <code>GlobalSetting</code>, which applies the callback to all workers across all Automa instances in your application.</p> In\u00a0[3]: Copied! <pre>from bridgic.core.automa import GraphAutoma, worker\nfrom bridgic.core.automa.worker import WorkerCallbackBuilder\nfrom bridgic.core.config import GlobalSetting\n\nGlobalSetting.set(callback_builders=[WorkerCallbackBuilder(LoggingCallback)])\n\nclass MyAutoma(GraphAutoma):\n    @worker(is_start=True)\n    async def step1(self, x: int) -&gt; int:\n        return x + 1\n\nautoma = MyAutoma(name=\"test-automa\")  # Will log for all workers inside.\nawait automa.arun(x=10)\n</pre> from bridgic.core.automa import GraphAutoma, worker from bridgic.core.automa.worker import WorkerCallbackBuilder from bridgic.core.config import GlobalSetting  GlobalSetting.set(callback_builders=[WorkerCallbackBuilder(LoggingCallback)])  class MyAutoma(GraphAutoma):     @worker(is_start=True)     async def step1(self, x: int) -&gt; int:         return x + 1  automa = MyAutoma(name=\"test-automa\")  # Will log for all workers inside. await automa.arun(x=10) <pre>[START] test-automa args={'args': (), 'kwargs': {'x': 10}, 'feedback_data': None}\n[START] step1 args={'args': (), 'kwargs': {'x': 10}}\n[END] step1 result=11\n[END] test-automa result=None\n</pre> <p>Automa-Level Configuration</p> <p>The following example shows how to configure callbacks at the automa level using <code>RunningOptions</code>, which applies the callback to all workers within a specific Automa instance.</p> In\u00a0[4]: Copied! <pre>from bridgic.core.automa import GraphAutoma, RunningOptions, worker\nfrom bridgic.core.automa.worker import WorkerCallbackBuilder\n\n# Because this example is under jupyter-notebook environment, GlobalSetting needs to be reset.\n# In real deveopment, this line is not necessary to have a default global setting.\nGlobalSetting.set(callback_builders=[])\n\nclass MyAutoma(GraphAutoma):\n    @worker(is_start=True)\n    async def step1(self, x: int) -&gt; int:\n        return x + 1\n\nrunning_options = RunningOptions(callback_builders=[WorkerCallbackBuilder(LoggingCallback)])\nautoma = MyAutoma(name=\"test-automa\", running_options=running_options)  # Will log for all workers inside.\nawait automa.arun(x=10)\n</pre> from bridgic.core.automa import GraphAutoma, RunningOptions, worker from bridgic.core.automa.worker import WorkerCallbackBuilder  # Because this example is under jupyter-notebook environment, GlobalSetting needs to be reset. # In real deveopment, this line is not necessary to have a default global setting. GlobalSetting.set(callback_builders=[])  class MyAutoma(GraphAutoma):     @worker(is_start=True)     async def step1(self, x: int) -&gt; int:         return x + 1  running_options = RunningOptions(callback_builders=[WorkerCallbackBuilder(LoggingCallback)]) automa = MyAutoma(name=\"test-automa\", running_options=running_options)  # Will log for all workers inside. await automa.arun(x=10) <pre>[START] test-automa args={'args': (), 'kwargs': {'x': 10}, 'feedback_data': None}\n[START] step1 args={'args': (), 'kwargs': {'x': 10}}\n[END] step1 result=11\n[END] test-automa result=None\n</pre> <p>Worker-Level Configuration</p> <p>The following example shows how to configure callbacks at the worker level by passing <code>callback_builders</code> to the <code>@worker</code> decorator, which applies the callback only to the specific worker.</p> In\u00a0[5]: Copied! <pre>from bridgic.core.automa import GraphAutoma, worker\nfrom bridgic.core.automa.worker import WorkerCallbackBuilder\n\n# Because this example is under jupyter-notebook environment, GlobalSetting needs to be reset.\n# In real deveopment, this line is not necessary to have a default global setting.\nGlobalSetting.set(callback_builders=[])\n\nclass MyAutoma(GraphAutoma):\n    @worker(\n        is_start=True,\n        callback_builders=[WorkerCallbackBuilder(LoggingCallback)],\n    )\n    async def step1(self, x: int) -&gt; int:\n        return x + 1\n\nautoma = MyAutoma(name=\"test-automa\")  # Will only log for \"step1\" worker.\nawait automa.arun(x=10)\n</pre> from bridgic.core.automa import GraphAutoma, worker from bridgic.core.automa.worker import WorkerCallbackBuilder  # Because this example is under jupyter-notebook environment, GlobalSetting needs to be reset. # In real deveopment, this line is not necessary to have a default global setting. GlobalSetting.set(callback_builders=[])  class MyAutoma(GraphAutoma):     @worker(         is_start=True,         callback_builders=[WorkerCallbackBuilder(LoggingCallback)],     )     async def step1(self, x: int) -&gt; int:         return x + 1  automa = MyAutoma(name=\"test-automa\")  # Will only log for \"step1\" worker. await automa.arun(x=10)  <pre>[START] step1 args={'args': (), 'kwargs': {'x': 10}}\n[END] step1 result=11\n</pre> In\u00a0[6]: Copied! <pre>from bridgic.core.automa import GraphAutoma, RunningOptions, worker\nfrom bridgic.core.automa.worker import WorkerCallback, WorkerCallbackBuilder\nfrom bridgic.core.config import GlobalSetting\n\n# Top-level automa\nclass TopAutoma(GraphAutoma):\n    @worker(is_start=True)\n    async def top_worker(self, x: int) -&gt; int:\n        return x + 1\n\n# Inner automa (will be used as a nested worker)\nclass InnerAutoma(GraphAutoma):\n    @worker(is_start=True)\n    async def inner_worker(self, x: int) -&gt; int:\n        return x * 2\n\n# Configure callback at global setting, with &lt;Global&gt; tag.\nGlobalSetting.set(\n    callback_builders=[\n        WorkerCallbackBuilder(LoggingCallback, init_kwargs={\"tag\": \"&lt;Global&gt;\"}),\n    ]\n)\n\n# Configure callback at top-level automa, with &lt;Automa&gt; tag.\nrunning_options = RunningOptions(\n    callback_builders=[\n        WorkerCallbackBuilder(LoggingCallback, init_kwargs={\"tag\": \"&lt;Automa&gt;\"})\n    ]\n)\nautoma = TopAutoma(name=\"top-automa\", running_options=running_options)\n\n# Add a instance of InnerAutoma as a worker.\nautoma.add_worker(\"nested_automa_as_worker\", InnerAutoma(name=\"inner-automa\"), dependencies=[\"top_worker\"])\n\n# When executed:\n# - Callbacks that from GlobalSetting will be propagated to all workers application-wide.\n# - Callbacks that from RunningOptions will be propagated to all workers inside the \"top-level\" automa.\nawait automa.arun(x=10)\n</pre> from bridgic.core.automa import GraphAutoma, RunningOptions, worker from bridgic.core.automa.worker import WorkerCallback, WorkerCallbackBuilder from bridgic.core.config import GlobalSetting  # Top-level automa class TopAutoma(GraphAutoma):     @worker(is_start=True)     async def top_worker(self, x: int) -&gt; int:         return x + 1  # Inner automa (will be used as a nested worker) class InnerAutoma(GraphAutoma):     @worker(is_start=True)     async def inner_worker(self, x: int) -&gt; int:         return x * 2  # Configure callback at global setting, with  tag. GlobalSetting.set(     callback_builders=[         WorkerCallbackBuilder(LoggingCallback, init_kwargs={\"tag\": \"\"}),     ] )  # Configure callback at top-level automa, with  tag. running_options = RunningOptions(     callback_builders=[         WorkerCallbackBuilder(LoggingCallback, init_kwargs={\"tag\": \"\"})     ] ) automa = TopAutoma(name=\"top-automa\", running_options=running_options)  # Add a instance of InnerAutoma as a worker. automa.add_worker(\"nested_automa_as_worker\", InnerAutoma(name=\"inner-automa\"), dependencies=[\"top_worker\"])  # When executed: # - Callbacks that from GlobalSetting will be propagated to all workers application-wide. # - Callbacks that from RunningOptions will be propagated to all workers inside the \"top-level\" automa. await automa.arun(x=10)  <pre>&lt;Global&gt;[START] top-automa args={'args': (), 'kwargs': {'x': 10}, 'feedback_data': None}\n&lt;Automa&gt;[START] top-automa args={'args': (), 'kwargs': {'x': 10}, 'feedback_data': None}\n&lt;Global&gt;[START] top_worker args={'args': (), 'kwargs': {'x': 10}}\n&lt;Automa&gt;[START] top_worker args={'args': (), 'kwargs': {'x': 10}}\n&lt;Global&gt;[END] top_worker result=11\n&lt;Automa&gt;[END] top_worker result=11\n&lt;Global&gt;[START] nested_automa_as_worker args={'args': (11,), 'kwargs': {'feedback_data': None, 'x': 10}}\n&lt;Automa&gt;[START] nested_automa_as_worker args={'args': (11,), 'kwargs': {'feedback_data': None, 'x': 10}}\n&lt;Global&gt;[START] inner_worker args={'args': (11,), 'kwargs': {}}\n&lt;Automa&gt;[START] inner_worker args={'args': (11,), 'kwargs': {}}\n&lt;Global&gt;[END] inner_worker result=22\n&lt;Automa&gt;[END] inner_worker result=22\n&lt;Global&gt;[END] nested_automa_as_worker result=None\n&lt;Automa&gt;[END] nested_automa_as_worker result=None\n&lt;Global&gt;[END] top-automa result=None\n&lt;Automa&gt;[END] top-automa result=None\n</pre> In\u00a0[7]: Copied! <pre>import warnings\n\nfrom typing import Union\nfrom bridgic.core.automa import Automa\nfrom bridgic.core.automa.worker import WorkerCallback\n\nclass ValueErrorHandler(WorkerCallback):\n    async def on_worker_error(\n        self,\n        key: str,\n        is_top_level: bool = False,\n        parent: Optional[Automa] = None,\n        arguments: Dict[str, Any] = None,\n        error: ValueError = None\n    ) -&gt; bool:\n        warnings.warn(\"ValueError in %s: %s\", key, error)\n        return True  # Swallow ValueError.\n\nclass MultipleErrorHandler(WorkerCallback):\n    async def on_worker_error(\n        self,\n        key: str,\n        is_top_level: bool = False,\n        parent: Optional[Automa] = None,\n        arguments: Dict[str, Any] = None,\n        error: Union[KeyError, TypeError] = None\n    ) -&gt; bool:\n        warnings.warn(\"Recoverable issue: %s\", error)\n        return False  # Re-raise it.\n</pre> import warnings  from typing import Union from bridgic.core.automa import Automa from bridgic.core.automa.worker import WorkerCallback  class ValueErrorHandler(WorkerCallback):     async def on_worker_error(         self,         key: str,         is_top_level: bool = False,         parent: Optional[Automa] = None,         arguments: Dict[str, Any] = None,         error: ValueError = None     ) -&gt; bool:         warnings.warn(\"ValueError in %s: %s\", key, error)         return True  # Swallow ValueError.  class MultipleErrorHandler(WorkerCallback):     async def on_worker_error(         self,         key: str,         is_top_level: bool = False,         parent: Optional[Automa] = None,         arguments: Dict[str, Any] = None,         error: Union[KeyError, TypeError] = None     ) -&gt; bool:         warnings.warn(\"Recoverable issue: %s\", error)         return False  # Re-raise it. <p>The framework calls every matching callback, so you can compose specialized handlers with broader \"catch-all\" callbacks.</p>"},{"location":"tutorials/items/core_mechanism/worker_callback/#worker-callback","title":"Worker Callback\u00b6","text":""},{"location":"tutorials/items/core_mechanism/worker_callback/#introduction","title":"Introduction\u00b6","text":"<p>The Worker Callback mechanism will invoke pre-defined hook methods at key points during worker execution, allowing you to add cross-cutting concerns such as logging, validation, monitoring, and error handling without modifying your business logic. This tutorial will guide you through understanding and using the Worker Callback mechanism.</p> <p><code>WorkerCallback</code> is the base class of all callback instances that are invokded around a worker. You can implement the following three methods to subclass <code>WorkerCallback</code>:</p> Method Description <code>on_worker_start()</code> Called before worker execution. Use for input validation, logging, or monitoring <code>on_worker_end()</code> Called after successful execution. Use for result logging, event publishing, or metrics <code>on_worker_error()</code> Called when an exception is raised. Use for error handling, logging, or suppression"},{"location":"tutorials/items/core_mechanism/worker_callback/#creating-a-custom-callback","title":"Creating a Custom Callback\u00b6","text":""},{"location":"tutorials/items/core_mechanism/worker_callback/#step-1-define-a-class","title":"Step 1: Define a Class\u00b6","text":"<p>To create a custom callback, simply subclass <code>WorkerCallback</code> and implement the methods above. You don't need to implement all three methods, but only implement the ones that you need. The base <code>WorkerCallback</code> class provides default implementations that do nothing. Here we define a <code>LoggingCallback</code> class that implements all of the three hook methods.</p>"},{"location":"tutorials/items/core_mechanism/worker_callback/#step-2-choose-building-mode","title":"Step 2: Choose Building Mode\u00b6","text":"<p>Callbacks are instantiated through <code>WorkerCallbackBuilder</code>, which delays construction until the worker is created and lets you control its sharing mode.</p> <ul> <li>Shared instance mode (<code>is_shared=True</code>, default): all workers within the declaration scope reuse the same callback instance. This ideal for stateful integrations (e.g., a tracing client).</li> <li>Independent instance mode (<code>is_shared=False</code>): every worker receives its own callback instance. This is useful when you need isolated state or thread safety.</li> </ul>"},{"location":"tutorials/items/core_mechanism/worker_callback/#step-3-decide-the-scope","title":"Step 3: Decide the Scope\u00b6","text":"<p>Choose where the builder should take effect. Bridgic merges builders from the widest scope to the narrowest one:</p> Level Registration Method Scope of Effect Global-level <code>GlobalSetting</code> Applies to all workers across all Automa instances Automa-level <code>RunningOptions</code> Applies to every worker inside one Automa instance Worker-level <code>@worker</code> or <code>add_worker()</code> Applies only to the targeted worker"},{"location":"tutorials/items/core_mechanism/worker_callback/#features-you-need-to-know","title":"Features you Need to Know\u00b6","text":""},{"location":"tutorials/items/core_mechanism/worker_callback/#callback-propagation","title":"Callback Propagation\u00b6","text":"<p>Nested automa will inherit callbacks from their parent (even higher ancestor) scope by reading the initialized running options. This ensures instrumentation remains consistent across multi levels. When you set a callback in the <code>RunningOptions</code> during automa initialization, all workers in any nested automata will automatically inherit that callback:</p> <p>In the following example, the <code>LoggingCallback</code> configured at the top-level automa propagates to:</p> <ul> <li>All workers directly in <code>TopAutoma</code> (<code>top_worker</code>, <code>nested_automa_worker</code>)</li> <li>All workers inside the nested <code>InnerAutoma</code> (<code>inner_worker</code>)</li> </ul>"},{"location":"tutorials/items/core_mechanism/worker_callback/#dynamical-topology-support","title":"Dynamical Topology Support\u00b6","text":"<p><code>GraphAutoma.add_worker()</code> and related APIs allow you to modify the topology at runtime. When a new worker is added, Bridgic automatically builds its callback list using the current global builders, the builders from its ancestors' running options, and the builders passed with the <code>add_worker()</code> call. As a result:</p> <ul> <li>Dynamically added workers receive the same instrumentation guarantees as statically declared ones.</li> <li>Nested automa inserted later as a new worker inherits callbacks from its ancestors scopes.</li> </ul> <p>This design keeps long-running agentic systems observable even as they grow or reconfigure themselves during execution.</p>"},{"location":"tutorials/items/core_mechanism/worker_callback/#exception-handling","title":"Exception Handling\u00b6","text":""},{"location":"tutorials/items/core_mechanism/worker_callback/#exception-type-matching","title":"Exception Type Matching\u00b6","text":"<p>The <code>on_worker_error()</code> method allows for fine-grained and flexible error handling by inspecting the type annotation of its <code>error</code> parameter. You can indicate exactly which exception types you want your handler to respond to, by annotating the <code>error</code> parameter with a specific exception type. At runtime, the Bridgic framework will automatically match and invoke your callback only for those exceptions that match the annotation.</p> <p>Below is a simple example comparison table:</p> Type annotation of <code>error</code> The matched exception type(s) to trigger <code>on_worker_error</code> <code>Exception</code> All exceptions <code>ValueError</code> <code>ValueError</code> and its subclasses (e.g., <code>UnicodeDecodeError</code>) <code>Union[Type1, Type2, ...]</code> <code>Type1</code> and <code>Type2</code> will be considered to be matched"},{"location":"tutorials/items/core_mechanism/worker_callback/#exception-suppression","title":"Exception Suppression\u00b6","text":"<p>The return value of <code>on_worker_error</code> will determine whether to suppress the captured exception:</p> Value Behavior <code>True</code> Means to suppress the exception; the worker result becomes <code>None</code>. <code>False</code> Means to observe only; the framework re-raises after all callbacks finish. <p>Specially, to ensure human-in-the-loop flows stay intact, <code>InteractionException</code> should never be suppressed.</p>"},{"location":"tutorials/items/core_mechanism/worker_callback/#best-practices","title":"Best Practices\u00b6","text":"<ol> <li><p>Keep callbacks lightweight: Callback methods are called for each worker that they are responsible for, so keep them fast and avoid blocking operations.</p> </li> <li><p>Use appropriate scope: Use global-level for application-wide concerns, automa-level for specific instance, and worker-level for fine-grained control.</p> </li> <li><p>Handle exceptions carefully: Be thoughtful about which exceptions to suppress. Suppressing exceptions can hide bugs and make debugging difficult.</p> </li> <li><p>Use shared instances wisely: Shared instances are great for maintaining connections or state, but be aware of thread-safety concerns.</p> </li> <li><p>Leverage the parent parameter: The <code>parent</code> parameter gives you access to the automa's context, allowing you to post events, request feedback, or interact with the automa's state.</p> </li> </ol>"},{"location":"tutorials/items/core_mechanism/worker_callback/#next-steps","title":"Next Steps\u00b6","text":"<ul> <li>Learn about Observability to see how callbacks enable system transparency</li> <li>Explore Callback Integrations for ready-to-use callback implementations</li> </ul>"},{"location":"tutorials/items/model_integration/","title":"Model","text":""},{"location":"tutorials/items/model_integration/#model-integration","title":"Model Integration","text":""},{"location":"tutorials/items/model_integration/#our-philosophy","title":"Our Philosophy","text":"<ul> <li>Model Neutrality</li> <li>Protocol Driven Design</li> </ul> <p>Bridgic is designed as a model-neutral framework that treats all LLM providers - whether they are commercial vendors (OpenAI, etc.) or inference engines (vLLM, etc.) - as equals. This architectural decision will reduce the extra effort developers need to put in when switching models.</p> <p></p>"},{"location":"tutorials/items/model_integration/#model-neutrality","title":"Model Neutrality","text":"<p>Bridgic\u2019s API is fundamentally model-neutral. When designing our interfaces, we deliberately avoid assumptions about any particular model provider or technology. Instead, we focus on delivering:</p> <ul> <li>Functional Simplicity: The APIs are intuitive and easy to use, allowing you to interact with any supported model provider through clear and minimal calls, with no provider-specific complexity.</li> <li>Consistent Experience: Regardless of which underlying LLM or model engine you use, Bridgic ensures a uniform invocation model, standardized error handling, and predictable behavior.</li> </ul> <p>This model-agnostic approach allows you to swap, extend, or combine providers with minimal friction, promoting genuine flexibility and future-proof integration in your agent pipelines.</p>"},{"location":"tutorials/items/model_integration/#protocol-driven-design","title":"Protocol Driven Design","text":"<p>At the heart of Bridgic's model integration lies the Protocol Pattern, which defines clear behavioral contracts without imposing implementation details. This design enables:</p> <ol> <li>Extensibility: New model providers can be integrated by implementing well-defined protocols (<code>StructuredOutput</code>, <code>ToolSelection</code>, etc.)</li> <li>Capability Declaration: Each provider explicitly declares its capabilities through protocol implementation, making it clear what features are available</li> <li>Flexibility: Providers can implement only the protocols they support, avoiding forced compatibility with unsupported features</li> <li>Type Safety: Protocols provide compile-time type checking and IDE support, improving developer experience</li> </ol> <p> </p> <p>This architecture means that when you work with a model provider in Bridgic, you explicitly know what capabilities it offers, and the framework enforces these contracts at both development and runtime.</p>"},{"location":"tutorials/items/model_integration/llm_integration/","title":"LLM","text":"In\u00a0[2]: Copied! <pre>import os\nfrom dotenv import load_dotenv\nfrom bridgic.llms.openai import OpenAILlm, OpenAIConfiguration\n\nload_dotenv()\n\n_api_key = os.environ.get(\"OPENAI_API_KEY\")\n\nllm = OpenAILlm(\n    api_key=_api_key,\n)\n\nconfig = OpenAIConfiguration(\n    model=\"gpt-4o\",\n    temperature=0.7,\n    max_tokens=2000,\n)\n\nllm = OpenAILlm(\n    api_key=_api_key,\n    configuration=config,\n    timeout=30.0,\n)\n</pre> import os from dotenv import load_dotenv from bridgic.llms.openai import OpenAILlm, OpenAIConfiguration  load_dotenv()  _api_key = os.environ.get(\"OPENAI_API_KEY\")  llm = OpenAILlm(     api_key=_api_key, )  config = OpenAIConfiguration(     model=\"gpt-4o\",     temperature=0.7,     max_tokens=2000, )  llm = OpenAILlm(     api_key=_api_key,     configuration=config,     timeout=30.0, ) In\u00a0[6]: Copied! <pre>from bridgic.core.model.types import Message, Role\n\n# Create messages\nmessages = [\n    Message.from_text(\"You are a helpful assistant.\", role=Role.SYSTEM),\n    Message.from_text(\"What is the capital of France?\", role=Role.USER),\n]\n\n# Get response\nresponse = llm.chat(\n    messages=messages,\n    model=\"gpt-4o\",\n    temperature=0.7,\n)\n\nprint(response.message.content)\n</pre> from bridgic.core.model.types import Message, Role  # Create messages messages = [     Message.from_text(\"You are a helpful assistant.\", role=Role.SYSTEM),     Message.from_text(\"What is the capital of France?\", role=Role.USER), ]  # Get response response = llm.chat(     messages=messages,     model=\"gpt-4o\",     temperature=0.7, )  print(response.message.content)  <pre>The capital of France is Paris.\n</pre> In\u00a0[8]: Copied! <pre># Stream response chunks\nfor chunk in llm.stream(messages=messages, model=\"gpt-4o\"):\n    print(chunk.delta, end=\"|\", flush=True)  # Print each chunk as it arrives\n</pre> # Stream response chunks for chunk in llm.stream(messages=messages, model=\"gpt-4o\"):     print(chunk.delta, end=\"|\", flush=True)  # Print each chunk as it arrives <pre>The| capital| of| France| is| Paris|.|</pre> In\u00a0[14]: Copied! <pre>from pydantic import BaseModel, Field\nfrom bridgic.core.model.protocols import PydanticModel, JsonSchema\n\n# Option 1: Using Pydantic Models\nclass MathProblemSolution(BaseModel):\n    \"\"\"Solution to a math problem with reasoning\"\"\"\n    reasoning: str = Field(description=\"Step-by-step reasoning\")\n    answer: int = Field(description=\"Final numerical answer\")\n\nmessages = [\n    Message.from_text(\"What is 15 * 23?\", role=Role.USER)\n]\n\n# Get structured output\nsolution = llm.structured_output(\n    messages=messages,\n    constraint=PydanticModel(model=MathProblemSolution),\n    model=\"gpt-4o\",\n)\n\nprint(f\"REASONING:\\n\\n{solution.reasoning}\\n\")\nprint(f\"ANSWER: {solution.answer}\\n\")\n</pre> from pydantic import BaseModel, Field from bridgic.core.model.protocols import PydanticModel, JsonSchema  # Option 1: Using Pydantic Models class MathProblemSolution(BaseModel):     \"\"\"Solution to a math problem with reasoning\"\"\"     reasoning: str = Field(description=\"Step-by-step reasoning\")     answer: int = Field(description=\"Final numerical answer\")  messages = [     Message.from_text(\"What is 15 * 23?\", role=Role.USER) ]  # Get structured output solution = llm.structured_output(     messages=messages,     constraint=PydanticModel(model=MathProblemSolution),     model=\"gpt-4o\", )  print(f\"REASONING:\\n\\n{solution.reasoning}\\n\") print(f\"ANSWER: {solution.answer}\\n\") <pre>REASONING:\n\n15 multiplied by 23 can be broken down into smaller, more manageable calculations using the distributive property of multiplication. Here's how:\n\n1. **Break down 23:**\n   - 23 can be expressed as 20 + 3.\n\n2. **Apply the distributive property:**\n   - 15 * 23 = 15 * (20 + 3)\n   - According to the distributive property, this can be expanded to:\n     - 15 * 20 + 15 * 3\n\n3. **Calculate the individual products:**\n   - **15 * 20**\n     - 15 * 2 = 30\n     - Append a zero (since you are multiplying by 20, which is 10 times 2):\n     - 15 * 20 = 300\n   \n   - **15 * 3**\n     - 15 * 3 = 45\n\n4. **Add the two results together:**\n   - 300 + 45 = 345\n\nThus, using the distributive property and breaking down the numbers into simpler parts, we find that 15 multiplied by 23 equals 345.\n\nANSWER: 345\n\n</pre> In\u00a0[15]: Copied! <pre>from bridgic.core.model.types import Tool\n\n# Define available tools\ntools = [\n    Tool(\n        name=\"get_weather\",\n        description=\"Get the current weather for a location\",\n        parameters={\n            \"type\": \"object\",\n            \"properties\": {\n                \"location\": {\n                    \"type\": \"string\",\n                    \"description\": \"City name, e.g., 'San Francisco, CA'\"\n                },\n                \"unit\": {\n                    \"type\": \"string\",\n                    \"enum\": [\"celsius\", \"fahrenheit\"],\n                    \"description\": \"Temperature unit\"\n                }\n            },\n            \"required\": [\"location\"]\n        }\n    ),\n    Tool(\n        name=\"calculate\",\n        description=\"Perform mathematical calculations\",\n        parameters={\n            \"type\": \"object\",\n            \"properties\": {\n                \"expression\": {\n                    \"type\": \"string\",\n                    \"description\": \"Mathematical expression to evaluate\"\n                }\n            },\n            \"required\": [\"expression\"]\n        }\n    )\n]\n\n# Model selects appropriate tool\nmessages = [\n    Message.from_text(\"What's the weather like in Paris?\", role=Role.USER)\n]\n\ntool_calls, content = llm.select_tool(\n    messages=messages,\n    tools=tools,\n    model=\"gpt-4o\",\n    tool_choice=\"auto\",\n)\n\n# Process tool calls\nfor tool_call in tool_calls:\n    print(f\"Tool: {tool_call.name}\")\n    print(f\"Arguments: {tool_call.arguments}\")\n    print(f\"Call ID: {tool_call.id}\")\n</pre> from bridgic.core.model.types import Tool  # Define available tools tools = [     Tool(         name=\"get_weather\",         description=\"Get the current weather for a location\",         parameters={             \"type\": \"object\",             \"properties\": {                 \"location\": {                     \"type\": \"string\",                     \"description\": \"City name, e.g., 'San Francisco, CA'\"                 },                 \"unit\": {                     \"type\": \"string\",                     \"enum\": [\"celsius\", \"fahrenheit\"],                     \"description\": \"Temperature unit\"                 }             },             \"required\": [\"location\"]         }     ),     Tool(         name=\"calculate\",         description=\"Perform mathematical calculations\",         parameters={             \"type\": \"object\",             \"properties\": {                 \"expression\": {                     \"type\": \"string\",                     \"description\": \"Mathematical expression to evaluate\"                 }             },             \"required\": [\"expression\"]         }     ) ]  # Model selects appropriate tool messages = [     Message.from_text(\"What's the weather like in Paris?\", role=Role.USER) ]  tool_calls, content = llm.select_tool(     messages=messages,     tools=tools,     model=\"gpt-4o\",     tool_choice=\"auto\", )  # Process tool calls for tool_call in tool_calls:     print(f\"Tool: {tool_call.name}\")     print(f\"Arguments: {tool_call.arguments}\")     print(f\"Call ID: {tool_call.id}\")  <pre>Tool: get_weather\nArguments: {'location': 'Paris'}\nCall ID: call_aLv7xon4zhsNVMcnLmxsGJ3v\n</pre>"},{"location":"tutorials/items/model_integration/llm_integration/#llm","title":"LLM\u00b6","text":""},{"location":"tutorials/items/model_integration/llm_integration/#installation","title":"Installation\u00b6","text":"<p>Bridgic uses a modular installation strategy\u2014install only the components you require.</p> <p>Each model integration is available as a separate package, so you can minimize dependencies and keep your environment streamlined.</p> Package <code>BaseLlm</code> <code>StructuredOutput</code> <code>ToolSelection</code> <code>bridgic-llms-openai-like</code> \u2705 \u274c \u274c <code>bridgic-llms-openai</code> \u2705 \u2705 \u2705 <code>bridgic-llms-vllm</code> \u2705 \u2705 \u2705"},{"location":"tutorials/items/model_integration/llm_integration/#llm-usage","title":"LLM Usage\u00b6","text":"<p>This section demonstrates the complete lifecycle of working with models in Bridgic, from initialization to advanced features.</p>"},{"location":"tutorials/items/model_integration/llm_integration/#1-initialization","title":"1. Initialization\u00b6","text":"<p>Model initialization is straightforward and follows a consistent pattern across all providers.</p>"},{"location":"tutorials/items/model_integration/llm_integration/#2-basic-interfaces","title":"2. Basic Interfaces\u00b6","text":"<p>All LLM providers in Bridgic implement the <code>BaseLlm</code> abstract class, which defines the fundamental <code>chat</code>/<code>stream</code> interfaces and their asynchronous variant.</p>"},{"location":"tutorials/items/model_integration/llm_integration/#21-chat","title":"2.1 Chat\u00b6","text":"<p>The most basic interface for getting a complete response from the model:</p>"},{"location":"tutorials/items/model_integration/llm_integration/#22-streaming","title":"2.2 Streaming\u00b6","text":"<p>For real-time response generation:</p>"},{"location":"tutorials/items/model_integration/llm_integration/#3-advanced-protocols","title":"3. Advanced Protocols\u00b6","text":"<p>Advanced interfaces are provided through optional protocols that providers can implement based on their capabilities.</p>"},{"location":"tutorials/items/model_integration/llm_integration/#31-structured-output-structuredoutput-protocol","title":"3.1 Structured Output (<code>StructuredOutput</code> Protocol)\u00b6","text":"<p>Generate outputs that conform to specific schemas or formats:</p>"},{"location":"tutorials/items/model_integration/llm_integration/#32-tool-selection-toolselection-protocol","title":"3.2 Tool Selection (<code>ToolSelection</code> Protocol)\u00b6","text":"<p>Enable models to select and use tools (function calling):</p>"},{"location":"tutorials/items/observability_integration/","title":"Observability","text":""},{"location":"tutorials/items/observability_integration/#observability-integration","title":"Observability Integration","text":""},{"location":"tutorials/items/observability_integration/#why-observability-matters","title":"Why Observability Matters","text":"<p>Developing and managing agentic systems comes with its own challenges. Unlike traditional software where execution flows are predictable, agentic systems involve dynamic decision-making, complex workflows, and interactions with external services. Without proper observability, these systems can become \"black boxes\"\u2014you know inputs and outputs, but have little insight into what happens in between.</p> <p>Effective observability provides several critical benefits:</p> <ul> <li> <p>Understanding Runtime Behavior: See what's actually happening as your system executes, including which tasks are running, their execution order, and how they interact with each other.</p> </li> <li> <p>Task Level Visibility: Track inputs and outputs at the granularity of individual task units, enabling you to understand data flow, identify where transformations occur, and debug issues at the right level of detail.</p> </li> <li> <p>Performance Optimization: Help measure execution times and identify performance bottlenecks of your agentic system built with Bridgic. This visibility is essential for continuously optimizing agentic systems.</p> </li> </ul>"},{"location":"tutorials/items/observability_integration/#how-observability-works","title":"How Observability Works","text":""},{"location":"tutorials/items/observability_integration/#worker-execution","title":"Worker Execution","text":"<p>In the Bridgic framework, each task execution happens at the Worker granularity. Worker is the basic unit that represents a discrete task which can be executed. This worker-centric architecture provides natural boundaries for observability instrumentation, regardless of complexity. Whether you're building a simple sequential workflow or a complex graph-based agentic system (like ReAct), every operation runs as a worker.</p>"},{"location":"tutorials/items/observability_integration/#callback-mechanism","title":"Callback Mechanism","text":"<p>Bridgic provides a flexible callback mechanism that hooks into worker execution at key lifecycle points. This callback system is designed to be non-intrusive\u2014callbacks observe and instrument execution without modifying your core business logic. Refer to Worker Callback Mechanism for more about how this mechanism works in detail.</p>"},{"location":"tutorials/items/observability_integration/#available-third-party-integrations","title":"Available Third-Party Integrations","text":"<p>Bridgic currently provides integrations with the following observability platforms:</p> <ul> <li>Opik Integration</li> <li>LangWatch Integration</li> </ul>"},{"location":"tutorials/items/observability_integration/lang_watch_integration/","title":"LangWatch Integration","text":""},{"location":"tutorials/items/observability_integration/lang_watch_integration/#langwatch-integration","title":"LangWatch Integration","text":""},{"location":"tutorials/items/observability_integration/lang_watch_integration/#overview","title":"Overview","text":"<p>LangWatch is a comprehensive observability platform designed for LLM applications. The <code>bridgic-traces-langwatch</code> package enables seamless integration of LangWatch into your Bridgic-based agentic workflows.</p> <p>This integration is primarily supported by <code>LangWatchTraceCallback</code>, a WorkerCallback implementation that automatically instruments the worker execution with LangWatch tracing, which provides comprehensive observability by:</p> <ul> <li>Worker Execution Traces Tracking: Record the execution of each worker as a span in LangWatch, allowing to visualize start/end time, duration.</li> <li>Worker Execution Data Reporting: Capture the input, output and other necessary information and then log to the LangWatch platform.</li> <li>Hierarchical Trace Structure: Organize execution traces in a hierarchy that reflects the nesting between automa layers, making it straightforward to see how top-level automa is composed of the execution of multiple nested workers.</li> </ul>"},{"location":"tutorials/items/observability_integration/lang_watch_integration/#prerequisites","title":"Prerequisites","text":"<p>LangWatch provides a hosted version of the platform, or you can run the platform locally.</p> <ul> <li>To use the hosted version, you need to create a LangWatch account and grab your API Keyfrom the dashboard.</li> <li>To run LangWatch locally, see the self-hosting guide for more information.</li> </ul>"},{"location":"tutorials/items/observability_integration/lang_watch_integration/#using-langwatch-in-bridgic","title":"Using LangWatch in Bridgic","text":""},{"location":"tutorials/items/observability_integration/lang_watch_integration/#step-1-install-package","title":"Step 1: Install package","text":"<pre><code># Install the LangWatch tracing package\npip install bridgic-traces-langwatch\n</code></pre>"},{"location":"tutorials/items/observability_integration/lang_watch_integration/#step-2-configure-langwatch","title":"Step 2: Configure LangWatch","text":"<p>Using Environment Variables</p> <p>Set the following environment variables:</p> <pre><code>export LANGWATCH_API_KEY=\"your-api-key-here\"\nexport LANGWATCH_ENDPOINT=\"https://app.langwatch.ai\"  # Optional, defaults to https://app.langwatch.ai\n</code></pre>"},{"location":"tutorials/items/observability_integration/lang_watch_integration/#step-3-register-the-callback","title":"Step 3: Register the callback","text":"<p>You can enable LangWatch tracing globally with a single helper call (recommended), or wire the callback manually when you need custom behavior. Bridgic also lets you scope tracing to a single automa via <code>RunningOptions</code>.</p>"},{"location":"tutorials/items/observability_integration/lang_watch_integration/#method-1-application-wide-registration-helper-or-manual","title":"Method 1: Application-wide registration (helper or manual)","text":"<p>Choose whichever snippet fits your setup\u2014they produce the same effect.</p> start_langwatch_traceGlobalSetting <pre><code>from bridgic.traces.langwatch import start_langwatch_trace\n\nstart_langwatch_trace(\n    api_key=None,            # defaults to LANGWATCH_API_KEY env var\n    endpoint_url=None,       # defaults to LANGWATCH_ENDPOINT or https://app.langwatch.ai\n    base_attributes=None     # optional: shared attributes applied to every trace\n)\n</code></pre> <pre><code>from bridgic.core.automa.worker import WorkerCallbackBuilder\nfrom bridgic.core.config import GlobalSetting\nfrom bridgic.traces.langwatch import LangWatchTraceCallback\n\nGlobalSetting.set(callback_builders=[WorkerCallbackBuilder(\n    LangWatchTraceCallback,\n    init_kwargs={\n        \"api_key\": None,\n        \"endpoint_url\": None,\n        \"base_attributes\": None\n    }\n)])\n</code></pre> <pre><code>from bridgic.core.automa import GraphAutoma, worker\n\nclass DataAnalysisAutoma(GraphAutoma):\n    @worker(is_start=True)\n    async def collect_data(self, topic: str) -&gt; dict:\n        \"\"\"Collect data for the given topic.\"\"\"\n        # Simulate data collection\n        await asyncio.sleep(1)\n        return {\n            \"topic\": topic,\n            \"data_points\": [\"point1\", \"point2\", \"point3\"],\n            \"timestamp\": \"2024-01-01\"\n        }\n\n    @worker(dependencies=[\"collect_data\"])\n    async def analyze_trends(self, data: dict) -&gt; dict:\n        \"\"\"Analyze trends in the collected data.\"\"\"\n        # Simulate trend analysis\n        await asyncio.sleep(1)\n        return {\n            \"trends\": [\"trend1\", \"trend2\"],\n            \"confidence\": 0.85,\n            \"source_data\": data\n        }\n\n    @worker(dependencies=[\"analyze_trends\"], is_output=True)\n    async def generate_report(self, analysis: dict) -&gt; str:\n        \"\"\"Generate a final report.\"\"\"\n        await asyncio.sleep(1)\n        return f\"Report: Found {len(analysis['trends'])} trends with {analysis['confidence']} confidence.\"\n\nasync def automa_arun():\n    # Call start_langwatch_trace(...) or configure GlobalSetting(...) once at startup\n    from bridgic.traces.langwatch import start_langwatch_trace\n\n    start_langwatch_trace(\n        api_key=None,\n        endpoint_url=None,\n        base_attributes=None\n    )\n\n    automa = DataAnalysisAutoma()\n    result = await automa.arun(topic=\"market analysis\")\n    print(result)\n\nif __name__ == \"__main__\":\n    import asyncio\n    asyncio.run(automa_arun())\n</code></pre>"},{"location":"tutorials/items/observability_integration/lang_watch_integration/#method-2-per-automa-scope-with-runningoptions","title":"Method 2: Per-automa scope with <code>RunningOptions</code>","text":"<p>When only a specific automa needs LangWatch tracing, configure the callback through <code>RunningOptions</code>.</p> <pre><code>from bridgic.core.automa import GraphAutoma, RunningOptions, worker\nfrom bridgic.core.automa.worker import WorkerCallbackBuilder\nfrom bridgic.traces.langwatch import LangWatchTraceCallback\n\nclass DataAnalysisAutoma(GraphAutoma):\n    @worker(is_start=True)\n    async def collect_data(self, topic: str) -&gt; dict:\n        \"\"\"Collect data for the given topic.\"\"\"\n        # Simulate data collection\n        await asyncio.sleep(1)\n        return {\n            \"topic\": topic,\n            \"data_points\": [\"point1\", \"point2\", \"point3\"],\n            \"timestamp\": \"2024-01-01\"\n        }\n\n    @worker(dependencies=[\"collect_data\"])\n    async def analyze_trends(self, data: dict) -&gt; dict:\n        \"\"\"Analyze trends in the collected data.\"\"\"\n        # Simulate trend analysis\n        await asyncio.sleep(1)\n        return {\n            \"trends\": [\"trend1\", \"trend2\"],\n            \"confidence\": 0.85,\n            \"source_data\": data\n        }\n\n    @worker(dependencies=[\"analyze_trends\"], is_output=True)\n    async def generate_report(self, analysis: dict) -&gt; str:\n        \"\"\"Generate a final report.\"\"\"\n        await asyncio.sleep(1)\n        return f\"Report: Found {len(analysis['trends'])} trends with {analysis['confidence']} confidence.\"\n\nasync def automa_arun():\n    builder = WorkerCallbackBuilder(LangWatchTraceCallback, init_kwargs={\n        \"api_key\": None,\n        \"endpoint_url\": None,\n        \"base_attributes\": None\n    })\n    running_options = RunningOptions(callback_builders=[builder])\n    automa = DataAnalysisAutoma(running_options=running_options)\n    result = await automa.arun(topic=\"market analysis\")\n    print(result)\n\nif __name__ == \"__main__\":\n    import asyncio\n    asyncio.run(automa_arun())\n</code></pre> <p>Once your Bridgic application has finished running, traces will be automatically sent to LangWatch. You can view them in the LangWatch dashboard to explore rich visual insights and detailed traces of your workflow.</p>"},{"location":"tutorials/items/observability_integration/opik_integration/","title":"Opik Integration","text":""},{"location":"tutorials/items/observability_integration/opik_integration/#opik-integration","title":"Opik Integration","text":""},{"location":"tutorials/items/observability_integration/opik_integration/#overview","title":"Overview","text":"<p>Comet Opik is a comprehensive observability platform designed for agentic systems. The <code>bridgic-traces-opik</code> package enables seamless integration of Opik into your Bridgic-based agentic workflows.</p> <p>This integration is primarily supported by <code>OpikTraceCallback</code>, a WorkerCallback implementation that automatically instruments the worker execution with Opik tracing, which provides comprehensive observability by:</p> <ul> <li>Worker Execution Traces Tracking: Record the execution of each worker as a span in Opik, allowing to visualize start/end time, duration.</li> <li>Worker Execution Data Reporting: Capture the input, output and other necessary information and then log to the opik platform.</li> <li>Hierarchical Trace Structure: Organize execution traces in a hierarchy that reflects the nesting between automa layers, making it straightforward to see how top-level automa is composed of the execution of multiple nested workers.</li> </ul>"},{"location":"tutorials/items/observability_integration/opik_integration/#prerequisites","title":"Prerequisites","text":"<p>Comet provides a hosted version of the Opik platform, or you can run the platform locally.</p> <ul> <li>To use the hosted version, you need to create a Comet account and grab your API Key.</li> <li>To run the Opik platform locally, see the installation guide for more information.</li> </ul>"},{"location":"tutorials/items/observability_integration/opik_integration/#using-opik-in-bridgic","title":"Using Opik in Bridgic","text":""},{"location":"tutorials/items/observability_integration/opik_integration/#step-1-install-package","title":"Step 1: Install package","text":"<pre><code># Automatically install the Opik package\npip install bridgic-traces-opik\n</code></pre>"},{"location":"tutorials/items/observability_integration/opik_integration/#step-2-register-the-callback","title":"Step 2: Register the callback","text":"<p>You can register Opik tracing at the scope that best fits your application. <code>start_opik_trace</code> is the fastest path (a single line that configures global tracing via <code>GlobalSetting</code>). When you want to customize the same global setup or target only a specific automa, Bridgic exposes direct hooks for both use cases.</p>"},{"location":"tutorials/items/observability_integration/opik_integration/#method-1-application-wide-registration-helper-or-manual","title":"Method 1: Application-wide registration (helper or manual)","text":"<p>Pick one of the three options below\u2014they produce the exact same runtime behavior:</p> start_opik_trace for opik cloudstart_opik_trace for self-hostedGlobalSetting <pre><code>from bridgic.traces.opik import start_opik_trace\nstart_opik_trace(project_name=\"bridgic-integration-demo\", api_key=\"your-api-key\")\n</code></pre> <pre><code>from bridgic.traces.opik import start_opik_trace\nstart_opik_trace(project_name=\"bridgic-integration-demo\", use_local=True, host=\"http://localhost:5173/api\")\n</code></pre> <pre><code>from bridgic.core.automa.worker import WorkerCallbackBuilder\nfrom bridgic.core.config import GlobalSetting\nfrom bridgic.traces.opik import OpikTraceCallback\n\nGlobalSetting.set(callback_builders=[WorkerCallbackBuilder(\n    OpikTraceCallback,\n    init_kwargs={\"project_name\": \"bridgic-integration-demo\", \"api_key\": \"your-api-key\"}\n)])\n</code></pre> <pre><code>from bridgic.core.automa import GraphAutoma, worker\nfrom bridgic.traces.opik import start_opik_trace\n\nclass DataAnalysisAutoma(GraphAutoma):\n    @worker(is_start=True)\n    async def collect_data(self, topic: str) -&gt; dict:\n        \"\"\"Collect data for the given topic.\"\"\"\n        # Simulate data collection\n        await asyncio.sleep(1)\n        return {\n            \"topic\": topic,\n            \"data_points\": [\"point1\", \"point2\", \"point3\"],\n            \"timestamp\": \"2024-01-01\"\n        }\n\n    @worker(dependencies=[\"collect_data\"])\n    async def analyze_trends(self, data: dict) -&gt; dict:\n        \"\"\"Analyze trends in the collected data.\"\"\"\n        # Simulate trend analysis\n        await asyncio.sleep(1)\n        return {\n            \"trends\": [\"trend1\", \"trend2\"],\n            \"confidence\": 0.85,\n            \"source_data\": data\n        }\n\n    @worker(dependencies=[\"analyze_trends\"], is_output=True)\n    async def generate_report(self, analysis: dict) -&gt; str:\n        \"\"\"Generate a final report.\"\"\"\n        await asyncio.sleep(1)\n        return f\"Report: Found {len(analysis['trends'])} trends with {analysis['confidence']} confidence.\"\n\nasync def automa_arun():\n    # Call either start_opik_trace(...) or GlobalSetting.set(...) once at startup\n    start_opik_trace(project_name=\"bridgic-integration-demo\", api_key=\"your-api-key\")\n    automa = DataAnalysisAutoma()\n    result = await automa.arun(topic=\"market analysis\")\n    print(result)\n\nif __name__ == \"__main__\":\n    import asyncio\n    asyncio.run(automa_arun())\n</code></pre>"},{"location":"tutorials/items/observability_integration/opik_integration/#method-2-per-automa-scope-with-runningoptions","title":"Method 2: Per-automa scope with <code>RunningOptions</code>","text":"<p>When only a specific automa needs tracing, configure the callback through <code>RunningOptions</code>. Each automa gets its own callback instance, leaving other automa untouched.</p> <pre><code>from bridgic.core.automa import GraphAutoma, RunningOptions, worker\nfrom bridgic.core.automa.worker import WorkerCallbackBuilder\nfrom bridgic.traces.opik import OpikTraceCallback\n\nclass DataAnalysisAutoma(GraphAutoma):\n    @worker(is_start=True)\n    async def collect_data(self, topic: str) -&gt; dict:\n        \"\"\"Collect data for the given topic.\"\"\"\n        # Simulate data collection\n        await asyncio.sleep(1)\n        return {\n            \"topic\": topic,\n            \"data_points\": [\"point1\", \"point2\", \"point3\"],\n            \"timestamp\": \"2024-01-01\"\n        }\n\n    @worker(dependencies=[\"collect_data\"])\n    async def analyze_trends(self, data: dict) -&gt; dict:\n        \"\"\"Analyze trends in the collected data.\"\"\"\n        # Simulate trend analysis\n        await asyncio.sleep(1)\n        return {\n            \"trends\": [\"trend1\", \"trend2\"],\n            \"confidence\": 0.85,\n            \"source_data\": data\n        }\n\n    @worker(dependencies=[\"analyze_trends\"], is_output=True)\n    async def generate_report(self, analysis: dict) -&gt; str:\n        \"\"\"Generate a final report.\"\"\"\n        await asyncio.sleep(1)\n        return f\"Report: Found {len(analysis['trends'])} trends with {analysis['confidence']} confidence.\"\n\nasync def automa_arun():\n    builder = WorkerCallbackBuilder(OpikTraceCallback, init_kwargs={\"project_name\": \"bridgic-integration-demo\", \"api_key\": \"your-api-key\"})\n    running_options = RunningOptions(callback_builders=[builder])\n    automa = DataAnalysisAutoma(running_options=running_options)\n    result = await automa.arun(topic=\"market analysis\")\n    print(result)\n\nif __name__ == \"__main__\":\n    import asyncio\n    asyncio.run(automa_arun())\n</code></pre> <p>Once your Bridgic application has finished running, your terminal might display the following message:</p> <pre><code>$ python bridgic-demo/demo.py \nOPIK: Started logging traces to the \"bridgic-integration-demo\" project at http://localhost:5173/api/v1/session/redirect/projects/?trace_id=019a9709-e437-7b30-861e-76006b75e969&amp;path=aHR0cDovL2xvY2FsaG9zdDo1MTczL2FwaS8=\nReport: Found 2 trends with 0.85 confidence.\n</code></pre> <p>You can dive into the Opik app to explore rich visual insights and detailed traces of your workflow.</p>"},{"location":"tutorials/items/protocol_integration/","title":"Protocol","text":""},{"location":"tutorials/items/protocol_integration/#protocol-integration","title":"Protocol Integration","text":""},{"location":"tutorials/items/protocol_integration/#introduction","title":"Introduction","text":"<p>Protocol integration modules in Bridgic enable seamless connecting with third-party protocols, allowing AI applications to leverage external tools, resources or services. This tutorial provides a basic overview of how to integrate with external protocols or SDKs and the benefits that will be brought to AI application development.</p>"},{"location":"tutorials/items/protocol_integration/#the-value-of-protocol-integration","title":"The Value of Protocol Integration","text":"<p>Developing AI applications often requires interacting with various external services and resources. Because there are so many different platforms and systems, developers frequently need to work with specific protocols or SDKs for each one, which can increase the learning curve and add complexity to the development process.</p> <p>In the world of Bridgic, every component of a workflow can exist as an fine-grained worker. Multiple workers can be orchestrated into a more complex automa object (typically in a dynamic graph form), which results in a bigger part of the complete AI system.</p> <p>What if there were a design that turned the integration of third-party protocols or SDKs into simply using workers and orchestrating automas? This approach would significantly reduce developers' mental overhead while paving the way for smoother development, debugging, observability, and troubleshooting!</p> <p>Bridgic addresses this need by:</p> <ul> <li>Standard Neutrality: Bridgic implements an integration layer tailored to its core architecture, adapting to the features and abstractions of each third-party protocol.</li> <li>Unified Experience: Developers use the same familiar Bridgic interfaces and patterns to access protocol integrations\u2014no need to learn a separate API for each protocol.</li> </ul>"},{"location":"tutorials/items/protocol_integration/#available-integrations","title":"Available Integrations","text":"<p>Integrating external protocols will greatly expand the capabilities of your AI applications. Bridgic has already provided some important protocol integration packages, enabling you to unlock new possibilities.</p> Protocol Package Description MCP <code>bridgic-protocols-mcp</code> Model Context Protocol integration <p>We warmly invite developers from across the community to join us in expanding Bridgic's protocol integrations. Your contribution will help shape a richer ecosystem, benefiting everyone building AI application more easily.</p>"},{"location":"tutorials/items/protocol_integration/mcp_quick_start/","title":"MCP","text":"In\u00a0[\u00a0]: Copied! <pre>pip install bridgic-protocols-mcp\n</pre> pip install bridgic-protocols-mcp In\u00a0[1]: Copied! <pre>import os\nimport tempfile\n\nfrom bridgic.protocols.mcp import McpServerConnectionStdio\n\n# Create a temporary directory for the filesystem MCP server\ntemp_dir = os.path.realpath(tempfile.mkdtemp())\nprint(f\"Using temporary directory: {temp_dir}\")\n\n# Create a connection to a filesystem MCP server\n# Note: This requires Node.js and npx to be installed\nfilesystem_connection = McpServerConnectionStdio(\n    name=\"connection-filesystem-stdio\",\n    command=\"npx\",\n    args=[\"-y\", \"@modelcontextprotocol/server-filesystem\", temp_dir],\n)\n\n# Establish the connection\nfilesystem_connection.connect()\n\n# Verify connection\nprint(f\"\u2713 Connected to MCP server: {filesystem_connection.name}\")\nprint(f\"  Connection status: {filesystem_connection.is_connected}\")\n\n# List available tools\ntools = filesystem_connection.list_tools()\nprint(f\"\\n\u2713 Found {len(tools)} available tools:\")\nfor tool in tools:\n    print(f\"  - {tool.tool_name}: {tool.tool_description[:50]}...\")\n</pre> import os import tempfile  from bridgic.protocols.mcp import McpServerConnectionStdio  # Create a temporary directory for the filesystem MCP server temp_dir = os.path.realpath(tempfile.mkdtemp()) print(f\"Using temporary directory: {temp_dir}\")  # Create a connection to a filesystem MCP server # Note: This requires Node.js and npx to be installed filesystem_connection = McpServerConnectionStdio(     name=\"connection-filesystem-stdio\",     command=\"npx\",     args=[\"-y\", \"@modelcontextprotocol/server-filesystem\", temp_dir], )  # Establish the connection filesystem_connection.connect()  # Verify connection print(f\"\u2713 Connected to MCP server: {filesystem_connection.name}\") print(f\"  Connection status: {filesystem_connection.is_connected}\")  # List available tools tools = filesystem_connection.list_tools() print(f\"\\n\u2713 Found {len(tools)} available tools:\") for tool in tools:     print(f\"  - {tool.tool_name}: {tool.tool_description[:50]}...\") <pre>Using temporary directory: /private/var/folders/9t/5r9fms9s5q33p6xty_0_k1mw0000gn/T/tmpeuov9ggi\n\u2713 Connected to MCP server: connection-filesystem-stdio\n  Connection status: True\n\n\u2713 Found 14 available tools:\n  - read_file: Read the complete contents of a file as text. DEPR...\n  - read_text_file: Read the complete contents of a file from the file...\n  - read_media_file: Read an image or audio file. Returns the base64 en...\n  - read_multiple_files: Read the contents of multiple files simultaneously...\n  - write_file: Create a new file or completely overwrite an exist...\n  - edit_file: Make line-based edits to a text file. Each edit re...\n  - create_directory: Create a new directory or ensure a directory exist...\n  - list_directory: Get a detailed listing of all files and directorie...\n  - list_directory_with_sizes: Get a detailed listing of all files and directorie...\n  - directory_tree: Get a recursive tree view of files and directories...\n  - move_file: Move or rename files and directories. Can move fil...\n  - search_files: Recursively search for files and directories match...\n  - get_file_info: Retrieve detailed metadata about a file or directo...\n  - list_allowed_directories: Returns the list of directories that this server i...\n</pre> <p>You can also connect to an MCP server via streamable HTTP transport. Below is an example of how to connect to a remote Github MCP Server and view the tools it supports:</p> In\u00a0[2]: Copied! <pre>import os\nimport dotenv\n\nfrom mcp.shared._httpx_utils import create_mcp_http_client\nfrom bridgic.protocols.mcp import McpServerConnectionStreamableHttp\n\ndotenv.load_dotenv()\n\ngithub_mcp_url = os.environ.get(\"GITHUB_MCP_HTTP_URL\", \"https://api.githubcopilot.com/mcp/\")\ngithub_token = os.environ.get(\"GITHUB_TOKEN\")\n\nhttp_client = create_mcp_http_client(\n    headers={\"Authorization\": f\"Bearer {github_token}\"},\n)\n\ngithub_connection = McpServerConnectionStreamableHttp(\n    name=\"connection-github-streamable-http\",\n    url=github_mcp_url,\n    http_client=http_client,\n    request_timeout=15,\n)\n\ngithub_connection.connect()\n\n# Verify connection\nprint(f\"\u2713 Connected to MCP server: {github_connection.name}\")\nprint(f\"  Connection status: {github_connection.is_connected}\")\n\n# List available tools\ntools = github_connection.list_tools()\nprint(f\"\\n\u2713 Found {len(tools)} available tools:\")\nfor tool in tools:\n    print(f\"  - {tool.tool_name}: {tool.tool_description[:50]}...\")\n</pre> import os import dotenv  from mcp.shared._httpx_utils import create_mcp_http_client from bridgic.protocols.mcp import McpServerConnectionStreamableHttp  dotenv.load_dotenv()  github_mcp_url = os.environ.get(\"GITHUB_MCP_HTTP_URL\", \"https://api.githubcopilot.com/mcp/\") github_token = os.environ.get(\"GITHUB_TOKEN\")  http_client = create_mcp_http_client(     headers={\"Authorization\": f\"Bearer {github_token}\"}, )  github_connection = McpServerConnectionStreamableHttp(     name=\"connection-github-streamable-http\",     url=github_mcp_url,     http_client=http_client,     request_timeout=15, )  github_connection.connect()  # Verify connection print(f\"\u2713 Connected to MCP server: {github_connection.name}\") print(f\"  Connection status: {github_connection.is_connected}\")  # List available tools tools = github_connection.list_tools() print(f\"\\n\u2713 Found {len(tools)} available tools:\") for tool in tools:     print(f\"  - {tool.tool_name}: {tool.tool_description[:50]}...\") <pre>\u2713 Connected to MCP server: connection-github-streamable-http\n  Connection status: True\n\n\u2713 Found 40 available tools:\n  - add_comment_to_pending_review: Add review comment to the requester's latest pendi...\n  - add_issue_comment: Add a comment to a specific issue in a GitHub repo...\n  - assign_copilot_to_issue: Assign Copilot to a specific issue in a GitHub rep...\n  - create_branch: Create a new branch in a GitHub repository...\n  - create_or_update_file: Create or update a single file in a GitHub reposit...\n  - create_pull_request: Create a new pull request in a GitHub repository....\n  - create_repository: Create a new GitHub repository in your account or ...\n  - delete_file: Delete a file from a GitHub repository...\n  - fork_repository: Fork a GitHub repository to your account or specif...\n  - get_commit: Get details for a commit from a GitHub repository...\n  - get_file_contents: Get the contents of a file or directory from a Git...\n  - get_label: Get a specific label from a repository....\n  - get_latest_release: Get the latest release in a GitHub repository...\n  - get_me: Get details of the authenticated GitHub user. Use ...\n  - get_release_by_tag: Get a specific release by its tag name in a GitHub...\n  - get_tag: Get details about a specific git tag in a GitHub r...\n  - get_team_members: Get member usernames of a specific team in an orga...\n  - get_teams: Get details of the teams the user is a member of. ...\n  - issue_read: Get information about a specific issue in a GitHub...\n  - issue_write: Create a new or update an existing issue in a GitH...\n  - list_branches: List branches in a GitHub repository...\n  - list_commits: Get list of commits of a branch in a GitHub reposi...\n  - list_issue_types: List supported issue types for repository owner (o...\n  - list_issues: List issues in a GitHub repository. For pagination...\n  - list_pull_requests: List pull requests in a GitHub repository. If the ...\n  - list_releases: List releases in a GitHub repository...\n  - list_tags: List git tags in a GitHub repository...\n  - merge_pull_request: Merge a pull request in a GitHub repository....\n  - pull_request_read: Get information on a specific pull request in GitH...\n  - pull_request_review_write: Create and/or submit, delete review of a pull requ...\n  - push_files: Push multiple files to a GitHub repository in a si...\n  - request_copilot_review: Request a GitHub Copilot code review for a pull re...\n  - search_code: Fast and precise code search across ALL GitHub rep...\n  - search_issues: Search for issues in GitHub repositories using iss...\n  - search_pull_requests: Search for pull requests in GitHub repositories us...\n  - search_repositories: Find GitHub repositories by name, description, rea...\n  - search_users: Find GitHub users by username, real name, or other...\n  - sub_issue_write: Add a sub-issue to a parent issue in a GitHub repo...\n  - update_pull_request: Update an existing pull request in a GitHub reposi...\n  - update_pull_request_branch: Update the branch of a pull request with the lates...\n</pre> In\u00a0[3]: Copied! <pre>import datetime\nimport mcp\n\nfrom bridgic.core.automa import GraphAutoma, RunningOptions, worker\nfrom bridgic.core.automa.args import System\n\n# List the tools via the server connection\ntools = filesystem_connection.list_tools()\n\n# Filter the needed one which will create the real worker\nwrite_tool = next(t for t in tools if t.tool_name == \"write_file\")\nread_tool = next(t for t in tools if t.tool_name == \"read_file\")\nmeta_tool = next(t for t in tools if t.tool_name == \"get_file_info\")\n\nclass FileWriter(GraphAutoma):\n    def __init__(self, name: str, running_options: RunningOptions = None):\n        super().__init__(name=name, running_options=running_options)\n        self.add_worker(\"write\", write_tool.create_worker())\n        self.add_worker(\"read\", read_tool.create_worker())\n        self.add_worker(\"meta\", meta_tool.create_worker())\n\n    @worker(is_start=True)\n    def start(self, title: str, content: str, rtx = System(\"runtime_context\")):\n        # Get the current time\n        now_time = datetime.datetime.now()\n\n        # Get the content and path of the file to be written\n        file_path = f\"{temp_dir}/{title}.txt\"\n        file_content = (\n            f\"Time: {now_time.strftime('%Y-%m-%d %H:%M:%S')}\\n\"\n            f\"Content: {content}\\n\"\n        )\n\n        # Write the file at the next step\n        self.ferry_to(\"write\", content=file_content, path=file_path)\n\n        return file_path\n\n    @worker(dependencies=[\"start\", \"write\"])\n    def after_write(self, file_path: str, write_info: mcp.types.CallToolResult):\n        self.ferry_to(\"read\", path=file_path)\n        self.ferry_to(\"meta\", path=file_path)\n\n    @worker(is_output=True, dependencies=[\"start\", \"read\", \"meta\"])\n    def output(self, file_path: str, read_info: mcp.types.CallToolResult, meta_info: mcp.types.CallToolResult) -&gt; str:\n        return (\n            f\"\u2713 Finnished writting!\"\n            f\"\\nFile path: {file_path}\"\n            f\"\\n{meta_info.content[0].text}\"\n        )\n\nfile_processor = FileWriter(name=\"file-processor\")\n\nfor content in [\n    (\"1.txt\", \"Hello, Bridgic!\"),\n    (\"2.txt\", \"Hello, MCP!\"),\n]:\n    result = await file_processor.arun(title=content[0], content=content[1])\n    print(f\"\\n{result}\")\n</pre> import datetime import mcp  from bridgic.core.automa import GraphAutoma, RunningOptions, worker from bridgic.core.automa.args import System  # List the tools via the server connection tools = filesystem_connection.list_tools()  # Filter the needed one which will create the real worker write_tool = next(t for t in tools if t.tool_name == \"write_file\") read_tool = next(t for t in tools if t.tool_name == \"read_file\") meta_tool = next(t for t in tools if t.tool_name == \"get_file_info\")  class FileWriter(GraphAutoma):     def __init__(self, name: str, running_options: RunningOptions = None):         super().__init__(name=name, running_options=running_options)         self.add_worker(\"write\", write_tool.create_worker())         self.add_worker(\"read\", read_tool.create_worker())         self.add_worker(\"meta\", meta_tool.create_worker())      @worker(is_start=True)     def start(self, title: str, content: str, rtx = System(\"runtime_context\")):         # Get the current time         now_time = datetime.datetime.now()          # Get the content and path of the file to be written         file_path = f\"{temp_dir}/{title}.txt\"         file_content = (             f\"Time: {now_time.strftime('%Y-%m-%d %H:%M:%S')}\\n\"             f\"Content: {content}\\n\"         )          # Write the file at the next step         self.ferry_to(\"write\", content=file_content, path=file_path)          return file_path      @worker(dependencies=[\"start\", \"write\"])     def after_write(self, file_path: str, write_info: mcp.types.CallToolResult):         self.ferry_to(\"read\", path=file_path)         self.ferry_to(\"meta\", path=file_path)      @worker(is_output=True, dependencies=[\"start\", \"read\", \"meta\"])     def output(self, file_path: str, read_info: mcp.types.CallToolResult, meta_info: mcp.types.CallToolResult) -&gt; str:         return (             f\"\u2713 Finnished writting!\"             f\"\\nFile path: {file_path}\"             f\"\\n{meta_info.content[0].text}\"         )  file_processor = FileWriter(name=\"file-processor\")  for content in [     (\"1.txt\", \"Hello, Bridgic!\"),     (\"2.txt\", \"Hello, MCP!\"), ]:     result = await file_processor.arun(title=content[0], content=content[1])     print(f\"\\n{result}\") <pre>\n\u2713 Finnished writting!\nFile path: /private/var/folders/9t/5r9fms9s5q33p6xty_0_k1mw0000gn/T/tmpeuov9ggi/1.txt.txt\nsize: 51\ncreated: Sun Jan 25 2026 23:09:15 GMT+0800 (China Standard Time)\nmodified: Sun Jan 25 2026 23:09:15 GMT+0800 (China Standard Time)\naccessed: Sun Jan 25 2026 23:09:15 GMT+0800 (China Standard Time)\nisDirectory: false\nisFile: true\npermissions: 644\n\n\u2713 Finnished writting!\nFile path: /private/var/folders/9t/5r9fms9s5q33p6xty_0_k1mw0000gn/T/tmpeuov9ggi/2.txt.txt\nsize: 47\ncreated: Sun Jan 25 2026 23:09:15 GMT+0800 (China Standard Time)\nmodified: Sun Jan 25 2026 23:09:15 GMT+0800 (China Standard Time)\naccessed: Sun Jan 25 2026 23:09:15 GMT+0800 (China Standard Time)\nisDirectory: false\nisFile: true\npermissions: 644\n</pre> In\u00a0[4]: Copied! <pre>from bridgic.protocols.mcp import McpServerConnectionStdio\n\nweather_connection = McpServerConnectionStdio(\n    name=\"connection-weather-stdio\",\n    command=\"npx\",\n    args=[\"-y\", \"@mariox/weather-mcp-server\"],\n)\nweather_connection.connect()\n</pre> from bridgic.protocols.mcp import McpServerConnectionStdio  weather_connection = McpServerConnectionStdio(     name=\"connection-weather-stdio\",     command=\"npx\",     args=[\"-y\", \"@mariox/weather-mcp-server\"], ) weather_connection.connect() <p>Then you can initialize an agentic automa which utilizes the weather tool(s) to answer the weather questions.</p> In\u00a0[5]: Copied! <pre>import os\nimport dotenv\n\nfrom bridgic.llms.openai import OpenAILlm, OpenAIConfiguration\n\ndotenv.load_dotenv()\n\n# Prepare the LLM (set these env vars before running this cell)\n_api_key = os.environ.get(\"OPENAI_API_KEY\")\n_api_base = os.environ.get(\"OPENAI_API_BASE\", \"https://api.openai.com/v1\")\n_model_name = os.environ.get(\"OPENAI_MODEL_NAME\", \"gpt-4o-mini\")\n\n# Initialize LLM instance\nllm = OpenAILlm(\n    api_key=_api_key,\n    api_base=_api_base,\n    configuration=OpenAIConfiguration(model=_model_name),\n    timeout=180,\n)\n</pre> import os import dotenv  from bridgic.llms.openai import OpenAILlm, OpenAIConfiguration  dotenv.load_dotenv()  # Prepare the LLM (set these env vars before running this cell) _api_key = os.environ.get(\"OPENAI_API_KEY\") _api_base = os.environ.get(\"OPENAI_API_BASE\", \"https://api.openai.com/v1\") _model_name = os.environ.get(\"OPENAI_MODEL_NAME\", \"gpt-4o-mini\")  # Initialize LLM instance llm = OpenAILlm(     api_key=_api_key,     api_base=_api_base,     configuration=OpenAIConfiguration(model=_model_name),     timeout=180, ) In\u00a0[6]: Copied! <pre>from bridgic.core.automa import RunningOptions\nfrom bridgic.core.agentic.recent import ReCentAutoma, StopCondition\n\n# Pass weather tools in directly to build an agentic automa as a weather agent\nweather_agent = ReCentAutoma(\n    llm=llm,\n    tools=weather_connection.list_tools(),\n    stop_condition=StopCondition(max_iteration=5),\n    running_options=RunningOptions(debug=True),\n)\n\n# Ask the weather agent for the weather in Shanghai\nresult = await weather_agent.arun(goal=\"Get the weather in Shanghai.\")\nprint(result)\n</pre> from bridgic.core.automa import RunningOptions from bridgic.core.agentic.recent import ReCentAutoma, StopCondition  # Pass weather tools in directly to build an agentic automa as a weather agent weather_agent = ReCentAutoma(     llm=llm,     tools=weather_connection.list_tools(),     stop_condition=StopCondition(max_iteration=5),     running_options=RunningOptions(debug=True), )  # Ask the weather agent for the weather in Shanghai result = await weather_agent.arun(goal=\"Get the weather in Shanghai.\") print(result)  <pre>\n[ReCentAutoma]-[ReCentAutoma-171bd997] is started.\n[ReCentAutoma]-[ReCentAutoma-171bd997] [__dynamic_step__] driving [initialize_task_goal]\n[ReCentAutoma]-[ReCentAutoma-171bd997] [__automa__] triggers [initialize_task_goal]\n[ReCentAutoma]-[ReCentAutoma-171bd997] \ud83c\udfaf Task Goal\nGet the weather in Shanghai.\n[ReCentAutoma]-[ReCentAutoma-171bd997] [__dynamic_step__] driving [observe]\n[ReCentAutoma]-[ReCentAutoma-171bd997] [initialize_task_goal] triggers [observe]\n[ReCentAutoma]-[ReCentAutoma-171bd997] \ud83d\udc40 Observation\n    Iteration: 1\n    Achieved: False\n    Thinking: The task goal is to get the weather in Shanghai. However, the conversation history does not show that any information regarding the current weather in Shanghai has been provided or gathered so far. Therefore, there is a significant gap because the specific weather details are still missing. The goal has not been achieved yet.\n[ReCentAutoma]-[ReCentAutoma-171bd997] [__dynamic_step__] driving [select_tools, compress_memory]\n[ReCentAutoma]-[ReCentAutoma-171bd997] [observe] triggers [select_tools]\n[ReCentAutoma]-[ReCentAutoma-171bd997] [observe] triggers [compress_memory]\n[ReCentAutoma]-[ReCentAutoma-171bd997] \ud83e\udded Memory Check\n    Compression Needed: False\n[ReCentAutoma]-[ReCentAutoma-171bd997] \ud83d\udd27 Tool Selection\n    (No tools selected)\n\n    LLM Response: To achieve the task goal of getting the weather in Shanghai, the most appropriate tool to use is the `get_weather` function, specifying \"Shanghai\" as the location.\n\nI will proceed to execute this function to retrieve the current weather information for Shanghai. \n\nHere we go!\n[ReCentAutoma]-[ReCentAutoma-171bd997] [__dynamic_step__] driving [observe]\n[ReCentAutoma]-[ReCentAutoma-171bd997] [select_tools] triggers [observe]\n[ReCentAutoma]-[ReCentAutoma-171bd997] \ud83d\udc40 Observation\n    Iteration: 2\n    Achieved: False\n    Thinking: The task goal is to obtain the weather information for Shanghai. As of now, there have been no updates or details regarding the weather in Shanghai provided. Consequently, there remains a critical gap as the necessary weather information has not been collected or presented. Therefore, the goal has not been achieved.\n[ReCentAutoma]-[ReCentAutoma-171bd997] [__dynamic_step__] driving [select_tools, compress_memory]\n[ReCentAutoma]-[ReCentAutoma-171bd997] [observe] triggers [select_tools]\n[ReCentAutoma]-[ReCentAutoma-171bd997] [observe] triggers [compress_memory]\n[ReCentAutoma]-[ReCentAutoma-171bd997] \ud83e\udded Memory Check\n    Compression Needed: False\n[ReCentAutoma]-[ReCentAutoma-171bd997] \ud83d\udd27 Tool Selection\n    Tool 1: get_weather\n      id: tool_bfecfd4db550446d8631f200e\n      arguments: {'location': '\u4e0a\u6d77'}\n[ReCentAutoma]-[ReCentAutoma-171bd997] [__dynamic_step__] driving [tool-&lt;get_weather&gt;-&lt;tool_bfecfd4db550446d8631f200e&gt;]\n[ReCentAutoma]-[ReCentAutoma-171bd997] [select_tools] triggers [tool-&lt;get_weather&gt;-&lt;tool_bfecfd4db550446d8631f200e&gt;]\n[ReCentAutoma]-[ReCentAutoma-171bd997] [__dynamic_step__] driving [collect_results-&lt;07038e9e&gt;]\n[ReCentAutoma]-[ReCentAutoma-171bd997] [tool-&lt;get_weather&gt;-&lt;tool_bfecfd4db550446d8631f200e&gt;] triggers [collect_results-&lt;07038e9e&gt;]\n[ReCentAutoma]-[ReCentAutoma-171bd997] \ud83d\udea9 Tool Results\n    Tool 1: get_weather\n      id: tool_bfecfd4db550446d8631f200e\n      result: meta=None content=[TextContent(type='text', text='\ud83c\udf24\ufe0f **\u4e0a\u6d77 \u5929\u6c14\u4fe1\u606f**\\n\\n\ud83d\udccd **\u4f4d\u7f6e\u4fe1\u606f:**\\n\ud83d\uddfa\ufe0f \u5730\u70b9: \u4e0a\u6d77\u5e02, \u4e2d\u56fd\\n\ud83d\udcca \u5750\u6807: 31.2304, 121.4737\\n\\n\ud83c\udf24\ufe0f **\u5929\u6c14\u4fe1\u606f:**\\n\ud83c\udf21\ufe0f \u6e29\u5ea6: 8.6\u00b0C\\n\u2601\ufe0f \u5929\u6c14: \u9634\u5929\\n\ud83d\udca8 \u98ce\u901f: 12.3 km/h\\n\ud83e\udded \u98ce\u5411: 93\u00b0\\n\ud83d\udca7 \u6e7f\u5ea6: 77%\\n\ud83d\udcca \u6c14\u538b: 1026.7 hPa\\n\ud83d\udd50 \u65f6\u95f4: 2026-01-25T23:00\\n\\n\ud83d\udce1 \u6570\u636e\u6e90: Open-Meteo (\u514d\u8d39API)', annotations=None, meta=None)] structuredContent=None isError=False\n[ReCentAutoma]-[ReCentAutoma-171bd997] [__dynamic_step__] driving [observe]\n[ReCentAutoma]-[ReCentAutoma-171bd997] [collect_results-&lt;07038e9e&gt;] triggers [observe]\n[ReCentAutoma]-[ReCentAutoma-171bd997] \ud83d\udc40 Observation\n    Iteration: 3\n    Achieved: True\n    Thinking: The current goal was to get the weather information for Shanghai. The weather data has now been successfully retrieved, including temperature, weather condition, wind speed, humidity, and pressure. There are no remaining gaps as the goal has been fully achieved with all necessary details provided.\n[ReCentAutoma]-[ReCentAutoma-171bd997] [__dynamic_step__] driving [finalize_answer]\n[ReCentAutoma]-[ReCentAutoma-171bd997] [observe] triggers [finalize_answer]\n[ReCentAutoma]-[ReCentAutoma-171bd997] is finished.\n### Current Weather in Shanghai\n\n- **Location:** Shanghai, China\n- **Coordinates:** 31.2304\u00b0 N, 121.4737\u00b0 E\n\n#### Weather Information:\n- **Temperature:** 8.6\u00b0C\n- **Condition:** Overcast\n- **Wind Speed:** 12.3 km/h\n- **Wind Direction:** 93\u00b0 (East)\n- **Humidity:** 77%\n- **Pressure:** 1026.7 hPa\n\n#### Data Source:\n- The information is sourced from Open-Meteo (free API).\n\n### Summary\nThe current weather in Shanghai indicates an overcast day with a temperature of 8.6\u00b0C, moderate winds from the east, and high humidity levels.\n</pre> In\u00a0[7]: Copied! <pre>import json\n\nfrom bridgic.protocols.mcp import McpPromptTemplate\n\nprompts: list[McpPromptTemplate] = github_connection.list_prompts()\n\nfor prompt in prompts:\n    description = prompt.prompt_info.description\n    arguments = [f\"[required={arg.required}] {arg.name}: {arg.description}\" for arg in prompt.prompt_info.arguments]\n    print(\n        f\"name: {prompt.prompt_name}:\\n\"\n        f\"description: {description}\\n\"\n        f\"parameters: {json.dumps(arguments, indent=2)}\\n\"\n    )\n</pre> import json  from bridgic.protocols.mcp import McpPromptTemplate  prompts: list[McpPromptTemplate] = github_connection.list_prompts()  for prompt in prompts:     description = prompt.prompt_info.description     arguments = [f\"[required={arg.required}] {arg.name}: {arg.description}\" for arg in prompt.prompt_info.arguments]     print(         f\"name: {prompt.prompt_name}:\\n\"         f\"description: {description}\\n\"         f\"parameters: {json.dumps(arguments, indent=2)}\\n\"     ) <pre>name: AssignCodingAgent:\ndescription: Assign GitHub Coding Agent to multiple tasks in a GitHub repository.\nparameters: [\n  \"[required=True] repo: The repository to assign tasks in (owner/repo).\"\n]\n\nname: issue_to_fix_workflow:\ndescription: Create an issue for a problem and then generate a pull request to fix it\nparameters: [\n  \"[required=True] owner: Repository owner\",\n  \"[required=True] repo: Repository name\",\n  \"[required=True] title: Issue title\",\n  \"[required=True] description: Issue description\",\n  \"[required=None] labels: Comma-separated list of labels to apply (optional)\",\n  \"[required=None] assignees: Comma-separated list of assignees (optional)\"\n]\n\n</pre> <p>We now know that there is a prompt template named \"issue_to_fix_workflow\" available. This template is designed to generate instructions for LLM to help it to use tools to create an issue and a pull request on GitHub. It requires the following parameters: <code>owner</code>, <code>repo</code>, <code>title</code>, and <code>description</code>.</p> <p>Let's fill in these arguments to render the prompt:</p> In\u00a0[8]: Copied! <pre>fix_issue_template = next(p for p in prompts if p.prompt_name == \"issue_to_fix_workflow\")\n\nmessages = fix_issue_template.format_messages(\n    owner=\"somebody\",\n    repo=\"awesome-project\",\n    title=\"A New bug\",\n    description=\"The bug is really annoying and it have to be fixed.\",\n)\nprint(messages)\n</pre> fix_issue_template = next(p for p in prompts if p.prompt_name == \"issue_to_fix_workflow\")  messages = fix_issue_template.format_messages(     owner=\"somebody\",     repo=\"awesome-project\",     title=\"A New bug\",     description=\"The bug is really annoying and it have to be fixed.\", ) print(messages) <pre>[Message(role=&lt;Role.USER: 'user'&gt;, blocks=[TextBlock(block_type='text', text='You are a development workflow assistant helping to create GitHub issues and generate corresponding pull requests to fix them. You should: 1) Create a well-structured issue with clear problem description, 2) Assign it to Copilot coding agent to generate a solution, and 3) Monitor the PR creation process.')], extras={}), Message(role=&lt;Role.USER: 'user'&gt;, blocks=[TextBlock(block_type='text', text=\"I need to create an issue titled 'A New bug' in somebody/awesome-project and then have a PR generated to fix it. The issue description is: The bug is really annoying and it have to be fixed.\")], extras={}), Message(role=&lt;Role.AI: 'assistant'&gt;, blocks=[TextBlock(block_type='text', text=\"I'll help you create the issue 'A New bug' in somebody/awesome-project and then coordinate with Copilot to generate a fix. Let me start by creating the issue with the provided details.\")], extras={}), Message(role=&lt;Role.USER: 'user'&gt;, blocks=[TextBlock(block_type='text', text='Perfect! Please:\\n1. Create the issue with the title, description, labels, and assignees\\n2. Once created, assign it to Copilot coding agent to generate a solution\\n3. Monitor the process and let me know when the PR is ready for review')], extras={}), Message(role=&lt;Role.AI: 'assistant'&gt;, blocks=[TextBlock(block_type='text', text=\"Excellent plan! Here's what I'll do:\\n\\n1. \u2705 Create the issue with all specified details\\n2. \ud83e\udd16 Assign to Copilot coding agent for automated fix\\n3. \ud83d\udccb Monitor progress and notify when PR is created\\n4. \ud83d\udd0d Provide PR details for your review\\n\\nLet me start by creating the issue.\")], extras={})]\n</pre> <p>The rendered messages can now be used with your LLM. You can pass them directly to the LLM's chat method or tool-selection method, or even use them as part of a larger conversation context.</p> <p>The following example shows</p> In\u00a0[9]: Copied! <pre># Convert MCP tool specifications to standard tool objects\nmodel_tools = [tool.to_tool() for tool in github_connection.list_tools()]\n\n# Use the rendered messages to help select tool(s)\ntool_calls, _ = await llm.aselect_tool(\n    messages=messages,\n    tools=model_tools,\n)\nprint(tool_calls)\n</pre> # Convert MCP tool specifications to standard tool objects model_tools = [tool.to_tool() for tool in github_connection.list_tools()]  # Use the rendered messages to help select tool(s) tool_calls, _ = await llm.aselect_tool(     messages=messages,     tools=model_tools, ) print(tool_calls)   <pre>[ToolCall(id='tool_7d890ee2619842b0add42624e', name='issue_write', arguments={'title': 'A New bug', 'body': 'The bug is really annoying and it have to be fixed.', 'method': 'create', 'owner': 'somebody', 'repo': 'awesome-project'})]\n</pre> In\u00a0[\u00a0]: Copied! <pre>import os\nimport tempfile\n\nfrom bridgic.protocols.mcp import (\n    McpServerConnectionStdio,\n    McpServerConnectionManager,\n)\n\ntemp_dir = os.path.realpath(tempfile.mkdtemp())\n\n# Create a file with written content\nwith open(os.path.join(temp_dir, \"dream.txt\"), \"w\", encoding=\"utf-8\") as f:\n    f.write(\"Bridging Logic and Magic\")\n\ncli_connection = McpServerConnectionStdio(\n    name=\"connection-cli-stdio\",\n    command=\"uvx\",\n    args=[\"cli-mcp-server\"],\n    env={\n        \"ALLOWED_DIR\": temp_dir,\n        \"ALLOWED_COMMANDS\": \"ls,cat,wc,pwd,echo\",\n        \"ALLOWED_FLAGS\": \"all\",\n        \"ALLOW_SHELL_OPERATORS\": \"true\",\n    },\n)\n\nplaywright_connection = McpServerConnectionStdio(\n    name=\"connection-playwright-stdio\",\n    command=\"npx\",\n    args=[\n        \"@playwright/mcp@latest\",\n    ],\n    request_timeout=60,\n)\n\n# Register the two connection in different connection manager\n# In this way, their operations will never block each others\nMcpServerConnectionManager.get_instance(\"terminal-use\").register_connection(cli_connection)\nMcpServerConnectionManager.get_instance(\"browser-use\").register_connection(playwright_connection)\n\n# Note: registration have be done before calling `connect()` method\ncli_connection.connect()\nplaywright_connection.connect()\n\n# Retrieve connections by their names\nprint(\"Cli MCP server connected:\", McpServerConnectionManager.get_connection(\"connection-cli-stdio\").is_connected)\nprint(\"Playwright MCP server connected:\", McpServerConnectionManager.get_connection(\"connection-playwright-stdio\").is_connected)\n</pre> import os import tempfile  from bridgic.protocols.mcp import (     McpServerConnectionStdio,     McpServerConnectionManager, )  temp_dir = os.path.realpath(tempfile.mkdtemp())  # Create a file with written content with open(os.path.join(temp_dir, \"dream.txt\"), \"w\", encoding=\"utf-8\") as f:     f.write(\"Bridging Logic and Magic\")  cli_connection = McpServerConnectionStdio(     name=\"connection-cli-stdio\",     command=\"uvx\",     args=[\"cli-mcp-server\"],     env={         \"ALLOWED_DIR\": temp_dir,         \"ALLOWED_COMMANDS\": \"ls,cat,wc,pwd,echo\",         \"ALLOWED_FLAGS\": \"all\",         \"ALLOW_SHELL_OPERATORS\": \"true\",     }, )  playwright_connection = McpServerConnectionStdio(     name=\"connection-playwright-stdio\",     command=\"npx\",     args=[         \"@playwright/mcp@latest\",     ],     request_timeout=60, )  # Register the two connection in different connection manager # In this way, their operations will never block each others McpServerConnectionManager.get_instance(\"terminal-use\").register_connection(cli_connection) McpServerConnectionManager.get_instance(\"browser-use\").register_connection(playwright_connection)  # Note: registration have be done before calling `connect()` method cli_connection.connect() playwright_connection.connect()  # Retrieve connections by their names print(\"Cli MCP server connected:\", McpServerConnectionManager.get_connection(\"connection-cli-stdio\").is_connected) print(\"Playwright MCP server connected:\", McpServerConnectionManager.get_connection(\"connection-playwright-stdio\").is_connected) <pre>Cli MCP server connected: True\nPlaywright MCP server connected: True\n</pre> In\u00a0[12]: Copied! <pre>import uuid\n\nfrom bridgic.core.automa import GraphAutoma, worker, RunningOptions\nfrom bridgic.core.automa.interaction import Event, InteractionFeedback, InteractionException\nfrom bridgic.core.utils._console import printer\n\n# One MCP connection across multiple interrupt\u2013resume cycles. Specifically, \n# calling interact_with_human() pauses the automa and alling arun(feedback_data=...) \n# resumes it. The same connection (established in the previous cell) is reused\n# on every turn to run the CLI tool.\n\n# Define an Automa which supports human-interaction\nclass CliAutoma(GraphAutoma):\n    @worker(is_start=True)\n    def start(self):\n        printer.print(f\"Welcome to the example CLI Automa.\", color=\"gray\")\n        self.ferry_to(\"human_input\")\n\n    @worker()\n    def human_input(self):\n        # Interrupt\u2013resume:\n        # - on first run this pauses (raising InteractionException);\n        # - on resume we receive feedback (the human command) and continue.\n        event = Event(event_type=\"get_human_command\")\n        feedback: InteractionFeedback = self.interact_with_human(event)\n        human_command = feedback.data\n\n        printer.print(f\"&gt; {human_command}\")\n\n        if human_command in [\"quit\", \"exit\"]:\n            self.ferry_to(\"end\")\n        else:\n            tool_key = f\"tool-&lt;{uuid.uuid4().hex[:8]}&gt;\"\n            collect_key = f\"collect-&lt;{uuid.uuid4().hex[:8]}&gt;\"\n\n            async def _collect_command_result(command_result: mcp.types.CallToolResult):\n                printer.print(f\"{command_result.content[0].text.strip()}\\n\", color=\"gray\")\n                self.ferry_to(\"human_input\")\n\n            # Reuse the same connection across all interrupt\u2013resume cycles.\n            # It was established once (previous cell) and stays open.\n            # Each turn we fetch it here and it outlives cycle of running.\n            real_connection = McpServerConnectionManager.get_connection(\"connection-cli-stdio\")\n\n            # Filter the \"run_command\" tool spec from cli-mcp-server.\n            command_tool = next(t for t in real_connection.list_tools() if t.tool_name == \"run_command\")\n\n            # Use the tool specification to create worker instance and then add it dynamically.\n            self.add_worker(tool_key, command_tool.create_worker())\n            self.add_func_as_worker(collect_key, _collect_command_result, dependencies=[tool_key])\n            self.ferry_to(tool_key, command=human_command)\n\n    @worker(is_output=True)\n    def end(self):\n        printer.print(f\"See you again.\\n\", color=\"gray\")\n\nhi_automa = CliAutoma(name=\"human-interaction-automa\", running_options=RunningOptions(debug=False))\n\ninteraction_id = None\ninteraction_feedback = None\n\nasync def continue_automa(feedback_data = None) -&gt; str:\n    try:\n        await hi_automa.arun(feedback_data=feedback_data)\n    except InteractionException as e:\n        interaction_id = e.interactions[0].interaction_id\n        return interaction_id\n\n# First run: automa reaches human_input, calls interact_with_human, pauses (InteractionException).\n# We obtain interaction_id for the next resume.\ninteraction_id = await continue_automa()\n\n# Each iteration we send the human command as feedback to resume the execution.\ncommands = [\n    \"pwd\",\n    \"ls -l\",\n    \"wc -l dream.txt\",\n    \"cat dream.txt\",\n    \"exit\",\n]\nfor command in commands:\n    interaction_feedback = InteractionFeedback(\n        interaction_id=interaction_id,\n        data=command\n    )\n    interaction_id = await continue_automa(interaction_feedback)\n</pre> import uuid  from bridgic.core.automa import GraphAutoma, worker, RunningOptions from bridgic.core.automa.interaction import Event, InteractionFeedback, InteractionException from bridgic.core.utils._console import printer  # One MCP connection across multiple interrupt\u2013resume cycles. Specifically,  # calling interact_with_human() pauses the automa and alling arun(feedback_data=...)  # resumes it. The same connection (established in the previous cell) is reused # on every turn to run the CLI tool.  # Define an Automa which supports human-interaction class CliAutoma(GraphAutoma):     @worker(is_start=True)     def start(self):         printer.print(f\"Welcome to the example CLI Automa.\", color=\"gray\")         self.ferry_to(\"human_input\")      @worker()     def human_input(self):         # Interrupt\u2013resume:         # - on first run this pauses (raising InteractionException);         # - on resume we receive feedback (the human command) and continue.         event = Event(event_type=\"get_human_command\")         feedback: InteractionFeedback = self.interact_with_human(event)         human_command = feedback.data          printer.print(f\"&gt; {human_command}\")          if human_command in [\"quit\", \"exit\"]:             self.ferry_to(\"end\")         else:             tool_key = f\"tool-&lt;{uuid.uuid4().hex[:8]}&gt;\"             collect_key = f\"collect-&lt;{uuid.uuid4().hex[:8]}&gt;\"              async def _collect_command_result(command_result: mcp.types.CallToolResult):                 printer.print(f\"{command_result.content[0].text.strip()}\\n\", color=\"gray\")                 self.ferry_to(\"human_input\")              # Reuse the same connection across all interrupt\u2013resume cycles.             # It was established once (previous cell) and stays open.             # Each turn we fetch it here and it outlives cycle of running.             real_connection = McpServerConnectionManager.get_connection(\"connection-cli-stdio\")              # Filter the \"run_command\" tool spec from cli-mcp-server.             command_tool = next(t for t in real_connection.list_tools() if t.tool_name == \"run_command\")              # Use the tool specification to create worker instance and then add it dynamically.             self.add_worker(tool_key, command_tool.create_worker())             self.add_func_as_worker(collect_key, _collect_command_result, dependencies=[tool_key])             self.ferry_to(tool_key, command=human_command)      @worker(is_output=True)     def end(self):         printer.print(f\"See you again.\\n\", color=\"gray\")  hi_automa = CliAutoma(name=\"human-interaction-automa\", running_options=RunningOptions(debug=False))  interaction_id = None interaction_feedback = None  async def continue_automa(feedback_data = None) -&gt; str:     try:         await hi_automa.arun(feedback_data=feedback_data)     except InteractionException as e:         interaction_id = e.interactions[0].interaction_id         return interaction_id  # First run: automa reaches human_input, calls interact_with_human, pauses (InteractionException). # We obtain interaction_id for the next resume. interaction_id = await continue_automa()  # Each iteration we send the human command as feedback to resume the execution. commands = [     \"pwd\",     \"ls -l\",     \"wc -l dream.txt\",     \"cat dream.txt\",     \"exit\", ] for command in commands:     interaction_feedback = InteractionFeedback(         interaction_id=interaction_id,         data=command     )     interaction_id = await continue_automa(interaction_feedback) <pre>Welcome to the example CLI Automa.\n&gt; pwd\n/private/var/folders/9t/5r9fms9s5q33p6xty_0_k1mw0000gn/T/tmpr7ghhwn0\n\n&gt; ls -l\ntotal 8\n-rw-r--r--  1 xushili  staff  24 Jan 25 23:09 dream.txt\n\n&gt; wc -l dream.txt\n0 dream.txt\n\n&gt; cat dream.txt\nBridging Logic and Magic\n\n&gt; exit\nSee you again.\n\n</pre> <p>Before shutting down your application, make sure to properly close all your connections. Finally, let's close all the connections we've created to conclude this tutorial:</p> In\u00a0[16]: Copied! <pre>filesystem_connection.close()\nweather_connection.close()\ngithub_connection.close()\ncli_connection.close()\nplaywright_connection.close()\n\nall_closed = all([\n    not filesystem_connection.is_connected,\n    not weather_connection.is_connected,\n    not github_connection.is_connected,\n    not cli_connection.is_connected,\n    not playwright_connection.is_connected,\n])\nprint(\"All connections closed:\", all_closed)\n</pre> filesystem_connection.close() weather_connection.close() github_connection.close() cli_connection.close() playwright_connection.close()  all_closed = all([     not filesystem_connection.is_connected,     not weather_connection.is_connected,     not github_connection.is_connected,     not cli_connection.is_connected,     not playwright_connection.is_connected, ]) print(\"All connections closed:\", all_closed) <pre>All connections closed: True\n</pre>"},{"location":"tutorials/items/protocol_integration/mcp_quick_start/#mcp","title":"MCP\u00b6","text":"<p>This tutorial will demonstrate how to integrate Model Context Protocol (MCP) with Bridgic to enhance your development in building agentic applications.</p>"},{"location":"tutorials/items/protocol_integration/mcp_quick_start/#introduction","title":"Introduction\u00b6","text":"<p>Model Context Protocol (MCP) enables AI applications to access external resources and tools. By integrating MCP with Bridgic, you can:</p> <ul> <li>Connect to MCP Servers: Access a wide range of external services and resources through standardized MCP servers</li> <li>Get and Use MCP Tools: Leverage tools provided by MCP servers as workers in your Bridgic workflows</li> <li>Get and Use MCP Prompts: Utilize pre-configured prompt templates from MCP servers</li> <li>Greatly Enhance your Agentic Module: Enable LLM-driven agents to autonomously select and use MCP tools</li> </ul> <p>This tutorial will walk you through the essentials of integrating MCP with Bridgic, from basic installation to advanced usage, along with easy-to-understand examples.</p>"},{"location":"tutorials/items/protocol_integration/mcp_quick_start/#installation","title":"Installation\u00b6","text":"<p>First, install the <code>bridgic-protocols-mcp</code> package. Since the MCP Python SDK requires Python 3.12 or newer. Please ensure you are using a compatible Python version before installation.</p>"},{"location":"tutorials/items/protocol_integration/mcp_quick_start/#basic-usage","title":"Basic Usage\u00b6","text":""},{"location":"tutorials/items/protocol_integration/mcp_quick_start/#connecting-to-an-mcp-server","title":"Connecting to an MCP Server\u00b6","text":"<p>MCP servers can be connected via different transport. The most common is stdio transport, which runs the MCP server as a subprocess. Let's connect to a filesystem MCP server as an example and see what tools it offers:</p>"},{"location":"tutorials/items/protocol_integration/mcp_quick_start/#using-a-mcp-tool-as-a-worker","title":"Using a MCP Tool as a Worker\u00b6","text":"<p>MCP tools can be converted to Bridgic workers and integrated into <code>GraphAutoma</code> building. This allows you to orchestrate MCP tool calls alongside other workers in your application.</p> <p>Why don't we just run the tool directly, but instead execute it as a worker? In Bridgic's view, every execution process in a workflow program (or an even more agentic system) can be decomposed into fine-grained workers, which can then be orchestrated and scheduled. Standardizing the execution process in this way simplifies development and debugging, and enhances observability during execution. Currently, too many frameworks separate agent operation from programmable orchestration, causing tools and developer-defined work units to hold unequal positions. This leads to two very different development and debugging experiences.</p> <p>In Bridgic, tools have distinct specifications, but all tool executions are carried out by converting them into workers. This approach standardizes different kinds of tools as uniform and orchestratable units, facilitating their integration and scheduling alongside other workers to accomplish more complex tasks.</p> <p>Let's create a simple workflow that uses MCP tools to read and write files:</p>"},{"location":"tutorials/items/protocol_integration/mcp_quick_start/#using-mcp-tools-in-an-agentic-automa","title":"Using MCP Tools in an agentic Automa\u00b6","text":"<p>In more scenarios, you may prefer to use an LLM-powered automa that can determine which MCP tools to utilize, adapting its choices to a specific goal and the evolving context during execution.</p> <p><code>ReCentAutoma</code> is such an agentic automa. By passing MCP tools into it, you can:</p> <ul> <li>Keep the orchestration logic in Bridgic while delegating decisions to the LLM.</li> <li>Let the LLM select appropriate tools at each step</li> <li>Collect the results of tool calls and incorporate them as part of a dynamic process</li> </ul> <p>Below is a minimal example that uses the weather MCP tools inside a <code>ReCentAutoma</code>.</p> <p>First of all, you have to connect to an MCP server that contains the weather tool.</p>"},{"location":"tutorials/items/protocol_integration/mcp_quick_start/#using-mcp-prompts-to-render-your-context","title":"Using MCP Prompts to render your context\u00b6","text":"<p>MCP servers can also provide prompt templates that can be used to render context for your LLM applications. These prompts are useful for standardizing how you format messages before sending them to an LLM.</p> <p>You can check the available prompt templates from the server by running:</p>"},{"location":"tutorials/items/protocol_integration/mcp_quick_start/#advanced-usage","title":"Advanced Usage\u00b6","text":""},{"location":"tutorials/items/protocol_integration/mcp_quick_start/#multiple-server-connection-management","title":"Multiple Server Connection Management\u00b6","text":"<p>When building complex applications, you may need to connect to multiple MCP servers simultaneously. Bridgic provides <code>McpServerConnectionManager</code> to help you manage multiple connections efficiently.</p> <p>A connection manager:</p> <ul> <li>Shares a common event loop across for the connections registered in it</li> <li>Handles the lifecycle of connections within the same event loop</li> <li>Allows to retrieve any connection by its name across your application</li> </ul> <p>When you call the <code>connect()</code> method on a connection, it is actually automatically registered with the default manager. All operations on an MCP server connection, such as <code>list_tools()</code>, <code>list_prompt()</code> or their asynchronous peers, are internally managed by the connection manager.</p> <p>If you want more control, you can explicitly choose which manager to register your connection(s) with by calling the <code>register_connection()</code>. This is particularly useful when it's necessary to isolate connections and their operations of certain MCP servers that contain some time-consuming tools. The execution isolation is at the thread level.</p> <p>For example, browser and terminal usage are relatively time-consuming, so it's necessary to use a separate manager for connection management to prevent their execution from blocking the use of other MCP tools.</p> <p>The following example demonstrates:</p> <ol> <li>Connecting to both a Cli MCP server and a Playwright MCP server simultaneously.</li> <li>Assigning each connection to a separate manager to keep their operations isolated.</li> </ol>"},{"location":"tutorials/items/protocol_integration/mcp_quick_start/#pay-attention-to-the-connection-lifecycle","title":"Pay attention to the Connection Lifecycle\u00b6","text":"<p>The lifecycle of an MCP server connection is independent from the execution of an automa: neither <code>interact_with_human()</code> (which pauses and raises <code>InteractionException</code>) nor <code>arun()</code> / <code>arun(feedback_data=...)</code> (which runs or resumes the automa) affects the connection. Once a connection is established and managed by a connection manager, it remains open until you close it.</p> <p>A practical implication is that one connection can serve many executions, which is important for the development of application. The automa may pause at <code>interact_with_human()</code> and be resumed later with <code>arun(feedback_data=...)</code>; each cycle can use MCP tools over the same connection without reconnecting.</p> <p>The following example demonstrates a simple CLI loop: in each turn, the automa requests a human command (interrupt), the application provides the command as feedback (resume), the automa executes the CLI MCP tool, and then requests for the next command\u2014repeating this process to simulate user's multi-turn input. Across all these turns, the connection to the CLI MCP server is created only once (in the previous cell) and reused each time.</p> <p>Please note that this example specifically simulates multi-turn human\u2013computer interactions by mimicking user command input; in real-world development, developers are free to customize their own human-in-the-loop interaction flow as needed.</p>"},{"location":"tutorials/items/quick_start/quick_start/","title":"Quick Start","text":"<p>Execute the following commands in the shell:</p> <pre>export OPENAI_API_KEY=\"&lt;your_openai_api_key&gt;\"\nexport OPENAI_MODEL_NAME=\"&lt;the_model_name&gt;\"\n</pre> In\u00a0[\u00a0]: Copied! <pre># Get the environment variables.\nimport os\n\n# Import the necessary packages.\nfrom bridgic.core.automa import GraphAutoma, worker\nfrom bridgic.core.model.types import Message, Role\n\n# Here we use OpenAILikeLlm because the package `bridgic-llms-openai-like` is installed automatically \n# when you install Bridgic. This makes sure the OpenAI-like model integration works out of the box.\nfrom bridgic.llms.openai_like import OpenAILikeLlm, OpenAILikeConfiguration\n\n\n# In this tutorial, we use OpenAI as an example. \n# You can freely replace these model settings to use any LLM provider you like.\n_api_key = os.environ.get(\"OPENAI_API_KEY\")\n_api_base = os.environ.get(\"OPENAI_API_BASE\")\n_model_name = os.environ.get(\"OPENAI_MODEL_NAME\")\n\nllm = OpenAILikeLlm(\n    api_key=_api_key,\n    api_base=_api_base,\n    configuration=OpenAILikeConfiguration(model=_model_name),\n    timeout=20,\n)\n</pre> # Get the environment variables. import os  # Import the necessary packages. from bridgic.core.automa import GraphAutoma, worker from bridgic.core.model.types import Message, Role  # Here we use OpenAILikeLlm because the package `bridgic-llms-openai-like` is installed automatically  # when you install Bridgic. This makes sure the OpenAI-like model integration works out of the box. from bridgic.llms.openai_like import OpenAILikeLlm, OpenAILikeConfiguration   # In this tutorial, we use OpenAI as an example.  # You can freely replace these model settings to use any LLM provider you like. _api_key = os.environ.get(\"OPENAI_API_KEY\") _api_base = os.environ.get(\"OPENAI_API_BASE\") _model_name = os.environ.get(\"OPENAI_MODEL_NAME\")  llm = OpenAILikeLlm(     api_key=_api_key,     api_base=_api_base,     configuration=OpenAILikeConfiguration(model=_model_name),     timeout=20, ) In\u00a0[19]: Copied! <pre>class WordLearningAssistant(GraphAutoma):\n    @worker(is_start=True)\n    async def generate_derivatives(self, word: str):\n        print(f\"------Generating derivatives for {word}------\")\n        response = await llm.achat(\n            model=_model_name,\n            messages=[\n                Message.from_text(text=\"You are a word learning assistant. Generate derivatives of the input word in a list.\", role=Role.SYSTEM),\n                Message.from_text(text=word, role=Role.USER),\n            ]\n        )\n        print(response.message.content)\n        print(f\"------End of generating derivatives------\\n\")\n        return response.message.content\n\n    @worker(dependencies=[\"generate_derivatives\"], is_output=True)\n    async def make_sentences(self, derivatives):\n        print(f\"------Making sentences with------\")\n        response = await llm.achat(\n            model=_model_name,\n            messages=[\n                Message.from_text(text=\"You are a word learning assistant. Make sentences with the input derivatives in a list.\", role=Role.SYSTEM),\n                Message.from_text(text=derivatives, role=Role.USER),\n            ]\n        )\n        print(response.message.content)\n        print(f\"------End of making sentences------\\n\")\n        return response.message.content\n\nword_learning_assistant = WordLearningAssistant()\n</pre> class WordLearningAssistant(GraphAutoma):     @worker(is_start=True)     async def generate_derivatives(self, word: str):         print(f\"------Generating derivatives for {word}------\")         response = await llm.achat(             model=_model_name,             messages=[                 Message.from_text(text=\"You are a word learning assistant. Generate derivatives of the input word in a list.\", role=Role.SYSTEM),                 Message.from_text(text=word, role=Role.USER),             ]         )         print(response.message.content)         print(f\"------End of generating derivatives------\\n\")         return response.message.content      @worker(dependencies=[\"generate_derivatives\"], is_output=True)     async def make_sentences(self, derivatives):         print(f\"------Making sentences with------\")         response = await llm.achat(             model=_model_name,             messages=[                 Message.from_text(text=\"You are a word learning assistant. Make sentences with the input derivatives in a list.\", role=Role.SYSTEM),                 Message.from_text(text=derivatives, role=Role.USER),             ]         )         print(response.message.content)         print(f\"------End of making sentences------\\n\")         return response.message.content  word_learning_assistant = WordLearningAssistant() In\u00a0[20]: Copied! <pre>res = await word_learning_assistant.arun(word=\"happy\")\n</pre> res = await word_learning_assistant.arun(word=\"happy\") <pre>------Generating derivatives for happy------\nHere are some derivatives of the word \"happy\":\n\n1. Happiness\n2. Happily\n3. Happier\n4. Happiest\n5. Unhappy\n6. Unhappiness\n7. Happinesses (plural form)\n8. Happifying (gerund form)\n9. Happify (verb form)\n\nFeel free to ask for derivatives of another word!\n------End of generating derivatives------\n\n------Making sentences with------\nSure! Here are sentences using each of the derivatives of the word \"happy\":\n\n1. **Happiness**: The pursuit of happiness is a common goal for many people.\n2. **Happily**: She smiled happily as she opened her birthday gifts.\n3. **Happier**: After taking a vacation, I felt much happier than I had in months.\n4. **Happiest**: That day was the happiest moment of my life when my daughter graduated.\n5. **Unhappy**: He seemed unhappy at the party and left early.\n6. **Unhappiness**: Her unhappiness was evident in her quiet demeanor.\n7. **Happinesses**: Different people find happinesses in various aspects of life, like family, work, and hobbies.\n8. **Happifying**: The act of volunteering can be a happifying experience for both the giver and the receiver.\n9. **Happify**: Listening to uplifting music can help to happify your day.\n\nLet me know if you need sentences for another word!\n------End of making sentences------\n\n</pre> <p>Congratulations! We have successfully completed the word learning assistant, which performed the task exactly according to our requirements.</p> In\u00a0[\u00a0]: Copied! <pre>class MyAutoma(GraphAutoma):\n    @worker(is_start=True)\n    async def start(self, x: int):\n        return x    \n</pre> class MyAutoma(GraphAutoma):     @worker(is_start=True)     async def start(self, x: int):         return x     <p>Or, you can also use it like this:</p> In\u00a0[\u00a0]: Copied! <pre>class MyAutoma(GraphAutoma): ...\nmy_automa = MyAutoma()\n\n# Add the function as a worker with worker decorator in the instance of the automa\n@my_automa.worker(is_start=True)\nasync def start(x: int):\n    return x\n</pre> class MyAutoma(GraphAutoma): ... my_automa = MyAutoma()  # Add the function as a worker with worker decorator in the instance of the automa @my_automa.worker(is_start=True) async def start(x: int):     return x  <p>Another API <code>add_func_as_worker()</code> can also be used to add workers into a <code>GraphAutoma</code>.</p> In\u00a0[\u00a0]: Copied! <pre>async def start(x: int):\n    return x\n\nclass MyAutoma(GraphAutoma): ...\nmy_automa = MyAutoma()\n\n# Add the function as a worker\nmy_automa.add_func_as_worker(\n    key=\"start\",\n    func=start,\n    is_start=True,\n)\n</pre> async def start(x: int):     return x  class MyAutoma(GraphAutoma): ... my_automa = MyAutoma()  # Add the function as a worker my_automa.add_func_as_worker(     key=\"start\",     func=start,     is_start=True, )  <p>In addition to functions being convertible to workers, subclasses that inherit from <code>Worker</code> and override either <code>run()</code> or <code>arun()</code> can also be used directly as workers, whose instances can be added into a <code>GraphAutoma</code> by the <code>add_worker()</code> API.</p> In\u00a0[\u00a0]: Copied! <pre>from bridgic.core.automa.worker import Worker\n\nclass MyWorker(Worker):\n    async def arun(self, x: int):\n        return x\n\nmy_worker = MyWorker()\n\n# Add the worker to the automa\nclass MyAutoma(GraphAutoma): ...\nmy_automa = MyAutoma()\nmy_automa.add_worker(\n    key=\"my_worker\",\n    worker=my_worker,\n    is_start=True,\n)\n\n# Run the worker\nres = await my_automa.arun(x=1)\nprint(res)\n</pre> from bridgic.core.automa.worker import Worker  class MyWorker(Worker):     async def arun(self, x: int):         return x  my_worker = MyWorker()  # Add the worker to the automa class MyAutoma(GraphAutoma): ... my_automa = MyAutoma() my_automa.add_worker(     key=\"my_worker\",     worker=my_worker,     is_start=True, )  # Run the worker res = await my_automa.arun(x=1) print(res) <p>Note:</p> <ol> <li>A specific worker that inherits from <code>Worker</code> must override either the <code>run()</code> or <code>arun()</code> method.</li> <li>Bridgic is a framework primarily designed for asynchronous execution, if both <code>run()</code> and <code>arun()</code> of a worker are overridden, <code>arun()</code> will take precedence. Refer to <code>Worker</code> for details.</li> </ol> <p>In any of these ways the workers can be correctly added into <code>MyAutoma</code>.</p> <p>Whether using decorator syntax or the corresponding API, there are usually some parameters:</p> <ol> <li><code>key</code>: A string used as the worker key. As the unique identifier of a worker in the current automa, it must be ensured that there are no duplicate keys within the same automa. Function or class names are used by default.</li> <li><code>func</code>(in <code>add_func_as_worker()</code>) or <code>worker</code>(in <code>add_worker()</code>): The actual callable object. The decorator syntax does not need this parameter.</li> <li><code>is_start</code>: <code>True</code> or <code>False</code>. Marking the worker as the start worker of the automa. It can be set for multiple workers.</li> <li><code>dependencies</code>: A list of worker keys. Marking the preceding workers that the worker depends on.</li> <li><code>is_output</code>: <code>True</code> or <code>False</code>. Marking the worker as the output worker of the automa. Only one output worker can be set per execution branch.</li> <li><code>args_mapping_rule</code>: The arguments mapping rule. For detailed information on the parameter binding between workers, please refer to the tutorial: Parameter Binding</li> </ol> <p>Note: In Bridgic, a worker must be added to an automa before it can be scheduled and executed. In another word, you shouldn\u2019t directly call <code>worker.arun()</code> or <code>worker.run()</code> to run a worker.</p> In\u00a0[3]: Copied! <pre># Write workers in MyAutoma\nclass MyAutoma(GraphAutoma):\n    @worker(is_start=True)\n    async def worker_0(self, a, b, x, y):\n        print(f\"worker_0: a={a}, b={b}, x={x}, y={y}\")\n\n    @worker(is_start=True)\n    async def worker_1(self, x, y):\n        print(f\"worker_1: x={x}, y={y}\")\n</pre> # Write workers in MyAutoma class MyAutoma(GraphAutoma):     @worker(is_start=True)     async def worker_0(self, a, b, x, y):         print(f\"worker_0: a={a}, b={b}, x={x}, y={y}\")      @worker(is_start=True)     async def worker_1(self, x, y):         print(f\"worker_1: x={x}, y={y}\") <p>After all the required workers are defined in an automa, the automa can be called with <code>await automa_obj.arun(*args, **kwargs)</code> to start the entire scheduling process.</p> <p>Bridgic is a framework built on asynchronous programming. Thus <code>Graphautoma</code> must be started using arun(). However, workers may execute in concurrency mode when needed.</p> <p>At startup, the arguments of <code>automa_obj.arun(*args, **kwargs)</code> will be distributed to the worker and is as follows:</p> <ul> <li>positional parameters: The positional arguments passed to <code>arun</code> are mapped to the parameters of the workers marked with <code>is_start=True</code>, following the order in which they are provided. An error will be raised if the parameter list of some worker is shorter than the number of positional arguments passed to <code>arun()</code>.</li> <li>keyword parameters: The keyword arguments passed to <code>arun</code> are regarded as propagation arguments. In all workers, as long as the same parameter name is declared in the parameter signature, the corresponding arguments will be obtained in the priority of parameter acceptance.</li> <li>priority: Positional arguments take precedence over keyword arguments(propagation arguments)..</li> </ul> <p>For example: we pass positional arguments <code>1</code> and <code>2</code>, and keyword arguments <code>x=3</code>, <code>y=4</code>.</p> In\u00a0[4]: Copied! <pre>my_automa = MyAutoma()\nawait my_automa.arun(1, 2, x=3, y=4)\n</pre> my_automa = MyAutoma() await my_automa.arun(1, 2, x=3, y=4) <pre>worker_0: a=1, b=2, x=3, y=4\nworker_1: x=1, y=2\n</pre> <p><code>1</code> and <code>2</code> were received in order by the first and second parameters of <code>worker_0</code> and <code>worker_1</code> respectively. Because positional arguments take precedence over keyword arguments, even if the parameter names of <code>worker_1</code> are the same as the input keyword parameters, they will still preferentially receive positional arguments.</p> <p>An error will be raised if the parameter list of some worker is shorter than the number of positional arguments passed to <code>arun</code>.</p> In\u00a0[\u00a0]: Copied! <pre>my_automa = MyAutoma()\nawait my_automa.arun(1, 2, 3, y=4)  # worker_1 raises an error\n</pre> my_automa = MyAutoma() await my_automa.arun(1, 2, 3, y=4)  # worker_1 raises an error  <p>If all arguments are passed in keyword format, each worker will obtain the corresponding arguments based on the Inputs Propagation mechanism of the inputs.</p> In\u00a0[10]: Copied! <pre>my_automa = MyAutoma()\nawait my_automa.arun(a=1, b=2, x=3, y=4)\n</pre> my_automa = MyAutoma() await my_automa.arun(a=1, b=2, x=3, y=4) <pre>worker_0: a=1, b=2, x=3, y=4\nworker_1: x=3, y=4\n</pre> <p>Now, we can start building our Bridgic project!</p>"},{"location":"tutorials/items/quick_start/quick_start/#quick-start","title":"Quick Start\u00b6","text":"<p>In this tutorial, we assume that Bridgic is already installed on your system. If that\u2019s not the case, see Installation.</p> <p>Let's start by building a simple word learning assistant. You provide a word, and the assistant will generate its derived forms and create sentences with them. This example will also show how to use Bridgic in practice.</p>"},{"location":"tutorials/items/quick_start/quick_start/#word-learning-assistant","title":"Word learning assistant\u00b6","text":""},{"location":"tutorials/items/quick_start/quick_start/#1-model-initialization","title":"1. Model Initialization\u00b6","text":"<p>Before getting started, let's set up our environment. In this quick start, we'll use the integration out of the box. For an in-depth explanation of model integration, see: LLM Integration.</p>"},{"location":"tutorials/items/quick_start/quick_start/#2-automa-orchestration","title":"2. Automa Orchestration\u00b6","text":"<p>There are two steps to complete the word learning assistant:</p> <ol> <li>Generate derivatives of the input word.</li> <li>Make sentences with derivatives.</li> </ol>"},{"location":"tutorials/items/quick_start/quick_start/#3-agent-running","title":"3. Agent Running\u00b6","text":"<p>Let's run this assistant, via <code>arun</code> method:</p>"},{"location":"tutorials/items/quick_start/quick_start/#what-have-we-learnt","title":"What have we learnt?\u00b6","text":"<p>The above example idemonstrates a typical way to write an agent application with Bridgic. Let's now explore some of its components.</p>"},{"location":"tutorials/items/quick_start/quick_start/#worker","title":"Worker\u00b6","text":"<p>Any callable object (such as functions, methods, etc.) can be converted into a worker object which serve as the basic execution unit in Bridgic for scheduling and orchestration.</p> <p>Just as in the example of the word learning assistant, we can use a decorator syntax <code>@worker</code> to wrap functions and methods into a worker object.</p>"},{"location":"tutorials/items/quick_start/quick_start/#graphautoma","title":"GraphAutoma\u00b6","text":"<p>An automa is an entity that manages and orchestrates a group of workers, serving as the scheduling engine . In the example of the word learning assistant above, we used the subclass of <code>Automa</code>, i.e. <code>GraphAutoma</code>, which performs the scheduling according to the topological sorting among workers.</p> <p>You should subclass <code>GraphAutoma</code> and declare methods as workers with <code>@worker</code>.</p>"},{"location":"understanding/basics/","title":"Basics","text":"<p>Autonomous agents, powered by modern large language models, represent a major advance in the AI industry. However, when these \"AI agents\" are deployed in real-world production environments, many workflows remain deterministic and are not all that \"agentic\". Therefore, the community has largely reached a consensus to use the term \"agentic system\" to refer to the entire spectrum of AI systems, from deterministic workflows to fully autonomous agents.</p> <p>Bridgic provides a consistent and unified approach for developing all types of agentic systems\u2014no matter how agentic they are. This is precisely why the name \"Bridgic\" was chosen\u2014it bridges logic and magic! The following sections will introduce and explain the fundamental concepts that form the foundation of Bridgic.</p>"},{"location":"understanding/basics/#worker-and-automa","title":"Worker and Automa","text":"<p>Bridgic has two core concepts:</p> <ul> <li>Worker: the basic execution unit in Bridgic.</li> <li>Automa: an entity that manages and orchestrates a group of workers. An automa itself is also a worker, which enables the nesting of automa instances within each other.</li> </ul> <p>A worker is an entity that actually performs tasks. In real-world systems, a worker can represent a precise execution logic, such as a function or an API call, or it can be something highly autonomous, like an agent. In other words, a worker can be any entity capable of carrying out actions, regardless of its level of autonomy.</p> <p>In Bridgic, the <code>Worker</code> class is defined in Python as follows:</p> <pre><code>class Worker:\n    async def arun(self, *args, **kwargs) -&gt; Any:\n        ...\n</code></pre> <p>The <code>arun</code> method of the <code>Worker</code> class is called to execute a task. You can pass any required arguments to <code>arun</code>, and it will return a value as the result of the task.</p> <p>Tips</p> <p>In fact, in addition to the <code>arun</code> method, a <code>Worker</code> also has a <code>run</code> method. This relates to Bridgic's concurrency mode. Please refer to the relevant sections for more details.</p> <p>Besides worker, \"automa\" is another core concept in Bridgic. An automa acts as a container for a group of workers. Instead of performing tasks by itself, an automa schedules and orchestrates the workers it contains, running them according to a predefined or dynamic execution flow in order to accomplish the overall task.</p> <p>In Bridgic, the <code>Automa</code> class is defined in Python as follows:</p> <pre><code>class Automa(Worker):\n        ...\n</code></pre> <p>Note that the <code>Automa</code> class inherits from <code>Worker</code>. This means that every automa is also a worker and can be seamlessly nested within another automa. This design abstraction allows for powerful, modular programming by enabling automa to be composed layer by layer. We will elaborate on this later.</p>"},{"location":"understanding/basics/#graphautoma","title":"GraphAutoma","text":"<p><code>GraphAutoma</code> is a concrete implementation of the automa concept, where workers are organized into a directed graph (DG) with workers as nodes and their relationships as edges.</p>"},{"location":"understanding/basics/#predefined-dependencies","title":"Predefined Dependencies","text":"<p>As shown in the diagram above, the execution order of workers primarily depends on two factors:</p> <ul> <li>The designation of one or more start workers.</li> <li>The predefined dependencies established between workers. They are represented by the solid arrows in the diagram.</li> </ul> <p>The execution flow starts with the start worker (i.e., <code>worker_1</code>), followed by the concurrent execution of <code>worker_2</code> and <code>worker_3</code>, and finally <code>worker_4</code> is executed.</p> <p>Notably, <code>worker_4</code> will be triggered only after both <code>worker_2</code> and <code>worker_3</code> have completed their execution. In other words, within a <code>GraphAutoma</code>, if a worker has multiple predecessor workers, it adopts an \"AND\" execution logic\u2014meaning that the worker will not start until all its direct predecessors have finished.</p> <p>The execution flow shown above can be implemented with the following code:</p> <pre><code>from bridgic.core.automa import GraphAutoma, worker\n\nclass MyFlow(GraphAutoma):\n    @worker(is_start=True)\n    async def worker_1(self):\n        ...\n\n    @worker(dependencies=[\"worker_1\"])\n    async def worker_2(self):\n        ...\n\n    @worker(dependencies=[\"worker_1\"])\n    async def worker_3(self):\n        ...\n\n    @worker(dependencies=[\"worker_2\", \"worker_3\"])\n    async def worker_4(self):\n        ...\n</code></pre> <p>This code uses the <code>@worker</code> decorator to transform a regular Python method into a worker, and uses the <code>dependencies</code> parameter to define the partial order among these workers.</p>"},{"location":"understanding/basics/#dynamic-routing","title":"Dynamic Routing","text":"<p>To support the development of autonomous agents, in addition to the predefined dependencies mentioned above, Bridgic also provides an easy-to-use <code>ferry_to()</code> API for implementing dynamic routing.</p> <p>As illustrated in the diagram above, the dashed arrows represent potential execution paths. After <code>worker_1</code> finishes execution, the orchestrator of <code>GraphAutoma</code> determines at runtime\u2014based on parameters or context\u2014whether to proceed with <code>worker_2</code>, <code>worker_3</code>, or both.</p> <p>Here is the code:</p> <pre><code>from bridgic.core.automa import GraphAutoma, worker\n\nclass MyFlow(GraphAutoma):\n    @worker(is_start=True)\n    async def worker_1(self, x):\n        ...\n        if x &gt; 0:\n            self.ferry_to(\"worker_2\", y=1)\n        else\n            self.ferry_to(\"worker_3\", z=2)\n\n    @worker()\n    async def worker_2(self, y):\n        ...\n\n    @worker()\n    async def worker_3(self, z):\n        ...\n</code></pre> <p>With the <code>ferry_to()</code> mechanism, you can implement dynamic routing in a natural, intuitive way\u2014just like calling a regular function.</p> <p>Tips</p> <p>The name of the <code>ferry_to</code> API comes from the idea of \"ferrying\" between various \"islands\" in a directed graph.</p> <p>With the help of the dynamic routing mechanism provided by <code>ferry_to</code>, you can easily implement looping logic in your workflow. The diagram below illustrates how this works:</p> <p>Code:</p> <pre><code>from bridgic.core.automa import GraphAutoma, worker\n\nclass MyFlow(GraphAutoma):\n    @worker(is_start=True)\n    async def worker_1(self, x):\n        return x\n\n    @worker(dependencies=[\"worker_1\"])\n    async def worker_2(self, x):\n        return x + 1\n\n    @worker(dependencies=[\"worker_2\"])\n    async def worker_3(self, x):\n        if x &lt; 0:\n            self.ferry_to(\"worker_2\", x)\n        else:\n            self.ferry_to(\"worker_4\", x)\n\n    @worker(is_output=True)\n    async def worker_4(self, x):\n        return x\n</code></pre> <p>In this code, <code>worker_3</code> decides whether to run <code>worker_2</code> or <code>worker_4</code> next based on different conditions.</p> <p>It's important to note that when <code>ferry_to(\"worker_2\", x)</code> is called, <code>worker_2</code> will execute immediately in the next event loop iteration, without waiting for its predefined dependency <code>worker_1</code> to complete. This behavior arises from the interplay between dynamic routing and predefined dependencies\u2014an approach that allows Bridgic to seamlessly combine static orchestration with flexible, dynamic control flow.</p>"},{"location":"understanding/basics/#api","title":"API","text":"<p><code>GraphAutoma</code> provides two types of APIs that are used to manage the graph topology:</p> <ul> <li> <p>The core API: <code>add_worker</code>, <code>add_func_as_worker</code>, <code>remove_worker</code>, and <code>add_dependency</code>.</p> </li> <li> <p>The declarative API: <code>@worker</code> decorator.</p> </li> </ul> <p>For more code examples, please refer to the Tutorials section.</p>"},{"location":"understanding/basics/#dynamic-directed-graph","title":"Dynamic Directed Graph","text":"<p><code>GraphAutoma</code> implements a Dynamic Directed Graph (DDG) that orchestrates the execution of its internal workers in an asynchronous and dynamic manner using asyncio.</p> <p>A DDG is a directed graph whose topology can be changed at runtime. Its scheduler divides the orchestration process into several dynamic steps (DS), each executed in a single event loop iteration. At the end of each DS, the scheduler prepares the next set of workers to run based on the predefined dependencies or the dynamic <code>ferry_to</code> calls. Any topology changes triggered by <code>add_worker</code> or <code>remove_worker</code> takes effect in the next DS.</p> <p>Taking the above diagram as an example, the entire execution is divided into three dynamic steps:</p> <ul> <li>DS 1: <code>worker_1</code> is executed.</li> <li>DS 2: both <code>worker_2</code> and <code>worker_3</code> are executed.</li> <li>DS 3: <code>worker_4</code> is executed.</li> </ul>"},{"location":"understanding/basics/#modulirity-and-netsting","title":"Modulirity and Netsting","text":"<p>In Bridgic, an automa itself is also a worker, allowing one automa to be added into another. This design enables the construction of complex agentic systems by reusing components through hierarchical nesting, introducing a new paradigm of modular and component-based programming in agent-based development.</p> <p>For more code examples on modularity, please refer to the \"Modularity\" section in the Tutorials.</p>"},{"location":"understanding/concepts/","title":"Concepts","text":"<p>Bridgic has two core concepts:</p> <ul> <li>Worker: the basic execution unit in Bridgic.</li> <li>Automa: an entity that manages and orchestrates a group of workers. An Automa itself is also a Worker, which enables the nesting of Automa instances within each other.</li> </ul>"},{"location":"understanding/concepts/#worker","title":"Worker","text":"<p>A worker is the basic execution unit in the Bridgic framework, representing a specific task node.</p> <p>A worker typically corresponds to a function (which can be synchronous or asynchronous) that performs an independent business logic or processing step. Workers are automatically linked through <code>dependencies</code> to form a complete workflow. The design of Workers makes task decomposition, reuse, and composition simple and flexible, serving as the core foundation for implementing automated processing and complex business orchestration.</p> <p>There are two ways to define worker:</p> <ol> <li>Use the <code>@worker</code> decorator to decorate member methods (regular or asynchronous) of a <code>GraphAutoma</code> class, thereby registering them as Workers.</li> <li>Inherit from the <code>Worker</code> base class and implement its <code>run</code> or <code>arun</code> method.</li> </ol>"},{"location":"understanding/concepts/#graphautoma","title":"GraphAutoma","text":"<p><code>GraphAutoma</code> is an implementation of <code>Automa</code> based on Dynamic Directed Graph (abbreviated as DDG). It is the core class in Bridgic. It is not just a simple task scheduler, but a highly abstracted workflow engine that helps developers organize, manage, and run complex asynchronous or synchronous task flows in a declarative manner. Compared to traditional flow control or manual orchestration, <code>GraphAutoma</code> offers the following significant advantages:</p> <ol> <li> <p>Declarative Dependency Modeling    With the <code>@worker</code> decorator, developers can define each node (worker) and its dependencies as if building blocks, without writing tedious scheduling logic. Each worker can be a synchronous or asynchronous function, and GraphAutoma will automatically recognize dependencies and construct the complete task graph.</p> </li> <li> <p>Call-based Branch Management    In addition to automatic scheduling based on dependencies, GraphAutoma also supports \"active jump\" or \"call-based branching\" control via the <code>ferry_to</code> method. Developers can call <code>ferry_to(worker_key, *args, **kwargs)</code> inside any worker to directly switch the control flow to the specified worker and pass parameters. This mechanism is similar to \"goto\" or \"event-driven jump\", but operates at the granularity of complete workers, while still maintaining asynchronous safety and context consistency. This feature is suitable for complex scenarios requiring dynamic branching, making it easy for developers to create logic orchestration programs that depend on runtime conditions. The call to <code>ferry_to</code> is not constrained by dependencies; the target worker will be scheduled for asynchronous execution in the next dynamic step, greatly enhancing the flexibility and controllability of the workflow.</p> </li> <li> <p>Automatic Driving and Scheduling    Once dependencies are defined, GraphAutoma will automatically drive the execution of each worker according to the dependency topology. As long as dependencies are satisfied, workers will be automatically scheduled, eliminating the need for developers to manually manage execution order or state transitions, thus greatly reducing error rates and maintenance costs.</p> </li> <li> <p>Asynchronous and Concurrent Support    GraphAutoma natively supports asynchronous workers, fully leveraging Python's async features for efficient concurrent execution. For I/O-intensive tasks, it can also be combined with thread pools to achieve true parallel processing and improve overall throughput.</p> </li> <li> <p>Human Interaction and Event Mechanism    In complex business scenarios, some nodes may need to wait for manual input or external events. GraphAutoma has a built-in human interaction mechanism, supporting task pausing and resuming after receiving user feedback. At the same time, it integrates Bridgic's event system, enabling real-time communication between workers and the application layer, greatly enhancing the system's interactivity and flexibility.</p> </li> <li> <p>Serialization and Persistence    GraphAutoma supports complete serialization and deserialization, allowing the current workflow state to be persisted to disk or a database, enabling advanced features such as checkpointing and fault recovery. This is especially important for long-running or highly reliable business processes.</p> </li> <li> <p>Dynamic Topology Changes    Supports dynamically adding or removing workers at runtime, flexibly adapting to changes and expansion needs in business processes. There is no need to restart or refactor the entire workflow, greatly improving system maintainability and scalability.</p> </li> <li> <p>Layerizable and Editable    GraphAutoma supports deep customization through inheritance and secondary development. Developers can flexibly add new worker nodes based on existing GraphAutoma subclasses to extend their behavior and functionality, enabling personalized customization of business processes. By inheriting, you can reuse existing dependencies and scheduling logic, and then stack new business nodes on top, quickly building more complex automated workflows. This layered and editable design allows GraphAutoma to be continuously expanded and evolved like Lego blocks, greatly enhancing the system's maintainability and evolutionary capability.</p> </li> <li> <p>Nestable and Reusable    GraphAutoma is not just a top-level workflow engine; it can itself act as a \"super worker\" and be nested within another, larger GraphAutoma. Each GraphAutoma instance can be scheduled, passed parameters, and reused just like a regular worker, enabling recursive composition and hierarchical management of workflows. This nesting mechanism allows developers to use the composition pattern to orchestrate even larger and more complex business processes.</p> </li> </ol> <p>In summary, <code>GraphAutoma</code> enables developers to quickly build robust, flexible, interactive, and persistent complex workflow systems with minimal mental overhead, making it a powerful cornerstone for modern automation and intelligent application development.</p>"},{"location":"understanding/introduction/","title":"Introduction","text":"<p>Bridgic is an agentic programming framework built around a novel dynamic topology orchestration model and a component-oriented paradigm that is realized through ASL (Agent Structure Language)\u2014a powerful declarative DSL for composing, reusing, and nesting agentic structures. Together, these elements make it possible to develop the entire spectrum of agentic systems, ranging from deterministic workflows to autonomous agents.</p>"},{"location":"understanding/introduction/#core-features","title":"Core Features","text":"<ul> <li>Orchestration: Bridgic introduces a novel orchestration model based on DDG (Dynamic Directed Graph).</li> <li>Dynamic Routing: Bridgic enables conditional branching and dynamic orchestration through an easy-to-use <code>ferry_to()</code> API.</li> <li>Dynamic Topology: The DDG-based orchestration topology can be changed at runtime in Bridgic to support highly autonomous AI applications.</li> <li>ASL: ASL (Agent Structure Language) is a powerful declarative DSL that embodies a component-oriented paradigm and is even capable of supporting dynamic topologies.</li> <li>Modularity &amp; Componentization: In Bridgic, a complex agentic system can be composed by reusing components through hierarchical nesting.</li> <li>Parameter Resolving: Two mechanisms are designed to pass data among workers/automas\u2014thereby eliminating the complexity of global state management when necessary.</li> <li>Human-in-the-Loop: A Bridgic-style agentic system can request feedback from human whenever needed to dynamically adjust its execution logic.</li> <li>Serialization: Bridgic employs a scalable serialization and deserialization mechanism to achieve state persistence and recovery, enabling human-in-the-loop in long-running AI systems.</li> <li>Systematic Integration: A wide range of tools, LLMs and tracing functionalities can be seamlessly integrated into the Bridgic world, in a systematic way.</li> <li>Customization: What Bridgic provides is not a \"black box\" approach. You have full control over every aspect of your AI applications, such as prompts, context windows, the control flow, and more.</li> </ul>"}]}