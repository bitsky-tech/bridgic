{"config":{"lang":["en"],"separator":"[\\s\\-,:!=\\[\\]()\"`/]+|\\.(?!\\d)|&[lg]t;|(?!\\b)(?=[A-Z][a-z])","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to the world of Bridgic!","text":"<p>Bridgic is an innovative programming framework designed to create agentic systems, ranging from deterministic workflows to autonomous agents. It introduces a new paradigm that simplifies the development of agentic systems.</p>"},{"location":"#what-does-the-word-bridgic-mean","title":"What does the word \"Bridgic\" mean?","text":"<p>The name Bridgic is inspired by the idea of \u201cbridging logic and magic\u201d \u2014 bringing together the precision of logic and the creativity of intelligence.</p> <ul> <li>Logic stands for deterministic execution flows.</li> <li>Magic stands for highly autonomous AI agents.</li> </ul> <p>Refer to the relevant sections of the documentation for details:</p> <ul> <li>Installation.</li> <li>Tutorials.</li> <li>Understanding.</li> <li>API Reference.</li> </ul>"},{"location":"extras/llms/","title":"LLM Integration","text":""},{"location":"extras/llms/#overview","title":"Overview","text":"<p>Bridgic's model integration module (<code>bridgic.llms</code>) employs a Provider-centric integration strategy, adapting different model service providers and inference frameworks as independent integration packages. Each package adheres to the same interface standards, ensuring model neutrality and consistent interface definitions.</p>"},{"location":"extras/llms/#design-philosophy","title":"Design Philosophy","text":""},{"location":"extras/llms/#provider-centric-integration","title":"Provider-Centric Integration","text":"<p>Bridgic designs model integration with Providers as the fundamental unit:</p> <ul> <li>Independent Packaging: Each model provider (such as OpenAI, vLLM, other model vendors, or inference frameworks) has its own dedicated integration package</li> <li>Unified Interface: All packages implement the same core interface to ensure consistent user experience</li> <li>On-Demand Installation: Developers only need to install the required Provider packages, avoiding unnecessary dependencies</li> </ul>"},{"location":"extras/llms/#model-neutrality","title":"Model Neutrality","text":"<p>Through Provider-centric design, Bridgic achieves true model neutrality:</p> <ul> <li>No Vendor Lock-in: Application code is not tied to specific model providers, enabling easy switching</li> <li>Seamless Switching: When changing model providers, application code requires minimal changes\u2014simply swap the model object</li> </ul>"},{"location":"extras/llms/#protocol-driven-design","title":"Protocol-Driven Design","text":"<p>Each Provider declares its capabilities by implementing predefined protocols:</p> <ul> <li>Capability Declaration: Protocol based implementations clearly declare the supported functional features</li> <li>Progressive Enhancement: Providers can implement partial protocols to support incremental enhancement</li> <li>Extensibility: New Providers can be seamlessly integrated by implementing the appropriate protocols</li> </ul>"},{"location":"extras/llms/#available-integrations","title":"Available Integrations","text":"Provider Package Description OpenAI <code>bridgic-llms-openai</code> Official OpenAI API integration for GPT-4, GPT-3.5, and other models vLLM <code>bridgic-llms-vllm</code> Self-hosted large language model inference engine - <code>bridgic-llms-openai-like</code> Thin wrapper for OpenAI compatible model services"},{"location":"extras/traces/","title":"Trace Integration","text":""},{"location":"extras/traces/#overview","title":"Overview","text":"<p>Any agentic system needs a reliable observability layer to stay debuggable and trustworthy. The full picture of observability spans two complementary consists of these two parts:</p> <ul> <li>Passive Tracing: To Keep watching over every worker lifecycle event without code intrusion.</li> <li>Active Tracing: To enable application to emit custom spans or metrics whenever richer context is needed.</li> </ul> <p>Together they form a holistic observability surface.</p> <p>Bridgic fully supports passive tracing based on Worker Callback Mechanism, and provides seamless integration with several third-party platforms. More convient programming tools that support active tracing will also be introduced in the future.</p>"},{"location":"extras/traces/#available-integrations","title":"Available Integrations","text":"Package Description <code>bridgic-traces-opik</code> Opik-powered observability and runtime tracing adapter. <code>bridgic-traces-langwatch</code> LangWatch-powered observability and runtime tracing adapter."},{"location":"home/basics/","title":"Basics","text":"<p>Autonomous agents, powered by modern large language models, represent a major advance in the AI industry. However, when these \"AI agents\" are deployed in real-world production environments, many workflows remain deterministic and are not all that \"agentic\". Therefore, the community has largely reached a consensus to use the term \"agentic system\" to refer to the entire spectrum of AI systems, from deterministic workflows to fully autonomous agents.</p> <p>Bridgic provides a consistent and unified approach for developing all types of agentic systems\u2014no matter how agentic they are. This is precisely why the name \"Bridgic\" was chosen\u2014it bridges logic and magic! The following sections will introduce and explain the fundamental concepts that form the foundation of Bridgic.</p>"},{"location":"home/basics/#worker-and-automa","title":"Worker and Automa","text":"<p>Bridgic has two core concepts:</p> <ul> <li>Worker: the basic execution unit in Bridgic.</li> <li>Automa: an entity that manages and orchestrates a group of workers. An automa itself is also a worker, which enables the nesting of automa instances within each other.</li> </ul> <p>A worker is an entity that actually performs tasks. In real-world systems, a worker can represent a precise execution logic, such as a function or an API call, or it can be something highly autonomous, like an agent. In other words, a worker can be any entity capable of carrying out actions, regardless of its level of autonomy.</p> <p>In Bridgic, the <code>Worker</code> class is defined in Python as follows:</p> <pre><code>class Worker:\n    async def arun(self, *args, **kwargs) -&gt; Any:\n        ...\n</code></pre> <p>The <code>arun</code> method of the <code>Worker</code> class is called to execute a task. You can pass any required arguments to <code>arun</code>, and it will return a value as the result of the task.</p> <p>Tips</p> <p>In fact, in addition to the <code>arun</code> method, a <code>Worker</code> also has a <code>run</code> method. This relates to Bridgic's concurrency mode. Please refer to the relevant sections for more details.</p> <p>Besides worker, \"automa\" is another core concept in Bridgic. An automa acts as a container for a group of workers. Instead of performing tasks by itself, an automa schedules and orchestrates the workers it contains, running them according to a predefined or dynamic execution flow in order to accomplish the overall task.</p> <p>In Bridgic, the <code>Automa</code> class is defined in Python as follows:</p> <pre><code>class Automa(Worker):\n        ...\n</code></pre> <p>Note that the <code>Automa</code> class inherits from <code>Worker</code>. This means that every automa is also a worker and can be seamlessly nested within another automa. This design abstraction allows for powerful, modular programming by enabling automa to be composed layer by layer. We will elaborate on this later.</p>"},{"location":"home/basics/#graphautoma","title":"GraphAutoma","text":"<p><code>GraphAutoma</code> is a concrete implementation of the automa concept, where workers are organized into a directed graph (DG) with workers as nodes and their relationships as edges.</p>"},{"location":"home/basics/#predefined-dependencies","title":"Predefined Dependencies","text":"<p>As shown in the diagram above, the execution order of workers primarily depends on two factors:</p> <ul> <li>The designation of one or more start workers.</li> <li>The predefined dependencies established between workers. They are represented by the solid arrows in the diagram.</li> </ul> <p>The execution flow starts with the start worker (i.e., <code>worker_1</code>), followed by the concurrent execution of <code>worker_2</code> and <code>worker_3</code>, and finally <code>worker_4</code> is executed.</p> <p>Notably, <code>worker_4</code> will be triggered only after both <code>worker_2</code> and <code>worker_3</code> have completed their execution. In other words, within a <code>GraphAutoma</code>, if a worker has multiple predecessor workers, it adopts an \"AND\" execution logic\u2014meaning that the worker will not start until all its direct predecessors have finished.</p> <p>The execution flow shown above can be implemented with the following code:</p> <pre><code>from bridgic.core.automa import GraphAutoma, worker\n\nclass MyFlow(GraphAutoma):\n    @worker(is_start=True)\n    async def worker_1(self):\n        ...\n\n    @worker(dependencies=[\"worker_1\"])\n    async def worker_2(self):\n        ...\n\n    @worker(dependencies=[\"worker_1\"])\n    async def worker_3(self):\n        ...\n\n    @worker(dependencies=[\"worker_2\", \"worker_3\"])\n    async def worker_4(self):\n        ...\n</code></pre> <p>This code uses the <code>@worker</code> decorator to transform a regular Python method into a worker, and uses the <code>dependencies</code> parameter to define the partial order among these workers.</p>"},{"location":"home/basics/#dynamic-routing","title":"Dynamic Routing","text":"<p>To support the development of autonomous agents, in addition to the predefined dependencies mentioned above, Bridgic also provides an easy-to-use <code>ferry_to()</code> API for implementing dynamic routing.</p> <p>As illustrated in the diagram above, the dashed arrows represent potential execution paths. After <code>worker_1</code> finishes execution, the orchestrator of <code>GraphAutoma</code> determines at runtime\u2014based on parameters or context\u2014whether to proceed with <code>worker_2</code>, <code>worker_3</code>, or both.</p> <p>Here is the code:</p> <pre><code>from bridgic.core.automa import GraphAutoma, worker\n\nclass MyFlow(GraphAutoma):\n    @worker(is_start=True)\n    async def worker_1(self, x):\n        ...\n        if x &gt; 0:\n            self.ferry_to(\"worker_2\", y=1)\n        else\n            self.ferry_to(\"worker_3\", z=2)\n\n    @worker()\n    async def worker_2(self, y):\n        ...\n\n    @worker()\n    async def worker_3(self, z):\n        ...\n</code></pre> <p>With the <code>ferry_to()</code> mechanism, you can implement dynamic routing in a natural, intuitive way\u2014just like calling a regular function.</p> <p>Tips</p> <p>The name of the <code>ferry_to</code> API comes from the idea of \"ferrying\" between various \"islands\" in a directed graph.</p> <p>With the help of the dynamic routing mechanism provided by <code>ferry_to</code>, you can easily implement looping logic in your workflow. The diagram below illustrates how this works:</p> <p>Code:</p> <pre><code>from bridgic.core.automa import GraphAutoma, worker\n\nclass MyFlow(GraphAutoma):\n    @worker(is_start=True)\n    async def worker_1(self, x):\n        return x\n\n    @worker(dependencies=[\"worker_1\"])\n    async def worker_2(self, x):\n        return x + 1\n\n    @worker(dependencies=[\"worker_2\"])\n    async def worker_3(self, x):\n        if x &lt; 0:\n            self.ferry_to(\"worker_2\", x)\n        else:\n            self.ferry_to(\"worker_4\", x)\n\n    @worker(is_output=True)\n    async def worker_4(self, x):\n        return x\n</code></pre> <p>In this code, <code>worker_3</code> decides whether to run <code>worker_2</code> or <code>worker_4</code> next based on different conditions.</p> <p>It\u2019s important to note that when <code>ferry_to(\"worker_2\", x)</code> is called, <code>worker_2</code> will execute immediately in the next event loop iteration, without waiting for its predefined dependency <code>worker_1</code> to complete. This behavior arises from the interplay between dynamic routing and predefined dependencies\u2014an approach that allows Bridgic to seamlessly combine static orchestration with flexible, dynamic control flow.</p>"},{"location":"home/basics/#api","title":"API","text":"<p><code>GraphAutoma</code> provides two types of APIs that are used to manage the graph topology:</p> <ul> <li> <p>The core API: <code>add_worker</code>, <code>add_func_as_worker</code>, <code>remove_worker</code>, and <code>add_dependency</code>.</p> </li> <li> <p>The declarative API: <code>@worker</code> decorator.</p> </li> </ul> <p>For more code examples, please refer to the Tutorials section.</p>"},{"location":"home/basics/#dynamic-directed-graph","title":"Dynamic Directed Graph","text":"<p><code>GraphAutoma</code> implements a Dynamic Directed Graph (DDG) that orchestrates the execution of its internal workers in an asynchronous and dynamic manner using asyncio.</p> <p>A DDG is a directed graph whose topology can be changed at runtime. Its scheduler divides the orchestration process into several dynamic steps (DS), each executed in a single event loop iteration. At the end of each DS, the scheduler prepares the next set of workers to run based on the predefined dependencies or the dynamic <code>ferry_to</code> calls. Any topology changes triggered by <code>add_worker</code> or <code>remove_worker</code> takes effect in the next DS.</p> <p>Taking the above diagram as an example, the entire execution is divided into three dynamic steps:</p> <ul> <li>DS 1: <code>worker_1</code> is executed.</li> <li>DS 2: both <code>worker_2</code> and <code>worker_3</code> are executed.</li> <li>DS 3: <code>worker_4</code> is executed.</li> </ul>"},{"location":"home/basics/#modulirity-and-netsting","title":"Modulirity and Netsting","text":"<p>In Bridgic, an automa itself is also a worker, allowing one automa to be added into another. This design enables the construction of complex agentic systems by reusing components through hierarchical nesting, introducing a new paradigm of modular and component-based programming in agent-based development.</p> <p>For more code examples on modularity, please refer to the \"Modularity\" section in the Tutorials.</p>"},{"location":"home/concepts/","title":"Concepts","text":"<p>Bridgic has two core concepts:</p> <ul> <li>Worker: the basic execution unit in Bridgic.</li> <li>Automa: an entity that manages and orchestrates a group of workers. An Automa itself is also a Worker, which enables the nesting of Automa instances within each other.</li> </ul>"},{"location":"home/concepts/#worker","title":"Worker","text":"<p>A worker is the basic execution unit in the Bridgic framework, representing a specific task node.</p> <p>A worker typically corresponds to a function (which can be synchronous or asynchronous) that performs an independent business logic or processing step. Workers are automatically linked through <code>dependencies</code> to form a complete workflow. The design of Workers makes task decomposition, reuse, and composition simple and flexible, serving as the core foundation for implementing automated processing and complex business orchestration.</p> <p>There are two ways to define worker:</p> <ol> <li>Use the <code>@worker</code> decorator to decorate member methods (regular or asynchronous) of a <code>GraphAutoma</code> class, thereby registering them as Workers.</li> <li>Inherit from the <code>Worker</code> base class and implement its <code>run</code> or <code>arun</code> method.</li> </ol>"},{"location":"home/concepts/#graphautoma","title":"GraphAutoma","text":"<p><code>GraphAutoma</code> is an implementation of <code>Automa</code> based on Dynamic Directed Graph (abbreviated as DDG). It is the core class in Bridgic. It is not just a simple task scheduler, but a highly abstracted workflow engine that helps developers organize, manage, and run complex asynchronous or synchronous task flows in a declarative manner. Compared to traditional flow control or manual orchestration, <code>GraphAutoma</code> offers the following significant advantages:</p> <ol> <li> <p>Declarative Dependency Modeling    With the <code>@worker</code> decorator, developers can define each node (worker) and its dependencies as if building blocks, without writing tedious scheduling logic. Each worker can be a synchronous or asynchronous function, and GraphAutoma will automatically recognize dependencies and construct the complete task graph.</p> </li> <li> <p>Call-based Branch Management    In addition to automatic scheduling based on dependencies, GraphAutoma also supports \"active jump\" or \"call-based branching\" control via the <code>ferry_to</code> method. Developers can call <code>ferry_to(worker_key, *args, **kwargs)</code> inside any worker to directly switch the control flow to the specified worker and pass parameters. This mechanism is similar to \"goto\" or \"event-driven jump\", but operates at the granularity of complete workers, while still maintaining asynchronous safety and context consistency. This feature is suitable for complex scenarios requiring dynamic branching, making it easy for developers to create logic orchestration programs that depend on runtime conditions. The call to <code>ferry_to</code> is not constrained by dependencies; the target worker will be scheduled for asynchronous execution in the next dynamic step, greatly enhancing the flexibility and controllability of the workflow.</p> </li> <li> <p>Automatic Driving and Scheduling    Once dependencies are defined, GraphAutoma will automatically drive the execution of each worker according to the dependency topology. As long as dependencies are satisfied, workers will be automatically scheduled, eliminating the need for developers to manually manage execution order or state transitions, thus greatly reducing error rates and maintenance costs.</p> </li> <li> <p>Asynchronous and Concurrent Support    GraphAutoma natively supports asynchronous workers, fully leveraging Python's async features for efficient concurrent execution. For I/O-intensive tasks, it can also be combined with thread pools to achieve true parallel processing and improve overall throughput.</p> </li> <li> <p>Human Interaction and Event Mechanism    In complex business scenarios, some nodes may need to wait for manual input or external events. GraphAutoma has a built-in human interaction mechanism, supporting task pausing and resuming after receiving user feedback. At the same time, it integrates Bridgic's event system, enabling real-time communication between workers and the application layer, greatly enhancing the system's interactivity and flexibility.</p> </li> <li> <p>Serialization and Persistence    GraphAutoma supports complete serialization and deserialization, allowing the current workflow state to be persisted to disk or a database, enabling advanced features such as checkpointing and fault recovery. This is especially important for long-running or highly reliable business processes.</p> </li> <li> <p>Dynamic Topology Changes    Supports dynamically adding or removing workers at runtime, flexibly adapting to changes and expansion needs in business processes. There is no need to restart or refactor the entire workflow, greatly improving system maintainability and scalability.</p> </li> <li> <p>Layerizable and Editable    GraphAutoma supports deep customization through inheritance and secondary development. Developers can flexibly add new worker nodes based on existing GraphAutoma subclasses to extend their behavior and functionality, enabling personalized customization of business processes. By inheriting, you can reuse existing dependencies and scheduling logic, and then stack new business nodes on top, quickly building more complex automated workflows. This layered and editable design allows GraphAutoma to be continuously expanded and evolved like Lego blocks, greatly enhancing the system's maintainability and evolutionary capability.</p> </li> <li> <p>Nestable and Reusable    GraphAutoma is not just a top-level workflow engine; it can itself act as a \"super worker\" and be nested within another, larger GraphAutoma. Each GraphAutoma instance can be scheduled, passed parameters, and reused just like a regular worker, enabling recursive composition and hierarchical management of workflows. This nesting mechanism allows developers to use the composition pattern to orchestrate even larger and more complex business processes.</p> </li> </ol> <p>In summary, <code>GraphAutoma</code> enables developers to quickly build robust, flexible, interactive, and persistent complex workflow systems with minimal mental overhead, making it a powerful cornerstone for modern automation and intelligent application development.</p>"},{"location":"home/introduction/","title":"Introduction","text":"<p>Bridgic is an innovative programming framework designed to create agentic systems, ranging from deterministic workflows to autonomous agents. It introduces a new paradigm that simplifies the development of agentic systems.</p>"},{"location":"home/introduction/#core-features","title":"Core Features","text":"<ul> <li>Orchestration: Bridgic helps manage the execution flow of your AI applications by leveraging both predefined dependencies and dynamic routing.</li> <li>Parameter Binding: There are three ways to pass data among workers\u2014Arguments Mapping, Arguments Injection, and Inputs Propagation\u2014thereby eliminating the complexity of global state management.</li> <li>Dynamic Routing: Bridgic enables conditional branching and intelligent decision-making through an easy-to-use <code>ferry_to()</code> API that adapts to runtime dynamics.</li> <li>Dynamic Topology: The topology can be changed at runtime in Bridgic to support highly autonomous AI applications.</li> <li>Modularity: In Bridgic, a complex agentic system can be composed by reusing components through hierarchical nesting.</li> <li>Human-in-the-Loop: A Bridgic-style agentic system can request feedback from human whenever needed to dynamically adjust its execution logic.</li> <li>Serialization: Bridgic employs a scalable serialization and deserialization mechanism to achieve state persistence and recovery, enabling human-in-the-loop in long-running AI systems.</li> <li>Systematic Integration: A wide range of tools and LLMs can be seamlessly integrated into the Bridgic world, in a systematic way.</li> <li>Customization: What Bridgic provides is not a \"black box\" approach. You have full control over every aspect of your AI applications, such as prompts, context windows, the control flow, and more.</li> </ul>"},{"location":"reference/bridgic-core/bridgic/core/","title":"core","text":""},{"location":"reference/bridgic-core/bridgic/core/agentic/","title":"agentic","text":"<p>The Agentic module provides core components for building intelligent agent systems.</p> <p>This module contains various Automa implementations for orchestrating and executing  LLM-based workflows or agents. These Automa implementations are typically  composed together to build complex intelligent agents with advanced capabilities.</p>"},{"location":"reference/bridgic-core/bridgic/core/agentic/#bridgic.core.agentic.ConcurrentAutoma","title":"ConcurrentAutoma","text":"<p>               Bases: <code>GraphAutoma</code></p> <p>This class is to provide concurrent execution of multiple workers.</p> <p>In accordance with the defined \"Concurrency Model of Worker\", each worker within  a ConcurrentAutoma can be configured to operate in one of two concurrency modes:</p> <ol> <li>Async Mode: Workers execute concurrently in an asynchronous fashion, driven  by the event loop of the main thread. This execution mode corresponds to the <code>arun()</code>  method of the Worker.</li> <li>Parallel Mode: Workers execute synchronously, each running in a dedicated  thread within a thread pool managed by the ConcurrentAutoma. This execution mode  corresponds to the <code>run()</code> method of the Worker.</li> </ol> <p>Upon completion of all worker tasks, the concurrent automa instance aggregates  the result outputs from each worker into a single list, which is then returned  to the caller.</p> Source code in <code>bridgic/core/agentic/_concurrent_automa.py</code> <pre><code>class ConcurrentAutoma(GraphAutoma):\n    \"\"\"\n    This class is to provide concurrent execution of multiple workers.\n\n    In accordance with the defined \"Concurrency Model of Worker\", each worker within \n    a ConcurrentAutoma can be configured to operate in one of two concurrency modes:\n\n    1. **Async Mode**: Workers execute concurrently in an asynchronous fashion, driven \n    by the event loop of the main thread. This execution mode corresponds to the `arun()` \n    method of the Worker.\n    2. **Parallel Mode**: Workers execute synchronously, each running in a dedicated \n    thread within a thread pool managed by the ConcurrentAutoma. This execution mode \n    corresponds to the `run()` method of the Worker.\n\n    Upon completion of all worker tasks, the concurrent automa instance aggregates \n    the result outputs from each worker into a single list, which is then returned \n    to the caller.\n    \"\"\"\n\n    # Automa type.\n    AUTOMA_TYPE: ClassVar[AutomaType] = AutomaType.Concurrent\n\n    _MERGER_WORKER_KEY: Final[str] = \"__merger__\"\n\n    def __init__(\n        self,\n        name: Optional[str] = None,\n        thread_pool: Optional[ThreadPoolExecutor] = None,\n        running_options: Optional[RunningOptions] = None,\n    ):\n        super().__init__(name=name, thread_pool=thread_pool, running_options=running_options)\n\n        # Implementation notes:\n        # There are two types of workers in the concurrent automa:\n        # 1. Concurrent workers: These workers will be concurrently executed with each other.\n        # 2. The Merger worker: This worker will merge the results of all the concurrent workers.\n\n        cls = type(self)\n        if cls.AUTOMA_TYPE == AutomaType.Concurrent:\n            # The _registered_worker_funcs data are from @worker decorators.\n            # Initialize the decorated concurrent workers.\n            for worker_key, worker_func in self._registered_worker_funcs.items():\n                super().add_func_as_worker(\n                    key=worker_key,\n                    func=worker_func,\n                    is_start=True,\n                )\n\n        # Add a hidden worker as the merger worker, which will merge the results of all the start workers.\n        super().add_func_as_worker(\n            key=self._MERGER_WORKER_KEY,\n            func=self._merge_workers_results,\n            dependencies=super().all_workers(),\n            is_output=True,\n            args_mapping_rule=ArgsMappingRule.MERGE,\n        )\n\n    def _merge_workers_results(self, results: List[Any]) -&gt; List[Any]:\n        return results\n\n    @override\n    def add_worker(\n        self,\n        key: str,\n        worker: Worker,\n    ) -&gt; None:\n        \"\"\"\n        Add a concurrent worker to the concurrent automa. This worker will be concurrently executed with other concurrent workers.\n\n        Parameters\n        ----------\n        key : str\n            The key of the worker.\n        worker : Worker\n            The worker instance to be registered.\n        \"\"\"\n        if key == self._MERGER_WORKER_KEY:\n            raise AutomaRuntimeError(f\"the reserved key `{key}` is not allowed to be used by `add_worker()`\")\n        # Implementation notes:\n        # Concurrent workers are implemented as start workers in the underlying graph automa.\n        super().add_worker(key=key, worker=worker, is_start=True)\n        super().add_dependency(self._MERGER_WORKER_KEY, key)\n\n    @override\n    def add_func_as_worker(\n        self,\n        key: str,\n        func: Callable,\n    ) -&gt; None:\n        \"\"\"\n        Add a function or method as a concurrent worker to the concurrent automa. This worker will be concurrently executed with other concurrent workers.\n\n        Parameters\n        ----------\n        key : str\n            The key of the function worker.\n        func : Callable\n            The function to be added as a concurrent worker to the automa.\n        \"\"\"\n        if key == self._MERGER_WORKER_KEY:\n            raise AutomaRuntimeError(f\"the reserved key `{key}` is not allowed to be used by `add_func_as_worker()`\")\n        # Implementation notes:\n        # Concurrent workers are implemented as start workers in the underlying graph automa.\n        super().add_func_as_worker(key=key, func=func, is_start=True)\n        super().add_dependency(self._MERGER_WORKER_KEY, key)\n\n    @override\n    def worker(\n        self,\n        *,\n        key: Optional[str] = None,\n    ) -&gt; Callable:\n        \"\"\"\n        This is a decorator to mark a function or method as a concurrent worker of the concurrent automa. This worker will be concurrently executed with other concurrent workers.\n\n        Parameters\n        ----------\n        key : str\n            The key of the worker. If not provided, the name of the decorated callable will be used.\n        \"\"\"\n        if key == self._MERGER_WORKER_KEY:\n            raise AutomaRuntimeError(f\"the reserved key `{key}` is not allowed to be used by `automa.worker()`\")\n\n        super_automa = super()\n        def wrapper(func: Callable):\n            super_automa.add_func_as_worker(key=key, func=func, is_start=True)\n            super_automa.add_dependency(self._MERGER_WORKER_KEY, key)\n\n        return wrapper\n\n    @override\n    def remove_worker(self, key: str) -&gt; None:\n        \"\"\"\n        Remove a concurrent worker from the concurrent automa.\n\n        Parameters\n        ----------\n        key : str\n            The key of the worker to be removed.\n        \"\"\"\n        if key == self._MERGER_WORKER_KEY:\n            raise AutomaRuntimeError(f\"the merge worker is not allowed to be removed from the concurrent automa\")\n        super().remove_worker(key=key)\n\n    @override\n    def add_dependency(\n        self,\n        key: str,\n        dependency: str,\n    ) -&gt; None:\n        raise AutomaRuntimeError(f\"add_dependency() is not allowed to be called on a concurrent automa\")\n\n    def all_workers(self) -&gt; List[str]:\n        \"\"\"\n        Gets a list containing the keys of all concurrent workers registered in this concurrent automa.\n\n        Returns\n        -------\n        List[str]\n            A list of concurrent worker keys.\n        \"\"\"\n        keys_list = super().all_workers()\n        # Implementation notes:\n        # Hide the merger worker from the list of concurrent workers.\n        return list(filter(lambda key: key != self._MERGER_WORKER_KEY, keys_list))\n\n    def ferry_to(self, worker_key: str, /, *args, **kwargs):\n        raise AutomaRuntimeError(f\"ferry_to() is not allowed to be called on a concurrent automa\")\n\n    async def arun(\n        self, \n        *args: Tuple[Any, ...],\n        feedback_data: Optional[Union[InteractionFeedback, List[InteractionFeedback]]] = None,\n        **kwargs: Dict[str, Any]\n    ) -&gt; List[Any]:\n        result = await super().arun(\n            *args,\n            feedback_data=feedback_data,\n            **kwargs\n        )\n        return cast(List[Any], result)\n</code></pre>"},{"location":"reference/bridgic-core/bridgic/core/agentic/#bridgic.core.agentic.ConcurrentAutoma.add_worker","title":"add_worker","text":"<pre><code>add_worker(key: str, worker: Worker) -&gt; None\n</code></pre> <p>Add a concurrent worker to the concurrent automa. This worker will be concurrently executed with other concurrent workers.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>The key of the worker.</p> required <code>worker</code> <code>Worker</code> <p>The worker instance to be registered.</p> required Source code in <code>bridgic/core/agentic/_concurrent_automa.py</code> <pre><code>@override\ndef add_worker(\n    self,\n    key: str,\n    worker: Worker,\n) -&gt; None:\n    \"\"\"\n    Add a concurrent worker to the concurrent automa. This worker will be concurrently executed with other concurrent workers.\n\n    Parameters\n    ----------\n    key : str\n        The key of the worker.\n    worker : Worker\n        The worker instance to be registered.\n    \"\"\"\n    if key == self._MERGER_WORKER_KEY:\n        raise AutomaRuntimeError(f\"the reserved key `{key}` is not allowed to be used by `add_worker()`\")\n    # Implementation notes:\n    # Concurrent workers are implemented as start workers in the underlying graph automa.\n    super().add_worker(key=key, worker=worker, is_start=True)\n    super().add_dependency(self._MERGER_WORKER_KEY, key)\n</code></pre>"},{"location":"reference/bridgic-core/bridgic/core/agentic/#bridgic.core.agentic.ConcurrentAutoma.add_func_as_worker","title":"add_func_as_worker","text":"<pre><code>add_func_as_worker(key: str, func: Callable) -&gt; None\n</code></pre> <p>Add a function or method as a concurrent worker to the concurrent automa. This worker will be concurrently executed with other concurrent workers.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>The key of the function worker.</p> required <code>func</code> <code>Callable</code> <p>The function to be added as a concurrent worker to the automa.</p> required Source code in <code>bridgic/core/agentic/_concurrent_automa.py</code> <pre><code>@override\ndef add_func_as_worker(\n    self,\n    key: str,\n    func: Callable,\n) -&gt; None:\n    \"\"\"\n    Add a function or method as a concurrent worker to the concurrent automa. This worker will be concurrently executed with other concurrent workers.\n\n    Parameters\n    ----------\n    key : str\n        The key of the function worker.\n    func : Callable\n        The function to be added as a concurrent worker to the automa.\n    \"\"\"\n    if key == self._MERGER_WORKER_KEY:\n        raise AutomaRuntimeError(f\"the reserved key `{key}` is not allowed to be used by `add_func_as_worker()`\")\n    # Implementation notes:\n    # Concurrent workers are implemented as start workers in the underlying graph automa.\n    super().add_func_as_worker(key=key, func=func, is_start=True)\n    super().add_dependency(self._MERGER_WORKER_KEY, key)\n</code></pre>"},{"location":"reference/bridgic-core/bridgic/core/agentic/#bridgic.core.agentic.ConcurrentAutoma.worker","title":"worker","text":"<pre><code>worker(*, key: Optional[str] = None) -&gt; Callable\n</code></pre> <p>This is a decorator to mark a function or method as a concurrent worker of the concurrent automa. This worker will be concurrently executed with other concurrent workers.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>The key of the worker. If not provided, the name of the decorated callable will be used.</p> <code>None</code> Source code in <code>bridgic/core/agentic/_concurrent_automa.py</code> <pre><code>@override\ndef worker(\n    self,\n    *,\n    key: Optional[str] = None,\n) -&gt; Callable:\n    \"\"\"\n    This is a decorator to mark a function or method as a concurrent worker of the concurrent automa. This worker will be concurrently executed with other concurrent workers.\n\n    Parameters\n    ----------\n    key : str\n        The key of the worker. If not provided, the name of the decorated callable will be used.\n    \"\"\"\n    if key == self._MERGER_WORKER_KEY:\n        raise AutomaRuntimeError(f\"the reserved key `{key}` is not allowed to be used by `automa.worker()`\")\n\n    super_automa = super()\n    def wrapper(func: Callable):\n        super_automa.add_func_as_worker(key=key, func=func, is_start=True)\n        super_automa.add_dependency(self._MERGER_WORKER_KEY, key)\n\n    return wrapper\n</code></pre>"},{"location":"reference/bridgic-core/bridgic/core/agentic/#bridgic.core.agentic.ConcurrentAutoma.remove_worker","title":"remove_worker","text":"<pre><code>remove_worker(key: str) -&gt; None\n</code></pre> <p>Remove a concurrent worker from the concurrent automa.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>The key of the worker to be removed.</p> required Source code in <code>bridgic/core/agentic/_concurrent_automa.py</code> <pre><code>@override\ndef remove_worker(self, key: str) -&gt; None:\n    \"\"\"\n    Remove a concurrent worker from the concurrent automa.\n\n    Parameters\n    ----------\n    key : str\n        The key of the worker to be removed.\n    \"\"\"\n    if key == self._MERGER_WORKER_KEY:\n        raise AutomaRuntimeError(f\"the merge worker is not allowed to be removed from the concurrent automa\")\n    super().remove_worker(key=key)\n</code></pre>"},{"location":"reference/bridgic-core/bridgic/core/agentic/#bridgic.core.agentic.ConcurrentAutoma.all_workers","title":"all_workers","text":"<pre><code>all_workers() -&gt; List[str]\n</code></pre> <p>Gets a list containing the keys of all concurrent workers registered in this concurrent automa.</p> <p>Returns:</p> Type Description <code>List[str]</code> <p>A list of concurrent worker keys.</p> Source code in <code>bridgic/core/agentic/_concurrent_automa.py</code> <pre><code>def all_workers(self) -&gt; List[str]:\n    \"\"\"\n    Gets a list containing the keys of all concurrent workers registered in this concurrent automa.\n\n    Returns\n    -------\n    List[str]\n        A list of concurrent worker keys.\n    \"\"\"\n    keys_list = super().all_workers()\n    # Implementation notes:\n    # Hide the merger worker from the list of concurrent workers.\n    return list(filter(lambda key: key != self._MERGER_WORKER_KEY, keys_list))\n</code></pre>"},{"location":"reference/bridgic-core/bridgic/core/agentic/#bridgic.core.agentic.SequentialAutoma","title":"SequentialAutoma","text":"<p>               Bases: <code>GraphAutoma</code></p> <p>This class is to provide an easy way to orchestrate workers in a strictly  sequential manner.</p> <p>Each worker within the SequentialAutoma is invoked in the precise order determined  by their positional index, ensuring a linear workflow where the output of one worker  can serve as the input to the next.</p> <p>Upon the completion of all registered workers, the SequentialAutoma returns the output  produced by the final worker in the sequence as the overall result to the caller. This  design enforces ordered, step-wise processing, making the SequentialAutoma particularly  suitable for use cases that require strict procedural dependencies among constituent tasks.</p> Source code in <code>bridgic/core/agentic/_sequential_automa.py</code> <pre><code>class SequentialAutoma(GraphAutoma):\n    \"\"\"\n    This class is to provide an easy way to orchestrate workers in a strictly \n    sequential manner.\n\n    Each worker within the SequentialAutoma is invoked in the precise order determined \n    by their positional index, ensuring a linear workflow where the output of one worker \n    can serve as the input to the next.\n\n    Upon the completion of all registered workers, the SequentialAutoma returns the output \n    produced by the final worker in the sequence as the overall result to the caller. This \n    design enforces ordered, step-wise processing, making the SequentialAutoma particularly \n    suitable for use cases that require strict procedural dependencies among constituent tasks.\n    \"\"\"\n\n    # Automa type.\n    AUTOMA_TYPE: ClassVar[AutomaType] = AutomaType.Sequential\n\n    _TAIL_WORKER_KEY: Final[str] = \"__tail__\"\n    _last_worker_key: Optional[str]\n\n    def __init__(\n        self,\n        name: Optional[str] = None,\n        thread_pool: Optional[ThreadPoolExecutor] = None,\n        running_options: Optional[RunningOptions] = None,\n    ):\n        super().__init__(name=name, thread_pool=thread_pool, running_options=running_options)\n\n        cls = type(self)\n        self._last_worker_key = None\n        if cls.AUTOMA_TYPE == AutomaType.Sequential:\n            # The _registered_worker_funcs data are from @worker decorators.\n            # Initialize the decorated sequential workers.\n            for worker_key, worker_func in self._registered_worker_funcs.items():\n                is_start = self._last_worker_key is None\n                dependencies = [] if self._last_worker_key is None else [self._last_worker_key]\n                super().add_func_as_worker(\n                    key=worker_key,\n                    func=worker_func,\n                    dependencies=dependencies,\n                    is_start=is_start,\n                    args_mapping_rule=worker_func.__args_mapping_rule__,\n                )\n                self._last_worker_key = worker_key\n\n        if self._last_worker_key is not None:\n            # Add a hidden worker as the tail worker.\n            super().add_func_as_worker(\n                key=self._TAIL_WORKER_KEY,\n                func=self._tail_worker,\n                dependencies=[self._last_worker_key],\n                is_output=True,\n                args_mapping_rule=ArgsMappingRule.AS_IS,\n            )\n\n    def _tail_worker(self, result: Any) -&gt; Any:\n        # Return the result of the last worker without any modification.\n        return result\n\n    @override\n    def dump_to_dict(self) -&gt; Dict[str, Any]:\n        state_dict = super().dump_to_dict()\n        state_dict[\"last_worker_key\"] = self._last_worker_key\n        return state_dict\n\n    @override\n    def load_from_dict(self, state_dict: Dict[str, Any]) -&gt; None:\n        super().load_from_dict(state_dict)\n        self._last_worker_key = state_dict[\"last_worker_key\"]\n\n    def __add_worker_internal(\n        self,\n        key: str,\n        func_or_worker: Union[Callable, Worker],\n        *,\n        args_mapping_rule: ArgsMappingRule = ArgsMappingRule.AS_IS,\n    ) -&gt; None:\n        is_start = self._last_worker_key is None\n        dependencies = [] if self._last_worker_key is None else [self._last_worker_key]\n        if isinstance(func_or_worker, Callable):\n            super().add_func_as_worker(\n                key=key, \n                func=func_or_worker,\n                dependencies=dependencies,\n                is_start=is_start,\n                args_mapping_rule=args_mapping_rule,\n            )\n        else:\n            super().add_worker(\n                key=key, \n                worker=func_or_worker,\n                dependencies=dependencies,\n                is_start=is_start,\n                args_mapping_rule=args_mapping_rule,\n            )\n        if self._last_worker_key is not None:\n            # Remove the old hidden tail worker.\n            super().remove_worker(self._TAIL_WORKER_KEY)\n\n        # Add a new hidden tail worker.\n        self._last_worker_key = key\n        super().add_func_as_worker(\n            key=self._TAIL_WORKER_KEY,\n            func=self._tail_worker,\n            dependencies=[self._last_worker_key],\n            is_output=True,\n            args_mapping_rule=ArgsMappingRule.AS_IS,\n        )\n\n    @override\n    def add_worker(\n        self,\n        key: str,\n        worker: Worker,\n        *,\n        args_mapping_rule: ArgsMappingRule = ArgsMappingRule.AS_IS,\n    ) -&gt; None:\n        \"\"\"\n        Add a sequential worker to the sequential automa at the end of the automa.\n\n        Parameters\n        ----------\n        key : str\n            The key of the worker.\n        worker : Worker\n            The worker instance to be registered.\n        args_mapping_rule : ArgsMappingRule\n            The rule of arguments mapping.\n        \"\"\"\n        if key == self._TAIL_WORKER_KEY:\n            raise AutomaRuntimeError(f\"the reserved key `{key}` is not allowed to be used by `add_worker()`\")\n\n        self.__add_worker_internal(\n            key, \n            worker, \n            args_mapping_rule=args_mapping_rule\n        )\n\n    @override\n    def add_func_as_worker(\n        self,\n        key: str,\n        func: Callable,\n        *,\n        args_mapping_rule: ArgsMappingRule = ArgsMappingRule.AS_IS,\n    ) -&gt; None:\n        \"\"\"\n        Add a function or method as a sequential worker to the sequential automa at the end of the automa.\n\n        Parameters\n        ----------\n        key : str\n            The key of the worker.\n        func : Callable\n            The function to be added as a sequential worker to the automa.\n        args_mapping_rule : ArgsMappingRule\n            The rule of arguments mapping.\n        \"\"\"\n        if key == self._TAIL_WORKER_KEY:\n            raise AutomaRuntimeError(f\"the reserved key `{key}` is not allowed to be used by `add_func_as_worker()`\")\n\n        self.__add_worker_internal(\n            key, \n            func, \n            args_mapping_rule=args_mapping_rule\n        )\n\n    @override\n    def worker(\n        self,\n        *,\n        key: Optional[str] = None,\n        args_mapping_rule: ArgsMappingRule = ArgsMappingRule.AS_IS,\n    ) -&gt; Callable:\n        \"\"\"\n        This is a decorator to mark a function or method as a sequential worker of the sequential automa, at the end of the automa.\n\n        Parameters\n        ----------\n        key : str\n            The key of the worker. If not provided, the name of the decorated callable will be used.\n        args_mapping_rule: ArgsMappingRule\n            The rule of arguments mapping.\n        \"\"\"\n        if key == self._TAIL_WORKER_KEY:\n            raise AutomaRuntimeError(f\"the reserved key `{key}` is not allowed to be used by `automa.worker()`\")\n\n        def wrapper(func: Callable):\n            self.__add_worker_internal(\n                key, \n                func, \n                args_mapping_rule=args_mapping_rule\n            )\n\n        return wrapper\n\n    @override\n    def remove_worker(self, key: str) -&gt; None:\n        raise AutomaRuntimeError(f\"remove_worker() is not allowed to be called on a sequential automa\")\n\n    @override\n    def add_dependency(\n        self,\n        key: str,\n        depends: str,\n    ) -&gt; None:\n        raise AutomaRuntimeError(f\"add_dependency() is not allowed to be called on a sequential automa\")\n\n    def ferry_to(self, worker_key: str, /, *args, **kwargs):\n        raise AutomaRuntimeError(f\"ferry_to() is not allowed to be called on a sequential automa\")\n</code></pre>"},{"location":"reference/bridgic-core/bridgic/core/agentic/#bridgic.core.agentic.SequentialAutoma.add_worker","title":"add_worker","text":"<pre><code>add_worker(\n    key: str,\n    worker: Worker,\n    *,\n    args_mapping_rule: ArgsMappingRule = AS_IS\n) -&gt; None\n</code></pre> <p>Add a sequential worker to the sequential automa at the end of the automa.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>The key of the worker.</p> required <code>worker</code> <code>Worker</code> <p>The worker instance to be registered.</p> required <code>args_mapping_rule</code> <code>ArgsMappingRule</code> <p>The rule of arguments mapping.</p> <code>AS_IS</code> Source code in <code>bridgic/core/agentic/_sequential_automa.py</code> <pre><code>@override\ndef add_worker(\n    self,\n    key: str,\n    worker: Worker,\n    *,\n    args_mapping_rule: ArgsMappingRule = ArgsMappingRule.AS_IS,\n) -&gt; None:\n    \"\"\"\n    Add a sequential worker to the sequential automa at the end of the automa.\n\n    Parameters\n    ----------\n    key : str\n        The key of the worker.\n    worker : Worker\n        The worker instance to be registered.\n    args_mapping_rule : ArgsMappingRule\n        The rule of arguments mapping.\n    \"\"\"\n    if key == self._TAIL_WORKER_KEY:\n        raise AutomaRuntimeError(f\"the reserved key `{key}` is not allowed to be used by `add_worker()`\")\n\n    self.__add_worker_internal(\n        key, \n        worker, \n        args_mapping_rule=args_mapping_rule\n    )\n</code></pre>"},{"location":"reference/bridgic-core/bridgic/core/agentic/#bridgic.core.agentic.SequentialAutoma.add_func_as_worker","title":"add_func_as_worker","text":"<pre><code>add_func_as_worker(\n    key: str,\n    func: Callable,\n    *,\n    args_mapping_rule: ArgsMappingRule = AS_IS\n) -&gt; None\n</code></pre> <p>Add a function or method as a sequential worker to the sequential automa at the end of the automa.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>The key of the worker.</p> required <code>func</code> <code>Callable</code> <p>The function to be added as a sequential worker to the automa.</p> required <code>args_mapping_rule</code> <code>ArgsMappingRule</code> <p>The rule of arguments mapping.</p> <code>AS_IS</code> Source code in <code>bridgic/core/agentic/_sequential_automa.py</code> <pre><code>@override\ndef add_func_as_worker(\n    self,\n    key: str,\n    func: Callable,\n    *,\n    args_mapping_rule: ArgsMappingRule = ArgsMappingRule.AS_IS,\n) -&gt; None:\n    \"\"\"\n    Add a function or method as a sequential worker to the sequential automa at the end of the automa.\n\n    Parameters\n    ----------\n    key : str\n        The key of the worker.\n    func : Callable\n        The function to be added as a sequential worker to the automa.\n    args_mapping_rule : ArgsMappingRule\n        The rule of arguments mapping.\n    \"\"\"\n    if key == self._TAIL_WORKER_KEY:\n        raise AutomaRuntimeError(f\"the reserved key `{key}` is not allowed to be used by `add_func_as_worker()`\")\n\n    self.__add_worker_internal(\n        key, \n        func, \n        args_mapping_rule=args_mapping_rule\n    )\n</code></pre>"},{"location":"reference/bridgic-core/bridgic/core/agentic/#bridgic.core.agentic.SequentialAutoma.worker","title":"worker","text":"<pre><code>worker(\n    *,\n    key: Optional[str] = None,\n    args_mapping_rule: ArgsMappingRule = AS_IS\n) -&gt; Callable\n</code></pre> <p>This is a decorator to mark a function or method as a sequential worker of the sequential automa, at the end of the automa.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>The key of the worker. If not provided, the name of the decorated callable will be used.</p> <code>None</code> <code>args_mapping_rule</code> <code>ArgsMappingRule</code> <p>The rule of arguments mapping.</p> <code>AS_IS</code> Source code in <code>bridgic/core/agentic/_sequential_automa.py</code> <pre><code>@override\ndef worker(\n    self,\n    *,\n    key: Optional[str] = None,\n    args_mapping_rule: ArgsMappingRule = ArgsMappingRule.AS_IS,\n) -&gt; Callable:\n    \"\"\"\n    This is a decorator to mark a function or method as a sequential worker of the sequential automa, at the end of the automa.\n\n    Parameters\n    ----------\n    key : str\n        The key of the worker. If not provided, the name of the decorated callable will be used.\n    args_mapping_rule: ArgsMappingRule\n        The rule of arguments mapping.\n    \"\"\"\n    if key == self._TAIL_WORKER_KEY:\n        raise AutomaRuntimeError(f\"the reserved key `{key}` is not allowed to be used by `automa.worker()`\")\n\n    def wrapper(func: Callable):\n        self.__add_worker_internal(\n            key, \n            func, \n            args_mapping_rule=args_mapping_rule\n        )\n\n    return wrapper\n</code></pre>"},{"location":"reference/bridgic-core/bridgic/core/agentic/#bridgic.core.agentic.ReActAutoma","title":"ReActAutoma","text":"<p>               Bases: <code>GraphAutoma</code></p> <p>A react automa is a subclass of graph automa that implements the ReAct prompting framework.</p> Source code in <code>bridgic/core/agentic/react/_react_automa.py</code> <pre><code>class ReActAutoma(GraphAutoma):\n    \"\"\"\n    A react automa is a subclass of graph automa that implements the [ReAct](https://arxiv.org/abs/2210.03629) prompting framework.\n    \"\"\"\n\n    _llm: ToolSelection\n    \"\"\" The LLM to be used by the react automa. \"\"\"\n    _tools: Optional[List[ToolSpec]]\n    \"\"\" The candidate tools to be used by the react automa. \"\"\"\n    _system_prompt: Optional[SystemMessage]\n    \"\"\" The system prompt to be used by the react automa. \"\"\"\n    _max_iterations: int\n    \"\"\" The maximum number of iterations for the react automa. \"\"\"\n    _prompt_template: str\n    \"\"\" The template file for the react automa. \"\"\"\n    _jinja_env: Environment\n    \"\"\" The Jinja environment to be used by the react automa. \"\"\"\n    _jinja_template: Template\n    \"\"\" The Jinja template to be used by the react automa. \"\"\"\n\n    def __init__(\n        self,\n        llm: ToolSelection,\n        system_prompt: Optional[Union[str, SystemMessage]] = None,\n        tools: Optional[List[Union[Callable, Automa, ToolSpec]]] = None,\n        name: Optional[str] = None,\n        thread_pool: Optional[ThreadPoolExecutor] = None,\n        running_options: Optional[RunningOptions] = None,\n        max_iterations: int = DEFAULT_MAX_ITERATIONS,\n        prompt_template: str = DEFAULT_TEMPLATE_FILE,\n    ):\n        super().__init__(name=name, thread_pool=thread_pool, running_options=running_options)\n\n        self._llm = llm\n        if system_prompt:\n            # Validate SystemMessage...\n            if isinstance(system_prompt, str):\n                system_prompt = SystemMessage(role=\"system\", content=system_prompt)\n            elif (\"role\" not in system_prompt) or (system_prompt[\"role\"] != \"system\"):\n                raise ValueError(f\"Invalid `system_prompt` value received: {system_prompt}. It should contain `role`=`system`.\")\n\n        self._system_prompt = system_prompt\n        if tools:\n            self._tools = [self._ensure_tool_spec(tool) for tool in tools]\n        else:\n            self._tools = None\n        self._max_iterations = max_iterations\n        self._prompt_template = prompt_template\n        self._jinja_env = Environment(loader=PackageLoader(\"bridgic.core.agentic.react\"))\n        self._jinja_template = self._jinja_env.get_template(prompt_template)\n\n        self.add_worker(\n            key=\"tool_selector\",\n            worker=ToolSelectionWorker(tool_selection_llm=llm),\n            dependencies=[\"assemble_context\"],\n            args_mapping_rule=ArgsMappingRule.UNPACK,\n        )\n\n    @override\n    def dump_to_dict(self) -&gt; Dict[str, Any]:\n        state_dict = super().dump_to_dict()\n        state_dict[\"tools\"] = self._tools\n        state_dict[\"system_prompt\"] = self._system_prompt\n        state_dict[\"llm\"] = self._llm\n        state_dict[\"max_iterations\"] = self._max_iterations\n        state_dict[\"prompt_template\"] = self._prompt_template\n        return state_dict\n\n    @override\n    def load_from_dict(self, state_dict: Dict[str, Any]) -&gt; None:\n        super().load_from_dict(state_dict)\n        self._max_iterations = state_dict[\"max_iterations\"]\n        self._prompt_template = state_dict[\"prompt_template\"]\n        self._tools = state_dict[\"tools\"]\n        self._system_prompt = state_dict[\"system_prompt\"]\n        self._llm = state_dict[\"llm\"]\n        self._jinja_env = Environment(loader=PackageLoader(\"bridgic.core.agentic.react\"))\n        self._jinja_template = self._jinja_env.get_template(self._prompt_template)\n\n    @property\n    def max_iterations(self) -&gt; int:\n        return self._max_iterations\n\n    @max_iterations.setter\n    def max_iterations(self, max_iterations: int) -&gt; None:\n        self._max_iterations = max_iterations\n\n    @property\n    def prompt_template(self) -&gt; str:\n        return self._prompt_template\n\n    @prompt_template.setter\n    def prompt_template(self, prompt_template: str) -&gt; None:\n        self._prompt_template = prompt_template\n\n    @override\n    async def arun(\n        self,\n        user_msg: Optional[Union[str, UserTextMessage]] = None,\n        *,\n        chat_history: Optional[List[Union[UserTextMessage, AssistantTextMessage, ToolMessage]]] = None,\n        messages: Optional[List[ChatMessage]] = None,\n        tools: Optional[List[Union[Callable, Automa, ToolSpec]]] = None,\n        feedback_data: Optional[Union[InteractionFeedback, List[InteractionFeedback]]] = None,\n    ) -&gt; Any:\n        return await super().arun(\n            user_msg=user_msg,\n            chat_history=chat_history,\n            messages=messages,\n            tools=tools,\n            feedback_data=feedback_data,\n        )\n\n    @worker(is_start=True)\n    async def validate_and_transform(\n        self,\n        user_msg: Optional[Union[str, UserTextMessage]] = None,\n        *,\n        chat_history: Optional[List[Union[UserTextMessage, AssistantTextMessage, ToolMessage]]] = None,\n        messages: Optional[List[ChatMessage]] = None,\n        tools: Optional[List[Union[Callable, Automa, ToolSpec]]] = None,\n    ) -&gt; Dict[str, Any]:\n        \"\"\"\n        Validate and transform the input messages and tools to the canonical format.\n        \"\"\"\n\n        # Part One: validate and transform the input messages.\n        # Unify input messages of various types to the `ChatMessage` format.\n        chat_messages: List[ChatMessage] = []\n        if messages:\n            # If `messages` is provided, use it directly.\n            chat_messages = messages\n        elif user_msg:\n            # Since `messages` is not provided, join the system prompt + `chat_history` + `user_msg`\n            # First, append the `system_prompt`\n            if self._system_prompt:\n                chat_messages.append(self._system_prompt)\n\n            # Second, append the `chat_history`\n            if chat_history:\n                for history_msg in chat_history:\n                    # Validate the history messages...\n                    role = history_msg[\"role\"]\n                    if role == \"user\" or role == \"assistant\" or role == \"tool\":\n                        chat_messages.append(history_msg)\n                    else:\n                        raise ValueError(f\"Invalid role: `{role}` received in history message: `{history_msg}`, expected `user`, `assistant`, or `tool`.\")\n\n            # Third, append the `user_msg`\n            if isinstance(user_msg, str):\n                chat_messages.append(UserTextMessage(role=\"user\", content=user_msg))\n            elif isinstance(user_msg, dict):\n                if \"role\" in user_msg and user_msg[\"role\"] == \"user\":\n                    chat_messages.append(user_msg)\n                else:\n                    raise ValueError(f\"`role` must be `user` in user message: `{user_msg}`.\")\n        else:\n            raise ValueError(f\"Either `messages` or `user_msg` must be provided.\")\n\n        # Part Two: validate and transform the intput tools.\n        # Unify input tools of various types to the `ToolSpec` format.\n        if self._tools:\n            tool_spec_list = self._tools\n        elif tools:\n            tool_spec_list = [self._ensure_tool_spec(tool) for tool in tools]\n        else:\n            # TODO: whether to support empty tool list?\n            tool_spec_list = []\n\n        return {\n            \"initial_messages\": chat_messages,\n            \"candidate_tools\": tool_spec_list,\n        }\n\n    @worker(dependencies=[\"validate_and_transform\"], args_mapping_rule=ArgsMappingRule.UNPACK)\n    async def assemble_context(\n        self,\n        *,\n        initial_messages: Optional[List[ChatMessage]] = None,\n        candidate_tools: Optional[List[ToolSpec]] = None,\n        tool_selection_outputs: Tuple[List[ToolCall], Optional[str]] = From(\"tool_selector\", default=None),\n        tool_result_messages: Optional[List[ToolMessage]] = None,\n        rtx = System(\"runtime_context\"),\n    ) -&gt; Dict[str, Any]:\n        # print(f\"\\n******* ReActAutoma.assemble_context *******\\n\")\n        # print(f\"initial_messages: {initial_messages}\")\n        # print(f\"candidate_tools: {candidate_tools}\")\n        # print(f\"tool_selection_outputs: {tool_selection_outputs}\")\n        # print(f\"tool_result_messages: {tool_result_messages}\")\n\n        local_space = self.get_local_space(rtx)\n        # Build messages memory with help of local space.\n        messages_memory: List[ChatMessage] = []\n        if initial_messages:\n            # If `messages` is provided, use it to re-initialize the messages memory.\n            messages_memory = initial_messages.copy()\n        else:\n            messages_memory = local_space.get(\"messages_memory\", [])\n        if tool_selection_outputs:\n            # Transform tools_calls format:\n            tool_calls = tool_selection_outputs[0]\n            tool_calls_list = [\n                FunctionToolCall(\n                    id=tool_call.id,\n                    type=\"function\",\n                    function=Function(\n                        name=tool_call.name,\n                        arguments=tool_call.arguments,\n                    ),\n                ) for tool_call in tool_calls\n            ]\n            llm_response = tool_selection_outputs[1]\n            assistant_message = AssistantTextMessage(\n                role=\"assistant\",\n                # TOD: name?\n                content=llm_response,\n                tool_calls=tool_calls_list,\n            )\n            messages_memory.append(assistant_message)\n        if tool_result_messages:\n            messages_memory.extend(tool_result_messages)\n        local_space[\"messages_memory\"] = messages_memory\n        # print(\"--------------------------------\")\n        # print(f\"messages_memory: {messages_memory}\")\n\n        # Save &amp; retrieve tools with help of local space.\n        if candidate_tools:\n            local_space[\"tools\"] = candidate_tools\n        else:\n            candidate_tools = local_space.get(\"tools\", [])\n\n        # Note: here 'messages' and `tools` are injected into the template as variables.\n        raw_prompt = self._jinja_template.render(messages=messages_memory, tools=candidate_tools)\n        # print(f\"\\n ##### raw_prompt ##### \\n{raw_prompt}\")\n\n        # Note: the jinjia template must conform to the TypedDict `ChatMessage` format (in json).\n        llm_messages = cast(List[ChatMessage], json.loads(raw_prompt))\n        llm_tools: List[Tool] = [tool.to_tool() for tool in candidate_tools]\n\n        return {\n            \"messages\": llm_messages,\n            \"tools\": llm_tools,\n        }\n\n    @worker(dependencies=[\"tool_selector\"], args_mapping_rule=ArgsMappingRule.UNPACK)\n    async def plan_next_step(\n        self,\n        tool_calls: List[ToolCall],\n        llm_response: Optional[str] = None,\n        messages_and_tools: dict = From(\"validate_and_transform\"),\n        rtx = System(\"runtime_context\"),\n    ) -&gt; None:\n        local_space = self.get_local_space(rtx)\n        iterations_count = local_space.get(\"iterations_count\", 0)\n        iterations_count += 1\n        local_space[\"iterations_count\"] = iterations_count\n        if iterations_count &gt; self._max_iterations:\n            # TODO: how to report this to users?\n            self.ferry_to(\n                \"finally_summarize\", \n                final_answer=f\"Sorry, I am unable to answer your question after {self._max_iterations} iterations. Please try again later.\"\n            )\n            return\n\n        # TODO: maybe hand over the control flow to users?\n        # print(f\"\\n******* ReActAutoma.plan_next_step *******\\n\")\n        # print(f\"tool_calls: {tool_calls}\")\n        # print(f\"llm_response: {llm_response}\")\n        if tool_calls:\n            tool_spec_list = messages_and_tools[\"candidate_tools\"]\n            matched_list = self._match_tool_calls_and_tool_specs(tool_calls, tool_spec_list)\n            if matched_list:\n                matched_tool_calls = []\n                tool_worker_keys = []\n                for tool_call, tool_spec in matched_list:\n                    matched_tool_calls.append(tool_call)\n                    tool_worker = tool_spec.create_worker()\n                    worker_key = f\"tool_{tool_call.name}_{tool_call.id}\"\n                    self.add_worker(\n                        key=worker_key,\n                        worker=tool_worker,\n                    )\n                    # TODO: convert tool_call.arguments to the tool parameters types\n                    # TODO: validate the arguments against the tool parameters / json schema\n                    self.ferry_to(worker_key, **tool_call.arguments)\n                    tool_worker_keys.append(worker_key)\n                self.add_func_as_worker(\n                    key=\"merge_tools_results\",\n                    func=self.merge_tools_results,\n                    dependencies=tool_worker_keys,\n                    args_mapping_rule=ArgsMappingRule.MERGE,\n                )\n                return matched_tool_calls\n            else:\n                # TODO\n                ...\n        else:\n            # Got final answer from the LLM.\n            self.ferry_to(\"finally_summarize\", final_answer=llm_response)\n\n    async def merge_tools_results(\n        self, \n        tool_results: List[Any],\n        tool_calls: List[ToolCall] = From(\"plan_next_step\"),\n    ) -&gt; List[ToolMessage]:\n        \"\"\"\n        Merge the results of the tools.\n        \"\"\"\n        # print(f\"\\n******* ReActAutoma.merge_tools_results *******\\n\")\n        # print(f\"tool_results: {tool_results}\")\n        # print(f\"tool_calls: {tool_calls}\")\n        assert len(tool_results) == len(tool_calls)\n        tool_messages = []\n        for tool_result, tool_call in zip(tool_results, tool_calls):\n            tool_messages.append(ToolMessage(\n                role=\"tool\", \n                # Note: Convert the tool result to string, since a tool can return any type of data.\n                # TODO: maybe we can use a better way to serialize the tool result?\n                content=str(tool_result), \n                tool_call_id=tool_call.id\n            ))\n            # Remove the tool workers\n            self.remove_worker(f\"tool_{tool_call.name}_{tool_call.id}\")\n        # Remove self...\n        self.remove_worker(\"merge_tools_results\")\n        self.ferry_to(\"assemble_context\", tool_result_messages=tool_messages)\n        return tool_messages\n\n    @worker(is_output=True)\n    async def finally_summarize(self, final_answer: str) -&gt; str:\n        return final_answer\n\n    def _ensure_tool_spec(self, tool: Union[Callable, Automa, ToolSpec]) -&gt; ToolSpec:\n        if isinstance(tool, ToolSpec):\n            return tool\n        elif isinstance(tool, type) and issubclass(tool, Automa):\n            return AutomaToolSpec.from_raw(tool)\n        elif isinstance(tool, Callable):\n            # Note: this test against `Callable` should be placed at last.\n            return FunctionToolSpec.from_raw(tool)\n        else:\n            raise TypeError(f\"Invalid tool type: {type(tool)} detected, expected `Callable`, `Automa`, or `ToolSpec`.\")\n\n    def _match_tool_calls_and_tool_specs(\n        self,\n        tool_calls: List[ToolCall],\n        tool_spec_list: List[ToolSpec],\n    ) -&gt; List[Tuple[ToolCall, ToolSpec]]:\n        \"\"\"\n        This function is used to match the tool calls and the tool specs based on the tool name.\n\n        Parameters\n        ----------\n        tool_calls : List[ToolCall]\n            The tool calls to match.\n        tool_spec_list : List[ToolSpec]\n            The tool specs to match.\n\n        Returns\n        -------\n        List[(ToolCall, ToolSpec)]\n            The matched tool calls and tool specs.\n        \"\"\"\n        matched_list: List[Tuple[ToolCall, ToolSpec]] = []\n        for tool_call in tool_calls:\n            for tool_spec in tool_spec_list:\n                if tool_call.name == tool_spec.tool_name:\n                    matched_list.append((tool_call, tool_spec))\n        return matched_list\n</code></pre>"},{"location":"reference/bridgic-core/bridgic/core/agentic/#bridgic.core.agentic.ReActAutoma.validate_and_transform","title":"validate_and_transform","text":"<code>async</code> <pre><code>validate_and_transform(\n    user_msg: Optional[Union[str, UserTextMessage]] = None,\n    *,\n    chat_history: Optional[\n        List[\n            Union[\n                UserTextMessage,\n                AssistantTextMessage,\n                ToolMessage,\n            ]\n        ]\n    ] = None,\n    messages: Optional[List[ChatMessage]] = None,\n    tools: Optional[\n        List[Union[Callable, Automa, ToolSpec]]\n    ] = None\n) -&gt; Dict[str, Any]\n</code></pre> <p>Validate and transform the input messages and tools to the canonical format.</p> Source code in <code>bridgic/core/agentic/react/_react_automa.py</code> <pre><code>@worker(is_start=True)\nasync def validate_and_transform(\n    self,\n    user_msg: Optional[Union[str, UserTextMessage]] = None,\n    *,\n    chat_history: Optional[List[Union[UserTextMessage, AssistantTextMessage, ToolMessage]]] = None,\n    messages: Optional[List[ChatMessage]] = None,\n    tools: Optional[List[Union[Callable, Automa, ToolSpec]]] = None,\n) -&gt; Dict[str, Any]:\n    \"\"\"\n    Validate and transform the input messages and tools to the canonical format.\n    \"\"\"\n\n    # Part One: validate and transform the input messages.\n    # Unify input messages of various types to the `ChatMessage` format.\n    chat_messages: List[ChatMessage] = []\n    if messages:\n        # If `messages` is provided, use it directly.\n        chat_messages = messages\n    elif user_msg:\n        # Since `messages` is not provided, join the system prompt + `chat_history` + `user_msg`\n        # First, append the `system_prompt`\n        if self._system_prompt:\n            chat_messages.append(self._system_prompt)\n\n        # Second, append the `chat_history`\n        if chat_history:\n            for history_msg in chat_history:\n                # Validate the history messages...\n                role = history_msg[\"role\"]\n                if role == \"user\" or role == \"assistant\" or role == \"tool\":\n                    chat_messages.append(history_msg)\n                else:\n                    raise ValueError(f\"Invalid role: `{role}` received in history message: `{history_msg}`, expected `user`, `assistant`, or `tool`.\")\n\n        # Third, append the `user_msg`\n        if isinstance(user_msg, str):\n            chat_messages.append(UserTextMessage(role=\"user\", content=user_msg))\n        elif isinstance(user_msg, dict):\n            if \"role\" in user_msg and user_msg[\"role\"] == \"user\":\n                chat_messages.append(user_msg)\n            else:\n                raise ValueError(f\"`role` must be `user` in user message: `{user_msg}`.\")\n    else:\n        raise ValueError(f\"Either `messages` or `user_msg` must be provided.\")\n\n    # Part Two: validate and transform the intput tools.\n    # Unify input tools of various types to the `ToolSpec` format.\n    if self._tools:\n        tool_spec_list = self._tools\n    elif tools:\n        tool_spec_list = [self._ensure_tool_spec(tool) for tool in tools]\n    else:\n        # TODO: whether to support empty tool list?\n        tool_spec_list = []\n\n    return {\n        \"initial_messages\": chat_messages,\n        \"candidate_tools\": tool_spec_list,\n    }\n</code></pre>"},{"location":"reference/bridgic-core/bridgic/core/agentic/#bridgic.core.agentic.ReActAutoma.merge_tools_results","title":"merge_tools_results","text":"<code>async</code> <pre><code>merge_tools_results(\n    tool_results: List[Any],\n    tool_calls: List[ToolCall] = From(\"plan_next_step\"),\n) -&gt; List[ToolMessage]\n</code></pre> <p>Merge the results of the tools.</p> Source code in <code>bridgic/core/agentic/react/_react_automa.py</code> <pre><code>async def merge_tools_results(\n    self, \n    tool_results: List[Any],\n    tool_calls: List[ToolCall] = From(\"plan_next_step\"),\n) -&gt; List[ToolMessage]:\n    \"\"\"\n    Merge the results of the tools.\n    \"\"\"\n    # print(f\"\\n******* ReActAutoma.merge_tools_results *******\\n\")\n    # print(f\"tool_results: {tool_results}\")\n    # print(f\"tool_calls: {tool_calls}\")\n    assert len(tool_results) == len(tool_calls)\n    tool_messages = []\n    for tool_result, tool_call in zip(tool_results, tool_calls):\n        tool_messages.append(ToolMessage(\n            role=\"tool\", \n            # Note: Convert the tool result to string, since a tool can return any type of data.\n            # TODO: maybe we can use a better way to serialize the tool result?\n            content=str(tool_result), \n            tool_call_id=tool_call.id\n        ))\n        # Remove the tool workers\n        self.remove_worker(f\"tool_{tool_call.name}_{tool_call.id}\")\n    # Remove self...\n    self.remove_worker(\"merge_tools_results\")\n    self.ferry_to(\"assemble_context\", tool_result_messages=tool_messages)\n    return tool_messages\n</code></pre>"},{"location":"reference/bridgic-core/bridgic/core/agentic/tool_specs/","title":"tool_specs","text":"<p>The Tool Specs module provides definitions and implementations of tool specifications.</p> <p>This module contains various tool specification classes that support transforming  \"tool ingredients\" such as Python functions and Automa workflows into LLM-callable tools,  enabling callable objects to be seamlessly used in agentic systems.</p>"},{"location":"reference/bridgic-core/bridgic/core/agentic/tool_specs/#bridgic.core.agentic.tool_specs.ToolSpec","title":"ToolSpec","text":"<p>               Bases: <code>Serializable</code></p> <p>ToolSpec is an abstract class that represents a tool specification that describes all necessary information about a tool used by the LLM. </p> <p>ToolSpec and its subclasses are responsible for providing four categories of interfaces: 1. Transformations to LLM Tool: <code>to_tool</code>. 2. Worker Creation: <code>create_worker</code>. 3. Serialization and Deserialization. 4. ToolSpec initialization from raw resources: <code>from_raw</code>.</p> Source code in <code>bridgic/core/agentic/tool_specs/_base_tool_spec.py</code> <pre><code>class ToolSpec(Serializable):\n    \"\"\"\n    ToolSpec is an abstract class that represents a tool specification that describes all necessary information about a tool used by the LLM. \n\n    ToolSpec and its subclasses are responsible for providing four categories of interfaces:\n    1. Transformations to LLM Tool: `to_tool`.\n    2. Worker Creation: `create_worker`.\n    3. Serialization and Deserialization.\n    4. ToolSpec initialization from raw resources: `from_raw`.\n    \"\"\"\n    _tool_id: Optional[Union[str, int]]\n    \"\"\"The unique ID of the tool, used to uniquely identify a tool across the entire system. This tool can be of various types.\"\"\"\n    _tool_name: Optional[str]\n    \"\"\"The name of the tool to be called\"\"\"\n    _tool_description: Optional[str]\n    \"\"\"A description of what the tool does, used by the model to choose when and how to call the tool.\"\"\"\n    _tool_parameters: Optional[Dict[str, Any]]\n    \"\"\"The JSON schema of the tool's parameters\"\"\"\n\n    def __init__(\n        self,\n        tool_name: Optional[str] = None,\n        tool_description: Optional[str] = None,\n        tool_parameters: Optional[Dict[str, Any]] = None,\n    ):\n        self._tool_id = None\n        self._tool_name = tool_name\n        self._tool_description = tool_description\n        self._tool_parameters = tool_parameters\n\n    @property\n    def tool_name(self) -&gt; Optional[str]:\n        return self._tool_name\n\n    @property\n    def tool_description(self) -&gt; Optional[str]:\n        return self._tool_description\n\n    @property\n    def tool_parameters(self) -&gt; Optional[Dict[str, Any]]:\n        return self._tool_parameters\n\n    def __str__(self) -&gt; str:\n        return f\"{self.__class__.__name__}(tool_name={self._tool_name}, tool_description={self._tool_description}, tool_parameters={self._tool_parameters})\"\n\n    def __repr__(self) -&gt; str:\n        return f\"&lt;{self.__class__.__name__}(tool_name={self._tool_name}, tool_description={self._tool_description}, tool_parameters={self._tool_parameters})&gt;\"\n\n    ###############################################################\n    ######## Part One of interfaces: Transformations to Tool ######\n    ###############################################################\n\n    @abstractmethod\n    def to_tool(self) -&gt; Tool:\n        \"\"\"\n        Transform this ToolSpec to a `Tool` object used by LLM.\n\n        Returns\n        -------\n        Tool\n            A `Tool` object that can be used by LLM.\n        \"\"\"\n        ...\n\n    ###############################################################\n    ######## Part Two of interfaces: Worker Creation ##############\n    ###############################################################\n\n    @abstractmethod\n    def create_worker(self) -&gt; Worker:\n        \"\"\"\n        Create a Worker from the information included in this ToolSpec.\n\n        Returns\n        -------\n        Worker\n            A new `Worker` object that can be added to an Automa to execute the tool.\n        \"\"\"\n        ...\n\n    ###############################################################\n    ######## Part Three of interfaces: \n    ######## Serialization and Deserialization ####################\n    ###############################################################\n\n    @override\n    def dump_to_dict(self) -&gt; Dict[str, Any]:\n        state_dict = {}\n        if self._tool_id:\n            state_dict[\"tool_id\"] = self._tool_id\n        if self._tool_name:\n            state_dict[\"tool_name\"] = self._tool_name\n        if self._tool_description:\n            state_dict[\"tool_description\"] = self._tool_description\n        if self._tool_parameters:\n            state_dict[\"tool_parameters\"] = self._tool_parameters\n        return state_dict\n\n    @override\n    def load_from_dict(self, state_dict: Dict[str, Any]) -&gt; None:\n        self._tool_id = state_dict.get(\"tool_id\")\n        self._tool_name = state_dict.get(\"tool_name\")\n        self._tool_description = state_dict.get(\"tool_description\")\n        self._tool_parameters = state_dict.get(\"tool_parameters\")\n</code></pre>"},{"location":"reference/bridgic-core/bridgic/core/agentic/tool_specs/#bridgic.core.agentic.tool_specs.ToolSpec.to_tool","title":"to_tool","text":"<code>abstractmethod</code> <pre><code>to_tool() -&gt; Tool\n</code></pre> <p>Transform this ToolSpec to a <code>Tool</code> object used by LLM.</p> <p>Returns:</p> Type Description <code>Tool</code> <p>A <code>Tool</code> object that can be used by LLM.</p> Source code in <code>bridgic/core/agentic/tool_specs/_base_tool_spec.py</code> <pre><code>@abstractmethod\ndef to_tool(self) -&gt; Tool:\n    \"\"\"\n    Transform this ToolSpec to a `Tool` object used by LLM.\n\n    Returns\n    -------\n    Tool\n        A `Tool` object that can be used by LLM.\n    \"\"\"\n    ...\n</code></pre>"},{"location":"reference/bridgic-core/bridgic/core/agentic/tool_specs/#bridgic.core.agentic.tool_specs.ToolSpec.create_worker","title":"create_worker","text":"<code>abstractmethod</code> <pre><code>create_worker() -&gt; Worker\n</code></pre> <p>Create a Worker from the information included in this ToolSpec.</p> <p>Returns:</p> Type Description <code>Worker</code> <p>A new <code>Worker</code> object that can be added to an Automa to execute the tool.</p> Source code in <code>bridgic/core/agentic/tool_specs/_base_tool_spec.py</code> <pre><code>@abstractmethod\ndef create_worker(self) -&gt; Worker:\n    \"\"\"\n    Create a Worker from the information included in this ToolSpec.\n\n    Returns\n    -------\n    Worker\n        A new `Worker` object that can be added to an Automa to execute the tool.\n    \"\"\"\n    ...\n</code></pre>"},{"location":"reference/bridgic-core/bridgic/core/agentic/tool_specs/#bridgic.core.agentic.tool_specs.FunctionToolSpec","title":"FunctionToolSpec","text":"<p>               Bases: <code>ToolSpec</code></p> Source code in <code>bridgic/core/agentic/tool_specs/_function_tool_spec.py</code> <pre><code>class FunctionToolSpec(ToolSpec):\n    _func: Callable\n    \"\"\"The python function to be used as a tool\"\"\"\n\n    def __init__(\n        self,\n        func: Callable,\n        tool_name: Optional[str] = None,\n        tool_description: Optional[str] = None,\n        tool_parameters: Optional[Dict[str, Any]] = None,\n    ):\n        super().__init__(\n            tool_name=tool_name,\n            tool_description=tool_description,\n            tool_parameters=tool_parameters\n        )\n        self._func = func\n\n    @classmethod\n    def from_raw(\n        cls,\n        func: Callable,\n        tool_name: Optional[str] = None,\n        tool_description: Optional[str] = None,\n        tool_parameters: Optional[Dict[str, Any]] = None,\n    ) -&gt; \"FunctionToolSpec\":\n        \"\"\"\n        Create a FunctionToolSpec from a python function. By default, the tool name, description and parameters' json schema will be extracted from the function's docstring and the parameters' type and description. However, these values can be customized by passing in the corresponding arguments.\n\n        Parameters\n        ----------\n        func : Callable\n            The python function to create a FunctionToolSpec from.\n        tool_name : Optional[str]\n            The name of the tool. If not provided, the function name will be used.\n        tool_description : Optional[str]\n            The description of the tool. If not provided, the function docstring will be used.\n        tool_parameters : Optional[Dict[str, Any]]\n            The JSON schema of the tool's parameters. If not provided, the JSON schema will be constructed properly from the parameters' annotations, the function's signature and/or docstring.\n\n        Returns\n        -------\n        FunctionToolSpec\n            A new `FunctionToolSpec` object.\n        \"\"\"\n        if isinstance(func, MethodType):\n            raise ValueError(f\"`func` is not allowed to be a bound method: {func}.\")\n\n        if not tool_name:\n            tool_name = func.__name__\n\n        if not tool_description:\n            tool_description = get_tool_description_from(func, tool_name)\n\n        if not tool_parameters:\n            tool_parameters = create_func_params_json_schema(func)\n            # TODO: whether to remove the `title` field of the params_schema?\n\n        return cls(\n            func=func,\n            tool_name=tool_name,\n            tool_description=tool_description,\n            tool_parameters=tool_parameters\n        )\n\n    @override\n    def to_tool(self) -&gt; Tool:\n        \"\"\"\n        Transform this FunctionToolSpec to a `Tool` object used by LLM.\n\n        Returns\n        -------\n        Tool\n            A `Tool` object that can be used by LLM.\n        \"\"\"\n        return Tool(\n            name=self._tool_name,\n            description=self._tool_description,\n            parameters=self._tool_parameters\n        )\n\n    @override\n    def create_worker(self) -&gt; Worker:\n        \"\"\"\n        Create a Worker from the information included in this FunctionToolSpec.\n\n        Returns\n        -------\n        Worker\n            A new `Worker` object that can be added to an Automa to execute the tool.\n        \"\"\"\n        # TODO: some initialization arguments may be needed in future, e.g., `bound_needed`.\n        return CallableWorker(self._func)\n\n    @override\n    def dump_to_dict(self) -&gt; Dict[str, Any]:\n        state_dict = super().dump_to_dict()\n        state_dict[\"func\"] = self._func.__module__ + \".\" + self._func.__qualname__\n        return state_dict\n\n    @override\n    def load_from_dict(self, state_dict: Dict[str, Any]) -&gt; None:\n        super().load_from_dict(state_dict)\n        self._func = load_qualified_class_or_func(state_dict[\"func\"])\n</code></pre>"},{"location":"reference/bridgic-core/bridgic/core/agentic/tool_specs/#bridgic.core.agentic.tool_specs.FunctionToolSpec.from_raw","title":"from_raw","text":"<code>classmethod</code> <pre><code>from_raw(\n    func: Callable,\n    tool_name: Optional[str] = None,\n    tool_description: Optional[str] = None,\n    tool_parameters: Optional[Dict[str, Any]] = None,\n) -&gt; FunctionToolSpec\n</code></pre> <p>Create a FunctionToolSpec from a python function. By default, the tool name, description and parameters' json schema will be extracted from the function's docstring and the parameters' type and description. However, these values can be customized by passing in the corresponding arguments.</p> <p>Parameters:</p> Name Type Description Default <code>func</code> <code>Callable</code> <p>The python function to create a FunctionToolSpec from.</p> required <code>tool_name</code> <code>Optional[str]</code> <p>The name of the tool. If not provided, the function name will be used.</p> <code>None</code> <code>tool_description</code> <code>Optional[str]</code> <p>The description of the tool. If not provided, the function docstring will be used.</p> <code>None</code> <code>tool_parameters</code> <code>Optional[Dict[str, Any]]</code> <p>The JSON schema of the tool's parameters. If not provided, the JSON schema will be constructed properly from the parameters' annotations, the function's signature and/or docstring.</p> <code>None</code> <p>Returns:</p> Type Description <code>FunctionToolSpec</code> <p>A new <code>FunctionToolSpec</code> object.</p> Source code in <code>bridgic/core/agentic/tool_specs/_function_tool_spec.py</code> <pre><code>@classmethod\ndef from_raw(\n    cls,\n    func: Callable,\n    tool_name: Optional[str] = None,\n    tool_description: Optional[str] = None,\n    tool_parameters: Optional[Dict[str, Any]] = None,\n) -&gt; \"FunctionToolSpec\":\n    \"\"\"\n    Create a FunctionToolSpec from a python function. By default, the tool name, description and parameters' json schema will be extracted from the function's docstring and the parameters' type and description. However, these values can be customized by passing in the corresponding arguments.\n\n    Parameters\n    ----------\n    func : Callable\n        The python function to create a FunctionToolSpec from.\n    tool_name : Optional[str]\n        The name of the tool. If not provided, the function name will be used.\n    tool_description : Optional[str]\n        The description of the tool. If not provided, the function docstring will be used.\n    tool_parameters : Optional[Dict[str, Any]]\n        The JSON schema of the tool's parameters. If not provided, the JSON schema will be constructed properly from the parameters' annotations, the function's signature and/or docstring.\n\n    Returns\n    -------\n    FunctionToolSpec\n        A new `FunctionToolSpec` object.\n    \"\"\"\n    if isinstance(func, MethodType):\n        raise ValueError(f\"`func` is not allowed to be a bound method: {func}.\")\n\n    if not tool_name:\n        tool_name = func.__name__\n\n    if not tool_description:\n        tool_description = get_tool_description_from(func, tool_name)\n\n    if not tool_parameters:\n        tool_parameters = create_func_params_json_schema(func)\n        # TODO: whether to remove the `title` field of the params_schema?\n\n    return cls(\n        func=func,\n        tool_name=tool_name,\n        tool_description=tool_description,\n        tool_parameters=tool_parameters\n    )\n</code></pre>"},{"location":"reference/bridgic-core/bridgic/core/agentic/tool_specs/#bridgic.core.agentic.tool_specs.FunctionToolSpec.to_tool","title":"to_tool","text":"<pre><code>to_tool() -&gt; Tool\n</code></pre> <p>Transform this FunctionToolSpec to a <code>Tool</code> object used by LLM.</p> <p>Returns:</p> Type Description <code>Tool</code> <p>A <code>Tool</code> object that can be used by LLM.</p> Source code in <code>bridgic/core/agentic/tool_specs/_function_tool_spec.py</code> <pre><code>@override\ndef to_tool(self) -&gt; Tool:\n    \"\"\"\n    Transform this FunctionToolSpec to a `Tool` object used by LLM.\n\n    Returns\n    -------\n    Tool\n        A `Tool` object that can be used by LLM.\n    \"\"\"\n    return Tool(\n        name=self._tool_name,\n        description=self._tool_description,\n        parameters=self._tool_parameters\n    )\n</code></pre>"},{"location":"reference/bridgic-core/bridgic/core/agentic/tool_specs/#bridgic.core.agentic.tool_specs.FunctionToolSpec.create_worker","title":"create_worker","text":"<pre><code>create_worker() -&gt; Worker\n</code></pre> <p>Create a Worker from the information included in this FunctionToolSpec.</p> <p>Returns:</p> Type Description <code>Worker</code> <p>A new <code>Worker</code> object that can be added to an Automa to execute the tool.</p> Source code in <code>bridgic/core/agentic/tool_specs/_function_tool_spec.py</code> <pre><code>@override\ndef create_worker(self) -&gt; Worker:\n    \"\"\"\n    Create a Worker from the information included in this FunctionToolSpec.\n\n    Returns\n    -------\n    Worker\n        A new `Worker` object that can be added to an Automa to execute the tool.\n    \"\"\"\n    # TODO: some initialization arguments may be needed in future, e.g., `bound_needed`.\n    return CallableWorker(self._func)\n</code></pre>"},{"location":"reference/bridgic-core/bridgic/core/agentic/tool_specs/#bridgic.core.agentic.tool_specs.AutomaToolSpec","title":"AutomaToolSpec","text":"<p>               Bases: <code>ToolSpec</code></p> Source code in <code>bridgic/core/agentic/tool_specs/_automa_tool_spec.py</code> <pre><code>class AutomaToolSpec(ToolSpec):\n    _automa_cls: Type[Automa]\n    \"\"\"The Automa class to be used as a tool\"\"\"\n    _automa_init_kwargs: Dict[str, Any]\n    \"\"\"The initialization arguments for the Automa\"\"\"\n\n    def __init__(\n        self,\n        automa_cls: Type[Automa],\n        tool_name: Optional[str] = None,\n        tool_description: Optional[str] = None,\n        tool_parameters: Optional[Dict[str, Any]] = None,\n        **automa_init_kwargs: Dict[str, Any],\n    ):\n        super().__init__(\n            tool_name=tool_name,\n            tool_description=tool_description,\n            tool_parameters=tool_parameters\n        )\n        self._automa_cls = automa_cls\n        self._automa_init_kwargs = automa_init_kwargs\n\n    @classmethod\n    def from_raw(\n        cls,\n        automa_cls: Type[Automa],\n        tool_name: Optional[str] = None,\n        tool_description: Optional[str] = None,\n        tool_parameters: Optional[Dict[str, Any]] = None,\n        **automa_init_kwargs: Dict[str, Any],\n    ) -&gt; \"AutomaToolSpec\":\n        \"\"\"\n        Create an AutomaToolSpec from an Automa class.\n        \"\"\"\n\n        def check_spec_func(automa_cls):\n            if hasattr(automa_cls, \"spec_func\") and isinstance(automa_cls.spec_func, Callable):\n                return\n            raise ValueError(f\"The Automa class {automa_cls} must be decorated with `@as_tool` in order to be used as a tool.\")\n\n        if (not tool_name) or (not tool_description) or (not tool_parameters):\n            check_spec_func(automa_cls)\n\n        if not tool_name:\n            tool_name = automa_cls.spec_func.__name__\n\n        if not tool_description:\n            tool_description = get_tool_description_from(automa_cls.spec_func, tool_name)\n\n        if not tool_parameters:\n            tool_parameters = create_func_params_json_schema(automa_cls.spec_func)\n            # TODO: whether to remove the `title` field of the params_schema?\n\n        return cls(\n            automa_cls=automa_cls,\n            tool_name=tool_name,\n            tool_description=tool_description,\n            tool_parameters=tool_parameters,\n            **automa_init_kwargs\n        )\n\n    @override\n    def to_tool(self) -&gt; Tool:\n        \"\"\"\n        Transform this AutomaToolSpec to a `Tool` object used by LLM.\n\n        Returns\n        -------\n        Tool\n            A `Tool` object that can be used by LLM.\n        \"\"\"\n        return Tool(\n            name=self._tool_name,\n            description=self._tool_description,\n            parameters=self._tool_parameters\n        )\n\n    @override\n    def create_worker(self) -&gt; Worker:\n        \"\"\"\n        Create a Worker from the information included in this AutomaToolSpec.\n\n        Returns\n        -------\n        Worker\n            A new `Worker` object that can be added to an Automa to execute the tool.\n        \"\"\"\n        return self._automa_cls(**self._automa_init_kwargs)\n\n    @override\n    def dump_to_dict(self) -&gt; Dict[str, Any]:\n        state_dict = super().dump_to_dict()\n        state_dict[\"automa_cls\"] = self._automa_cls.__module__ + \".\" + self._automa_cls.__qualname__\n        if self._automa_init_kwargs:\n            state_dict[\"automa_init_kwargs\"] = self._automa_init_kwargs\n        return state_dict\n\n    @override\n    def load_from_dict(self, state_dict: Dict[str, Any]) -&gt; None:\n        super().load_from_dict(state_dict)\n        self._automa_cls = load_qualified_class_or_func(state_dict[\"automa_cls\"])\n        self._automa_init_kwargs = state_dict.get(\"automa_init_kwargs\") or {}\n</code></pre>"},{"location":"reference/bridgic-core/bridgic/core/agentic/tool_specs/#bridgic.core.agentic.tool_specs.AutomaToolSpec.from_raw","title":"from_raw","text":"<code>classmethod</code> <pre><code>from_raw(\n    automa_cls: Type[Automa],\n    tool_name: Optional[str] = None,\n    tool_description: Optional[str] = None,\n    tool_parameters: Optional[Dict[str, Any]] = None,\n    **automa_init_kwargs: Dict[str, Any]\n) -&gt; AutomaToolSpec\n</code></pre> <p>Create an AutomaToolSpec from an Automa class.</p> Source code in <code>bridgic/core/agentic/tool_specs/_automa_tool_spec.py</code> <pre><code>@classmethod\ndef from_raw(\n    cls,\n    automa_cls: Type[Automa],\n    tool_name: Optional[str] = None,\n    tool_description: Optional[str] = None,\n    tool_parameters: Optional[Dict[str, Any]] = None,\n    **automa_init_kwargs: Dict[str, Any],\n) -&gt; \"AutomaToolSpec\":\n    \"\"\"\n    Create an AutomaToolSpec from an Automa class.\n    \"\"\"\n\n    def check_spec_func(automa_cls):\n        if hasattr(automa_cls, \"spec_func\") and isinstance(automa_cls.spec_func, Callable):\n            return\n        raise ValueError(f\"The Automa class {automa_cls} must be decorated with `@as_tool` in order to be used as a tool.\")\n\n    if (not tool_name) or (not tool_description) or (not tool_parameters):\n        check_spec_func(automa_cls)\n\n    if not tool_name:\n        tool_name = automa_cls.spec_func.__name__\n\n    if not tool_description:\n        tool_description = get_tool_description_from(automa_cls.spec_func, tool_name)\n\n    if not tool_parameters:\n        tool_parameters = create_func_params_json_schema(automa_cls.spec_func)\n        # TODO: whether to remove the `title` field of the params_schema?\n\n    return cls(\n        automa_cls=automa_cls,\n        tool_name=tool_name,\n        tool_description=tool_description,\n        tool_parameters=tool_parameters,\n        **automa_init_kwargs\n    )\n</code></pre>"},{"location":"reference/bridgic-core/bridgic/core/agentic/tool_specs/#bridgic.core.agentic.tool_specs.AutomaToolSpec.to_tool","title":"to_tool","text":"<pre><code>to_tool() -&gt; Tool\n</code></pre> <p>Transform this AutomaToolSpec to a <code>Tool</code> object used by LLM.</p> <p>Returns:</p> Type Description <code>Tool</code> <p>A <code>Tool</code> object that can be used by LLM.</p> Source code in <code>bridgic/core/agentic/tool_specs/_automa_tool_spec.py</code> <pre><code>@override\ndef to_tool(self) -&gt; Tool:\n    \"\"\"\n    Transform this AutomaToolSpec to a `Tool` object used by LLM.\n\n    Returns\n    -------\n    Tool\n        A `Tool` object that can be used by LLM.\n    \"\"\"\n    return Tool(\n        name=self._tool_name,\n        description=self._tool_description,\n        parameters=self._tool_parameters\n    )\n</code></pre>"},{"location":"reference/bridgic-core/bridgic/core/agentic/tool_specs/#bridgic.core.agentic.tool_specs.AutomaToolSpec.create_worker","title":"create_worker","text":"<pre><code>create_worker() -&gt; Worker\n</code></pre> <p>Create a Worker from the information included in this AutomaToolSpec.</p> <p>Returns:</p> Type Description <code>Worker</code> <p>A new <code>Worker</code> object that can be added to an Automa to execute the tool.</p> Source code in <code>bridgic/core/agentic/tool_specs/_automa_tool_spec.py</code> <pre><code>@override\ndef create_worker(self) -&gt; Worker:\n    \"\"\"\n    Create a Worker from the information included in this AutomaToolSpec.\n\n    Returns\n    -------\n    Worker\n        A new `Worker` object that can be added to an Automa to execute the tool.\n    \"\"\"\n    return self._automa_cls(**self._automa_init_kwargs)\n</code></pre>"},{"location":"reference/bridgic-core/bridgic/core/agentic/tool_specs/#bridgic.core.agentic.tool_specs.as_tool","title":"as_tool","text":"<pre><code>as_tool(spec_func: Callable) -&gt; Callable\n</code></pre> <p>A decorator that transforms a class to a tool that may be used by LLM.</p> <p>Parameters:</p> Name Type Description Default <code>spec_func</code> <code>Callable</code> <p>The function used to declare the tool spec. Note that this function is not intended to be called directly.</p> required Source code in <code>bridgic/core/agentic/tool_specs/_automa_tool_spec.py</code> <pre><code>def as_tool(spec_func: Callable) -&gt; Callable:\n    \"\"\"\n    A decorator that transforms a class to a tool that may be used by LLM.\n\n    Parameters\n    ----------\n    spec_func : Callable\n        The function used to declare the tool spec. Note that this function is not intended to be called directly.\n    \"\"\"\n    def decorator(cls):\n        if not isinstance(spec_func, Callable):\n            raise ValueError(f\"A function argument is expected, but got {type(spec_func)}.\")\n        if isinstance(spec_func, MethodType):\n            raise ValueError(f\"`spec_func` is not allowed to be a bound method: {spec_func}.\")\n        cls.spec_func = spec_func\n        return cls\n    return decorator\n</code></pre>"},{"location":"reference/bridgic-core/bridgic/core/agentic/types/","title":"types","text":"<p>The Agentic Types module defines foundational data structures for agentic systems.</p> <p>This module defines several important type definitions, such as <code>ToolSpec</code> and  <code>ChatMessage</code>, which are designed to be \"model-neutral\" as much as possible,  allowing developers to build agentic systems using different models.</p>"},{"location":"reference/bridgic-core/bridgic/core/agentic/types/#bridgic.core.agentic.types.Function","title":"Function","text":"<p>               Bases: <code>TypedDict</code></p> <p>The function that the model called.</p> Source code in <code>bridgic/core/agentic/types/_chat_message.py</code> <pre><code>class Function(TypedDict, total=True):\n    \"\"\"The function that the model called.\"\"\"\n\n    arguments: Required[str]\n    \"\"\"\n    The arguments to call the function with, as generated by the model in JSON\n    format. Note that the model does not always generate valid JSON, and may\n    hallucinate parameters not defined by your function schema. Validate the\n    arguments in your code before calling your function.\n    \"\"\"\n    name: Required[str]\n    \"\"\"The name of the function to call.\"\"\"\n</code></pre>"},{"location":"reference/bridgic-core/bridgic/core/agentic/types/#bridgic.core.agentic.types.Function.arguments","title":"arguments  <code>instance-attribute</code>","text":"<pre><code>arguments: Required[str]\n</code></pre> <p>The arguments to call the function with, as generated by the model in JSON format. Note that the model does not always generate valid JSON, and may hallucinate parameters not defined by your function schema. Validate the arguments in your code before calling your function.</p>"},{"location":"reference/bridgic-core/bridgic/core/agentic/types/#bridgic.core.agentic.types.Function.name","title":"name  <code>instance-attribute</code>","text":"<pre><code>name: Required[str]\n</code></pre> <p>The name of the function to call.</p>"},{"location":"reference/bridgic-core/bridgic/core/agentic/types/#bridgic.core.agentic.types.FunctionToolCall","title":"FunctionToolCall","text":"<p>               Bases: <code>TypedDict</code></p> <p>A call to a function tool created by the model.</p> Source code in <code>bridgic/core/agentic/types/_chat_message.py</code> <pre><code>class FunctionToolCall(TypedDict, total=True):\n    \"\"\"A call to a function tool created by the model.\"\"\"\n\n    id: Required[str]\n    \"\"\"The ID of the tool call.\"\"\"\n    function: Required[Function]\n    \"\"\"The function that the model called.\"\"\"\n    type: Required[Literal[\"function\"]]\n    \"\"\"The type of the tool. Currently, only `function` is supported.\"\"\"\n</code></pre>"},{"location":"reference/bridgic-core/bridgic/core/agentic/types/#bridgic.core.agentic.types.FunctionToolCall.id","title":"id  <code>instance-attribute</code>","text":"<pre><code>id: Required[str]\n</code></pre> <p>The ID of the tool call.</p>"},{"location":"reference/bridgic-core/bridgic/core/agentic/types/#bridgic.core.agentic.types.FunctionToolCall.function","title":"function  <code>instance-attribute</code>","text":"<pre><code>function: Required[Function]\n</code></pre> <p>The function that the model called.</p>"},{"location":"reference/bridgic-core/bridgic/core/agentic/types/#bridgic.core.agentic.types.FunctionToolCall.type","title":"type  <code>instance-attribute</code>","text":"<pre><code>type: Required[Literal['function']]\n</code></pre> <p>The type of the tool. Currently, only <code>function</code> is supported.</p>"},{"location":"reference/bridgic-core/bridgic/core/agentic/types/#bridgic.core.agentic.types.SystemMessage","title":"SystemMessage","text":"<p>               Bases: <code>TypedDict</code></p> <p>Developer-provided instructions that the model should follow, regardless of messages sent by the user.</p> Source code in <code>bridgic/core/agentic/types/_chat_message.py</code> <pre><code>class SystemMessage(TypedDict, total=False):\n    \"\"\"Developer-provided instructions that the model should follow, regardless of messages sent by the user.\"\"\"\n\n    role: Required[Literal[\"system\"]]\n    \"\"\"The role of the messages author, in this case `system`.\"\"\"\n    content: Required[str]\n    \"\"\"The contents of the system message, which is a text.\"\"\"\n</code></pre>"},{"location":"reference/bridgic-core/bridgic/core/agentic/types/#bridgic.core.agentic.types.SystemMessage.role","title":"role  <code>instance-attribute</code>","text":"<pre><code>role: Required[Literal['system']]\n</code></pre> <p>The role of the messages author, in this case <code>system</code>.</p>"},{"location":"reference/bridgic-core/bridgic/core/agentic/types/#bridgic.core.agentic.types.SystemMessage.content","title":"content  <code>instance-attribute</code>","text":"<pre><code>content: Required[str]\n</code></pre> <p>The contents of the system message, which is a text.</p>"},{"location":"reference/bridgic-core/bridgic/core/agentic/types/#bridgic.core.agentic.types.UserTextMessage","title":"UserTextMessage","text":"<p>               Bases: <code>TypedDict</code></p> <p>Messages sent by an end user, containing prompts.</p> Source code in <code>bridgic/core/agentic/types/_chat_message.py</code> <pre><code>class UserTextMessage(TypedDict, total=False):\n    \"\"\"Messages sent by an end user, containing prompts.\"\"\"\n\n    role: Required[Literal[\"user\"]]\n    \"\"\"The role of the messages author, in this case `user`.\"\"\"\n    content: Required[str]\n    \"\"\"The content of the user message, which is a text.\"\"\"\n</code></pre>"},{"location":"reference/bridgic-core/bridgic/core/agentic/types/#bridgic.core.agentic.types.UserTextMessage.role","title":"role  <code>instance-attribute</code>","text":"<pre><code>role: Required[Literal['user']]\n</code></pre> <p>The role of the messages author, in this case <code>user</code>.</p>"},{"location":"reference/bridgic-core/bridgic/core/agentic/types/#bridgic.core.agentic.types.UserTextMessage.content","title":"content  <code>instance-attribute</code>","text":"<pre><code>content: Required[str]\n</code></pre> <p>The content of the user message, which is a text.</p>"},{"location":"reference/bridgic-core/bridgic/core/agentic/types/#bridgic.core.agentic.types.AssistantTextMessage","title":"AssistantTextMessage","text":"<p>               Bases: <code>TypedDict</code></p> <p>Messages sent by the model in response to user messages.</p> Source code in <code>bridgic/core/agentic/types/_chat_message.py</code> <pre><code>class AssistantTextMessage(TypedDict, total=False):\n    \"\"\"Messages sent by the model in response to user messages.\"\"\"\n\n    role: Required[Literal[\"assistant\"]]\n    \"\"\"The role of the messages author, in this case `assistant`.\"\"\"\n    content: Optional[str]\n    \"\"\"The content of the assistant message, which is a text. Required unless `tool_calls` is specified.\"\"\"\n    tool_calls: Optional[Iterable[FunctionToolCall]]\n    \"\"\"The tool calls generated by the model, such as function calls.\"\"\"\n</code></pre>"},{"location":"reference/bridgic-core/bridgic/core/agentic/types/#bridgic.core.agentic.types.AssistantTextMessage.role","title":"role  <code>instance-attribute</code>","text":"<pre><code>role: Required[Literal['assistant']]\n</code></pre> <p>The role of the messages author, in this case <code>assistant</code>.</p>"},{"location":"reference/bridgic-core/bridgic/core/agentic/types/#bridgic.core.agentic.types.AssistantTextMessage.content","title":"content  <code>instance-attribute</code>","text":"<pre><code>content: Optional[str]\n</code></pre> <p>The content of the assistant message, which is a text. Required unless <code>tool_calls</code> is specified.</p>"},{"location":"reference/bridgic-core/bridgic/core/agentic/types/#bridgic.core.agentic.types.AssistantTextMessage.tool_calls","title":"tool_calls  <code>instance-attribute</code>","text":"<pre><code>tool_calls: Optional[Iterable[FunctionToolCall]]\n</code></pre> <p>The tool calls generated by the model, such as function calls.</p>"},{"location":"reference/bridgic-core/bridgic/core/agentic/types/#bridgic.core.agentic.types.ToolMessage","title":"ToolMessage","text":"<p>               Bases: <code>TypedDict</code></p> <p>Messages generated by tools.</p> Source code in <code>bridgic/core/agentic/types/_chat_message.py</code> <pre><code>class ToolMessage(TypedDict, total=False):\n    \"\"\"Messages generated by tools.\"\"\"\n\n    role: Required[Literal[\"tool\"]]\n    \"\"\"The role of the messages author, in this case `tool`.\"\"\"\n    content: Required[str]\n    \"\"\"The contents of the tool message.\"\"\"\n    tool_call_id: Required[str]\n    \"\"\"Tool call that this message is responding to.\"\"\"\n</code></pre>"},{"location":"reference/bridgic-core/bridgic/core/agentic/types/#bridgic.core.agentic.types.ToolMessage.role","title":"role  <code>instance-attribute</code>","text":"<pre><code>role: Required[Literal['tool']]\n</code></pre> <p>The role of the messages author, in this case <code>tool</code>.</p>"},{"location":"reference/bridgic-core/bridgic/core/agentic/types/#bridgic.core.agentic.types.ToolMessage.content","title":"content  <code>instance-attribute</code>","text":"<pre><code>content: Required[str]\n</code></pre> <p>The contents of the tool message.</p>"},{"location":"reference/bridgic-core/bridgic/core/agentic/types/#bridgic.core.agentic.types.ToolMessage.tool_call_id","title":"tool_call_id  <code>instance-attribute</code>","text":"<pre><code>tool_call_id: Required[str]\n</code></pre> <p>Tool call that this message is responding to.</p>"},{"location":"reference/bridgic-core/bridgic/core/agentic/workers/","title":"workers","text":"<p>The Agentic Workers module provides specialized implementation of Worker for agentic systems.</p> <p>This module provides specialized Worker implementations for specific functions to support  building Agentic systems with complex capabilities.</p>"},{"location":"reference/bridgic-core/bridgic/core/agentic/workers/#bridgic.core.agentic.workers.ToolSelectionWorker","title":"ToolSelectionWorker","text":"<p>               Bases: <code>Worker</code></p> <p>A worker that calls an LLM to select tools and/or generate a response.</p> Source code in <code>bridgic/core/agentic/workers/_tool_selection.py</code> <pre><code>class ToolSelectionWorker(Worker):\n    \"\"\"\n    A worker that calls an LLM to select tools and/or generate a response.\n    \"\"\"\n\n    # Note: the ToolSelection LLM instance need support serialization and deserialization.\n    _tool_selection_llm: ToolSelection\n    \"\"\"The LLM to be used for tool selection.\"\"\"\n\n    def __init__(self, tool_selection_llm: ToolSelection):\n        \"\"\"\n        Parameters\n        ----------\n        tool_selection_llm: ToolSelect\n            The LLM to be used for tool selection.\n        \"\"\"\n        super().__init__()\n        self._tool_selection_llm = tool_selection_llm\n\n    async def arun(\n        self,\n        messages: List[ChatMessage],\n        tools: List[Tool],\n    ) -&gt; Tuple[List[ToolCall], Optional[str]]:\n        \"\"\"\n        Run the worker.\n\n        Parameters\n        ----------\n        messages: List[ChatMessage]\n            The messages to send to the LLM.\n        tools: List[Tool]\n            The tool list for the LLM to select from.\n\n        Returns\n        -------\n        Tuple[List[ToolCall], Optional[str]]\n            * The first element is a list of `ToolCall` that the LLM selected.\n            * The second element is the text response from the LLM.\n        \"\"\"\n        # Validate and transform the input messages and tools to the format expected by the LLM.\n        llm_messages: List[Message] = []\n        for message in messages:\n            llm_messages.append(transform_chat_message_to_llm_message(message))\n        # print(f\"\\n******* ToolSelectionWorker.arun *******\\n\")\n        # print(f\"messages: {llm_messages}\")\n        # print(f\"tools: {tools}\")\n        tool_calls, llm_response = await self._tool_selection_llm.aselect_tool(\n            messages=llm_messages, \n            tools=tools, \n        )\n        return tool_calls, llm_response\n\n    @override\n    def dump_to_dict(self) -&gt; Dict[str, Any]:\n        state_dict = super().dump_to_dict()\n        state_dict[\"tool_selection_llm\"] = self._tool_selection_llm\n        return state_dict\n\n    @override\n    def load_from_dict(self, state_dict: Dict[str, Any]) -&gt; None:\n        super().load_from_dict(state_dict)\n        self._tool_selection_llm = state_dict[\"tool_selection_llm\"]\n</code></pre>"},{"location":"reference/bridgic-core/bridgic/core/agentic/workers/#bridgic.core.agentic.workers.ToolSelectionWorker.arun","title":"arun","text":"<code>async</code> <pre><code>arun(\n    messages: List[ChatMessage], tools: List[Tool]\n) -&gt; Tuple[List[ToolCall], Optional[str]]\n</code></pre> <p>Run the worker.</p> <p>Parameters:</p> Name Type Description Default <code>messages</code> <code>List[ChatMessage]</code> <p>The messages to send to the LLM.</p> required <code>tools</code> <code>List[Tool]</code> <p>The tool list for the LLM to select from.</p> required <p>Returns:</p> Type Description <code>Tuple[List[ToolCall], Optional[str]]</code> <ul> <li>The first element is a list of <code>ToolCall</code> that the LLM selected.</li> <li>The second element is the text response from the LLM.</li> </ul> Source code in <code>bridgic/core/agentic/workers/_tool_selection.py</code> <pre><code>async def arun(\n    self,\n    messages: List[ChatMessage],\n    tools: List[Tool],\n) -&gt; Tuple[List[ToolCall], Optional[str]]:\n    \"\"\"\n    Run the worker.\n\n    Parameters\n    ----------\n    messages: List[ChatMessage]\n        The messages to send to the LLM.\n    tools: List[Tool]\n        The tool list for the LLM to select from.\n\n    Returns\n    -------\n    Tuple[List[ToolCall], Optional[str]]\n        * The first element is a list of `ToolCall` that the LLM selected.\n        * The second element is the text response from the LLM.\n    \"\"\"\n    # Validate and transform the input messages and tools to the format expected by the LLM.\n    llm_messages: List[Message] = []\n    for message in messages:\n        llm_messages.append(transform_chat_message_to_llm_message(message))\n    # print(f\"\\n******* ToolSelectionWorker.arun *******\\n\")\n    # print(f\"messages: {llm_messages}\")\n    # print(f\"tools: {tools}\")\n    tool_calls, llm_response = await self._tool_selection_llm.aselect_tool(\n        messages=llm_messages, \n        tools=tools, \n    )\n    return tool_calls, llm_response\n</code></pre>"},{"location":"reference/bridgic-core/bridgic/core/automa/","title":"automa","text":"<p>This module contains the core Automa classes and functions.</p>"},{"location":"reference/bridgic-core/bridgic/core/automa/#bridgic.core.automa.Automa","title":"Automa","text":"<p>               Bases: <code>Worker</code></p> <p>Base class for an Automa.</p> <p>In Bridgic, an Automa is an entity that manages and orchestrates a group of workers. An Automa itself is also a Worker, which enables the nesting of Automa instances within each other.</p> Source code in <code>bridgic/core/automa/_automa.py</code> <pre><code>class Automa(Worker):\n    \"\"\"\n    Base class for an Automa.\n\n    In Bridgic, an Automa is an entity that manages and orchestrates a group of workers. An Automa itself is also a Worker, which enables the nesting of Automa instances within each other.\n    \"\"\"\n    _running_options: RunningOptions\n\n    # For event handling.\n    _event_handlers: Dict[str, EventHandlerType]\n    _default_event_handler: EventHandlerType\n\n    # For human interaction.\n    _worker_interaction_indices: Dict[str, int]\n\n    # Ongoing human interactions triggered by the `interact_with_human()` call from workers of the current Automa.\n    # worker_key -&gt; list of interactions.\n    _ongoing_interactions: Dict[str, List[_InteractionAndFeedback]]\n\n    _thread_pool: ThreadPoolExecutor\n    _main_thread_id: int\n    _main_loop: asyncio.AbstractEventLoop\n\n    # Cached callbacks for top-level automa execution, which are built once and reused across multiple arun() calls.\n    _cached_callbacks: Optional[List[WorkerCallback]] = None\n\n    def __init__(\n        self,\n        name: str = None,\n        thread_pool: Optional[ThreadPoolExecutor] = None,\n        running_options: Optional[RunningOptions] = None,\n    ):\n        super().__init__()\n\n        # Set parent to self for top-level Automa\n        self.parent = self\n\n        # Set the name of the Automa instance.\n        self.name = name or f\"{self.__class__.__name__}-{uuid.uuid4().hex[:8]}\"\n\n        # Initialize the shared running options.\n        self._running_options = running_options or RunningOptions()\n\n        # Initialize data structures for event handling and human interactions\n        self._event_handlers = {}\n        self._default_event_handler = None\n        self._worker_interaction_indices = {}\n        self._ongoing_interactions = {}\n\n        self._thread_pool = thread_pool\n\n    @override\n    def dump_to_dict(self) -&gt; Dict[str, Any]:\n        state_dict = super().dump_to_dict()\n        state_dict[\"name\"] = self.name\n        state_dict[\"running_options\"] = self._running_options\n        state_dict[\"ongoing_interactions\"] = self._ongoing_interactions\n        return state_dict\n\n    @override\n    def load_from_dict(self, state_dict: Dict[str, Any]) -&gt; None:\n        super().load_from_dict(state_dict)\n        self.name = state_dict[\"name\"]\n        self._running_options = state_dict[\"running_options\"]\n\n        self._event_handlers = {}\n        self._default_event_handler = None\n        self._worker_interaction_indices = {}\n        self._ongoing_interactions = state_dict[\"ongoing_interactions\"]\n        self._thread_pool = None\n\n    @classmethod\n    def load_from_snapshot(\n        cls, \n        snapshot: Snapshot,\n        thread_pool: Optional[ThreadPoolExecutor] = None,\n    ) -&gt; \"Automa\":\n        \"\"\"\n        Load an Automa instance from a snapshot.\n\n        Parameters\n        ----------\n        snapshot: Snapshot\n            The snapshot to load the Automa instance from.\n        thread_pool: Optional[ThreadPoolExecutor]\n            The thread pool for parallel running of I/O-bound tasks. If not provided, a default thread pool will be used.\n\n        Returns\n        -------\n        Automa\n            The loaded Automa instance.\n        \"\"\"\n        # Here you can compare snapshot.serialization_version with SERIALIZATION_VERSION, and handle any necessary version compatibility issues if needed.\n        automa = load_bytes(snapshot.serialized_bytes)\n        if thread_pool:\n            automa.thread_pool = thread_pool\n        return automa\n\n    @property\n    def thread_pool(self) -&gt; Optional[ThreadPoolExecutor]:\n        \"\"\"\n        Get/Set the thread pool for parallel running of I/O-bound tasks used by the current Automa instance and its nested Automa instances.\n\n        Note: If an Automa is nested within another Automa, the thread pool of the top-level Automa will be used, rather than the thread pool of the nested Automa.\n\n        Returns\n        -------\n        Optional[ThreadPoolExecutor]\n            The thread pool.\n        \"\"\"\n        return self._thread_pool\n\n    @thread_pool.setter\n    def thread_pool(self, executor: ThreadPoolExecutor) -&gt; None:\n        \"\"\"\n        Set the thread pool for parallel running of I/O-bound tasks.\n\n        Note: If an Automa is nested within another Automa, the thread pool of the top-level Automa will be used, rather than the thread pool of the nested Automa.\n        \"\"\"\n        self._thread_pool = executor\n\n    @abstractmethod\n    def _locate_interacting_worker(self) -&gt; Optional[str]:\n        \"\"\"\n        Locate the worker that is currently interacting with human.\n\n        Returns\n        -------\n        Optional[str]\n            The necessary identifier of the worker that is currently interacting with human.\n        \"\"\"\n        ...\n\n    @abstractmethod\n    def _get_worker_key(self, worker: Worker) -&gt; Optional[str]:\n        \"\"\"\n        Identify the worker key by the worker instance.\n        \"\"\"\n        ...\n\n    @abstractmethod\n    def _get_worker_instance(self, worker_key: str) -&gt; Worker:\n        \"\"\"\n        Get the worker instance by the worker key.\n        \"\"\"\n        ...\n\n    def set_running_options(\n        self,\n        debug: Optional[bool] = None,\n    ):\n        \"\"\"\n        Set runtime-configurable running options for this Automa instance.\n\n        This method only supports fields that can be changed at runtime. Fields that must be set \n        during initialization (such as `callback_builders`) cannot be changed here and must be \n        provided via the `running_options` parameter in the Automa constructor.\n\n        For fields that can be delayed to be set, some settings (like `debug`) from the outermost \n        (top-level) Automa will override the settings of all inner (nested) Automa instances.\n        For example, if the top-level Automa instance sets `debug = True` and the nested instances \n        sets `debug = False`, then the nested Automa instance will run in debug mode when the \n        top-level Automa instance is executed. We call it the Setting Penetration Mechanism.\n\n        Parameters\n        ----------\n        debug : bool, optional\n            Whether to enable debug mode. If None, the current value is not changed.\n            This field is subject to the Setting Penetration Mechanism.\n        \"\"\"\n        if debug is not None:\n            self._running_options.debug = debug\n\n    def _get_top_running_options(self) -&gt; RunningOptions:\n        if self.is_top_level():\n            # Here we are at the top-level automa.\n            return self._running_options\n        return self.parent._get_top_running_options()\n\n    def _collect_ancestor_callback_builders(self) -&gt; List[WorkerCallbackBuilder]:\n        \"\"\"\n        Collect callback builders from all ancestor automas in the ancestor chain.\n\n        This method traverses up the automa ancestor chain (from current to top-level)\n        to collect all callback builders from ancestor automas' RunningOptions, ensuring\n        that callbacks from all levels are propagated to nested workers. The current\n        automa's callbacks are included as the last in the chain.\n\n        The ancestor chain is the path from the current automa up to the top-level automa\n        through the parent relationship. This method collects callbacks from all automas\n        in this chain, ordered from top-level (first) to current level (last).\n\n        Returns\n        -------\n        List[WorkerCallbackBuilder]\n            A list of callback builders collected from all ancestor automas in the ancestor\n            chain and the current automa, ordered from top-level (first) to current level (last).\n        \"\"\"\n        # First, collect all callback builders by traversing up the ancestor chain\n        # (from current to top-level, stored in reverse order)\n        ancestor_callback_builders_reverse = []\n        current: Optional[Automa] = self\n\n        # Traverse up the ancestor chain to collect callback builders\n        while True:\n            ancestor_callback_builders_reverse.append(current._running_options.callback_builders)\n            if current.is_top_level():\n                break\n            else:\n                current = current.parent\n\n        # Reverse to get order from top-level (first) to current (last)\n        callback_builders = []\n        for builders in reversed(ancestor_callback_builders_reverse):\n            callback_builders.extend(builders)\n\n        return callback_builders\n\n    def _get_automa_callbacks(self) -&gt; List[WorkerCallback]:\n        \"\"\"\n        Get or build cached callback instances for top-level automa execution.\n\n        This method ensures that callback instances are built once and reused across\n        multiple arun() calls, respecting the is_shared setting of each builder.\n\n        Returns\n        -------\n        List[WorkerCallback]\n            List of callback instances for top-level automa execution.\n        \"\"\"\n        if self._cached_callbacks is None:\n            effective_builders = []\n            effective_builders.extend(GlobalSetting.read().callback_builders)\n            effective_builders.extend(self._running_options.callback_builders)\n            self._cached_callbacks = [cb.build() for cb in effective_builders]\n        return self._cached_callbacks\n\n    ###############################################################\n    ########## [Bridgic Event Handling Mechanism] starts ##########\n    ###############################################################\n\n    def register_event_handler(self, event_type: Optional[str], event_handler: EventHandlerType) -&gt; None:\n        \"\"\"\n        Register an event handler for the specified event type. If `event_type` is set to None, the event handler will be registered as the default handler that will handle all event types.\n\n        Note: Only event handlers registered on the top-level Automa will be invoked to handle events.\n\n        Parameters\n        ----------\n        event_type: Optional[str]\n            The type of event to be handled. If set to None, the event handler will be registered as the default handler and will be used to handle all event types.\n        event_handler: EventHandlerType\n            The event handler to be registered.\n        \"\"\"\n        if event_type is None:\n            self._default_event_handler = event_handler\n        else:\n            self._event_handlers[event_type] = event_handler\n\n    def unregister_event_handler(self, event_type: Optional[str]) -&gt; None:\n        \"\"\"\n        Unregister an event handler for the specified event type.\n\n        Parameters\n        ----------\n        event_type: Optional[str]\n            The type of event to be unregistered. If set to None, the default event handler will be unregistered.\n        \"\"\"\n        if event_type in self._event_handlers:\n            del self._event_handlers[event_type]\n        if event_type is None:\n            self._default_event_handler = None\n\n    def unregister_all_event_handlers(self) -&gt; None:\n        \"\"\"\n        Unregister all event handlers.\n        \"\"\"\n        self._event_handlers.clear()\n        self._default_event_handler = None\n\n    class _FeedbackSender(FeedbackSender):\n        def __init__(\n                self, \n                future: asyncio.Future[Feedback],\n                post_loop: asyncio.AbstractEventLoop,\n                ):\n            self._future = future\n            self._post_loop = post_loop\n\n        def send(self, feedback: Feedback) -&gt; None:\n            try:\n                current_loop = asyncio.get_running_loop()\n            except Exception:\n                current_loop = None\n            try:\n                if current_loop is self._post_loop:\n                    self._future.set_result(feedback)\n                else:\n                    self._post_loop.call_soon_threadsafe(self._future.set_result, feedback)\n            except asyncio.InvalidStateError:\n                # Suppress the InvalidStateError to be raised, maybe due to timeout.\n                import warnings\n                warnings.warn(f\"Feedback future already set. feedback: {feedback}\", FutureWarning)\n\n    @override\n    def post_event(self, event: Event) -&gt; None:\n        \"\"\"\n        Post an event to the application layer outside the Automa.\n\n        The event handler implemented by the application layer will be called in the same thread as the worker (maybe the main thread or a new thread from the thread pool).\n\n        Note that `post_event` can be called either in a non-async method or in an async method.\n\n        The event will be bubbled up to the top-level Automa, where it will be processed by the event handler registered with the event type.\n\n        Parameters\n        ----------\n        event: Event\n            The event to be posted.\n        \"\"\"\n        def _handler_need_feedback_sender(handler: EventHandlerType):\n            positional_param_names = get_param_names_by_kind(handler, Parameter.POSITIONAL_ONLY) + get_param_names_by_kind(handler, Parameter.POSITIONAL_OR_KEYWORD)\n            var_positional_param_names = get_param_names_by_kind(handler, Parameter.VAR_POSITIONAL)\n            return len(var_positional_param_names) &gt; 0 or len(positional_param_names) &gt; 1\n\n        if not self.is_top_level():\n            # Bubble up the event to the top-level Automa.\n            return self.parent.post_event(event)\n\n        # Here is the top-level Automa.\n        # Call event handlers\n        if event.event_type in self._event_handlers:\n            if _handler_need_feedback_sender(self._event_handlers[event.event_type]):\n                self._event_handlers[event.event_type](event, feedback_sender=None)\n            else:\n                self._event_handlers[event.event_type](event)\n        if self._default_event_handler is not None:\n            if _handler_need_feedback_sender(self._default_event_handler):\n                self._default_event_handler(event, feedback_sender=None)\n            else:\n                self._default_event_handler(event)\n\n    def request_feedback(\n        self, \n        event: Event,\n        timeout: Optional[float] = None\n    ) -&gt; Feedback:\n        \"\"\"\n        Request feedback for the specified event from the application layer outside the Automa. This method blocks the caller until the feedback is received.\n\n        Note that `request_feedback` should only be called from within a non-async method running in a new thread of the Automa thread pool.\n\n        Parameters\n        ----------\n        event: Event\n            The event to be posted to the event handler implemented by the application layer.\n        timeout: Optional[float]\n            A float or int number of seconds to wait for if the feedback is not received. If None, then there is no limit on the wait time.\n\n        Returns\n        -------\n        Feedback\n            The feedback received from the application layer.\n\n        Raises\n        ------\n        TimeoutError\n            If the feedback is not received before the timeout. Note that the raised exception is the built-in `TimeoutError` exception, instead of asyncio.TimeoutError or concurrent.futures.TimeoutError!\n        \"\"\"\n        if threading.get_ident() == self._main_thread_id:\n            raise AutomaRuntimeError(\n                f\"`request_feedback` should only be called in a different thread from the main thread of the {self.name}. \"\n            )\n        return asyncio.run_coroutine_threadsafe(\n            self.request_feedback_async(event, timeout),\n            self._main_loop\n        ).result()\n\n    async def request_feedback_async(\n        self, \n        event: Event,\n        timeout: Optional[float] = None\n    ) -&gt; Feedback:\n        \"\"\"\n        Request feedback for the specified event from the application layer outside the Automa. This method blocks the caller until the feedback is received.\n\n        The event handler implemented by the application layer will be called in the next event loop iteration, in the main thread.\n\n        Parameters\n        ----------\n        event: Event\n            The event to be posted to the event handler implemented by the application layer.\n        timeout: Optional[float]\n            A float or int number of seconds to wait for if the feedback is not received. If None, then there is no limit on the wait time.\n\n        Returns\n        -------\n        Feedback\n            The feedback received from the application layer.\n\n        Raises\n        ------\n        TimeoutError\n            If the feedback is not received before the timeout. Note that the raised exception is the built-in `TimeoutError` exception, instead of asyncio.TimeoutError!\n        \"\"\"\n        if not self.is_top_level():\n            # Bubble up the event to the top-level Automa.\n            return await self.parent.request_feedback_async(event, timeout)\n\n        # Here is the top-level Automa.\n        event_loop = asyncio.get_running_loop()\n        future = event_loop.create_future()\n        feedback_sender = self._FeedbackSender(future, event_loop)\n        # Call event handlers\n        if event.event_type in self._event_handlers:\n            self._event_handlers[event.event_type](event, feedback_sender)\n        if self._default_event_handler is not None:\n            self._default_event_handler(event, feedback_sender)\n\n        try:\n            return await asyncio.wait_for(future, timeout)\n        except TimeoutError as e:\n            # When python &gt;= 3.11 here.\n            raise TimeoutError(f\"No feedback is received before timeout in Automa[{self.name}]\") from e\n        except asyncio.TimeoutError as e:\n            # Version compatibility resolution: asyncio.wait_for raises asyncio.TimeoutError before python 3.11.\n            # https://docs.python.org/3/library/asyncio-task.html#asyncio.wait_for\n            raise TimeoutError(f\"No feedback is received before timeout in Automa[{self.name}]\") from e\n\n    ###############################################################\n    ########### [Bridgic Event Handling Mechanism] ends ###########\n    ###############################################################\n\n    ###############################################################\n    ######## [Bridgic Human Interaction Mechanism] starts #########\n    ###############################################################\n\n    def interact_with_human(\n        self,\n        event: Event,\n        interacting_worker: Optional[Worker] = None,\n    ) -&gt; InteractionFeedback:\n        \"\"\"\n        Trigger an interruption in the \"human-in-the-loop interaction\" during the execution of the Automa.\n\n        Parameters\n        ----------\n        event: Event\n            The event that triggered the interaction.\n        interacting_worker: Optional[Worker]\n            The worker instance that is currently interacting with human. If not provided, the worker will be located automatically.\n\n        Returns\n        -------\n        InteractionFeedback\n            The feedback received from the application layer.\n        \"\"\"\n        if not interacting_worker:\n            kickoff_worker_key: str = self._locate_interacting_worker()\n        else:\n            kickoff_worker_key = self._get_worker_key(interacting_worker)\n\n        if kickoff_worker_key:\n            return self.interact_with_human_from_worker_key(event, kickoff_worker_key)\n        raise AutomaRuntimeError(\n            f\"Get kickoff worker failed in Automa[{self.name}] \"\n            f\"when trying to interact with human with event: {event}\"\n        )\n\n    def interact_with_human_from_worker_key(\n        self,\n        event: Event,\n        worker_key: str\n    ) -&gt; InteractionFeedback:\n        # Match the interaction and feedback to see if it matches\n        matched_feedback: _InteractionAndFeedback = None\n        cur_interact_index = self._get_and_increment_interaction_index(worker_key)\n        if worker_key in self._ongoing_interactions:\n            interaction_and_feedbacks = self._ongoing_interactions[worker_key]\n            if cur_interact_index &lt; len(interaction_and_feedbacks):\n                matched_feedback = interaction_and_feedbacks[cur_interact_index]\n                # Check the event type\n                if event.event_type != matched_feedback.interaction.event.event_type:\n                    raise AutomaRuntimeError(\n                        f\"Event type mismatch! Automa[{self.name}-worker[{worker_key}]]. \"\n                        f\"interact_with_human passed-in event: {event}\\n\"\n                        f\"ongoing interaction &amp;&amp; feedback: {matched_feedback}\\n\"\n                    )\n        if matched_feedback is None or matched_feedback.feedback is None:\n            # Important: The interaction_id should be unique for each human interaction.\n            interaction_id = uuid.uuid4().hex if matched_feedback is None else matched_feedback.interaction.interaction_id\n            # Match failed, raise an exception to go into the human interactioin process.\n            raise _InteractionEventException(Interaction(\n                interaction_id=interaction_id,\n                event=event,\n            ))\n        else:\n            # Match the interaction and feedback succeeded, return it.\n            return matched_feedback.feedback\n\n    def _get_and_increment_interaction_index(self, worker_key: str) -&gt; int:\n        if worker_key not in self._worker_interaction_indices:\n            cur_index = 0\n            self._worker_interaction_indices[worker_key] = 0\n        else:\n            cur_index = self._worker_interaction_indices[worker_key]\n        self._worker_interaction_indices[worker_key] += 1\n        return cur_index\n\n    ###############################################################\n    ######### [Bridgic Human Interaction Mechanism] ends ##########\n    ###############################################################\n\n    def get_local_space(self, runtime_context: RuntimeContext) -&gt; Dict[str, Any]:\n        \"\"\"\n        Retrieve the local execution context (local space) associated with the current worker. \n        If you require the local space to be cleared after the completion of `automa.arun()`, \n        you may customize this behavior by overriding the `should_reset_local_space()` method.\n\n        Parameters\n        ----------\n        runtime_context : RuntimeContext\n            The runtime context.\n\n        Returns\n        -------\n        Dict[str, Any]\n            The local space.\n        \"\"\"\n        worker_key = runtime_context.worker_key\n        worker_obj = self._get_worker_instance(worker_key)\n        return worker_obj.local_space\n\n    def should_reset_local_space(self) -&gt; bool:\n        \"\"\"\n        This method indicates whether to reset the local space at the end of the arun method of Automa. \n        By default, it returns True, standing for resetting. Otherwise, it means doing nothing.\n\n        Examples:\n        --------\n        ```python\n        class MyAutoma(Automa):\n            def should_reset_local_space(self) -&gt; bool:\n                return False\n        ```\n        \"\"\"\n        return True\n</code></pre>"},{"location":"reference/bridgic-core/bridgic/core/automa/#bridgic.core.automa.Automa.thread_pool","title":"thread_pool  <code>property</code> <code>writable</code>","text":"<pre><code>thread_pool: Optional[ThreadPoolExecutor]\n</code></pre> <p>Get/Set the thread pool for parallel running of I/O-bound tasks used by the current Automa instance and its nested Automa instances.</p> <p>Note: If an Automa is nested within another Automa, the thread pool of the top-level Automa will be used, rather than the thread pool of the nested Automa.</p> <p>Returns:</p> Type Description <code>Optional[ThreadPoolExecutor]</code> <p>The thread pool.</p>"},{"location":"reference/bridgic-core/bridgic/core/automa/#bridgic.core.automa.Automa.load_from_snapshot","title":"load_from_snapshot","text":"<code>classmethod</code> <pre><code>load_from_snapshot(\n    snapshot: Snapshot,\n    thread_pool: Optional[ThreadPoolExecutor] = None,\n) -&gt; Automa\n</code></pre> <p>Load an Automa instance from a snapshot.</p> <p>Parameters:</p> Name Type Description Default <code>snapshot</code> <code>Snapshot</code> <p>The snapshot to load the Automa instance from.</p> required <code>thread_pool</code> <code>Optional[ThreadPoolExecutor]</code> <p>The thread pool for parallel running of I/O-bound tasks. If not provided, a default thread pool will be used.</p> <code>None</code> <p>Returns:</p> Type Description <code>Automa</code> <p>The loaded Automa instance.</p> Source code in <code>bridgic/core/automa/_automa.py</code> <pre><code>@classmethod\ndef load_from_snapshot(\n    cls, \n    snapshot: Snapshot,\n    thread_pool: Optional[ThreadPoolExecutor] = None,\n) -&gt; \"Automa\":\n    \"\"\"\n    Load an Automa instance from a snapshot.\n\n    Parameters\n    ----------\n    snapshot: Snapshot\n        The snapshot to load the Automa instance from.\n    thread_pool: Optional[ThreadPoolExecutor]\n        The thread pool for parallel running of I/O-bound tasks. If not provided, a default thread pool will be used.\n\n    Returns\n    -------\n    Automa\n        The loaded Automa instance.\n    \"\"\"\n    # Here you can compare snapshot.serialization_version with SERIALIZATION_VERSION, and handle any necessary version compatibility issues if needed.\n    automa = load_bytes(snapshot.serialized_bytes)\n    if thread_pool:\n        automa.thread_pool = thread_pool\n    return automa\n</code></pre>"},{"location":"reference/bridgic-core/bridgic/core/automa/#bridgic.core.automa.Automa.set_running_options","title":"set_running_options","text":"<pre><code>set_running_options(debug: Optional[bool] = None)\n</code></pre> <p>Set runtime-configurable running options for this Automa instance.</p> <p>This method only supports fields that can be changed at runtime. Fields that must be set  during initialization (such as <code>callback_builders</code>) cannot be changed here and must be  provided via the <code>running_options</code> parameter in the Automa constructor.</p> <p>For fields that can be delayed to be set, some settings (like <code>debug</code>) from the outermost  (top-level) Automa will override the settings of all inner (nested) Automa instances. For example, if the top-level Automa instance sets <code>debug = True</code> and the nested instances  sets <code>debug = False</code>, then the nested Automa instance will run in debug mode when the  top-level Automa instance is executed. We call it the Setting Penetration Mechanism.</p> <p>Parameters:</p> Name Type Description Default <code>debug</code> <code>bool</code> <p>Whether to enable debug mode. If None, the current value is not changed. This field is subject to the Setting Penetration Mechanism.</p> <code>None</code> Source code in <code>bridgic/core/automa/_automa.py</code> <pre><code>def set_running_options(\n    self,\n    debug: Optional[bool] = None,\n):\n    \"\"\"\n    Set runtime-configurable running options for this Automa instance.\n\n    This method only supports fields that can be changed at runtime. Fields that must be set \n    during initialization (such as `callback_builders`) cannot be changed here and must be \n    provided via the `running_options` parameter in the Automa constructor.\n\n    For fields that can be delayed to be set, some settings (like `debug`) from the outermost \n    (top-level) Automa will override the settings of all inner (nested) Automa instances.\n    For example, if the top-level Automa instance sets `debug = True` and the nested instances \n    sets `debug = False`, then the nested Automa instance will run in debug mode when the \n    top-level Automa instance is executed. We call it the Setting Penetration Mechanism.\n\n    Parameters\n    ----------\n    debug : bool, optional\n        Whether to enable debug mode. If None, the current value is not changed.\n        This field is subject to the Setting Penetration Mechanism.\n    \"\"\"\n    if debug is not None:\n        self._running_options.debug = debug\n</code></pre>"},{"location":"reference/bridgic-core/bridgic/core/automa/#bridgic.core.automa.Automa.register_event_handler","title":"register_event_handler","text":"<pre><code>register_event_handler(\n    event_type: Optional[str],\n    event_handler: EventHandlerType,\n) -&gt; None\n</code></pre> <p>Register an event handler for the specified event type. If <code>event_type</code> is set to None, the event handler will be registered as the default handler that will handle all event types.</p> <p>Note: Only event handlers registered on the top-level Automa will be invoked to handle events.</p> <p>Parameters:</p> Name Type Description Default <code>event_type</code> <code>Optional[str]</code> <p>The type of event to be handled. If set to None, the event handler will be registered as the default handler and will be used to handle all event types.</p> required <code>event_handler</code> <code>EventHandlerType</code> <p>The event handler to be registered.</p> required Source code in <code>bridgic/core/automa/_automa.py</code> <pre><code>def register_event_handler(self, event_type: Optional[str], event_handler: EventHandlerType) -&gt; None:\n    \"\"\"\n    Register an event handler for the specified event type. If `event_type` is set to None, the event handler will be registered as the default handler that will handle all event types.\n\n    Note: Only event handlers registered on the top-level Automa will be invoked to handle events.\n\n    Parameters\n    ----------\n    event_type: Optional[str]\n        The type of event to be handled. If set to None, the event handler will be registered as the default handler and will be used to handle all event types.\n    event_handler: EventHandlerType\n        The event handler to be registered.\n    \"\"\"\n    if event_type is None:\n        self._default_event_handler = event_handler\n    else:\n        self._event_handlers[event_type] = event_handler\n</code></pre>"},{"location":"reference/bridgic-core/bridgic/core/automa/#bridgic.core.automa.Automa.unregister_event_handler","title":"unregister_event_handler","text":"<pre><code>unregister_event_handler(event_type: Optional[str]) -&gt; None\n</code></pre> <p>Unregister an event handler for the specified event type.</p> <p>Parameters:</p> Name Type Description Default <code>event_type</code> <code>Optional[str]</code> <p>The type of event to be unregistered. If set to None, the default event handler will be unregistered.</p> required Source code in <code>bridgic/core/automa/_automa.py</code> <pre><code>def unregister_event_handler(self, event_type: Optional[str]) -&gt; None:\n    \"\"\"\n    Unregister an event handler for the specified event type.\n\n    Parameters\n    ----------\n    event_type: Optional[str]\n        The type of event to be unregistered. If set to None, the default event handler will be unregistered.\n    \"\"\"\n    if event_type in self._event_handlers:\n        del self._event_handlers[event_type]\n    if event_type is None:\n        self._default_event_handler = None\n</code></pre>"},{"location":"reference/bridgic-core/bridgic/core/automa/#bridgic.core.automa.Automa.unregister_all_event_handlers","title":"unregister_all_event_handlers","text":"<pre><code>unregister_all_event_handlers() -&gt; None\n</code></pre> <p>Unregister all event handlers.</p> Source code in <code>bridgic/core/automa/_automa.py</code> <pre><code>def unregister_all_event_handlers(self) -&gt; None:\n    \"\"\"\n    Unregister all event handlers.\n    \"\"\"\n    self._event_handlers.clear()\n    self._default_event_handler = None\n</code></pre>"},{"location":"reference/bridgic-core/bridgic/core/automa/#bridgic.core.automa.Automa.post_event","title":"post_event","text":"<pre><code>post_event(event: Event) -&gt; None\n</code></pre> <p>Post an event to the application layer outside the Automa.</p> <p>The event handler implemented by the application layer will be called in the same thread as the worker (maybe the main thread or a new thread from the thread pool).</p> <p>Note that <code>post_event</code> can be called either in a non-async method or in an async method.</p> <p>The event will be bubbled up to the top-level Automa, where it will be processed by the event handler registered with the event type.</p> <p>Parameters:</p> Name Type Description Default <code>event</code> <code>Event</code> <p>The event to be posted.</p> required Source code in <code>bridgic/core/automa/_automa.py</code> <pre><code>@override\ndef post_event(self, event: Event) -&gt; None:\n    \"\"\"\n    Post an event to the application layer outside the Automa.\n\n    The event handler implemented by the application layer will be called in the same thread as the worker (maybe the main thread or a new thread from the thread pool).\n\n    Note that `post_event` can be called either in a non-async method or in an async method.\n\n    The event will be bubbled up to the top-level Automa, where it will be processed by the event handler registered with the event type.\n\n    Parameters\n    ----------\n    event: Event\n        The event to be posted.\n    \"\"\"\n    def _handler_need_feedback_sender(handler: EventHandlerType):\n        positional_param_names = get_param_names_by_kind(handler, Parameter.POSITIONAL_ONLY) + get_param_names_by_kind(handler, Parameter.POSITIONAL_OR_KEYWORD)\n        var_positional_param_names = get_param_names_by_kind(handler, Parameter.VAR_POSITIONAL)\n        return len(var_positional_param_names) &gt; 0 or len(positional_param_names) &gt; 1\n\n    if not self.is_top_level():\n        # Bubble up the event to the top-level Automa.\n        return self.parent.post_event(event)\n\n    # Here is the top-level Automa.\n    # Call event handlers\n    if event.event_type in self._event_handlers:\n        if _handler_need_feedback_sender(self._event_handlers[event.event_type]):\n            self._event_handlers[event.event_type](event, feedback_sender=None)\n        else:\n            self._event_handlers[event.event_type](event)\n    if self._default_event_handler is not None:\n        if _handler_need_feedback_sender(self._default_event_handler):\n            self._default_event_handler(event, feedback_sender=None)\n        else:\n            self._default_event_handler(event)\n</code></pre>"},{"location":"reference/bridgic-core/bridgic/core/automa/#bridgic.core.automa.Automa.request_feedback","title":"request_feedback","text":"<pre><code>request_feedback(\n    event: Event, timeout: Optional[float] = None\n) -&gt; Feedback\n</code></pre> <p>Request feedback for the specified event from the application layer outside the Automa. This method blocks the caller until the feedback is received.</p> <p>Note that <code>request_feedback</code> should only be called from within a non-async method running in a new thread of the Automa thread pool.</p> <p>Parameters:</p> Name Type Description Default <code>event</code> <code>Event</code> <p>The event to be posted to the event handler implemented by the application layer.</p> required <code>timeout</code> <code>Optional[float]</code> <p>A float or int number of seconds to wait for if the feedback is not received. If None, then there is no limit on the wait time.</p> <code>None</code> <p>Returns:</p> Type Description <code>Feedback</code> <p>The feedback received from the application layer.</p> <p>Raises:</p> Type Description <code>TimeoutError</code> <p>If the feedback is not received before the timeout. Note that the raised exception is the built-in <code>TimeoutError</code> exception, instead of asyncio.TimeoutError or concurrent.futures.TimeoutError!</p> Source code in <code>bridgic/core/automa/_automa.py</code> <pre><code>def request_feedback(\n    self, \n    event: Event,\n    timeout: Optional[float] = None\n) -&gt; Feedback:\n    \"\"\"\n    Request feedback for the specified event from the application layer outside the Automa. This method blocks the caller until the feedback is received.\n\n    Note that `request_feedback` should only be called from within a non-async method running in a new thread of the Automa thread pool.\n\n    Parameters\n    ----------\n    event: Event\n        The event to be posted to the event handler implemented by the application layer.\n    timeout: Optional[float]\n        A float or int number of seconds to wait for if the feedback is not received. If None, then there is no limit on the wait time.\n\n    Returns\n    -------\n    Feedback\n        The feedback received from the application layer.\n\n    Raises\n    ------\n    TimeoutError\n        If the feedback is not received before the timeout. Note that the raised exception is the built-in `TimeoutError` exception, instead of asyncio.TimeoutError or concurrent.futures.TimeoutError!\n    \"\"\"\n    if threading.get_ident() == self._main_thread_id:\n        raise AutomaRuntimeError(\n            f\"`request_feedback` should only be called in a different thread from the main thread of the {self.name}. \"\n        )\n    return asyncio.run_coroutine_threadsafe(\n        self.request_feedback_async(event, timeout),\n        self._main_loop\n    ).result()\n</code></pre>"},{"location":"reference/bridgic-core/bridgic/core/automa/#bridgic.core.automa.Automa.request_feedback_async","title":"request_feedback_async","text":"<code>async</code> <pre><code>request_feedback_async(\n    event: Event, timeout: Optional[float] = None\n) -&gt; Feedback\n</code></pre> <p>Request feedback for the specified event from the application layer outside the Automa. This method blocks the caller until the feedback is received.</p> <p>The event handler implemented by the application layer will be called in the next event loop iteration, in the main thread.</p> <p>Parameters:</p> Name Type Description Default <code>event</code> <code>Event</code> <p>The event to be posted to the event handler implemented by the application layer.</p> required <code>timeout</code> <code>Optional[float]</code> <p>A float or int number of seconds to wait for if the feedback is not received. If None, then there is no limit on the wait time.</p> <code>None</code> <p>Returns:</p> Type Description <code>Feedback</code> <p>The feedback received from the application layer.</p> <p>Raises:</p> Type Description <code>TimeoutError</code> <p>If the feedback is not received before the timeout. Note that the raised exception is the built-in <code>TimeoutError</code> exception, instead of asyncio.TimeoutError!</p> Source code in <code>bridgic/core/automa/_automa.py</code> <pre><code>async def request_feedback_async(\n    self, \n    event: Event,\n    timeout: Optional[float] = None\n) -&gt; Feedback:\n    \"\"\"\n    Request feedback for the specified event from the application layer outside the Automa. This method blocks the caller until the feedback is received.\n\n    The event handler implemented by the application layer will be called in the next event loop iteration, in the main thread.\n\n    Parameters\n    ----------\n    event: Event\n        The event to be posted to the event handler implemented by the application layer.\n    timeout: Optional[float]\n        A float or int number of seconds to wait for if the feedback is not received. If None, then there is no limit on the wait time.\n\n    Returns\n    -------\n    Feedback\n        The feedback received from the application layer.\n\n    Raises\n    ------\n    TimeoutError\n        If the feedback is not received before the timeout. Note that the raised exception is the built-in `TimeoutError` exception, instead of asyncio.TimeoutError!\n    \"\"\"\n    if not self.is_top_level():\n        # Bubble up the event to the top-level Automa.\n        return await self.parent.request_feedback_async(event, timeout)\n\n    # Here is the top-level Automa.\n    event_loop = asyncio.get_running_loop()\n    future = event_loop.create_future()\n    feedback_sender = self._FeedbackSender(future, event_loop)\n    # Call event handlers\n    if event.event_type in self._event_handlers:\n        self._event_handlers[event.event_type](event, feedback_sender)\n    if self._default_event_handler is not None:\n        self._default_event_handler(event, feedback_sender)\n\n    try:\n        return await asyncio.wait_for(future, timeout)\n    except TimeoutError as e:\n        # When python &gt;= 3.11 here.\n        raise TimeoutError(f\"No feedback is received before timeout in Automa[{self.name}]\") from e\n    except asyncio.TimeoutError as e:\n        # Version compatibility resolution: asyncio.wait_for raises asyncio.TimeoutError before python 3.11.\n        # https://docs.python.org/3/library/asyncio-task.html#asyncio.wait_for\n        raise TimeoutError(f\"No feedback is received before timeout in Automa[{self.name}]\") from e\n</code></pre>"},{"location":"reference/bridgic-core/bridgic/core/automa/#bridgic.core.automa.Automa.interact_with_human","title":"interact_with_human","text":"<pre><code>interact_with_human(\n    event: Event,\n    interacting_worker: Optional[Worker] = None,\n) -&gt; InteractionFeedback\n</code></pre> <p>Trigger an interruption in the \"human-in-the-loop interaction\" during the execution of the Automa.</p> <p>Parameters:</p> Name Type Description Default <code>event</code> <code>Event</code> <p>The event that triggered the interaction.</p> required <code>interacting_worker</code> <code>Optional[Worker]</code> <p>The worker instance that is currently interacting with human. If not provided, the worker will be located automatically.</p> <code>None</code> <p>Returns:</p> Type Description <code>InteractionFeedback</code> <p>The feedback received from the application layer.</p> Source code in <code>bridgic/core/automa/_automa.py</code> <pre><code>def interact_with_human(\n    self,\n    event: Event,\n    interacting_worker: Optional[Worker] = None,\n) -&gt; InteractionFeedback:\n    \"\"\"\n    Trigger an interruption in the \"human-in-the-loop interaction\" during the execution of the Automa.\n\n    Parameters\n    ----------\n    event: Event\n        The event that triggered the interaction.\n    interacting_worker: Optional[Worker]\n        The worker instance that is currently interacting with human. If not provided, the worker will be located automatically.\n\n    Returns\n    -------\n    InteractionFeedback\n        The feedback received from the application layer.\n    \"\"\"\n    if not interacting_worker:\n        kickoff_worker_key: str = self._locate_interacting_worker()\n    else:\n        kickoff_worker_key = self._get_worker_key(interacting_worker)\n\n    if kickoff_worker_key:\n        return self.interact_with_human_from_worker_key(event, kickoff_worker_key)\n    raise AutomaRuntimeError(\n        f\"Get kickoff worker failed in Automa[{self.name}] \"\n        f\"when trying to interact with human with event: {event}\"\n    )\n</code></pre>"},{"location":"reference/bridgic-core/bridgic/core/automa/#bridgic.core.automa.Automa.get_local_space","title":"get_local_space","text":"<pre><code>get_local_space(\n    runtime_context: RuntimeContext,\n) -&gt; Dict[str, Any]\n</code></pre> <p>Retrieve the local execution context (local space) associated with the current worker.  If you require the local space to be cleared after the completion of <code>automa.arun()</code>,  you may customize this behavior by overriding the <code>should_reset_local_space()</code> method.</p> <p>Parameters:</p> Name Type Description Default <code>runtime_context</code> <code>RuntimeContext</code> <p>The runtime context.</p> required <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>The local space.</p> Source code in <code>bridgic/core/automa/_automa.py</code> <pre><code>def get_local_space(self, runtime_context: RuntimeContext) -&gt; Dict[str, Any]:\n    \"\"\"\n    Retrieve the local execution context (local space) associated with the current worker. \n    If you require the local space to be cleared after the completion of `automa.arun()`, \n    you may customize this behavior by overriding the `should_reset_local_space()` method.\n\n    Parameters\n    ----------\n    runtime_context : RuntimeContext\n        The runtime context.\n\n    Returns\n    -------\n    Dict[str, Any]\n        The local space.\n    \"\"\"\n    worker_key = runtime_context.worker_key\n    worker_obj = self._get_worker_instance(worker_key)\n    return worker_obj.local_space\n</code></pre>"},{"location":"reference/bridgic-core/bridgic/core/automa/#bridgic.core.automa.Automa.should_reset_local_space","title":"should_reset_local_space","text":"<pre><code>should_reset_local_space() -&gt; bool\n</code></pre> <p>This method indicates whether to reset the local space at the end of the arun method of Automa.  By default, it returns True, standing for resetting. Otherwise, it means doing nothing.</p> Examples: <pre><code>class MyAutoma(Automa):\n    def should_reset_local_space(self) -&gt; bool:\n        return False\n</code></pre> Source code in <code>bridgic/core/automa/_automa.py</code> <pre><code>def should_reset_local_space(self) -&gt; bool:\n    \"\"\"\n    This method indicates whether to reset the local space at the end of the arun method of Automa. \n    By default, it returns True, standing for resetting. Otherwise, it means doing nothing.\n\n    Examples:\n    --------\n    ```python\n    class MyAutoma(Automa):\n        def should_reset_local_space(self) -&gt; bool:\n            return False\n    ```\n    \"\"\"\n    return True\n</code></pre>"},{"location":"reference/bridgic-core/bridgic/core/automa/#bridgic.core.automa.Snapshot","title":"Snapshot","text":"<p>               Bases: <code>BaseModel</code></p> <p>A snapshot that represents the current state of an Automa. It is used when an Automa resumes after a human interaction.</p> Source code in <code>bridgic/core/automa/_automa.py</code> <pre><code>class Snapshot(BaseModel):\n    \"\"\"\n    A snapshot that represents the current state of an Automa. It is used when an Automa resumes after a human interaction.\n    \"\"\"\n    serialized_bytes: bytes\n    \"\"\"\n    The serialized bytes that represents the snapshot of the Automa.\n    \"\"\"\n    serialization_version: str\n    \"\"\"\n    The serialization version.\n    \"\"\"\n</code></pre>"},{"location":"reference/bridgic-core/bridgic/core/automa/#bridgic.core.automa.Snapshot.serialized_bytes","title":"serialized_bytes  <code>instance-attribute</code>","text":"<pre><code>serialized_bytes: bytes\n</code></pre> <p>The serialized bytes that represents the snapshot of the Automa.</p>"},{"location":"reference/bridgic-core/bridgic/core/automa/#bridgic.core.automa.Snapshot.serialization_version","title":"serialization_version  <code>instance-attribute</code>","text":"<pre><code>serialization_version: str\n</code></pre> <p>The serialization version.</p>"},{"location":"reference/bridgic-core/bridgic/core/automa/#bridgic.core.automa.RunningOptions","title":"RunningOptions","text":"<p>               Bases: <code>BaseModel</code></p> <p>Running options for an Automa instance.</p> <p>This class contains two types of fields:</p> <ol> <li>Runtime-configurable fields: Can be set at any time via <code>set_running_options()</code>.</li> <li> <p><code>debug</code>: Whether to enable debug mode.</p> </li> <li> <p>Initialization-only fields: Must be set during Automa instantiation via the <code>running_options</code> parameter.</p> </li> <li><code>callback_builders</code>: Callback builders at the Automa instance level. These will be merged with       global callback builders when workers are created during Automa initialization.</li> </ol> Source code in <code>bridgic/core/automa/_automa.py</code> <pre><code>class RunningOptions(BaseModel):\n    \"\"\"\n    Running options for an Automa instance.\n\n    This class contains two types of fields:\n\n    1. **Runtime-configurable fields**: Can be set at any time via `set_running_options()`.\n       - `debug`: Whether to enable debug mode.\n\n    2. **Initialization-only fields**: Must be set during Automa instantiation via the `running_options` parameter.\n       - `callback_builders`: Callback builders at the Automa instance level. These will be merged with \n         global callback builders when workers are created during Automa initialization.\n    \"\"\"\n    debug: bool = False\n    \"\"\"Whether to enable debug mode. Can be set at runtime via set_running_options().\"\"\"\n\n    callback_builders: List[WorkerCallbackBuilder] = []\n    \"\"\"A list of callback builders specific to this Automa instance.\"\"\"\n\n    model_config = {\"arbitrary_types_allowed\": True}\n</code></pre>"},{"location":"reference/bridgic-core/bridgic/core/automa/#bridgic.core.automa.RunningOptions.debug","title":"debug  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>debug: bool = False\n</code></pre> <p>Whether to enable debug mode. Can be set at runtime via set_running_options().</p>"},{"location":"reference/bridgic-core/bridgic/core/automa/#bridgic.core.automa.RunningOptions.callback_builders","title":"callback_builders  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>callback_builders: List[WorkerCallbackBuilder] = []\n</code></pre> <p>A list of callback builders specific to this Automa instance.</p>"},{"location":"reference/bridgic-core/bridgic/core/automa/#bridgic.core.automa.GraphAutoma","title":"GraphAutoma","text":"<p>               Bases: <code>Automa</code></p> <p>Dynamic Directed Graph (abbreviated as DDG) implementation of Automa. <code>GraphAutoma</code> manages  the running control flow between workers automatically, via <code>dependencies</code> and <code>ferry_to</code>. Outputs of workers can be mapped and passed to their successor workers in the runtime,  following <code>args_mapping_rule</code> and <code>result_dispatching_rule</code>.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>Optional[str]</code> <p>The name of the automa.</p> <code>None</code> <code>thread_pool</code> <code>Optional[ThreadPoolExecutor]</code> <p>The thread pool for parallel running of I/O-bound tasks.</p> <ul> <li> <p>If not provided, a default thread pool will be used. The maximum number of threads in the default thread pool dependends on the number of CPU cores. Please refer to  the ThreadPoolExecutor for detail.</p> </li> <li> <p>If provided, all workers (including all nested Automa instances) will be run in it. In this case, the  application layer code is responsible to create it and shut it down.</p> </li> </ul> <code>None</code> <p>Examples:</p> <p>The following example shows how to use <code>GraphAutoma</code> to create a simple graph automa that prints \"Hello, Bridgic\".</p> <pre><code>import asyncio\nfrom bridgic.core.automa import GraphAutoma, worker, ArgsMappingRule\n\nclass MyGraphAutoma(GraphAutoma):\n    @worker(is_start=True)\n    async def greet(self) -&gt; list[str]:\n        return [\"Hello\", \"Bridgic\"]\n\n    @worker(dependencies=[\"greet\"], args_mapping_rule=ArgsMappingRule.AS_IS, result_dispatching_rule=ResultDispatchingRule.AS_IS, is_output=True)\n    async def output(self, message: list[str]):\n        print(\"Echo: \" + \" \".join(message))\n\nasync def main():\n    automa_obj = MyGraphAutoma(name=\"my_graph_automa\")\n    await automa_obj.arun()\n\nasyncio.run(main())\n</code></pre> Source code in <code>bridgic/core/automa/_graph_automa.py</code> <pre><code>class GraphAutoma(Automa, metaclass=GraphMeta):\n    \"\"\"\n    Dynamic Directed Graph (abbreviated as DDG) implementation of Automa. `GraphAutoma` manages \n    the running control flow between workers automatically, via `dependencies` and `ferry_to`.\n    Outputs of workers can be mapped and passed to their successor workers in the runtime, \n    following `args_mapping_rule` and `result_dispatching_rule`.\n\n    Parameters\n    ----------\n    name : Optional[str]\n        The name of the automa.\n\n    thread_pool : Optional[ThreadPoolExecutor]\n        The thread pool for parallel running of I/O-bound tasks.\n\n        - If not provided, a default thread pool will be used.\n        The maximum number of threads in the default thread pool dependends on the number of CPU cores. Please refer to \n        the [ThreadPoolExecutor](https://docs.python.org/3/library/concurrent.futures.html#threadpoolexecutor) for detail.\n\n        - If provided, all workers (including all nested Automa instances) will be run in it. In this case, the \n        application layer code is responsible to create it and shut it down.\n\n    Examples\n    --------\n\n    The following example shows how to use `GraphAutoma` to create a simple graph automa that prints \"Hello, Bridgic\".\n\n    ```python\n    import asyncio\n    from bridgic.core.automa import GraphAutoma, worker, ArgsMappingRule\n\n    class MyGraphAutoma(GraphAutoma):\n        @worker(is_start=True)\n        async def greet(self) -&gt; list[str]:\n            return [\"Hello\", \"Bridgic\"]\n\n        @worker(dependencies=[\"greet\"], args_mapping_rule=ArgsMappingRule.AS_IS, result_dispatching_rule=ResultDispatchingRule.AS_IS, is_output=True)\n        async def output(self, message: list[str]):\n            print(\"Echo: \" + \" \".join(message))\n\n    async def main():\n        automa_obj = MyGraphAutoma(name=\"my_graph_automa\")\n        await automa_obj.arun()\n\n    asyncio.run(main())\n    ```\n    \"\"\"\n\n    # Automa type.\n    AUTOMA_TYPE: ClassVar[AutomaType] = AutomaType.Graph\n\n    # The initial topology defined by @worker functions.\n    _registered_worker_funcs: ClassVar[Dict[str, Callable]] = {}\n\n    # IMPORTANT: The entire states of a GraphAutoma instance include 2 part:\n    # \n    # Part-1 (for the states of topology structure):\n    #   1. Inner worker instances: self._workers\n    #   2. Relations between worker: self._worker_forwards\n    #   3. Dynamic states that serve as trigger of execution of workers: self._workers_dynamic_states\n    #   4. Execution result of inner workers: self._worker_output\n    #   5. Configurations of this automa instance: self._output_worker_key\n    # \n    # Part-2 (for the states of running states):\n    #   1. Records of Workers that are going to be kicked off: self._current_kickoff_workers\n    #   2. Records of running or deferred tasks:\n    #      - self._running_tasks\n    #      - self._topology_change_deferred_tasks\n    #      - self._ferry_deferred_tasks\n    #      - self._set_output_worker_deferred_task\n    #   3. Buffers of automa inputs: self._input_buffer\n    #   4. Ongoing human interactions: self._ongoing_interactions\n    #   ...\n\n    _workers: Dict[str, _GraphAdaptedWorker]\n    _worker_output: Dict[str, Any]\n    _worker_forwards: Dict[str, List[str]]\n\n    _current_kickoff_workers: List[_KickoffInfo]\n    _input_buffer: _AutomaInputBuffer\n    _workers_dynamic_states: Dict[str, _WorkerDynamicState]\n\n    # The whole running process of the DDG is divided into two main phases:\n    # 1. [Initialization Phase] The first phase (when _automa_running is False): the initial topology of DDG was constructed.\n    # 2. [Running Phase] The second phase (when _automa_running is True): the DDG is running, and the workers are executed in a dynamic step-by-step manner (DS loop).\n    _automa_running: bool\n\n    #########################################################\n    #### The following fields need not to be serialized. ####\n    #########################################################\n    _running_tasks: List[_RunnningTask]\n\n    # TODO: The following deferred task structures need to be thread-safe.\n    # TODO: Need to be refactored when parallelization features are added.\n    _topology_change_deferred_tasks: List[Union[_AddWorkerDeferredTask, _RemoveWorkerDeferredTask]]\n    _ferry_deferred_tasks: List[_FerryDeferredTask]\n\n    def __init__(\n        self,\n        name: Optional[str] = None,\n        thread_pool: Optional[ThreadPoolExecutor] = None,\n        running_options: Optional[RunningOptions] = None,\n    ):\n        \"\"\"\n        Parameters\n        ----------\n        name : Optional[str]\n            The name of the automa.\n\n        thread_pool : Optional[ThreadPoolExecutor]\n            The thread pool for parallel running of I/O-bound tasks.\n\n            - If not provided, a default thread pool will be used.\n            The maximum number of threads in the default thread pool dependends on the number of CPU cores. Please refer to \n            the [ThreadPoolExecutor](https://docs.python.org/3/library/concurrent.futures.html#threadpoolexecutor) for detail.\n\n            - If provided, all workers (including all nested Automa instances) will be run in it. In this case, the \n            application layer code is responsible to create it and shut it down.\n\n        running_options : Optional[RunningOptions]\n            Running options for this Automa instance, including callback_builders.\n            If None, uses default RunningOptions.\n\n        state_dict : Optional[Dict[str, Any]]\n            A dictionary for initializing the automa's runtime states. This parameter is designed for framework use only.\n        \"\"\"\n        super().__init__(name=name, thread_pool=thread_pool, running_options=running_options)\n\n        self._workers = {}\n        self._worker_outputs = {}\n        self._automa_running = False\n\n        # Initialize the states that need to be serialized.\n        self._normal_init()\n\n        # The list of the tasks that are currently being executed.\n        self._running_tasks = []\n        # deferred tasks\n        self._topology_change_deferred_tasks = []\n        self._ferry_deferred_tasks = []\n\n    def _normal_init(self):\n        ###############################################################################\n        # Initialization of [Part One: Topology-Related Runtime States] #### Strat ####\n        ###############################################################################\n\n        cls = type(self)\n\n        # _workers, _worker_forwards and _workers_dynamic_states will be initialized incrementally by add_worker()...\n        self._worker_forwards = {}\n        self._worker_output = {}\n        self._workers_dynamic_states = {}\n\n        if cls.AUTOMA_TYPE == AutomaType.Graph:\n            # The _registered_worker_funcs data are from @worker decorators.\n            for worker_key, worker_func in cls._registered_worker_funcs.items():\n                # The decorator based mechanism (i.e. @worker) is based on the add_worker() interface.\n                # Parameters check and other implementation details can be unified.\n                self._add_func_as_worker_internal(\n                    key=worker_key,\n                    func=worker_func,\n                    dependencies=worker_func.__dependencies__,\n                    is_start=worker_func.__is_start__,\n                    is_output=worker_func.__is_output__,\n                    args_mapping_rule=worker_func.__args_mapping_rule__,\n                    result_dispatching_rule=worker_func.__result_dispatching_rule__,\n                    callback_builders=worker_func.__callback_builders__,\n                )\n\n        ###############################################################################\n        # Initialization of [Part One: Topology-Related Runtime States] ##### End #####\n        ###############################################################################\n\n        ###############################################################################\n        # Initialization of [Part Two: Task-Related Runtime States] ###### Strat ######\n        ###############################################################################\n\n        # -- Current kickoff workers list.\n        # The key list of the workers that are ready to be immediately executed in the next DS (Dynamic Step). It will be lazily initialized in _compile_graph_and_detect_risks().\n        self._current_kickoff_workers = []\n        # -- Automa input buffer.\n        self._input_buffer = _AutomaInputBuffer()\n\n        ###############################################################################\n        # Initialization of [Part Two: Task-Related Runtime States] ####### End #######\n        ###############################################################################\n\n    ###############################################################\n    ########## [Bridgic Serialization Mechanism] starts ###########\n    ###############################################################\n\n    # The version of the serialization format.\n    SERIALIZATION_VERSION: str = \"1.0\"\n\n    @override\n    def dump_to_dict(self) -&gt; Dict[str, Any]:\n        state_dict = super().dump_to_dict()\n\n        state_dict[\"name\"] = self.name\n        state_dict[\"automa_running\"] = self._automa_running\n\n        # States related to workers.\n        state_dict[\"workers\"] = self._workers\n        state_dict[\"worker_forwards\"] = self._worker_forwards\n        state_dict[\"workers_dynamic_states\"] = self._workers_dynamic_states\n        state_dict[\"worker_output\"] = self._worker_output\n\n        # States related to interruption recovery.\n        state_dict[\"current_kickoff_workers\"] = self._current_kickoff_workers\n        state_dict[\"input_buffer\"] = self._input_buffer\n\n        return state_dict\n\n    @override\n    def load_from_dict(self, state_dict: Dict[str, Any]) -&gt; None:\n        super().load_from_dict(state_dict)\n\n        self.name = state_dict[\"name\"]\n        self._automa_running = state_dict[\"automa_running\"]\n\n        # States related to workers.\n        self._workers = state_dict[\"workers\"]\n        for worker in self._workers.values():\n            worker.parent = self\n        self._worker_forwards = state_dict[\"worker_forwards\"]\n        self._workers_dynamic_states = state_dict[\"workers_dynamic_states\"]\n        self._worker_output = state_dict[\"worker_output\"]\n\n        # States related to interruption recovery.\n        self._current_kickoff_workers = state_dict[\"current_kickoff_workers\"]\n        self._input_buffer = state_dict[\"input_buffer\"]\n\n        # The list of the tasks that are currently being executed.\n        self._running_tasks = []\n        # Deferred tasks\n        self._topology_change_deferred_tasks = []\n        self._set_output_worker_deferred_task = None\n        self._ferry_deferred_tasks = []\n\n    ###############################################################\n    ########### [Bridgic Serialization Mechanism] ends ############\n    ###############################################################\n\n    def _add_worker_incrementally(\n        self,\n        key: str,\n        worker: Worker,\n        *,\n        dependencies: List[str] = [],\n        is_start: bool = False,\n        is_output: bool = False,\n        args_mapping_rule: ArgsMappingRule = ArgsMappingRule.AS_IS,\n        result_dispatching_rule: ResultDispatchingRule = ResultDispatchingRule.AS_IS,\n        callback_builders: List[WorkerCallbackBuilder] = [],\n    ) -&gt; None:\n        \"\"\"\n        Incrementally add a worker into the automa. For internal use only.\n        This method is one of the very basic primitives of DDG for dynamic topology changes. \n        \"\"\"\n        if key in self._workers:\n            raise AutomaRuntimeError(\n                f\"duplicate workers with the same key '{key}' are not allowed to be added!\"\n            )\n\n        # Merge callback builders: Global -&gt; Ancestor Automa(s) -&gt; Current Automa -&gt; Nested Automa (if worker is automa) -&gt; Worker\n        effective_callback_builders = []\n        effective_callback_builders.extend(GlobalSetting.read().callback_builders)\n        # Collect callback builders from all ancestor automas in the ancestor chain (from top-level to current)\n        effective_callback_builders.extend(self._collect_ancestor_callback_builders())\n        # If the worker itself is an automa, include its own RunningOptions callback builders\n        if isinstance(worker, Automa):\n            effective_callback_builders.extend(worker._running_options.callback_builders)\n        # Include the callback builders from the worker itself.\n        effective_callback_builders.extend(callback_builders)\n\n        # Note: the dependencies argument must be a new copy of the list, created with list(dependencies).\n        # Refer to the Python documentation for more details:\n        # 1. https://docs.python.org/3/reference/compound_stmts.html#function-definitions\n        # \"Default parameter values are evaluated from left to right when the function definition is executed\"\n        # 2. https://docs.python.org/3/tutorial/controlflow.html#default-argument-values\n        # \"The default values are evaluated at the point of function definition in the defining scope\"\n        # \"Important warning: The default value is evaluated only once.\"\n        new_worker_obj = _GraphAdaptedWorker(\n            key=key,\n            worker=worker,\n            dependencies=list(dependencies),\n            is_start=is_start,\n            is_output=is_output,\n            args_mapping_rule=args_mapping_rule,\n            result_dispatching_rule=result_dispatching_rule,\n            callback_builders=effective_callback_builders,\n        )\n\n        # Register the worker_obj.\n        new_worker_obj.parent = self\n        self._workers[new_worker_obj.key] = new_worker_obj\n\n        # Incrementally update the dynamic states of added workers.\n        self._workers_dynamic_states[key] = _WorkerDynamicState(\n            dependency_triggers=set(dependencies)\n        )\n\n        # Incrementally update the forwards table.\n        for trigger in dependencies:\n            if trigger not in self._worker_forwards:\n                self._worker_forwards[trigger] = []\n            self._worker_forwards[trigger].append(key)\n\n        # If the added worker is an automa, recursively propagate callbacks to inner workers.\n        if new_worker_obj.is_automa():\n            nested_automa = new_worker_obj.get_decorated_worker()\n            if isinstance(nested_automa, GraphAutoma):\n                # Collect callback builders from all ancestor automas in the ancestor chain (from top-level to current)\n                ancestor_callback_builders = self._collect_ancestor_callback_builders()\n                # Append ancestor callbacks to the _cached_callbacks of the nested automa instance.\n                nested_automa._cached_callbacks = nested_automa._get_automa_callbacks() + [cb.build() for cb in ancestor_callback_builders]\n                # Recursively propagate ancestor callbacks to inner workers.\n                self._propagate_callbacks_to_nested_automa(\n                    nested_automa=nested_automa,\n                    callback_builders=ancestor_callback_builders,\n                )\n\n    def _propagate_callbacks_to_nested_automa(\n        self,\n        nested_automa: \"GraphAutoma\",\n        callback_builders: List[WorkerCallbackBuilder],\n    ) -&gt; None:\n        \"\"\"\n        Recursively propagate callback builders to all workers in a nested automa.\n\n        This method ensures that callbacks from all ancestor automas in the ancestor chain\n        are applied to all workers in nested automa instances, including deeply nested ones.\n\n        Parameters\n        ----------\n        nested_automa : GraphAutoma\n            The nested automa instance to propagate callbacks to.\n        callback_builders : List[WorkerCallbackBuilder]\n            The callback builders from all ancestor automas in the ancestor chain \n            (from top-level to current) to propagate.\n        \"\"\"\n        for worker_key in nested_automa.all_workers():\n            nested_worker = nested_automa._workers[worker_key]\n\n            # Add callback instances built from all ancestor automas' callback builders in the ancestor chain.\n            new_callbacks = [cb.build() for cb in callback_builders]\n            nested_worker._worker_callbacks += new_callbacks\n\n            # Check if the nested worker is also an automa, and recursively propagate.\n            if nested_worker.is_automa():\n                deeper_nested_automa = nested_worker.get_decorated_worker()\n                if isinstance(deeper_nested_automa, GraphAutoma):\n                    # Recursively propagate to deeper nested automas.\n                    # Include current nested automa's callbacks in the propagation chain,\n                    # so that deeper nested workers get callbacks from all ancestor automas.\n                    self._propagate_callbacks_to_nested_automa(\n                        nested_automa=deeper_nested_automa,\n                        callback_builders=callback_builders,\n                    )\n\n    def _remove_worker_incrementally(\n        self,\n        key: str\n    ) -&gt; None:\n        \"\"\"\n        Incrementally remove a worker from the automa. For internal use only.\n        This method is one of the very basic primitives of DDG for dynamic topology changes.\n        \"\"\"\n        if key not in self._workers:\n            raise AutomaRuntimeError(\n                f\"fail to remove worker '{key}' that does not exist!\"\n            )\n\n        worker_to_remove = self._workers[key]\n\n        # Remove the worker.\n        del self._workers[key]\n        # Incrementally update the dynamic states of removed workers.\n        del self._workers_dynamic_states[key]\n\n        if key in self._worker_forwards:\n            # Update the dependencies of the successor workers, if needed.\n            for successor in self._worker_forwards[key]:\n                self._workers[successor].dependencies.remove(key)\n                # Note this detail here: use discard() instead of remove() to avoid KeyError.\n                # This case occurs when a worker call remove_worker() to remove its predecessor worker.\n                self._workers_dynamic_states[successor].dependency_triggers.discard(key)\n            # Incrementally update the forwards table.\n            del self._worker_forwards[key]\n\n        # Remove from the forwards list of all dependencies worker.\n        for trigger in worker_to_remove.dependencies:\n            self._worker_forwards[trigger].remove(key)\n        if key in self._worker_interaction_indices:\n            del self._worker_interaction_indices[key]\n        if key in self._ongoing_interactions:\n            del self._ongoing_interactions[key]\n\n    def _add_dependency_incrementally(\n        self,\n        key: str,\n        dependency: str,\n    ) -&gt; None:\n        \"\"\"\n        Incrementally add a dependency from `key` to `depends`. For internal use only.\n        This method is one of the very basic primitives of DDG for dynamic topology changes.\n        \"\"\"\n        if key not in self._workers:\n            raise AutomaRuntimeError(\n                f\"fail to add dependency from a worker that does not exist: `{key}`!\"\n            )\n        if dependency not in self._workers:\n            raise AutomaRuntimeError(\n                f\"fail to add dependency to a worker that does not exist: `{dependency}`!\"\n            )\n        if dependency in self._workers[key].dependencies:\n            raise AutomaRuntimeError(\n                f\"dependency from '{key}' to '{dependency}' already exists!\"\n            )\n\n        self._workers[key].dependencies.append(dependency)\n        # Note this detail here for dynamic states change:\n        # The new dependency added here may be removed right away if the dependency is just the next kickoff worker. This is a valid behavior.\n        self._workers_dynamic_states[key].dependency_triggers.add(dependency)\n\n        if dependency not in self._worker_forwards:\n            self._worker_forwards[dependency] = []\n        self._worker_forwards[dependency].append(key)\n\n    def _add_worker_internal(\n        self,\n        key: str,\n        worker: Worker,\n        *,\n        dependencies: List[str] = [],\n        is_start: bool = False,\n        is_output: bool = False,\n        args_mapping_rule: ArgsMappingRule = ArgsMappingRule.AS_IS,\n        result_dispatching_rule: ResultDispatchingRule = ResultDispatchingRule.AS_IS,\n        callback_builders: List[WorkerCallbackBuilder] = [],\n    ) -&gt; None:\n        \"\"\"\n        The private version of the method `add_worker()`.\n        \"\"\"\n\n        def _basic_worker_params_check(key: str, worker_obj: Worker):\n            if not isinstance(worker_obj, Worker):\n                raise TypeError(\n                    f\"worker_obj to be registered must be a Worker, \"\n                    f\"but got {type(worker_obj)} for worker '{key}'\"\n                )\n\n            if not asyncio.iscoroutinefunction(worker_obj.arun):\n                raise WorkerSignatureError(\n                    f\"arun of Worker must be an async method, \"\n                    f\"but got {type(worker_obj.arun)} for worker '{key}'\"\n                )\n\n            if not isinstance(dependencies, list):\n                raise TypeError(\n                    f\"dependencies must be a list, \"\n                    f\"but got {type(dependencies)} for worker '{key}'\"\n                )\n            if not all([isinstance(d, str) for d in dependencies]):\n                raise ValueError(\n                    f\"dependencies must be a List of str, \"\n                    f\"but got {dependencies} for worker {key}\"\n                )\n\n            if args_mapping_rule not in ArgsMappingRule:\n                raise ValueError(\n                    f\"args_mapping_rule must be one of the following: {[e for e in ArgsMappingRule]}, \"\n                    f\"but got {args_mapping_rule} for worker {key}\"\n                )\n\n            if result_dispatching_rule not in ResultDispatchingRule:\n                raise ValueError(\n                    f\"result_dispatching_rule must be one of the following: {[e for e in ResultDispatchingRule]}, \"\n                    f\"but got {result_dispatching_rule} for worker {key}\"\n                )\n\n        # Ensure the parameters are valid.\n        _basic_worker_params_check(key, worker)\n\n        if not self._automa_running:\n            # Add worker during the [Initialization Phase].\n            self._add_worker_incrementally(\n                key=key,\n                worker=worker,\n                dependencies=dependencies,\n                is_start=is_start,\n                is_output=is_output,\n                args_mapping_rule=args_mapping_rule,\n                result_dispatching_rule=result_dispatching_rule,\n                callback_builders=callback_builders,\n            )\n        else:\n            # Add worker during the [Running Phase].\n            deferred_task = _AddWorkerDeferredTask(\n                worker_key=key,\n                worker_obj=worker,\n                dependencies=dependencies,\n                is_start=is_start,\n                is_output=is_output,\n                args_mapping_rule=args_mapping_rule,\n                result_dispatching_rule=result_dispatching_rule,\n                callback_builders=callback_builders,\n            )\n            # Note1: the execution order of topology change deferred tasks is important and is determined by the order of the calls of add_worker(), remove_worker() and add_dependency() in one DS.\n            # Note2: add_worker() and remove_worker() may be called in a new thread. But _topology_change_deferred_tasks is not necessary to be thread-safe due to Visibility Guarantees of the Bridgic Concurrency Model.\n            self._topology_change_deferred_tasks.append(deferred_task)\n\n    def _add_func_as_worker_internal(\n        self,\n        key: str,\n        func: Callable,\n        *,\n        dependencies: List[str] = [],\n        is_start: bool = False,\n        is_output: bool = False,\n        args_mapping_rule: ArgsMappingRule = ArgsMappingRule.AS_IS,\n        result_dispatching_rule: ResultDispatchingRule = ResultDispatchingRule.AS_IS,\n        callback_builders: List[WorkerCallbackBuilder] = [],\n    ) -&gt; None:\n        \"\"\"\n        The private version of the method `add_func_as_worker()`.\n        \"\"\"\n        if not isinstance(func, MethodType) and key in self._registered_worker_funcs:\n            func = MethodType(func, self)\n\n        # Validate: if func is a method, its bounded __self__ must be self when add_func_as_worker() is called.\n        if hasattr(func, \"__self__\") and func.__self__ is not self:\n            raise AutomaRuntimeError(\n                f\"the bounded instance of `func` must be the same as the instance of the GraphAutoma, \"\n                f\"but got {func.__self__}\"\n            )\n\n        # Register func as an instance of CallableWorker.\n        func_worker = CallableWorker(func)\n\n        self._add_worker_internal(\n            key=key,\n            worker=func_worker,\n            dependencies=dependencies,\n            is_start=is_start,\n            is_output=is_output,\n            args_mapping_rule=args_mapping_rule,\n            result_dispatching_rule=result_dispatching_rule,\n            callback_builders=callback_builders,\n        )\n\n    def all_workers(self) -&gt; List[str]:\n        \"\"\"\n        Gets a list containing the keys of all workers registered in this Automa.\n\n        Returns\n        -------\n        List[str]\n            A list of worker keys.\n        \"\"\"\n        return list(self._workers.keys())\n\n    def add_worker(\n        self,\n        key: str,\n        worker: Worker,\n        *,\n        dependencies: List[str] = [],\n        is_start: bool = False,\n        is_output: bool = False,\n        args_mapping_rule: ArgsMappingRule = ArgsMappingRule.AS_IS,\n        result_dispatching_rule: ResultDispatchingRule = ResultDispatchingRule.AS_IS,\n        callback_builders: List[WorkerCallbackBuilder] = [],\n    ) -&gt; None:\n        \"\"\"\n        This method is used to add a worker dynamically into the automa.\n\n        If this method is called during the [Initialization Phase], the worker will be added immediately. If this method is called during the [Running Phase], the worker will be added as a deferred task which will be executed in the next DS.\n\n        The dependencies can be added together with a worker. However, you can add a worker without any dependencies.\n\n        Note: args_mapping_rule and result_dispatching_rule could only be set when using worker-adding API. Even if the worker has no any dependencies.\n\n        Parameters\n        ----------\n        key : str\n            The key of the worker.\n        worker : Worker\n            The worker instance to be registered.\n        dependencies : List[str]\n            A list of worker keys that the worker depends on.\n        is_start : bool\n            Whether the worker is a start worker.\n        is_output : bool\n            Whether the worker is an output worker.\n        args_mapping_rule : ArgsMappingRule\n            The rule of arguments mapping.\n        result_dispatching_rule : ResultDispatchingRule\n            The rule of result dispatch.\n        callback_builders : List[WorkerCallbackBuilder]\n            A list of worker callback builders to be registered.\n            Callback instances will be created from builders when the worker is instantiated.\n        \"\"\"\n        self._add_worker_internal(\n            key=key,\n            worker=worker,\n            dependencies=dependencies,\n            is_start=is_start,\n            is_output=is_output,\n            args_mapping_rule=args_mapping_rule,\n            result_dispatching_rule=result_dispatching_rule,\n            callback_builders=callback_builders,\n        )\n\n    def add_func_as_worker(\n        self,\n        key: str,\n        func: Callable,\n        *,\n        dependencies: List[str] = [],\n        is_start: bool = False,\n        is_output: bool = False,\n        args_mapping_rule: ArgsMappingRule = ArgsMappingRule.AS_IS,\n        result_dispatching_rule: ResultDispatchingRule = ResultDispatchingRule.AS_IS,\n        callback_builders: List[WorkerCallbackBuilder] = [],\n    ) -&gt; None:\n        \"\"\"\n        This method is used to add a function as a worker into the automa.\n\n        The format of the parameters will follow that of the decorator @worker(...), so that the \n        behavior of the decorated function is consistent with that of normal CallableLandableWorker objects.\n\n        Parameters\n        ----------\n        key : str\n            The key of the function worker.\n        func : Callable\n            The function to be added as a worker to the automa.\n        dependencies : List[str]\n            A list of worker names that the decorated callable depends on.\n        is_start : bool\n            Whether the decorated callable is a start worker. True means it is, while False means it is not.\n        is_output : bool\n            Whether the decorated callable is an output worker. True means it is, while False means it is not.\n        args_mapping_rule : ArgsMappingRule\n            The rule of arguments mapping.\n        result_dispatching_rule : ResultDispatchingRule\n            The rule of result dispatch.\n        callback_builders : List[WorkerCallbackBuilder]\n            A list of worker callback builders to be registered.\n            Callback instances will be created from builders when the worker is instantiated.\n        \"\"\"\n        self._add_func_as_worker_internal(\n            key=key,\n            func=func,\n            dependencies=dependencies,\n            is_start=is_start,\n            is_output=is_output,\n            args_mapping_rule=args_mapping_rule,\n            result_dispatching_rule=result_dispatching_rule,\n            callback_builders=callback_builders,\n        )\n\n    def worker(\n        self,\n        *,\n        key: Optional[str] = None,\n        dependencies: List[str] = [],\n        is_start: bool = False,\n        is_output: bool = False,\n        args_mapping_rule: ArgsMappingRule = ArgsMappingRule.AS_IS,\n        result_dispatching_rule: ResultDispatchingRule = ResultDispatchingRule.AS_IS,\n        callback_builders: List[WorkerCallbackBuilder] = [],\n    ) -&gt; Callable:\n        \"\"\"\n        This is a decorator used to mark a function as an GraphAutoma detectable Worker. Dislike the \n        global decorator @worker(...), it is usally used after an GraphAutoma instance is initialized.\n\n        The format of the parameters will follow that of the decorator @worker(...), so that the \n        behavior of the decorated function is consistent with that of normal CallableLandableWorker objects.\n\n        Parameters\n        ----------\n        key : str\n            The key of the worker. If not provided, the name of the decorated callable will be used.\n        dependencies : List[str]\n            A list of worker names that the decorated callable depends on.\n        is_start : bool\n            Whether the decorated callable is a start worker. True means it is, while False means it is not.\n        is_output : bool\n            Whether the decorated callable is an output worker. True means it is, while False means it is not.\n        args_mapping_rule : str\n            The rule of arguments mapping. The options are: \"auto\", \"as_list\", \"as_dict\", \"suppressed\".\n        result_dispatching_rule : ResultDispatchingRule\n            The rule of result dispatch.\n        callback_builders : List[WorkerCallbackBuilder]\n            A list of worker callback builders to be registered.\n            Callback instances will be created from builders when the worker is instantiated.\n        \"\"\"\n        def wrapper(func: Callable):\n            self._add_func_as_worker_internal(\n                key=(key or func.__name__),\n                func=func,\n                dependencies=dependencies,\n                is_start=is_start,\n                is_output=is_output,\n                args_mapping_rule=args_mapping_rule,\n                result_dispatching_rule=result_dispatching_rule,\n                callback_builders=callback_builders,\n            )\n\n        return wrapper\n\n    def remove_worker(self, key: str) -&gt; None:\n        \"\"\"\n        Remove a worker from the Automa. This method can be called at any time to remove a worker from the Automa.\n\n        When a worker is removed, all dependencies related to this worker, including all the dependencies of the worker itself and the dependencies between the worker and its successor workers, will be also removed.\n\n        Parameters\n        ----------\n        key : str\n            The key of the worker to be removed.\n\n        Returns\n        -------\n        None\n\n        Raises\n        ------\n        AutomaDeclarationError\n            If the worker specified by key does not exist in the Automa, this exception will be raised.\n        \"\"\"\n        if not self._automa_running:\n            # remove immediately\n            self._remove_worker_incrementally(key)\n        else:\n            deferred_task = _RemoveWorkerDeferredTask(\n                worker_key=key,\n            )\n            # Note: the execution order of topology change deferred tasks is important and is determined by the order of the calls of add_worker(), remove_worker() and add_dependency() in one DS.\n            self._topology_change_deferred_tasks.append(deferred_task)\n\n    def add_dependency(\n        self,\n        key: str,\n        dependency: str,\n    ) -&gt; None:\n        \"\"\"\n        This method is used to dynamically add a dependency from `key` to `dependency`.\n\n        Note: args_mapping_rule and result_dispatching_rule is not allowed to be set by this method, \n        instead they should be set together with add_worker() or add_func_as_worker() when adding the worker.\n\n        Parameters\n        ----------\n        key : str\n            The key of the worker that will depend on the worker with key `dependency`.\n        dependency : str\n            The key of the worker on which the worker with key `key` will depend.\n        \"\"\"\n        ...\n        if not self._automa_running:\n            # add the dependency immediately\n            self._add_dependency_incrementally(key, dependency)\n        else:\n            deferred_task = _AddDependencyDeferredTask(\n                worker_key=key,\n                dependency=dependency,\n            )\n            # Note: the execution order of topology change deferred tasks is important and is determined by the order of the calls of add_worker(), remove_worker() and add_dependency() in one DS.\n            self._topology_change_deferred_tasks.append(deferred_task)\n\n    def _validate_canonical_graph(self):\n        \"\"\"\n        This method is used to validate that DDG graph is canonical.\n        \"\"\"\n        for worker_key, worker_obj in self._workers.items():\n            for dependency_key in worker_obj.dependencies:\n                if dependency_key not in self._workers:\n                    raise AutomaCompilationError(\n                        f\"the dependency `{dependency_key}` of worker `{worker_key}` does not exist\"\n                    )\n        assert set(self._workers.keys()) == set(self._workers_dynamic_states.keys())\n        for worker_key, worker_dynamic_state in self._workers_dynamic_states.items():\n            for dependency_key in worker_dynamic_state.dependency_triggers:\n                assert dependency_key in self._workers[worker_key].dependencies\n\n        for worker_key, worker_obj in self._workers.items():\n            for dependency_key in worker_obj.dependencies:\n                assert worker_key in self._worker_forwards[dependency_key]\n        for worker_key, successor_keys in self._worker_forwards.items():\n            for successor_key in successor_keys:\n                assert worker_key in self._workers[successor_key].dependencies\n\n    def _compile_graph_and_detect_risks(self):\n        \"\"\"\n        This method should be called at the very beginning of self.run() to ensure that:\n        1. The whole graph is built out of all of the following worker sources:\n            - Pre-defined workers, such as:\n                - Methods decorated with @worker(...)\n            - Post-added workers, such as:\n                - Functions decorated with @automa_obj.worker(...)\n                - Workers added via automa_obj.add_func_as_worker(...)\n                - Workers added via automa_obj.add_worker(...)\n        2. The dependencies of each worker are confirmed to satisfy the DAG constraints.\n        \"\"\"\n\n        # Validate the canonical graph.\n        self._validate_canonical_graph()\n        # Validate the DAG constraints.\n        GraphMeta.validate_dag_constraints(self._worker_forwards)\n        # TODO: More validations can be added here...\n\n        # Find all connected components of the whole automa graph.\n        self._find_connected_components()\n\n    def ferry_to(self, key: str, /, *args, **kwargs):\n        \"\"\"\n        Defer the invocation to the specified worker, passing any provided arguments. This creates a \n        delayed call, ensuring the worker will be scheduled to run asynchronously in the next event loop, \n        independent of its dependencies.\n\n        This primitive is commonly used for:\n\n        1. Implementing dynamic branching based on runtime conditions.\n        2. Creating logic that forms cyclic graphs.\n\n        Parameters\n        ----------\n        key : str\n            The key of the worker to run.\n        args : optional\n            Positional arguments to be passed.\n        kwargs : optional\n            Keyword arguments to be passed.\n\n        Examples\n        --------\n        ```python\n        class MyGraphAutoma(GraphAutoma):\n            @worker(is_start=True)\n            def start_worker(self):\n                number = random.randint(0, 1)\n                if number == 0:\n                    self.ferry_to(\"cond_1_worker\", number=number)\n                else:\n                    self.ferry_to(\"cond_2_worker\")\n\n            @worker()\n            def cond_1_worker(self, number: int):\n                print(f'Got {{number}}!')\n\n            @worker()\n            def cond_2_worker(self):\n                self.ferry_to(\"start_worker\")\n\n        automa = MyGraphAutoma()\n        await automa.arun()\n\n        # Output: Got 0!\n        ```\n        \"\"\"\n        # TODO: check worker_key is valid, maybe deferred check...\n        running_options = self._get_top_running_options()\n        # if debug is enabled, trace back the kickoff worker key from stacktrace.\n        kickoff_worker_key: str = self._trace_back_kickoff_worker_key_from_stack() if running_options.debug else None\n        deferred_task = _FerryDeferredTask(\n            ferry_to_worker_key=key,\n            kickoff_worker_key=kickoff_worker_key,\n            args=args,\n            kwargs=kwargs,\n        )\n        # Note: ferry_to() may be called in a new thread.\n        # But _ferry_deferred_tasks is not necessary to be thread-safe due to Visibility Guarantees of the Bridgic Concurrency Model.\n        self._ferry_deferred_tasks.append(deferred_task)\n\n    def _clean_all_worker_local_space(self):\n        \"\"\"\n        Clean the local space of all workers.\n        \"\"\"\n        for worker_obj in self._workers.values():\n            worker_obj.local_space = {}\n\n    async def arun(\n        self,\n        *args: Tuple[Any, ...],\n        feedback_data: Optional[Union[InteractionFeedback, List[InteractionFeedback]]] = None,\n        **kwargs: Dict[str, Any]\n    ) -&gt; Any:\n        \"\"\"\n        The entry point for running the constructed `GraphAutoma` instance.\n\n        This method serves as the entry point for both initial execution and resumption after \n        interruption of an automa instance. It automatically drives the execution of workers \n        based on their `dependencies` and explicit `ferry_to()` calls. Each execution will be \n        wrapped in an `asyncio.Task` to ensure context isolation.\n\n        **Automatic Scheduling**\n\n        The scheduling behavior in `GraphAutoma` is automatically driven by:\n\n        - Worker dependencies: Workers are scheduled to run only after all their necessary \n          dependencies are satisfied. The dependencies automatically drive the execution order.\n\n        - Calling ferry_to: During execution, a worker can explicitly trigger another worker \n          by calling `ferry_to()`, which enables dynamic flow control and conditional branching.\n\n        - Dynamic topology changes: When the graph topology is modified at runtime (such as \n          adding or removing workers or dependencies), the scheduling system seamlessly updates \n          to reflect the latest structure, ensuring that worker execution always follows the \n          current graph.\n\n        **Human Interaction Mechanism**\n\n        Workers can request human input by calling `interact_with_human()` during execution. \n        When this occurs:\n\n        - The execution will be paused after the running workers finish their execution.\n        - The Automa's state will be serialized into a `Snapshot` object.\n        - An `InteractionException` will be raised to the application layer. It contains both the \n          list of pending `Interaction` objects and the `Snapshot` object.\n        - The application layer may persist the `Snapshot` properly to resume the execution later.\n        - To resume execution, the application layer should reload the Automa state using \n          `load_from_snapshot()` with the saved `Snapshot` object and call `arun()` again with \n          `feedback_data` containing the user's feedback(s) to finish a complete interaction.\n\n        Parameters\n        ----------\n        args : optional\n            Positional arguments to be passed.\n        feedback_data : Optional[Union[InteractionFeedback, List[InteractionFeedback]]]\n            Feedbacks that are received from one or multiple human interactions occurred before the\n            Automa was paused. This argument may be of type `InteractionFeedback` or \n            `List[InteractionFeedback]`. If only one interaction occurred, `feedback_data` should be\n            of type `InteractionFeedback`. If multiple interactions occurred simultaneously, \n            `feedback_data` should be of type `List[InteractionFeedback]`.\n        kwargs : optional\n            Keyword arguments which may be further propagated to contained workers.\n\n        Returns\n        -------\n        Any\n            The execution result of the output-worker that has the setting `is_output=True`,\n            otherwise None.\n\n        Raises\n        ------\n        InteractionException\n            If the Automa is the top-level Automa and the `interact_with_human()` method is called\n            by one or more workers within the lastest event loop iteration, this exception will be\n            raised to the application layer.\n        \"\"\"\n        if self.is_top_level():\n            # For top-level automa, wrap in a task to ensure context isolation\n            task = asyncio.create_task(\n                self._arun_internal(*args, feedback_data=feedback_data, **kwargs),\n                name=f\"GraphAutoma-{self.name}-arun\"\n            )\n            return await task\n        else:\n            # For nested automa, directly call _arun_internal to avoid redundant task creation\n            return await self._arun_internal(*args, feedback_data=feedback_data, **kwargs)\n\n    async def _arun_internal(\n        self,\n        *args: Tuple[Any, ...],\n        feedback_data: Optional[Union[InteractionFeedback, List[InteractionFeedback]]] = None,\n        **kwargs: Dict[str, Any]\n    ) -&gt; Any:\n        \"\"\"\n        Internal implementation of `arun()` for `GraphAutoma`.\n\n        The scheduling behavior in `GraphAutoma` is automatically driven by:\n\n        1. **Worker dependencies**: Workers are scheduled to run only after all their necessary \n        dependencies are satisfied. The dependencies automatically drives the execution order.\n\n        2. **Calling ferry_to**: During execution, a worker can explicitly trigger another worker \n        with calling `ferry_to()`, which enables dynamic flow control and conditional branching.\n\n        3. **Dynamic topology changes**: When the graph topology is modified at runtime (such as adding \n        or removing workers or dependencies), the scheduling system seamlessly updates to reflect \n        the latest structure, ensuring that worker execution always follows the current graph.\n        \"\"\"\n\n        def _reinit_current_kickoff_workers_if_needed():\n            # Note: After deserialization, the _current_kickoff_workers must not be empty!\n            # Therefore, _current_kickoff_workers will only be reinitialized when the Automa is run for the first time or rerun.\n            # It is guaranteed that _current_kickoff_workers will not be reinitialized when the Automa is resumed after deserialization.\n            if not self._current_kickoff_workers:\n                self._current_kickoff_workers = [\n                    _KickoffInfo(\n                        worker_key=worker_key,\n                        last_kickoff=\"__automa__\"\n                    ) for worker_key, worker_obj in self._workers.items()\n                    if getattr(worker_obj, \"is_start\", False)\n                ]\n                # Each time the Automa re-runs, buffer the input arguments here.\n                self._input_buffer.args = args\n                self._input_buffer.kwargs = kwargs\n\n        def _execute_topology_change_deferred_tasks(tc_tasks: List[Union[_AddWorkerDeferredTask, _RemoveWorkerDeferredTask, _AddDependencyDeferredTask]]):\n            # update the control flow topology\n            for topology_task in tc_tasks:\n                if topology_task.task_type == \"add_worker\":\n                    self._add_worker_incrementally(\n                        key=topology_task.worker_key,\n                        worker=topology_task.worker_obj,\n                        dependencies=topology_task.dependencies,\n                        is_start=topology_task.is_start,\n                        is_output=topology_task.is_output,\n                        args_mapping_rule=topology_task.args_mapping_rule,\n                        result_dispatching_rule=topology_task.result_dispatching_rule,\n                        callback_builders=topology_task.callback_builders,\n                    )\n                elif topology_task.task_type == \"remove_worker\":\n                    self._remove_worker_incrementally(topology_task.worker_key)\n                elif topology_task.task_type == \"add_dependency\":\n                    self._add_dependency_incrementally(topology_task.worker_key, topology_task.dependency)\n\n            # update the data flow topology\n            args_manager.update_data_flow_topology(dynamic_tasks=tc_tasks)\n\n        def _set_worker_run_finished(worker_key: str):\n            for kickoff_info in self._current_kickoff_workers:\n                if kickoff_info.worker_key == worker_key:\n                    kickoff_info.run_finished = True\n                    break\n\n        def _check_and_normalize_interaction_params(\n            feedback_data: Optional[Union[InteractionFeedback, List[InteractionFeedback]]] = None,\n            interaction_feedback: Optional[InteractionFeedback] = None,\n            interaction_feedbacks: Optional[List[InteractionFeedback]] = None,\n        ):\n            if feedback_data:\n                if isinstance(feedback_data, list):\n                    rx_feedbacks = feedback_data\n                else:\n                    rx_feedbacks = [feedback_data]\n                return rx_feedbacks\n            # For backward compatibility with old parameter names. To be removed in the future.\n            if interaction_feedback and interaction_feedbacks:\n                raise AutomaRuntimeError(\n                    f\"Only one of interaction_feedback or interaction_feedbacks can be used. \"\n                    f\"But received interaction_feedback={interaction_feedback} and \\n\"\n                    f\"interaction_feedbacks={interaction_feedbacks}\"\n                )\n            if interaction_feedback:\n                rx_feedbacks = [interaction_feedback]\n            else:\n                rx_feedbacks = interaction_feedbacks\n            return rx_feedbacks\n\n        def _match_ongoing_interaction_and_feedbacks(rx_feedbacks:List[InteractionFeedback]):\n            match_left_feedbacks = []\n            for feedback in rx_feedbacks:\n                matched = False\n                for interaction_and_feedbacks in self._ongoing_interactions.values():\n                    for interaction_and_feedback in interaction_and_feedbacks:\n                        if interaction_and_feedback.interaction.interaction_id == feedback.interaction_id:\n                            matched = True\n                            # Note: Only one feedback is allowed for each interaction. Here we assume that only the first feedback is valid, which is a choice of implementation.\n                            if interaction_and_feedback.feedback is None:\n                                # Set feedback to self._ongoing_interactions\n                                interaction_and_feedback.feedback = feedback\n                            break\n                    if matched:\n                        break\n                if not matched:\n                    match_left_feedbacks.append(feedback)\n            return match_left_feedbacks\n\n        running_options = self._get_top_running_options()\n\n        self._main_loop = asyncio.get_running_loop()\n        self._main_thread_id = threading.get_ident()\n\n        if self.thread_pool is None:\n            self.thread_pool = ThreadPoolExecutor(thread_name_prefix=\"bridgic-thread\")\n\n        if not self._automa_running:\n            # Here is the last chance to compile and check the DDG in the end of the [Initialization Phase] (phase 1 just before the first DS).\n            self._compile_graph_and_detect_risks()\n            self._automa_running = True\n\n        # An Automa needs to be re-run with _current_kickoff_workers reinitialized.\n        _reinit_current_kickoff_workers_if_needed()\n\n        is_top_level = self.is_top_level()\n\n        # If this is the top-level automa, execute its callbacks separately.\n        if is_top_level:\n            automa_callbacks = self._get_automa_callbacks()\n\n            for callback in automa_callbacks:\n                await callback.on_worker_start(\n                    key=self.name,\n                    is_top_level=True,\n                    parent=self.parent,\n                    arguments={\n                        \"args\": self._input_buffer.args,\n                        \"kwargs\": self._input_buffer.kwargs,\n                        \"feedback_data\": feedback_data,\n                    },\n                )\n\n        # For backward compatibility with old parameter names. To be removed in the future.\n        interaction_feedback = kwargs.get(\"interaction_feedback\")\n        interaction_feedbacks = kwargs.get(\"interaction_feedbacks\")\n        rx_feedbacks = _check_and_normalize_interaction_params(feedback_data, interaction_feedback, interaction_feedbacks)\n        if rx_feedbacks:\n            rx_feedbacks = _match_ongoing_interaction_and_feedbacks(rx_feedbacks)\n\n        if running_options.debug:\n            printer.print(f\"\\n{type(self).__name__}-[{self.name}] is getting started.\", color=\"green\")\n\n        # Task loop divided into many dynamic steps (DS).\n        args_manager = ArgsManager(\n            input_args=self._input_buffer.args,\n            input_kwargs=self._input_buffer.kwargs,\n            worker_outputs=self._worker_output,\n            worker_forwards=self._worker_forwards,\n            worker_dict=self._workers\n        )\n        is_output_worker_keys = set()\n        while self._current_kickoff_workers:\n            # A new DS started.\n            if running_options.debug:\n                kickoff_worker_keys = [kickoff_info.worker_key for kickoff_info in self._current_kickoff_workers]\n                printer.print(f\"[DS][Before Tasks Started] kickoff workers: {kickoff_worker_keys}\", color=\"purple\")\n\n            for kickoff_info in self._current_kickoff_workers:\n                if kickoff_info.run_finished:\n                    # Skip finished workers. Here is the case that the Automa is resumed after a human interaction.\n                    if running_options.debug:\n                        printer.print(f\"[{kickoff_info.worker_key}] will be skipped - run finished\", color=\"blue\")\n                    continue\n\n                if running_options.debug:\n                    kickoff_name = kickoff_info.last_kickoff\n                    if kickoff_name == \"__automa__\":\n                        kickoff_name = f\"{kickoff_name}:({self.name})\"\n                    printer.print(f\"[{kickoff_name}] will kick off [{kickoff_info.worker_key}]\", color=\"cyan\")\n\n                # Arguments Mapping:\n                binding_args, binding_kwargs = args_manager.args_binding(\n                    last_worker_key=kickoff_info.last_kickoff,\n                    current_worker_key=kickoff_info.worker_key\n                ) if not kickoff_info.from_ferry else ((), {})\n                # Inputs Propagation\n                _, propagation_kwargs = args_manager.inputs_propagation(current_worker_key=kickoff_info.worker_key)\n                # Data injection.\n                _, injection_kwargs = args_manager.args_injection(\n                    current_worker_key=kickoff_info.worker_key, \n                    current_automa=self\n                )\n                # Ferry arguments.\n                ferry_args, ferry_kwargs = kickoff_info.args, kickoff_info.kwargs\n                # combine the arguments from the three steps.\n                # kwargs will cover priority follows: propagation_kwargs &lt; binding_kwargs &lt; injection_kwargs &lt; ferry_kwargs\n                next_args, next_kwargs = safely_map_args(\n                    (*binding_args, *ferry_args), \n                    {**propagation_kwargs, **binding_kwargs, **injection_kwargs, **ferry_kwargs}, \n                    self._workers[kickoff_info.worker_key].get_input_param_names()\n                )\n\n                # Collect the output worker keys.\n                if self._workers[kickoff_info.worker_key].is_output:\n                    is_output_worker_keys.add(kickoff_info.worker_key)\n                    if len(is_output_worker_keys) &gt; 1:\n                        raise AutomaRuntimeError(\n                            f\"It is not allowed to have more than one worker with `is_output=True` and \"\n                            f\"they are all considered as output-worker when the automa terminates and returns.\"\n                            f\"The current output-worker keys are: {is_output_worker_keys}.\"\n                            f\"If you want to collect the results of multiple workers simultaneously, \"\n                            f\"it is recommended that you add one worker to gather them.\"\n                        )\n\n                # Schedule task for each kickoff worker.\n                worker_obj = self._workers[kickoff_info.worker_key]\n                if worker_obj.is_automa():\n                    coro = worker_obj.arun(\n                        *next_args,\n                        feedback_data=rx_feedbacks,\n                        **next_kwargs,\n                    )\n                else:\n                    coro = worker_obj.arun(*next_args, **next_kwargs)\n\n                task = asyncio.create_task(\n                    # TODO1: arun() may need to be wrapped to support better interrupt...\n                    coro,\n                    name=f\"Task-{kickoff_info.worker_key}\"\n                )\n                self._running_tasks.append(_RunnningTask(\n                    worker_key=kickoff_info.worker_key,\n                    task=task,\n                ))\n\n            # Wait until all of the tasks are finished.\n            while True:\n                undone_tasks = [t.task for t in self._running_tasks if not t.task.done()]\n                if not undone_tasks:\n                    break\n                try:\n                    await undone_tasks[0]\n                except Exception as e:\n                    ...\n                    # The same exception will be raised again in the following task.result().\n                    # Note: A Task is done when the wrapped coroutine either returned a value, raised an exception, or the Task was cancelled.\n                    # Refer to: https://docs.python.org/3/library/asyncio-task.html#task-object\n\n            # Process graph topology change deferred tasks triggered by add_worker() and remove_worker().\n            _execute_topology_change_deferred_tasks(self._topology_change_deferred_tasks)\n\n            # Handle exceptions raised by all running tasks.\n            interaction_exceptions: List[_InteractionEventException] = []\n            non_interaction_exceptions: List[Exception] = []\n\n            for task in self._running_tasks:\n                try:\n                    # It will raise an exception if task failed.\n                    task_result = task.task.result()\n                    _set_worker_run_finished(task.worker_key)\n\n                    if task.worker_key in self._workers:\n                        # The current running worker may be removed.\n                        worker_obj = self._workers[task.worker_key]\n                        # Collect results of the finished tasks.\n                        self._worker_output[task.worker_key] = task_result\n                        # reset dynamic states of finished workers.\n                        self._workers_dynamic_states[task.worker_key].dependency_triggers = set(getattr(worker_obj, \"dependencies\", []))\n                        # Update the dynamic states of successor workers.\n                        for successor_key in self._worker_forwards.get(task.worker_key, []):\n                            self._workers_dynamic_states[successor_key].dependency_triggers.remove(task.worker_key)\n                        # Each time a worker is finished running, the ongoing interaction states should be cleared. Once it is re-run, the human interactions in the worker can be triggered again.\n                        if task.worker_key in self._worker_interaction_indices:\n                            del self._worker_interaction_indices[task.worker_key]\n                        if task.worker_key in self._ongoing_interactions:\n                            del self._ongoing_interactions[task.worker_key]\n                except Exception as e:\n                    if isinstance(e, _InteractionEventException):\n                        interaction_exceptions.append(e)\n                        if task.worker_key in self._workers and not self._workers[task.worker_key].is_automa():\n                            if task.worker_key not in self._ongoing_interactions:\n                                self._ongoing_interactions[task.worker_key] = []\n                            interaction=e.args[0]\n                            # Make sure the interaction_id is unique for each human interaction.\n                            found = False\n                            for iaf in self._ongoing_interactions[task.worker_key]:\n                                if iaf.interaction.interaction_id == interaction.interaction_id:\n                                    found = True\n                                    break\n                            if not found:\n                                self._ongoing_interactions[task.worker_key].append(_InteractionAndFeedback(\n                                    interaction=interaction,\n                                ))\n                    else:\n                        non_interaction_exceptions.append(e)\n\n            if len(self._topology_change_deferred_tasks) &gt; 0:\n                # Graph topology validation and risk detection. Only needed when topology changes.\n                # Guarantee the graph topology is valid and consistent after each DS.\n                # 1. Validate the canonical graph.\n                self._validate_canonical_graph()\n                # 2. Validate the DAG constraints.\n                GraphMeta.validate_dag_constraints(self._worker_forwards)\n                # TODO: more validations can be added here...\n\n            # TODO: Ferry-related risk detection may be added here...\n\n            # Handle exceptions with callbacks at the top-level automa before re-raising them.\n            if is_top_level:\n                # Get cached callbacks for top-level automa\n                automa_callbacks = self._get_automa_callbacks()\n\n                # Process interaction exceptions with callbacks (they cannot be suppressed, but callbacks can observe them)\n                for e in interaction_exceptions + non_interaction_exceptions:\n                    await try_handle_error_with_callbacks(\n                        callbacks=automa_callbacks,\n                        key=self.name,\n                        is_top_level=True,\n                        parent=self.parent,\n                        arguments={\n                            \"args\": self._input_buffer.args,\n                            \"kwargs\": self._input_buffer.kwargs,\n                            \"feedback_data\": feedback_data,\n                        },\n                        error=e,\n                    )\n\n            # For inner interaction exceptions, collect them and throw an InteractionException as a whole.\n            if len(interaction_exceptions) &gt; 0:\n                all_interactions: List[Interaction] = [interaction for e in interaction_exceptions for interaction in e.args]\n                if self.is_top_level():\n                    # This is the top-level Automa. Serialize the Automa and raise InteractionException to the application layer.\n                    serialized_automa = dump_bytes(self)\n                    snapshot = Snapshot(\n                        serialized_bytes=serialized_automa,\n                        serialization_version=GraphAutoma.SERIALIZATION_VERSION,\n                    )\n                    raise InteractionException(\n                        interactions=all_interactions,\n                        snapshot=snapshot,\n                    )\n                else:\n                    # Continue raise exception to the upper level Automa.\n                    raise _InteractionEventException(*all_interactions)\n\n            # For non-interaction exceptions, immediately raise the first one directly, since none of them are meant to be suppressed.\n            if len(non_interaction_exceptions) &gt; 0:\n                raise non_interaction_exceptions[0]\n\n            # Find next kickoff workers and rebuild _current_kickoff_workers\n            run_finished_worker_keys: List[str] = [kickoff_info.worker_key for kickoff_info in self._current_kickoff_workers if kickoff_info.run_finished]\n            assert len(run_finished_worker_keys) == len(self._current_kickoff_workers)\n            self._current_kickoff_workers = []\n            # New kickoff workers can be triggered by two ways:\n            # 1. The ferry_to() operation is called during current worker execution.\n            # 2. The dependencies are eliminated after all predecessor workers are finished.\n            # So,\n            # First add kickoff workers triggered by ferry_to();\n            for ferry_task in self._ferry_deferred_tasks:\n                self._current_kickoff_workers.append(_KickoffInfo(\n                    worker_key=ferry_task.ferry_to_worker_key,\n                    last_kickoff=ferry_task.kickoff_worker_key,\n                    from_ferry=True,\n                    args=ferry_task.args,\n                    kwargs=ferry_task.kwargs,\n                ))\n            # Then add kickoff workers triggered by dependencies elimination.\n            # Merge successor keys of all finished tasks.\n            successor_keys = set()\n            for worker_key in run_finished_worker_keys:\n                # Note: The `worker_key` worker may have been removed from the Automa.\n                for successor_key in self._worker_forwards.get(worker_key, []):\n                    if successor_key not in successor_keys:\n                        dependency_triggers = self._workers_dynamic_states[successor_key].dependency_triggers\n                        if not dependency_triggers:\n                            self._current_kickoff_workers.append(_KickoffInfo(\n                                worker_key=successor_key,\n                                last_kickoff=worker_key,\n                            ))\n                        successor_keys.add(successor_key)\n            if running_options.debug:\n                deferred_ferrys = [ferry_task.ferry_to_worker_key for ferry_task in self._ferry_deferred_tasks]\n                printer.print(f\"[DS][After Tasks Finished] successor workers: {successor_keys}, deferred ferrys: {deferred_ferrys}\", color=\"purple\")\n\n            # Clear running tasks after all finished.\n            self._running_tasks.clear()\n            self._ferry_deferred_tasks.clear()\n            self._topology_change_deferred_tasks.clear()\n\n        if running_options.debug:\n            printer.print(f\"{type(self).__name__}-[{self.name}] is finished.\", color=\"green\")\n\n        # After a complete run, reset all necessary states to allow the automa to re-run.\n        self._input_buffer = _AutomaInputBuffer()\n        if self.should_reset_local_space():\n            self._clean_all_worker_local_space()\n        self._ongoing_interactions.clear()\n        self._worker_interaction_indices.clear()\n        self._automa_running = False\n\n        # Get result before calling callbacks\n        if is_output_worker_keys:\n            result = self._worker_output.get(list(is_output_worker_keys)[0], None)\n        else:\n            result = None\n\n        # If this is the top-level automa, execute its callbacks separately.\n        if is_top_level:\n            automa_callbacks = self._get_automa_callbacks()\n            for callback in automa_callbacks:\n                await callback.on_worker_end(\n                    key=self.name,\n                    is_top_level=True,\n                    parent=self.parent,\n                    arguments={\n                        \"args\": self._input_buffer.args,\n                        \"kwargs\": self._input_buffer.kwargs,\n                        \"feedback_data\": feedback_data,\n                    },\n                    result=result,\n                )\n\n        return result\n\n    def _get_worker_dependencies(self, worker_key: str) -&gt; List[str]:\n        \"\"\"\n        Get the worker keys of all dependencies of the worker.\n        \"\"\"\n        deps = self._workers[worker_key].dependencies\n        return [] if deps is None else deps\n\n    def _find_connected_components(self):\n        \"\"\"\n        Find all of the connected components in the whole automa graph described by self._workers.\n        \"\"\"\n        visited = set()\n        component_list = []\n        component_idx = {}\n\n        def dfs(worker: str, component: List[str]):\n            visited.add(worker)\n            component.append(worker)\n            for target in self._worker_forwards.get(worker, []):\n                if target not in visited:\n                    dfs(target, component)\n\n        for worker in self._workers.keys():\n            if worker not in visited:\n                component_list.append([])\n                current_idx = len(component_list) - 1\n                current_component = component_list[current_idx]\n\n                dfs(worker, current_component)\n\n                for worker in current_component:\n                    component_idx[worker] = current_idx\n\n        # self._component_list, self._component_idx = component_list, component_idx\n        # TODO: check how to use _component_list and _component_idx...\n\n    @override\n    def _get_worker_key(self, worker: Worker) -&gt; Optional[str]:\n        for worker_key, worker_obj in self._workers.items():\n            if worker_obj == worker:\n                # Note: _GraphAdaptedWorker.__eq__() is overridden to support the '==' operator.\n                return worker_key\n        return None\n\n    @override\n    def _get_worker_instance(self, worker_key: str) -&gt; Worker:\n        return self._workers[worker_key]\n\n    @override\n    def _locate_interacting_worker(self) -&gt; Optional[str]:\n        return self._trace_back_kickoff_worker_key_from_stack()\n\n    def _trace_back_kickoff_worker_key_from_stack(self) -&gt; Optional[str]:\n        worker = self._get_current_running_worker_instance_by_stacktrace()\n        if worker:\n            return self._get_worker_key(worker)\n        return None\n\n    def _get_current_running_worker_instance_by_stacktrace(self) -&gt; Optional[Worker]:\n        for frame_info in inspect.stack():\n            frame = frame_info.frame\n            if 'self' in frame.f_locals:\n                self_obj = frame.f_locals['self']\n                if isinstance(self_obj, Worker) and (not isinstance(self_obj, Automa)) and (frame_info.function == \"arun\" or frame_info.function == \"run\"):\n                    return self_obj\n        return None\n\n    def __repr__(self) -&gt; str:\n        # TODO : It's good to make __repr__() of Automa compatible with eval().\n        # This feature depends on the implementation of __repr__() of workers.\n        class_name = self.__class__.__name__\n        workers_str = self._workers.__repr__()\n        return f\"{class_name}(workers={workers_str})\"\n\n    def __str__(self) -&gt; str:\n        d = {}\n        for k, v in self._workers.items():\n            d[k] = f\"{v} depends on {getattr(v, 'dependencies', [])}\"\n        return json.dumps(d, ensure_ascii=False, indent=4)\n</code></pre>"},{"location":"reference/bridgic-core/bridgic/core/automa/#bridgic.core.automa.GraphAutoma.all_workers","title":"all_workers","text":"<pre><code>all_workers() -&gt; List[str]\n</code></pre> <p>Gets a list containing the keys of all workers registered in this Automa.</p> <p>Returns:</p> Type Description <code>List[str]</code> <p>A list of worker keys.</p> Source code in <code>bridgic/core/automa/_graph_automa.py</code> <pre><code>def all_workers(self) -&gt; List[str]:\n    \"\"\"\n    Gets a list containing the keys of all workers registered in this Automa.\n\n    Returns\n    -------\n    List[str]\n        A list of worker keys.\n    \"\"\"\n    return list(self._workers.keys())\n</code></pre>"},{"location":"reference/bridgic-core/bridgic/core/automa/#bridgic.core.automa.GraphAutoma.add_worker","title":"add_worker","text":"<pre><code>add_worker(\n    key: str,\n    worker: Worker,\n    *,\n    dependencies: List[str] = [],\n    is_start: bool = False,\n    is_output: bool = False,\n    args_mapping_rule: ArgsMappingRule = AS_IS,\n    result_dispatching_rule: ResultDispatchingRule = AS_IS,\n    callback_builders: List[WorkerCallbackBuilder] = []\n) -&gt; None\n</code></pre> <p>This method is used to add a worker dynamically into the automa.</p> <p>If this method is called during the [Initialization Phase], the worker will be added immediately. If this method is called during the [Running Phase], the worker will be added as a deferred task which will be executed in the next DS.</p> <p>The dependencies can be added together with a worker. However, you can add a worker without any dependencies.</p> <p>Note: args_mapping_rule and result_dispatching_rule could only be set when using worker-adding API. Even if the worker has no any dependencies.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>The key of the worker.</p> required <code>worker</code> <code>Worker</code> <p>The worker instance to be registered.</p> required <code>dependencies</code> <code>List[str]</code> <p>A list of worker keys that the worker depends on.</p> <code>[]</code> <code>is_start</code> <code>bool</code> <p>Whether the worker is a start worker.</p> <code>False</code> <code>is_output</code> <code>bool</code> <p>Whether the worker is an output worker.</p> <code>False</code> <code>args_mapping_rule</code> <code>ArgsMappingRule</code> <p>The rule of arguments mapping.</p> <code>AS_IS</code> <code>result_dispatching_rule</code> <code>ResultDispatchingRule</code> <p>The rule of result dispatch.</p> <code>AS_IS</code> <code>callback_builders</code> <code>List[WorkerCallbackBuilder]</code> <p>A list of worker callback builders to be registered. Callback instances will be created from builders when the worker is instantiated.</p> <code>[]</code> Source code in <code>bridgic/core/automa/_graph_automa.py</code> <pre><code>def add_worker(\n    self,\n    key: str,\n    worker: Worker,\n    *,\n    dependencies: List[str] = [],\n    is_start: bool = False,\n    is_output: bool = False,\n    args_mapping_rule: ArgsMappingRule = ArgsMappingRule.AS_IS,\n    result_dispatching_rule: ResultDispatchingRule = ResultDispatchingRule.AS_IS,\n    callback_builders: List[WorkerCallbackBuilder] = [],\n) -&gt; None:\n    \"\"\"\n    This method is used to add a worker dynamically into the automa.\n\n    If this method is called during the [Initialization Phase], the worker will be added immediately. If this method is called during the [Running Phase], the worker will be added as a deferred task which will be executed in the next DS.\n\n    The dependencies can be added together with a worker. However, you can add a worker without any dependencies.\n\n    Note: args_mapping_rule and result_dispatching_rule could only be set when using worker-adding API. Even if the worker has no any dependencies.\n\n    Parameters\n    ----------\n    key : str\n        The key of the worker.\n    worker : Worker\n        The worker instance to be registered.\n    dependencies : List[str]\n        A list of worker keys that the worker depends on.\n    is_start : bool\n        Whether the worker is a start worker.\n    is_output : bool\n        Whether the worker is an output worker.\n    args_mapping_rule : ArgsMappingRule\n        The rule of arguments mapping.\n    result_dispatching_rule : ResultDispatchingRule\n        The rule of result dispatch.\n    callback_builders : List[WorkerCallbackBuilder]\n        A list of worker callback builders to be registered.\n        Callback instances will be created from builders when the worker is instantiated.\n    \"\"\"\n    self._add_worker_internal(\n        key=key,\n        worker=worker,\n        dependencies=dependencies,\n        is_start=is_start,\n        is_output=is_output,\n        args_mapping_rule=args_mapping_rule,\n        result_dispatching_rule=result_dispatching_rule,\n        callback_builders=callback_builders,\n    )\n</code></pre>"},{"location":"reference/bridgic-core/bridgic/core/automa/#bridgic.core.automa.GraphAutoma.add_func_as_worker","title":"add_func_as_worker","text":"<pre><code>add_func_as_worker(\n    key: str,\n    func: Callable,\n    *,\n    dependencies: List[str] = [],\n    is_start: bool = False,\n    is_output: bool = False,\n    args_mapping_rule: ArgsMappingRule = AS_IS,\n    result_dispatching_rule: ResultDispatchingRule = AS_IS,\n    callback_builders: List[WorkerCallbackBuilder] = []\n) -&gt; None\n</code></pre> <p>This method is used to add a function as a worker into the automa.</p> <p>The format of the parameters will follow that of the decorator @worker(...), so that the  behavior of the decorated function is consistent with that of normal CallableLandableWorker objects.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>The key of the function worker.</p> required <code>func</code> <code>Callable</code> <p>The function to be added as a worker to the automa.</p> required <code>dependencies</code> <code>List[str]</code> <p>A list of worker names that the decorated callable depends on.</p> <code>[]</code> <code>is_start</code> <code>bool</code> <p>Whether the decorated callable is a start worker. True means it is, while False means it is not.</p> <code>False</code> <code>is_output</code> <code>bool</code> <p>Whether the decorated callable is an output worker. True means it is, while False means it is not.</p> <code>False</code> <code>args_mapping_rule</code> <code>ArgsMappingRule</code> <p>The rule of arguments mapping.</p> <code>AS_IS</code> <code>result_dispatching_rule</code> <code>ResultDispatchingRule</code> <p>The rule of result dispatch.</p> <code>AS_IS</code> <code>callback_builders</code> <code>List[WorkerCallbackBuilder]</code> <p>A list of worker callback builders to be registered. Callback instances will be created from builders when the worker is instantiated.</p> <code>[]</code> Source code in <code>bridgic/core/automa/_graph_automa.py</code> <pre><code>def add_func_as_worker(\n    self,\n    key: str,\n    func: Callable,\n    *,\n    dependencies: List[str] = [],\n    is_start: bool = False,\n    is_output: bool = False,\n    args_mapping_rule: ArgsMappingRule = ArgsMappingRule.AS_IS,\n    result_dispatching_rule: ResultDispatchingRule = ResultDispatchingRule.AS_IS,\n    callback_builders: List[WorkerCallbackBuilder] = [],\n) -&gt; None:\n    \"\"\"\n    This method is used to add a function as a worker into the automa.\n\n    The format of the parameters will follow that of the decorator @worker(...), so that the \n    behavior of the decorated function is consistent with that of normal CallableLandableWorker objects.\n\n    Parameters\n    ----------\n    key : str\n        The key of the function worker.\n    func : Callable\n        The function to be added as a worker to the automa.\n    dependencies : List[str]\n        A list of worker names that the decorated callable depends on.\n    is_start : bool\n        Whether the decorated callable is a start worker. True means it is, while False means it is not.\n    is_output : bool\n        Whether the decorated callable is an output worker. True means it is, while False means it is not.\n    args_mapping_rule : ArgsMappingRule\n        The rule of arguments mapping.\n    result_dispatching_rule : ResultDispatchingRule\n        The rule of result dispatch.\n    callback_builders : List[WorkerCallbackBuilder]\n        A list of worker callback builders to be registered.\n        Callback instances will be created from builders when the worker is instantiated.\n    \"\"\"\n    self._add_func_as_worker_internal(\n        key=key,\n        func=func,\n        dependencies=dependencies,\n        is_start=is_start,\n        is_output=is_output,\n        args_mapping_rule=args_mapping_rule,\n        result_dispatching_rule=result_dispatching_rule,\n        callback_builders=callback_builders,\n    )\n</code></pre>"},{"location":"reference/bridgic-core/bridgic/core/automa/#bridgic.core.automa.GraphAutoma.worker","title":"worker","text":"<pre><code>worker(\n    *,\n    key: Optional[str] = None,\n    dependencies: List[str] = [],\n    is_start: bool = False,\n    is_output: bool = False,\n    args_mapping_rule: ArgsMappingRule = AS_IS,\n    result_dispatching_rule: ResultDispatchingRule = AS_IS,\n    callback_builders: List[WorkerCallbackBuilder] = []\n) -&gt; Callable\n</code></pre> <p>This is a decorator used to mark a function as an GraphAutoma detectable Worker. Dislike the  global decorator @worker(...), it is usally used after an GraphAutoma instance is initialized.</p> <p>The format of the parameters will follow that of the decorator @worker(...), so that the  behavior of the decorated function is consistent with that of normal CallableLandableWorker objects.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>The key of the worker. If not provided, the name of the decorated callable will be used.</p> <code>None</code> <code>dependencies</code> <code>List[str]</code> <p>A list of worker names that the decorated callable depends on.</p> <code>[]</code> <code>is_start</code> <code>bool</code> <p>Whether the decorated callable is a start worker. True means it is, while False means it is not.</p> <code>False</code> <code>is_output</code> <code>bool</code> <p>Whether the decorated callable is an output worker. True means it is, while False means it is not.</p> <code>False</code> <code>args_mapping_rule</code> <code>str</code> <p>The rule of arguments mapping. The options are: \"auto\", \"as_list\", \"as_dict\", \"suppressed\".</p> <code>AS_IS</code> <code>result_dispatching_rule</code> <code>ResultDispatchingRule</code> <p>The rule of result dispatch.</p> <code>AS_IS</code> <code>callback_builders</code> <code>List[WorkerCallbackBuilder]</code> <p>A list of worker callback builders to be registered. Callback instances will be created from builders when the worker is instantiated.</p> <code>[]</code> Source code in <code>bridgic/core/automa/_graph_automa.py</code> <pre><code>def worker(\n    self,\n    *,\n    key: Optional[str] = None,\n    dependencies: List[str] = [],\n    is_start: bool = False,\n    is_output: bool = False,\n    args_mapping_rule: ArgsMappingRule = ArgsMappingRule.AS_IS,\n    result_dispatching_rule: ResultDispatchingRule = ResultDispatchingRule.AS_IS,\n    callback_builders: List[WorkerCallbackBuilder] = [],\n) -&gt; Callable:\n    \"\"\"\n    This is a decorator used to mark a function as an GraphAutoma detectable Worker. Dislike the \n    global decorator @worker(...), it is usally used after an GraphAutoma instance is initialized.\n\n    The format of the parameters will follow that of the decorator @worker(...), so that the \n    behavior of the decorated function is consistent with that of normal CallableLandableWorker objects.\n\n    Parameters\n    ----------\n    key : str\n        The key of the worker. If not provided, the name of the decorated callable will be used.\n    dependencies : List[str]\n        A list of worker names that the decorated callable depends on.\n    is_start : bool\n        Whether the decorated callable is a start worker. True means it is, while False means it is not.\n    is_output : bool\n        Whether the decorated callable is an output worker. True means it is, while False means it is not.\n    args_mapping_rule : str\n        The rule of arguments mapping. The options are: \"auto\", \"as_list\", \"as_dict\", \"suppressed\".\n    result_dispatching_rule : ResultDispatchingRule\n        The rule of result dispatch.\n    callback_builders : List[WorkerCallbackBuilder]\n        A list of worker callback builders to be registered.\n        Callback instances will be created from builders when the worker is instantiated.\n    \"\"\"\n    def wrapper(func: Callable):\n        self._add_func_as_worker_internal(\n            key=(key or func.__name__),\n            func=func,\n            dependencies=dependencies,\n            is_start=is_start,\n            is_output=is_output,\n            args_mapping_rule=args_mapping_rule,\n            result_dispatching_rule=result_dispatching_rule,\n            callback_builders=callback_builders,\n        )\n\n    return wrapper\n</code></pre>"},{"location":"reference/bridgic-core/bridgic/core/automa/#bridgic.core.automa.GraphAutoma.remove_worker","title":"remove_worker","text":"<pre><code>remove_worker(key: str) -&gt; None\n</code></pre> <p>Remove a worker from the Automa. This method can be called at any time to remove a worker from the Automa.</p> <p>When a worker is removed, all dependencies related to this worker, including all the dependencies of the worker itself and the dependencies between the worker and its successor workers, will be also removed.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>The key of the worker to be removed.</p> required <p>Returns:</p> Type Description <code>None</code> <p>Raises:</p> Type Description <code>AutomaDeclarationError</code> <p>If the worker specified by key does not exist in the Automa, this exception will be raised.</p> Source code in <code>bridgic/core/automa/_graph_automa.py</code> <pre><code>def remove_worker(self, key: str) -&gt; None:\n    \"\"\"\n    Remove a worker from the Automa. This method can be called at any time to remove a worker from the Automa.\n\n    When a worker is removed, all dependencies related to this worker, including all the dependencies of the worker itself and the dependencies between the worker and its successor workers, will be also removed.\n\n    Parameters\n    ----------\n    key : str\n        The key of the worker to be removed.\n\n    Returns\n    -------\n    None\n\n    Raises\n    ------\n    AutomaDeclarationError\n        If the worker specified by key does not exist in the Automa, this exception will be raised.\n    \"\"\"\n    if not self._automa_running:\n        # remove immediately\n        self._remove_worker_incrementally(key)\n    else:\n        deferred_task = _RemoveWorkerDeferredTask(\n            worker_key=key,\n        )\n        # Note: the execution order of topology change deferred tasks is important and is determined by the order of the calls of add_worker(), remove_worker() and add_dependency() in one DS.\n        self._topology_change_deferred_tasks.append(deferred_task)\n</code></pre>"},{"location":"reference/bridgic-core/bridgic/core/automa/#bridgic.core.automa.GraphAutoma.add_dependency","title":"add_dependency","text":"<pre><code>add_dependency(key: str, dependency: str) -&gt; None\n</code></pre> <p>This method is used to dynamically add a dependency from <code>key</code> to <code>dependency</code>.</p> <p>Note: args_mapping_rule and result_dispatching_rule is not allowed to be set by this method,  instead they should be set together with add_worker() or add_func_as_worker() when adding the worker.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>The key of the worker that will depend on the worker with key <code>dependency</code>.</p> required <code>dependency</code> <code>str</code> <p>The key of the worker on which the worker with key <code>key</code> will depend.</p> required Source code in <code>bridgic/core/automa/_graph_automa.py</code> <pre><code>def add_dependency(\n    self,\n    key: str,\n    dependency: str,\n) -&gt; None:\n    \"\"\"\n    This method is used to dynamically add a dependency from `key` to `dependency`.\n\n    Note: args_mapping_rule and result_dispatching_rule is not allowed to be set by this method, \n    instead they should be set together with add_worker() or add_func_as_worker() when adding the worker.\n\n    Parameters\n    ----------\n    key : str\n        The key of the worker that will depend on the worker with key `dependency`.\n    dependency : str\n        The key of the worker on which the worker with key `key` will depend.\n    \"\"\"\n    ...\n    if not self._automa_running:\n        # add the dependency immediately\n        self._add_dependency_incrementally(key, dependency)\n    else:\n        deferred_task = _AddDependencyDeferredTask(\n            worker_key=key,\n            dependency=dependency,\n        )\n        # Note: the execution order of topology change deferred tasks is important and is determined by the order of the calls of add_worker(), remove_worker() and add_dependency() in one DS.\n        self._topology_change_deferred_tasks.append(deferred_task)\n</code></pre>"},{"location":"reference/bridgic-core/bridgic/core/automa/#bridgic.core.automa.GraphAutoma.ferry_to","title":"ferry_to","text":"<pre><code>ferry_to(key: str, /, *args, **kwargs)\n</code></pre> <p>Defer the invocation to the specified worker, passing any provided arguments. This creates a  delayed call, ensuring the worker will be scheduled to run asynchronously in the next event loop,  independent of its dependencies.</p> <p>This primitive is commonly used for:</p> <ol> <li>Implementing dynamic branching based on runtime conditions.</li> <li>Creating logic that forms cyclic graphs.</li> </ol> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>The key of the worker to run.</p> required <code>args</code> <code>optional</code> <p>Positional arguments to be passed.</p> <code>()</code> <code>kwargs</code> <code>optional</code> <p>Keyword arguments to be passed.</p> <code>{}</code> <p>Examples:</p> <pre><code>class MyGraphAutoma(GraphAutoma):\n    @worker(is_start=True)\n    def start_worker(self):\n        number = random.randint(0, 1)\n        if number == 0:\n            self.ferry_to(\"cond_1_worker\", number=number)\n        else:\n            self.ferry_to(\"cond_2_worker\")\n\n    @worker()\n    def cond_1_worker(self, number: int):\n        print(f'Got {{number}}!')\n\n    @worker()\n    def cond_2_worker(self):\n        self.ferry_to(\"start_worker\")\n\nautoma = MyGraphAutoma()\nawait automa.arun()\n\n# Output: Got 0!\n</code></pre> Source code in <code>bridgic/core/automa/_graph_automa.py</code> <pre><code>def ferry_to(self, key: str, /, *args, **kwargs):\n    \"\"\"\n    Defer the invocation to the specified worker, passing any provided arguments. This creates a \n    delayed call, ensuring the worker will be scheduled to run asynchronously in the next event loop, \n    independent of its dependencies.\n\n    This primitive is commonly used for:\n\n    1. Implementing dynamic branching based on runtime conditions.\n    2. Creating logic that forms cyclic graphs.\n\n    Parameters\n    ----------\n    key : str\n        The key of the worker to run.\n    args : optional\n        Positional arguments to be passed.\n    kwargs : optional\n        Keyword arguments to be passed.\n\n    Examples\n    --------\n    ```python\n    class MyGraphAutoma(GraphAutoma):\n        @worker(is_start=True)\n        def start_worker(self):\n            number = random.randint(0, 1)\n            if number == 0:\n                self.ferry_to(\"cond_1_worker\", number=number)\n            else:\n                self.ferry_to(\"cond_2_worker\")\n\n        @worker()\n        def cond_1_worker(self, number: int):\n            print(f'Got {{number}}!')\n\n        @worker()\n        def cond_2_worker(self):\n            self.ferry_to(\"start_worker\")\n\n    automa = MyGraphAutoma()\n    await automa.arun()\n\n    # Output: Got 0!\n    ```\n    \"\"\"\n    # TODO: check worker_key is valid, maybe deferred check...\n    running_options = self._get_top_running_options()\n    # if debug is enabled, trace back the kickoff worker key from stacktrace.\n    kickoff_worker_key: str = self._trace_back_kickoff_worker_key_from_stack() if running_options.debug else None\n    deferred_task = _FerryDeferredTask(\n        ferry_to_worker_key=key,\n        kickoff_worker_key=kickoff_worker_key,\n        args=args,\n        kwargs=kwargs,\n    )\n    # Note: ferry_to() may be called in a new thread.\n    # But _ferry_deferred_tasks is not necessary to be thread-safe due to Visibility Guarantees of the Bridgic Concurrency Model.\n    self._ferry_deferred_tasks.append(deferred_task)\n</code></pre>"},{"location":"reference/bridgic-core/bridgic/core/automa/#bridgic.core.automa.GraphAutoma.arun","title":"arun","text":"<code>async</code> <pre><code>arun(\n    *args: Tuple[Any, ...],\n    feedback_data: Optional[\n        Union[\n            InteractionFeedback, List[InteractionFeedback]\n        ]\n    ] = None,\n    **kwargs: Dict[str, Any]\n) -&gt; Any\n</code></pre> <p>The entry point for running the constructed <code>GraphAutoma</code> instance.</p> <p>This method serves as the entry point for both initial execution and resumption after  interruption of an automa instance. It automatically drives the execution of workers  based on their <code>dependencies</code> and explicit <code>ferry_to()</code> calls. Each execution will be  wrapped in an <code>asyncio.Task</code> to ensure context isolation.</p> <p>Automatic Scheduling</p> <p>The scheduling behavior in <code>GraphAutoma</code> is automatically driven by:</p> <ul> <li> <p>Worker dependencies: Workers are scheduled to run only after all their necessary    dependencies are satisfied. The dependencies automatically drive the execution order.</p> </li> <li> <p>Calling ferry_to: During execution, a worker can explicitly trigger another worker    by calling <code>ferry_to()</code>, which enables dynamic flow control and conditional branching.</p> </li> <li> <p>Dynamic topology changes: When the graph topology is modified at runtime (such as    adding or removing workers or dependencies), the scheduling system seamlessly updates    to reflect the latest structure, ensuring that worker execution always follows the    current graph.</p> </li> </ul> <p>Human Interaction Mechanism</p> <p>Workers can request human input by calling <code>interact_with_human()</code> during execution.  When this occurs:</p> <ul> <li>The execution will be paused after the running workers finish their execution.</li> <li>The Automa's state will be serialized into a <code>Snapshot</code> object.</li> <li>An <code>InteractionException</code> will be raised to the application layer. It contains both the    list of pending <code>Interaction</code> objects and the <code>Snapshot</code> object.</li> <li>The application layer may persist the <code>Snapshot</code> properly to resume the execution later.</li> <li>To resume execution, the application layer should reload the Automa state using    <code>load_from_snapshot()</code> with the saved <code>Snapshot</code> object and call <code>arun()</code> again with    <code>feedback_data</code> containing the user's feedback(s) to finish a complete interaction.</li> </ul> <p>Parameters:</p> Name Type Description Default <code>args</code> <code>optional</code> <p>Positional arguments to be passed.</p> <code>()</code> <code>feedback_data</code> <code>Optional[Union[InteractionFeedback, List[InteractionFeedback]]]</code> <p>Feedbacks that are received from one or multiple human interactions occurred before the Automa was paused. This argument may be of type <code>InteractionFeedback</code> or  <code>List[InteractionFeedback]</code>. If only one interaction occurred, <code>feedback_data</code> should be of type <code>InteractionFeedback</code>. If multiple interactions occurred simultaneously,  <code>feedback_data</code> should be of type <code>List[InteractionFeedback]</code>.</p> <code>None</code> <code>kwargs</code> <code>optional</code> <p>Keyword arguments which may be further propagated to contained workers.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Any</code> <p>The execution result of the output-worker that has the setting <code>is_output=True</code>, otherwise None.</p> <p>Raises:</p> Type Description <code>InteractionException</code> <p>If the Automa is the top-level Automa and the <code>interact_with_human()</code> method is called by one or more workers within the lastest event loop iteration, this exception will be raised to the application layer.</p> Source code in <code>bridgic/core/automa/_graph_automa.py</code> <pre><code>async def arun(\n    self,\n    *args: Tuple[Any, ...],\n    feedback_data: Optional[Union[InteractionFeedback, List[InteractionFeedback]]] = None,\n    **kwargs: Dict[str, Any]\n) -&gt; Any:\n    \"\"\"\n    The entry point for running the constructed `GraphAutoma` instance.\n\n    This method serves as the entry point for both initial execution and resumption after \n    interruption of an automa instance. It automatically drives the execution of workers \n    based on their `dependencies` and explicit `ferry_to()` calls. Each execution will be \n    wrapped in an `asyncio.Task` to ensure context isolation.\n\n    **Automatic Scheduling**\n\n    The scheduling behavior in `GraphAutoma` is automatically driven by:\n\n    - Worker dependencies: Workers are scheduled to run only after all their necessary \n      dependencies are satisfied. The dependencies automatically drive the execution order.\n\n    - Calling ferry_to: During execution, a worker can explicitly trigger another worker \n      by calling `ferry_to()`, which enables dynamic flow control and conditional branching.\n\n    - Dynamic topology changes: When the graph topology is modified at runtime (such as \n      adding or removing workers or dependencies), the scheduling system seamlessly updates \n      to reflect the latest structure, ensuring that worker execution always follows the \n      current graph.\n\n    **Human Interaction Mechanism**\n\n    Workers can request human input by calling `interact_with_human()` during execution. \n    When this occurs:\n\n    - The execution will be paused after the running workers finish their execution.\n    - The Automa's state will be serialized into a `Snapshot` object.\n    - An `InteractionException` will be raised to the application layer. It contains both the \n      list of pending `Interaction` objects and the `Snapshot` object.\n    - The application layer may persist the `Snapshot` properly to resume the execution later.\n    - To resume execution, the application layer should reload the Automa state using \n      `load_from_snapshot()` with the saved `Snapshot` object and call `arun()` again with \n      `feedback_data` containing the user's feedback(s) to finish a complete interaction.\n\n    Parameters\n    ----------\n    args : optional\n        Positional arguments to be passed.\n    feedback_data : Optional[Union[InteractionFeedback, List[InteractionFeedback]]]\n        Feedbacks that are received from one or multiple human interactions occurred before the\n        Automa was paused. This argument may be of type `InteractionFeedback` or \n        `List[InteractionFeedback]`. If only one interaction occurred, `feedback_data` should be\n        of type `InteractionFeedback`. If multiple interactions occurred simultaneously, \n        `feedback_data` should be of type `List[InteractionFeedback]`.\n    kwargs : optional\n        Keyword arguments which may be further propagated to contained workers.\n\n    Returns\n    -------\n    Any\n        The execution result of the output-worker that has the setting `is_output=True`,\n        otherwise None.\n\n    Raises\n    ------\n    InteractionException\n        If the Automa is the top-level Automa and the `interact_with_human()` method is called\n        by one or more workers within the lastest event loop iteration, this exception will be\n        raised to the application layer.\n    \"\"\"\n    if self.is_top_level():\n        # For top-level automa, wrap in a task to ensure context isolation\n        task = asyncio.create_task(\n            self._arun_internal(*args, feedback_data=feedback_data, **kwargs),\n            name=f\"GraphAutoma-{self.name}-arun\"\n        )\n        return await task\n    else:\n        # For nested automa, directly call _arun_internal to avoid redundant task creation\n        return await self._arun_internal(*args, feedback_data=feedback_data, **kwargs)\n</code></pre>"},{"location":"reference/bridgic-core/bridgic/core/automa/#bridgic.core.automa.WorkerSignatureError","title":"WorkerSignatureError","text":"<p>               Bases: <code>Exception</code></p> <p>Raised when invalid signature is detected in the case of defining a worker.</p> Source code in <code>bridgic/core/types/_error.py</code> <pre><code>class WorkerSignatureError(Exception):\n    \"\"\"\n    Raised when invalid signature is detected in the case of defining a worker.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/bridgic-core/bridgic/core/automa/#bridgic.core.automa.WorkerArgsMappingError","title":"WorkerArgsMappingError","text":"<p>               Bases: <code>Exception</code></p> <p>Raised when the parameters declaration of a worker does not meet the requirements of the arguments mapping rule.</p> Source code in <code>bridgic/core/types/_error.py</code> <pre><code>class WorkerArgsMappingError(Exception):\n    \"\"\"\n    Raised when the parameters declaration of a worker does not meet the requirements of the arguments mapping rule.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/bridgic-core/bridgic/core/automa/#bridgic.core.automa.WorkerArgsInjectionError","title":"WorkerArgsInjectionError","text":"<p>               Bases: <code>Exception</code></p> <p>Raised when the arguments injection mechanism encountered an error during operation.</p> Source code in <code>bridgic/core/types/_error.py</code> <pre><code>class WorkerArgsInjectionError(Exception):\n    \"\"\"\n    Raised when the arguments injection mechanism encountered an error during operation.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/bridgic-core/bridgic/core/automa/#bridgic.core.automa.WorkerRuntimeError","title":"WorkerRuntimeError","text":"<p>               Bases: <code>RuntimeError</code></p> <p>Raised when the worker encounters an unexpected error during runtime.</p> Source code in <code>bridgic/core/types/_error.py</code> <pre><code>class WorkerRuntimeError(RuntimeError):\n    \"\"\"\n    Raised when the worker encounters an unexpected error during runtime.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/bridgic-core/bridgic/core/automa/#bridgic.core.automa.AutomaDeclarationError","title":"AutomaDeclarationError","text":"<p>               Bases: <code>Exception</code></p> <p>Raised when the declaration of workers within an Automa is not valid.</p> Source code in <code>bridgic/core/types/_error.py</code> <pre><code>class AutomaDeclarationError(Exception):\n    \"\"\"\n    Raised when the declaration of workers within an Automa is not valid.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/bridgic-core/bridgic/core/automa/#bridgic.core.automa.AutomaCompilationError","title":"AutomaCompilationError","text":"<p>               Bases: <code>Exception</code></p> <p>Raised when the compilation or validation of an Automa fails.</p> Source code in <code>bridgic/core/types/_error.py</code> <pre><code>class AutomaCompilationError(Exception):\n    \"\"\"\n    Raised when the compilation or validation of an Automa fails.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/bridgic-core/bridgic/core/automa/#bridgic.core.automa.AutomaRuntimeError","title":"AutomaRuntimeError","text":"<p>               Bases: <code>RuntimeError</code></p> <p>Raised when the execution of an Automa encounters an unexpected error.</p> Source code in <code>bridgic/core/types/_error.py</code> <pre><code>class AutomaRuntimeError(RuntimeError):\n    \"\"\"\n    Raised when the execution of an Automa encounters an unexpected error.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/bridgic-core/bridgic/core/automa/#bridgic.core.automa.worker._worker_decorator.worker","title":"worker","text":"<pre><code>worker(**kwargs) -&gt; Callable\n</code></pre> <p>Decorator for marking a method inside an Automa class as a worker.</p> <p>To cover the need to declare workers in various Automa classes, this decorator actually  accepts a variable kwargs parameter. Through overloading, it further supports specifying  the parameters that need to be passed in when registering a worker under a specific Automa,  such as <code>ConcurrentAutoma</code>, <code>SequentialAutoma</code> and so on.</p> <p>Parameters:</p> Name Type Description Default <code>kwargs</code> <code>Dict[str, Any]</code> <p>The keyword arguments for the worker decorator.</p> <code>{}</code> Source code in <code>bridgic/core/automa/worker/_worker_decorator.py</code> <pre><code>def worker(**kwargs) -&gt; Callable:\n    \"\"\"\n    Decorator for marking a method inside an Automa class as a worker.\n\n    To cover the need to declare workers in various Automa classes, this decorator actually \n    accepts a variable kwargs parameter. Through overloading, it further supports specifying \n    the parameters that need to be passed in when registering a worker under a specific Automa, \n    such as `ConcurrentAutoma`, `SequentialAutoma` and so on.\n\n    Parameters\n    ----------\n    kwargs : Dict[str, Any]\n        The keyword arguments for the worker decorator.\n    \"\"\"\n    def wrapper(func: Callable):\n        setattr(func, \"__worker_kwargs__\", kwargs)\n        return func\n    return wrapper\n</code></pre>"},{"location":"reference/bridgic-core/bridgic/core/automa/args/","title":"args","text":"<p>The Args module provides Arguments Mapping and Arguments Injection mechanisms in Bridgic.</p>"},{"location":"reference/bridgic-core/bridgic/core/automa/args/#bridgic.core.automa.args.ArgsMappingRule","title":"ArgsMappingRule","text":"<p>               Bases: <code>Enum</code></p> <p>Enumeration of Arguments Mapping rules for worker parameter passing.</p> <p>ArgsMappingRule defines how the return values from predecessor workers are mapped  to the parameters of the current worker. This controls the data flow between workers  in an automa execution graph.</p> <p>Attributes:</p> Name Type Description <code>AS_IS</code> <code>Enum(default)</code> <p>Map the results of the previous workers to the corresponding parameters  in the order of dependency.</p> <code>MERGE</code> <code>Enum</code> <p>Merges all results from previous workers into a single tuple as the  only argument of the current worker.</p> <code>UNPACK</code> <code>Enum</code> <p>Unpacks the result from the previous worker and passes as individual  arguments. Only valid when the current worker has exactly one dependency and  the return value is a list/tuple or dict.</p> <code>SUPPRESSED</code> <code>Enum</code> <p>Suppresses all results from previous workers. No arguments are passed  to the current worker from its dependencies.</p> <p>Examples:</p> <pre><code>class MyAutoma(GraphAutoma):\n    @worker(is_start=True)\n    def worker_0(self, user_input: int) -&gt; int:\n        return user_input + 1\n\n    @worker(dependencies=[\"worker_0\"], args_mapping_rule=ArgsMappingRule.AS_IS)\n    def worker_1(self, worker_0_output: int) -&gt; int:\n        # Receives the exact return value from worker_0\n        return worker_0_output + 1\n\n    @worker(dependencies=[\"worker_0\"], args_mapping_rule=ArgsMappingRule.UNPACK)\n    def worker_2(self, user_input: int, result: int) -&gt; int:\n        # Unpacks the return value from worker_0 (assuming it returns a tuple)\n        return user_input + result\n\n    @worker(dependencies=[\"worker_0\", \"worker_1\"], args_mapping_rule=ArgsMappingRule.MERGE)\n    def worker_3(self, all_results: tuple) -&gt; int:\n        # Receives all results as a single tuple\n        return sum(all_results)\n\n    @worker(dependencies=[\"worker_3\"], args_mapping_rule=ArgsMappingRule.SUPPRESSED)\n    def worker_4(self, custom_input: int = 10) -&gt; int:\n        # Ignores return value from worker_3, uses custom input\n        return custom_input + 1\n</code></pre> Note <ol> <li>AS_IS is the default mapping rule when not specified</li> <li>UNPACK requires exactly one dependency and a list/tuple/dict return value</li> <li>MERGE combines all predecessor outputs into a single tuple argument</li> <li>SUPPRESSED allows workers to ignore dependency outputs completely</li> </ol> Source code in <code>bridgic/core/types/_common.py</code> <pre><code>class ArgsMappingRule(Enum):\n    \"\"\"\n    Enumeration of Arguments Mapping rules for worker parameter passing.\n\n    ArgsMappingRule defines how the return values from predecessor workers are mapped \n    to the parameters of the current worker. This controls the data flow between workers \n    in an automa execution graph.\n\n    Attributes\n    ----------\n    AS_IS: Enum (default)\n        Map the results of the previous workers to the corresponding parameters \n        in the order of dependency.\n    MERGE: Enum\n        Merges all results from previous workers into a single tuple as the \n        only argument of the current worker.\n    UNPACK: Enum\n        Unpacks the result from the previous worker and passes as individual \n        arguments. Only valid when the current worker has exactly one dependency and \n        the return value is a list/tuple or dict.\n    SUPPRESSED: Enum\n        Suppresses all results from previous workers. No arguments are passed \n        to the current worker from its dependencies.\n\n    Examples\n    --------\n    ```python\n    class MyAutoma(GraphAutoma):\n        @worker(is_start=True)\n        def worker_0(self, user_input: int) -&gt; int:\n            return user_input + 1\n\n        @worker(dependencies=[\"worker_0\"], args_mapping_rule=ArgsMappingRule.AS_IS)\n        def worker_1(self, worker_0_output: int) -&gt; int:\n            # Receives the exact return value from worker_0\n            return worker_0_output + 1\n\n        @worker(dependencies=[\"worker_0\"], args_mapping_rule=ArgsMappingRule.UNPACK)\n        def worker_2(self, user_input: int, result: int) -&gt; int:\n            # Unpacks the return value from worker_0 (assuming it returns a tuple)\n            return user_input + result\n\n        @worker(dependencies=[\"worker_0\", \"worker_1\"], args_mapping_rule=ArgsMappingRule.MERGE)\n        def worker_3(self, all_results: tuple) -&gt; int:\n            # Receives all results as a single tuple\n            return sum(all_results)\n\n        @worker(dependencies=[\"worker_3\"], args_mapping_rule=ArgsMappingRule.SUPPRESSED)\n        def worker_4(self, custom_input: int = 10) -&gt; int:\n            # Ignores return value from worker_3, uses custom input\n            return custom_input + 1\n    ```\n\n    Note\n    ----\n    1. AS_IS is the default mapping rule when not specified\n    2. UNPACK requires exactly one dependency and a list/tuple/dict return value\n    3. MERGE combines all predecessor outputs into a single tuple argument\n    4. SUPPRESSED allows workers to ignore dependency outputs completely\n    \"\"\"\n    AS_IS = \"as_is\"\n    MERGE = \"merge\"\n    UNPACK = \"unpack\"\n    SUPPRESSED = \"suppressed\"\n</code></pre>"},{"location":"reference/bridgic-core/bridgic/core/automa/args/#bridgic.core.automa.args.ResultDispatchingRule","title":"ResultDispatchingRule","text":"<p>               Bases: <code>Enum</code></p> <p>Enumeration of Result Dispatch rules for worker result passing.</p> <p>ResultDispatchingRule defines how the result from the current worker is dispatched to the next workers. This controls the data flow between workers in an automa execution graph.</p> <p>Attributes:</p> Name Type Description <code>AS_IS</code> <code>Enum(default)</code> <p>Gathers all results of current worker into a single tuple as the  only result to the next workers.</p> <code>IN_ORDER</code> <code>Enum</code> <p>Dispatch the current worker's results to the corresponding downstream  workers one by one according to the order they are declared or added.</p> Source code in <code>bridgic/core/types/_common.py</code> <pre><code>class ResultDispatchingRule(Enum):\n    \"\"\"\n    Enumeration of Result Dispatch rules for worker result passing.\n\n    ResultDispatchingRule defines how the result from the current worker is dispatched to the next workers.\n    This controls the data flow between workers in an automa execution graph.\n\n    Attributes\n    ----------\n    AS_IS: Enum (default)\n        Gathers all results of current worker into a single tuple as the \n        only result to the next workers.\n    IN_ORDER: Enum\n        Dispatch the current worker's results to the corresponding downstream \n        workers one by one according to the order they are declared or added.\n    \"\"\"\n    AS_IS = \"as_is\"\n    IN_ORDER = \"in_order\"\n</code></pre>"},{"location":"reference/bridgic-core/bridgic/core/automa/args/#bridgic.core.automa.args.From","title":"From  <code>dataclass</code>","text":"<p>               Bases: <code>ArgsDescriptor</code></p> <p>Implementing arguments injection for worker parameters with default value.</p> <p>When a worker needs the output of another worker but does not directly depend on  it in execution, you can use <code>From</code> to declare an arguments injection in  its parameters.</p> <p>Attributes:</p> Name Type Description <code>key</code> <code>str</code> <p>The key of the worker to inject arguments from.</p> <code>default</code> <code>Optional[Any]</code> <p>The default value of the arguments.</p> <p>Examples:</p> <pre><code>class MyAutoma(GraphAutoma):\n    @worker(is_start=True)\n    def worker_0(self, user_input: int) -&gt; int:\n        return user_input + 1\n\n    @worker(dependencies=[\"worker_0\"])\n    def worker_1(self, worker_0_output: int) -&gt; int:\n        return worker_0_output + 1\n\n    @worker(dependencies=[\"worker_1\"], is_output=True)\n    def worker_2(self, worker_1_output: int, worker_0_output: int = From(\"worker_0\", 1)) -&gt; int:\n        # needs the output of worker_0 but does not directly depend on it in execution\n        print(f'worker_0_output: {worker_0_output}')\n        return worker_1_output + 1\n</code></pre> <p>Returns:</p> Type Description <code>Any</code> <p>The output of the worker specified by the key.</p> <p>Raises:</p> Type Description <code>WorkerArgsInjectionError</code> <p>If the worker specified by the key does not exist and no default value is set.</p> Note: <ol> <li>Can set a default value for a <code>From</code> declaration, which will be returned when the specified worker does not exist.</li> <li>Will raise <code>WorkerArgsInjectionError</code> if the worker specified by the key does not exist and no default value is set.</li> </ol> Source code in <code>bridgic/core/automa/args/_args_descriptor.py</code> <pre><code>@dataclass\nclass From(ArgsDescriptor):\n    \"\"\"\n    Implementing arguments injection for worker parameters with default value.\n\n    When a worker needs the output of another worker but does not directly depend on \n    it in execution, you can use `From` to declare an arguments injection in \n    its parameters.\n\n    Attributes\n    ----------\n    key : str\n        The key of the worker to inject arguments from.\n    default : Optional[Any]\n        The default value of the arguments.\n\n    Examples\n    --------\n    ```python\n    class MyAutoma(GraphAutoma):\n        @worker(is_start=True)\n        def worker_0(self, user_input: int) -&gt; int:\n            return user_input + 1\n\n        @worker(dependencies=[\"worker_0\"])\n        def worker_1(self, worker_0_output: int) -&gt; int:\n            return worker_0_output + 1\n\n        @worker(dependencies=[\"worker_1\"], is_output=True)\n        def worker_2(self, worker_1_output: int, worker_0_output: int = From(\"worker_0\", 1)) -&gt; int:\n            # needs the output of worker_0 but does not directly depend on it in execution\n            print(f'worker_0_output: {worker_0_output}')\n            return worker_1_output + 1\n    ```\n\n    Returns\n    -------\n    Any\n        The output of the worker specified by the key.\n\n    Raises\n    ------\n    WorkerArgsInjectionError\n        If the worker specified by the key does not exist and no default value is set.\n\n    Note:\n    ------\n    1. Can set a default value for a `From` declaration, which will be returned when the specified worker does not exist.\n    2. Will raise `WorkerArgsInjectionError` if the worker specified by the key does not exist and no default value is set.\n    \"\"\"\n    key: str\n    default: Optional[Any] = InjectorNone()\n</code></pre>"},{"location":"reference/bridgic-core/bridgic/core/automa/args/#bridgic.core.automa.args.System","title":"System  <code>dataclass</code>","text":"<p>               Bases: <code>ArgsDescriptor</code></p> <p>Implementing system-level arguments injection for worker parameters.</p> <p>System provides access to automa-level resources and context through arguments  injection. It supports pattern matching for different types of system resources.</p> <p>Attributes:</p> Name Type Description <code>key</code> <code>str</code> <p>The system resource key to inject. Supported keys: - \"runtime_context\": Runtime context for data persistence across worker executions. - \"automa\": Current automa instance. - \"automa:worker_key\": Sub-automa instance in current automa.</p> <p>Examples:</p> <pre><code>def worker_1(x: int, current_automa = System(\"automa\")) -&gt; int:\n    # Access current automa instance\n    current_automa.add_worker(\n        key=\"sub_automa\",\n        worker=SubAutoma(),\n        dependencies=[\"worker_1\"]\n    )\n    return x + 1\n\nclass SubAutoma(GraphAutoma):\n    @worker(is_start=True)\n    def worker_0(self, user_input: int) -&gt; int:\n        return user_input + 1\n\nclass MyAutoma(GraphAutoma):\n    @worker(is_start=True)\n    def worker_0(self, user_input: int, rtx = System(\"runtime_context\")) -&gt; int:\n        # Access runtime context for data persistence\n        local_space = self.get_local_space(rtx)\n        count = local_space.get(\"count\", 0)\n        local_space[\"count\"] = count + 1\n\n        self.add_func_as_worker(\n            key=\"worker_1\",\n            func=worker_1,\n            dependencies=[\"worker_0\"]\n        )\n\n        return user_input + count\n\n    @worker(dependencies=[\"worker_1\"])\n    def worker_2(self, worker_1_output: int, sub_automa = System(\"automa:sub_automa\")) -&gt; int:\n        # Access sub-automa from worker_1\n        sub_automa.add_worker(\n            key=\"worker_3\",\n            worker=SubAutoma(),\n            dependencies=[\"worker_2\"],\n            is_output=True,\n        )\n        return worker_1_output + 1\n</code></pre> <p>Returns:</p> Type Description <code>Any</code> <p>The system resource specified by the key: - RuntimeContext: For \"runtime_context\" - AutomaInstance: For current automa instance or a sub-automa instance from the current automa.</p> <p>Raises:</p> Type Description <code>WorkerArgsInjectionError</code> <ul> <li>If the key pattern is not supported.</li> <li>If the specified resource does not exist.</li> <li>If the specified resource is not an Automa.</li> </ul> Note <ol> <li>\"runtime_context\" provides a <code>RuntimeContext</code> instance for data persistence</li> <li>\"automa\" provides access to the current automa instance</li> <li>\"automa:worker_key\" provides access to a sub-automa from the specified worker key</li> </ol> Source code in <code>bridgic/core/automa/args/_args_descriptor.py</code> <pre><code>@dataclass\nclass System(ArgsDescriptor):\n    \"\"\"\n    Implementing system-level arguments injection for worker parameters.\n\n    System provides access to automa-level resources and context through arguments \n    injection. It supports pattern matching for different types of system resources.\n\n    Attributes\n    ----------\n    key : str\n        The system resource key to inject. Supported keys:\n        - \"runtime_context\": Runtime context for data persistence across worker executions.\n        - \"automa\": Current automa instance.\n        - \"automa:worker_key\": Sub-automa instance in current automa.\n\n    Examples\n    --------\n    ```python\n    def worker_1(x: int, current_automa = System(\"automa\")) -&gt; int:\n        # Access current automa instance\n        current_automa.add_worker(\n            key=\"sub_automa\",\n            worker=SubAutoma(),\n            dependencies=[\"worker_1\"]\n        )\n        return x + 1\n\n    class SubAutoma(GraphAutoma):\n        @worker(is_start=True)\n        def worker_0(self, user_input: int) -&gt; int:\n            return user_input + 1\n\n    class MyAutoma(GraphAutoma):\n        @worker(is_start=True)\n        def worker_0(self, user_input: int, rtx = System(\"runtime_context\")) -&gt; int:\n            # Access runtime context for data persistence\n            local_space = self.get_local_space(rtx)\n            count = local_space.get(\"count\", 0)\n            local_space[\"count\"] = count + 1\n\n            self.add_func_as_worker(\n                key=\"worker_1\",\n                func=worker_1,\n                dependencies=[\"worker_0\"]\n            )\n\n            return user_input + count\n\n        @worker(dependencies=[\"worker_1\"])\n        def worker_2(self, worker_1_output: int, sub_automa = System(\"automa:sub_automa\")) -&gt; int:\n            # Access sub-automa from worker_1\n            sub_automa.add_worker(\n                key=\"worker_3\",\n                worker=SubAutoma(),\n                dependencies=[\"worker_2\"],\n                is_output=True,\n            )\n            return worker_1_output + 1\n    ```\n\n    Returns\n    -------\n    Any\n        The system resource specified by the key:\n        - RuntimeContext: For \"runtime_context\"\n        - AutomaInstance: For current automa instance or a sub-automa instance from the current automa.\n\n    Raises\n    ------\n    WorkerArgsInjectionError\n        - If the key pattern is not supported.\n        - If the specified resource does not exist.\n        - If the specified resource is not an Automa.\n\n    Note\n    ----\n    1. \"runtime_context\" provides a `RuntimeContext` instance for data persistence\n    2. \"automa\" provides access to the current automa instance\n    3. \"automa:worker_key\" provides access to a sub-automa from the specified worker key\n    \"\"\"\n    key: str\n\n    def __post_init__(self):\n        allowed_patterns = [\n            r\"^runtime_context$\",\n            r\"^automa:.*$\",\n            r\"^automa$\",\n        ]\n\n        if not any(re.match(pattern, self.key) for pattern in allowed_patterns):\n            raise WorkerArgsInjectionError(\n                f\"Key '{self.key}' is not supported. Supported keys: \\n\"\n                f\"- `runtime_context`: a context for data persistence of the current worker.\\n\"\n                f\"- `automa:&lt;worker_key&gt;`: a sub-automa in current automa.\\n\"\n                f\"- `automa`: the current automa instance.\\n\"\n            )\n</code></pre>"},{"location":"reference/bridgic-core/bridgic/core/automa/args/#bridgic.core.automa.args.InOrder","title":"InOrder  <code>dataclass</code>","text":"<p>A descriptor to indicate that data should be distributed to multiple workers. </p> <p>When is used to input arguments or worker with this descriptor, the data will be distributed to downstream workers instead of being gathered as a single value. Split the returned  Sequence object and dispatching them in-order and element-wise to the downstream workers  as their actual input.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Union[List, Tuple]</code> <p>The data to be distributed. Must be a list or tuple with length matching the number of workers that will receive it.</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>If the data is not a list or tuple.</p> Source code in <code>bridgic/core/automa/args/_args_binding.py</code> <pre><code>@dataclass\nclass InOrder:\n    \"\"\"\n    A descriptor to indicate that data should be distributed to multiple workers. \n\n    When is used to input arguments or worker with this descriptor, the data will be distributed\n    to downstream workers instead of being gathered as a single value. Split the returned \n    Sequence object and dispatching them in-order and element-wise to the downstream workers \n    as their actual input.\n\n    Parameters\n    ----------\n    data : Union[List, Tuple]\n        The data to be distributed. Must be a list or tuple with length matching\n        the number of workers that will receive it.\n\n    Raises\n    ------\n    ValueError\n        If the data is not a list or tuple.\n    \"\"\"\n    data: Union[List, Tuple]\n\n    def __post_init__(self):\n        if not isinstance(self.data, (list, tuple)):\n            raise ValueError(f\"The data must be a list or tuple, but got {type(self.data)}\")\n</code></pre>"},{"location":"reference/bridgic-core/bridgic/core/automa/interaction/","title":"interaction","text":"<p>The Interaction module provides mechanisms for handling human-machine interactions in Automa.</p> <p>This module contains several important interface definitions for implementing event handling,  feedback collection, and interaction control during Automa execution.</p> <p>There are two fundamental mechanisms for human-machine interactions in Automa:</p> <ul> <li>Feedback Request Mechanism: For simple interaction scenarios during Automa execution.<ul> <li><code>request_feedback_async()</code>, <code>request_feedback()</code></li> </ul> </li> <li>Human Interaction Mechanism: For long-running interaction scenarios that require interruption and resumption during Automa execution.<ul> <li><code>interact_with_human()</code>, <code>load_from_snapshot()</code></li> </ul> </li> </ul>"},{"location":"reference/bridgic-core/bridgic/core/automa/interaction/#bridgic.core.automa.interaction.EventHandlerType","title":"EventHandlerType  <code>module-attribute</code>","text":"<pre><code>EventHandlerType: TypeAlias = Union[\n    Callable[[Event, FeedbackSender], None],\n    Callable[[Event], None],\n]\n</code></pre> <p>The type of the event handler. It can be a function that takes an Event and a FeedbackSender as arguments, or a function that takes only an Event as an argument.</p>"},{"location":"reference/bridgic-core/bridgic/core/automa/interaction/#bridgic.core.automa.interaction.Event","title":"Event","text":"<p>               Bases: <code>BaseModel</code></p> <p>An event is a message that is sent from one worker inside the Automa to the application layer outside the Automa.</p> Source code in <code>bridgic/core/automa/interaction/_event_handling.py</code> <pre><code>class Event(BaseModel):\n    \"\"\"\n    An event is a message that is sent from one worker inside the Automa to the application layer outside the Automa.\n    \"\"\"\n    event_type: Optional[str] = None\n    \"\"\"The type of the event. The type of the event is used to identify the event handler registered to handle the event.\"\"\"\n    timestamp: datetime = datetime.now()\n    \"\"\"The timestamp of the event.\"\"\"\n    data: Optional[Any] = None\n    \"\"\"The data attached to the event.\"\"\"\n</code></pre>"},{"location":"reference/bridgic-core/bridgic/core/automa/interaction/#bridgic.core.automa.interaction.Event.event_type","title":"event_type  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>event_type: Optional[str] = None\n</code></pre> <p>The type of the event. The type of the event is used to identify the event handler registered to handle the event.</p>"},{"location":"reference/bridgic-core/bridgic/core/automa/interaction/#bridgic.core.automa.interaction.Event.timestamp","title":"timestamp  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>timestamp: datetime = now()\n</code></pre> <p>The timestamp of the event.</p>"},{"location":"reference/bridgic-core/bridgic/core/automa/interaction/#bridgic.core.automa.interaction.Event.data","title":"data  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>data: Optional[Any] = None\n</code></pre> <p>The data attached to the event.</p>"},{"location":"reference/bridgic-core/bridgic/core/automa/interaction/#bridgic.core.automa.interaction.Feedback","title":"Feedback","text":"<p>               Bases: <code>BaseModel</code></p> <p>A feedback is a message that is sent from the application layer outside the Automa to a worker inside the Automa.</p> Source code in <code>bridgic/core/automa/interaction/_event_handling.py</code> <pre><code>class Feedback(BaseModel):\n    \"\"\"\n    A feedback is a message that is sent from the application layer outside the Automa to a worker inside the Automa.\n    \"\"\"\n    data: Any\n    \"\"\"The data attached to the feedback.\"\"\"\n</code></pre>"},{"location":"reference/bridgic-core/bridgic/core/automa/interaction/#bridgic.core.automa.interaction.Feedback.data","title":"data  <code>instance-attribute</code>","text":"<pre><code>data: Any\n</code></pre> <p>The data attached to the feedback.</p>"},{"location":"reference/bridgic-core/bridgic/core/automa/interaction/#bridgic.core.automa.interaction.FeedbackSender","title":"FeedbackSender","text":"<p>               Bases: <code>ABC</code></p> <p>The appliction layer must use <code>FeedbackSender</code> to send back feedback to the worker inside the Automa.</p> Source code in <code>bridgic/core/automa/interaction/_event_handling.py</code> <pre><code>class FeedbackSender(ABC):\n    \"\"\"\n    The appliction layer must use `FeedbackSender` to send back feedback to the worker inside the Automa.\n    \"\"\"\n    @abstractmethod\n    def send(self, feedback: Feedback) -&gt; None:\n        \"\"\"\n        Send feedback to the Automa.\n        This method can be called only once for each event.\n\n        This `send` method can be safely called in several different scenarios:\n        - In the same asyncio Task of the same event loop as the event handler.\n        - In a different asyncio Task of the same event loop as the event handler.\n        - In a different thread from the event handler.\n\n        Parameters\n        ----------\n        feedback: Feedback\n            The feedback to be sent.\n        \"\"\"\n        ...\n</code></pre>"},{"location":"reference/bridgic-core/bridgic/core/automa/interaction/#bridgic.core.automa.interaction.FeedbackSender.send","title":"send","text":"<code>abstractmethod</code> <pre><code>send(feedback: Feedback) -&gt; None\n</code></pre> <p>Send feedback to the Automa. This method can be called only once for each event.</p> <p>This <code>send</code> method can be safely called in several different scenarios: - In the same asyncio Task of the same event loop as the event handler. - In a different asyncio Task of the same event loop as the event handler. - In a different thread from the event handler.</p> <p>Parameters:</p> Name Type Description Default <code>feedback</code> <code>Feedback</code> <p>The feedback to be sent.</p> required Source code in <code>bridgic/core/automa/interaction/_event_handling.py</code> <pre><code>@abstractmethod\ndef send(self, feedback: Feedback) -&gt; None:\n    \"\"\"\n    Send feedback to the Automa.\n    This method can be called only once for each event.\n\n    This `send` method can be safely called in several different scenarios:\n    - In the same asyncio Task of the same event loop as the event handler.\n    - In a different asyncio Task of the same event loop as the event handler.\n    - In a different thread from the event handler.\n\n    Parameters\n    ----------\n    feedback: Feedback\n        The feedback to be sent.\n    \"\"\"\n    ...\n</code></pre>"},{"location":"reference/bridgic-core/bridgic/core/automa/interaction/#bridgic.core.automa.interaction.InteractionFeedback","title":"InteractionFeedback","text":"<p>               Bases: <code>Feedback</code></p> <p>A feedback object that contains both the data provided by the user and the <code>interaction_id</code>, which uniquely identifies the corresponding interaction.</p> Source code in <code>bridgic/core/automa/interaction/_human_interaction.py</code> <pre><code>class InteractionFeedback(Feedback):\n    \"\"\"\n    A feedback object that contains both the data provided by the user and the `interaction_id`, which uniquely identifies the corresponding interaction.\n    \"\"\"\n    interaction_id: str\n    \"\"\" The unique identifier for the interaction.\"\"\"\n    timestamp: datetime = datetime.now()\n    \"\"\"The timestamp of the feedback.\"\"\"\n</code></pre>"},{"location":"reference/bridgic-core/bridgic/core/automa/interaction/#bridgic.core.automa.interaction.InteractionFeedback.interaction_id","title":"interaction_id  <code>instance-attribute</code>","text":"<pre><code>interaction_id: str\n</code></pre> <p>The unique identifier for the interaction.</p>"},{"location":"reference/bridgic-core/bridgic/core/automa/interaction/#bridgic.core.automa.interaction.InteractionFeedback.timestamp","title":"timestamp  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>timestamp: datetime = now()\n</code></pre> <p>The timestamp of the feedback.</p>"},{"location":"reference/bridgic-core/bridgic/core/automa/interaction/#bridgic.core.automa.interaction.InteractionException","title":"InteractionException","text":"<p>               Bases: <code>Exception</code></p> <p>An exception raised when the <code>interact_with_human</code> method is called one or more times within the latest event loop iteration, causing one or multiple human interactions to be triggered.</p> Source code in <code>bridgic/core/automa/interaction/_human_interaction.py</code> <pre><code>class InteractionException(Exception):\n    \"\"\"\n    An exception raised when the `interact_with_human` method is called one or more times within the latest event loop iteration, causing one or multiple human interactions to be triggered.\n    \"\"\"\n    _interactions: List[Interaction]\n    \"\"\"The list of interactions that occurred during the latest event loop iteration.\"\"\"\n    _snapshot: \"Snapshot\"\n    \"\"\"The snapshot of the Automa's current state.\"\"\"\n\n    def __init__(self, interactions: List[Interaction], snapshot: \"Snapshot\"):\n        self._interactions = interactions\n        self._snapshot = snapshot\n\n    @property\n    def interactions(self) -&gt; List[Interaction]:\n        \"\"\"\n        A list of `Interaction` objects that occurred during the latest event loop iteration.\n\n        Multiple `Interaction` objects may be generated because, within the latest event loop iteration, multiple workers calling the `interact_with_human` method might be running concurrently in parallel branches of the graph.\n        \"\"\"\n        return self._interactions\n\n    @property\n    def snapshot(self) -&gt; \"Snapshot\":\n        \"\"\"\n        A `Snapshot` of the Automa's current state.\n        The serialization is triggered automatically by the `interact_with_human` method.\n        \"\"\"\n        return self._snapshot\n</code></pre>"},{"location":"reference/bridgic-core/bridgic/core/automa/interaction/#bridgic.core.automa.interaction.InteractionException.interactions","title":"interactions  <code>property</code>","text":"<pre><code>interactions: List[Interaction]\n</code></pre> <p>A list of <code>Interaction</code> objects that occurred during the latest event loop iteration.</p> <p>Multiple <code>Interaction</code> objects may be generated because, within the latest event loop iteration, multiple workers calling the <code>interact_with_human</code> method might be running concurrently in parallel branches of the graph.</p>"},{"location":"reference/bridgic-core/bridgic/core/automa/interaction/#bridgic.core.automa.interaction.InteractionException.snapshot","title":"snapshot  <code>property</code>","text":"<pre><code>snapshot: Snapshot\n</code></pre> <p>A <code>Snapshot</code> of the Automa's current state. The serialization is triggered automatically by the <code>interact_with_human</code> method.</p>"},{"location":"reference/bridgic-core/bridgic/core/automa/interaction/#bridgic.core.automa.interaction.Interaction","title":"Interaction","text":"<p>               Bases: <code>BaseModel</code></p> <p>An object that represents a single interaction between the Automa and a human.  Each call to <code>interact_with_human</code> will generate an <code>Interaction</code> object which will be included in the <code>InteractionException</code> raised.</p> Source code in <code>bridgic/core/automa/interaction/_human_interaction.py</code> <pre><code>class Interaction(BaseModel):\n    \"\"\"\n    An object that represents a single interaction between the Automa and a human. \n    Each call to `interact_with_human` will generate an `Interaction` object which will be included in the `InteractionException` raised.\n    \"\"\"\n    interaction_id: str\n    \"\"\" The unique identifier for the interaction.\"\"\"\n    event: Event\n    \"\"\"The event that triggered the interaction.\"\"\"\n</code></pre>"},{"location":"reference/bridgic-core/bridgic/core/automa/interaction/#bridgic.core.automa.interaction.Interaction.interaction_id","title":"interaction_id  <code>instance-attribute</code>","text":"<pre><code>interaction_id: str\n</code></pre> <p>The unique identifier for the interaction.</p>"},{"location":"reference/bridgic-core/bridgic/core/automa/interaction/#bridgic.core.automa.interaction.Interaction.event","title":"event  <code>instance-attribute</code>","text":"<pre><code>event: Event\n</code></pre> <p>The event that triggered the interaction.</p>"},{"location":"reference/bridgic-core/bridgic/core/automa/worker/","title":"worker","text":"<p>The Worker module defines the Worker concept and its related implementations in Automa.</p> <p>This module provides the core abstractions and implementations of Worker, including:</p> <ul> <li>Worker: The base class for all workers, which is the basic execution unit in Automa,    defining the execution interface (<code>arun()</code> and <code>run()</code> methods) for nodes</li> <li>CallableWorker: A worker implementation for wrapping callable objects (functions or methods)</li> <li>WorkerCallback: A callback interface during worker execution, supporting validation,    monitoring, and log collection before and after execution</li> <li>WorkerCallbackBuilder: A builder for constructing and configuring worker callbacks</li> </ul>"},{"location":"reference/bridgic-core/bridgic/core/automa/worker/#bridgic.core.automa.worker.T_WorkerCallback","title":"T_WorkerCallback  <code>module-attribute</code>","text":"<pre><code>T_WorkerCallback = TypeVar(\n    \"T_WorkerCallback\", bound=\"WorkerCallback\"\n)\n</code></pre> <p>Type variable for WorkerCallback subclasses.</p>"},{"location":"reference/bridgic-core/bridgic/core/automa/worker/#bridgic.core.automa.worker.Worker","title":"Worker","text":"<p>               Bases: <code>Serializable</code></p> <p>This class is the base class for all workers.</p> <p><code>Worker</code> has two methods that may be overridden by the subclass:</p> <ol> <li> <p><code>arun()</code>: This asynchronous method should be implemented when your worker  does not require almost immediately scheduling after all its task dependencies  are fulfilled, and when overall workflow is not sensitive to the fair sharing  of CPU resources between workers. If workers can afford to retain and occupy  execution resources for their entire execution duration, and there is no  explicit need for fair CPU time-sharing or timely scheduling, you should  implement <code>arun()</code> and allow workers to run to completion as cooperative tasks  within the event loop.</p> </li> <li> <p><code>run()</code>: This synchronous method should be implemented when either of the  following holds:</p> <ul> <li> <p>a. The automa includes other workers that require timely access to CPU  resources (for example, workers that must respond quickly or are sensitive  to scheduling latency).</p> </li> <li> <p>b. The current worker itself should be scheduled as soon as all its task  dependencies are met, to maintain overall workflow responsiveness. In these  cases, <code>run()</code> enables the framework to offload your worker to a thread pool,  ensuring that CPU time is shared fairly among all such workers and the event  loop remains responsive.</p> </li> </ul> </li> </ol> <p>In summary, if you are unsure whether your task require quickly scheduling or not,  it is recommended to implement the <code>arun()</code> method. Otherwise, implement the  <code>run()</code> ONLY if you are certain that you agree to share CPU time slices  with other workers.</p> Source code in <code>bridgic/core/automa/worker/_worker.py</code> <pre><code>class Worker(Serializable):\n    \"\"\"\n    This class is the base class for all workers.\n\n    `Worker` has two methods that may be overridden by the subclass:\n\n    1. `arun()`: This asynchronous method should be implemented when your worker \n    does not require almost immediately scheduling after all its task dependencies \n    are fulfilled, and when overall workflow is not sensitive to the fair sharing \n    of CPU resources between workers. If workers can afford to retain and occupy \n    execution resources for their entire execution duration, and there is no \n    explicit need for fair CPU time-sharing or timely scheduling, you should \n    implement `arun()` and allow workers to run to completion as cooperative tasks \n    within the event loop.\n\n    2. `run()`: This synchronous method should be implemented when either of the \n    following holds:\n\n        - a. The automa includes other workers that require timely access to CPU \n        resources (for example, workers that must respond quickly or are sensitive \n        to scheduling latency).\n\n        - b. The current worker itself should be scheduled as soon as all its task \n        dependencies are met, to maintain overall workflow responsiveness. In these \n        cases, `run()` enables the framework to offload your worker to a thread pool, \n        ensuring that CPU time is shared fairly among all such workers and the event \n        loop remains responsive.\n\n    In summary, if you are unsure whether your task require quickly scheduling or not, \n    it is recommended to implement the `arun()` method. Otherwise, implement the \n    `run()` **ONLY** if you are certain that you agree to share CPU time slices \n    with other workers.\n    \"\"\"\n\n    # TODO : Maybe process pool of the Automa is needed.\n\n    __parent: \"Automa\"\n    __local_space: Dict[str, Any]\n\n    # Cached method signatures, with no need for serialization.\n    __cached_param_names_of_arun: Dict[_ParameterKind, List[Tuple[str, Any]]]\n    __cached_param_names_of_run: Dict[_ParameterKind, List[Tuple[str, Any]]]\n\n    def __init__(self):\n        self.__parent = self\n        self.__local_space = {}\n\n        # Cached method signatures, with no need for serialization.\n        self.__cached_param_names_of_arun = None\n        self.__cached_param_names_of_run = None\n\n    async def arun(self, *args: Tuple[Any, ...], **kwargs: Dict[str, Any]) -&gt; Any:\n        \"\"\"\n        The asynchronous method to run the worker.\n        \"\"\"\n        loop = asyncio.get_running_loop()\n        topest_automa = self._get_top_level_automa()\n        if topest_automa:\n            thread_pool = topest_automa.thread_pool\n            if thread_pool:\n                rx_param_names_dict = self.get_input_param_names()\n                rx_args, rx_kwargs = safely_map_args(args, kwargs, rx_param_names_dict)\n                # kwargs can only be passed by functools.partial.\n                return await loop.run_in_executor(thread_pool, partial(self.run, *rx_args, **rx_kwargs))\n\n        # Unexpected: No thread pool is available.\n        # Case 1: the worker is not inside an Automa (uncommon case).\n        # Case 2: no thread pool is setup by the top-level automa.\n        raise WorkerRuntimeError(f\"No thread pool is available for the worker {type(self)}\")\n\n    def run(self, *args: Tuple[Any, ...], **kwargs: Dict[str, Any]) -&gt; Any:\n        \"\"\"\n        The synchronous method to run the worker.\n        \"\"\"\n        raise NotImplementedError(f\"run() is not implemented in {type(self)}\")\n\n    def is_top_level(self) -&gt; bool:\n        \"\"\"\n        Check if the current worker is the top-level worker.\n\n        Returns\n        -------\n        bool\n            True if the current worker is the top-level worker (parent is self), False otherwise.\n        \"\"\"\n        return self.parent is self\n\n    def _get_top_level_automa(self) -&gt; Optional[\"Automa\"]:\n        \"\"\"\n        Get the top-level automa instance reference.\n        \"\"\"\n        # If the current automa is the top-level automa, return itself.\n        from bridgic.core.automa._automa import Automa\n        if isinstance(self, Automa):\n            top_level_automa = self\n        else:\n            top_level_automa = self.parent\n        while top_level_automa and (not top_level_automa.is_top_level()):\n            top_level_automa = top_level_automa.parent\n        return top_level_automa\n\n    def get_input_param_names(self) -&gt; Dict[_ParameterKind, List[Tuple[str, Any]]]:\n        \"\"\"\n        Get the names of input parameters of the worker.\n        Use cached result if available in order to improve performance.\n\n        This method intelligently detects whether the user has overridden the `run` method\n        or is using the default `arun` method, and returns the appropriate parameter signature.\n\n        Returns\n        -------\n        Dict[_ParameterKind, List[str]]\n            A dictionary of input parameter names by the kind of the parameter.\n            The key is the kind of the parameter, which is one of five possible values:\n\n            - inspect.Parameter.POSITIONAL_ONLY\n            - inspect.Parameter.POSITIONAL_OR_KEYWORD\n            - inspect.Parameter.VAR_POSITIONAL\n            - inspect.Parameter.KEYWORD_ONLY\n            - inspect.Parameter.VAR_KEYWORD\n        \"\"\"\n        # Check if user has overridden the arun method\n        if self._is_arun_overridden():\n            # User overrode arun method, return arun method parameters\n            if self.__cached_param_names_of_arun is None:\n                self.__cached_param_names_of_arun = get_param_names_all_kinds(self.arun)\n            return self.__cached_param_names_of_arun\n        else:\n            # User is using run method, return run method parameters\n            if self.__cached_param_names_of_run is None:\n                self.__cached_param_names_of_run = get_param_names_all_kinds(self.run)\n            return self.__cached_param_names_of_run\n\n    def _is_arun_overridden(self) -&gt; bool:\n        \"\"\"\n        Check if the user has overridden the arun method.\n        \"\"\"\n        # Compare method references - much faster than inspect.getsource()\n        return self.arun.__func__ is not Worker.arun\n\n    @property\n    def parent(self) -&gt; \"Automa\":\n        return self.__parent\n\n    @parent.setter\n    def parent(self, value: \"Automa\"):\n        self.__parent = value\n\n    @property\n    def local_space(self) -&gt; Dict[str, Any]:\n        return self.__local_space\n\n    @local_space.setter\n    def local_space(self, value: Dict[str, Any]):\n        self.__local_space = value\n\n    def get_report_info(self) -&gt; Dict[str, Any]:\n        report_info = {}\n        report_info[\"local_space\"] = self.__local_space\n        return report_info\n\n    @override\n    def dump_to_dict(self) -&gt; Dict[str, Any]:\n        state_dict = {}\n        state_dict[\"local_space\"] = self.__local_space\n        return state_dict\n\n    @override\n    def load_from_dict(self, state_dict: Dict[str, Any]) -&gt; None:\n        self.__parent = self\n        self.__local_space = state_dict[\"local_space\"]\n\n        # Cached method signatures, with no need for serialization.\n        self.__cached_param_names_of_arun = None\n        self.__cached_param_names_of_run = None\n\n    def ferry_to(self, key: str, /, *args, **kwargs):\n        \"\"\"\n        Handoff control flow to the specified worker, passing along any arguments as needed.\n        The specified worker will always start to run asynchronously in the next event loop, regardless of its dependencies.\n\n        Parameters\n        ----------\n        key : str\n            The key of the worker to run.\n        args : optional\n            Positional arguments to be passed.\n        kwargs : optional\n            Keyword arguments to be passed.\n        \"\"\"\n        if self.is_top_level():\n            raise WorkerRuntimeError(f\"`ferry_to` method can only be called by a worker inside an Automa\")\n        self.parent.ferry_to(key, *args, **kwargs)\n\n    def post_event(self, event: Event) -&gt; None:\n        \"\"\"\n        Post an event to the application layer outside the Automa.\n\n        The event handler implemented by the application layer will be called in the same thread as the worker (maybe the main thread or a new thread from the thread pool).\n\n        Note that `post_event` can be called in a non-async method or an async method.\n\n        The event will be bubbled up to the top-level Automa, where it will be processed by the event handler registered with the event type.\n\n        Parameters\n        ----------\n        event: Event\n            The event to be posted.\n        \"\"\"\n        if self.is_top_level():\n            raise WorkerRuntimeError(f\"`post_event` method can only be called by a worker inside an Automa\")\n        self.parent.post_event(event)\n\n    def request_feedback(\n        self, \n        event: Event,\n        timeout: Optional[float] = None\n    ) -&gt; Feedback:\n        \"\"\"\n        Request feedback for the specified event from the application layer outside the Automa. This method blocks the caller until the feedback is received.\n\n        Note that `post_event` should only be called from within a non-async method running in the new thread of the Automa thread pool.\n\n        Parameters\n        ----------\n        event: Event\n            The event to be posted to the event handler implemented by the application layer.\n        timeout: Optional[float]\n            A float or int number of seconds to wait for if the feedback is not received. If None, then there is no limit on the wait time.\n\n        Returns\n        -------\n        Feedback\n            The feedback received from the application layer.\n\n        Raises\n        ------\n        TimeoutError\n            If the feedback is not received before the timeout. Note that the raised exception is the built-in `TimeoutError` exception, instead of asyncio.TimeoutError or concurrent.futures.TimeoutError!\n        \"\"\"\n        if self.is_top_level():\n            raise WorkerRuntimeError(f\"`request_feedback` method can only be called by a worker inside an Automa\")\n        return self.parent.request_feedback(event, timeout)\n\n    async def request_feedback_async(\n        self, \n        event: Event,\n        timeout: Optional[float] = None\n    ) -&gt; Feedback:\n        \"\"\"\n        Request feedback for the specified event from the application layer outside the Automa. This method blocks the caller until the feedback is received.\n\n        The event handler implemented by the application layer will be called in the next event loop, in the main thread.\n\n        Note that `post_event` should only be called from within an asynchronous method running in the main event loop of the top-level Automa.\n\n        Parameters\n        ----------\n        event: Event\n            The event to be posted to the event handler implemented by the application layer.\n        timeout: Optional[float]\n            A float or int number of seconds to wait for if the feedback is not received. If None, then there is no limit on the wait time.\n\n        Returns\n        -------\n        Feedback\n            The feedback received from the application layer.\n\n        Raises\n        ------\n        TimeoutError\n            If the feedback is not received before the timeout. Note that the raised exception is the built-in `TimeoutError` exception, instead of asyncio.TimeoutError!\n        \"\"\"\n        if self.is_top_level():\n            raise WorkerRuntimeError(f\"`request_feedback_async` method can only be called by a worker inside an Automa\")\n        return await self.parent.request_feedback_async(event, timeout)\n\n    def interact_with_human(self, event: Event) -&gt; InteractionFeedback:\n        if self.is_top_level():\n            raise WorkerRuntimeError(f\"`interact_with_human` method can only be called by a worker inside an Automa\")\n        return self.parent.interact_with_human(event, self)\n</code></pre>"},{"location":"reference/bridgic-core/bridgic/core/automa/worker/#bridgic.core.automa.worker.Worker.arun","title":"arun","text":"<code>async</code> <pre><code>arun(\n    *args: Tuple[Any, ...], **kwargs: Dict[str, Any]\n) -&gt; Any\n</code></pre> <p>The asynchronous method to run the worker.</p> Source code in <code>bridgic/core/automa/worker/_worker.py</code> <pre><code>async def arun(self, *args: Tuple[Any, ...], **kwargs: Dict[str, Any]) -&gt; Any:\n    \"\"\"\n    The asynchronous method to run the worker.\n    \"\"\"\n    loop = asyncio.get_running_loop()\n    topest_automa = self._get_top_level_automa()\n    if topest_automa:\n        thread_pool = topest_automa.thread_pool\n        if thread_pool:\n            rx_param_names_dict = self.get_input_param_names()\n            rx_args, rx_kwargs = safely_map_args(args, kwargs, rx_param_names_dict)\n            # kwargs can only be passed by functools.partial.\n            return await loop.run_in_executor(thread_pool, partial(self.run, *rx_args, **rx_kwargs))\n\n    # Unexpected: No thread pool is available.\n    # Case 1: the worker is not inside an Automa (uncommon case).\n    # Case 2: no thread pool is setup by the top-level automa.\n    raise WorkerRuntimeError(f\"No thread pool is available for the worker {type(self)}\")\n</code></pre>"},{"location":"reference/bridgic-core/bridgic/core/automa/worker/#bridgic.core.automa.worker.Worker.run","title":"run","text":"<pre><code>run(\n    *args: Tuple[Any, ...], **kwargs: Dict[str, Any]\n) -&gt; Any\n</code></pre> <p>The synchronous method to run the worker.</p> Source code in <code>bridgic/core/automa/worker/_worker.py</code> <pre><code>def run(self, *args: Tuple[Any, ...], **kwargs: Dict[str, Any]) -&gt; Any:\n    \"\"\"\n    The synchronous method to run the worker.\n    \"\"\"\n    raise NotImplementedError(f\"run() is not implemented in {type(self)}\")\n</code></pre>"},{"location":"reference/bridgic-core/bridgic/core/automa/worker/#bridgic.core.automa.worker.Worker.is_top_level","title":"is_top_level","text":"<pre><code>is_top_level() -&gt; bool\n</code></pre> <p>Check if the current worker is the top-level worker.</p> <p>Returns:</p> Type Description <code>bool</code> <p>True if the current worker is the top-level worker (parent is self), False otherwise.</p> Source code in <code>bridgic/core/automa/worker/_worker.py</code> <pre><code>def is_top_level(self) -&gt; bool:\n    \"\"\"\n    Check if the current worker is the top-level worker.\n\n    Returns\n    -------\n    bool\n        True if the current worker is the top-level worker (parent is self), False otherwise.\n    \"\"\"\n    return self.parent is self\n</code></pre>"},{"location":"reference/bridgic-core/bridgic/core/automa/worker/#bridgic.core.automa.worker.Worker.get_input_param_names","title":"get_input_param_names","text":"<pre><code>get_input_param_names() -&gt; (\n    Dict[_ParameterKind, List[Tuple[str, Any]]]\n)\n</code></pre> <p>Get the names of input parameters of the worker. Use cached result if available in order to improve performance.</p> <p>This method intelligently detects whether the user has overridden the <code>run</code> method or is using the default <code>arun</code> method, and returns the appropriate parameter signature.</p> <p>Returns:</p> Type Description <code>Dict[_ParameterKind, List[str]]</code> <p>A dictionary of input parameter names by the kind of the parameter. The key is the kind of the parameter, which is one of five possible values:</p> <ul> <li>inspect.Parameter.POSITIONAL_ONLY</li> <li>inspect.Parameter.POSITIONAL_OR_KEYWORD</li> <li>inspect.Parameter.VAR_POSITIONAL</li> <li>inspect.Parameter.KEYWORD_ONLY</li> <li>inspect.Parameter.VAR_KEYWORD</li> </ul> Source code in <code>bridgic/core/automa/worker/_worker.py</code> <pre><code>def get_input_param_names(self) -&gt; Dict[_ParameterKind, List[Tuple[str, Any]]]:\n    \"\"\"\n    Get the names of input parameters of the worker.\n    Use cached result if available in order to improve performance.\n\n    This method intelligently detects whether the user has overridden the `run` method\n    or is using the default `arun` method, and returns the appropriate parameter signature.\n\n    Returns\n    -------\n    Dict[_ParameterKind, List[str]]\n        A dictionary of input parameter names by the kind of the parameter.\n        The key is the kind of the parameter, which is one of five possible values:\n\n        - inspect.Parameter.POSITIONAL_ONLY\n        - inspect.Parameter.POSITIONAL_OR_KEYWORD\n        - inspect.Parameter.VAR_POSITIONAL\n        - inspect.Parameter.KEYWORD_ONLY\n        - inspect.Parameter.VAR_KEYWORD\n    \"\"\"\n    # Check if user has overridden the arun method\n    if self._is_arun_overridden():\n        # User overrode arun method, return arun method parameters\n        if self.__cached_param_names_of_arun is None:\n            self.__cached_param_names_of_arun = get_param_names_all_kinds(self.arun)\n        return self.__cached_param_names_of_arun\n    else:\n        # User is using run method, return run method parameters\n        if self.__cached_param_names_of_run is None:\n            self.__cached_param_names_of_run = get_param_names_all_kinds(self.run)\n        return self.__cached_param_names_of_run\n</code></pre>"},{"location":"reference/bridgic-core/bridgic/core/automa/worker/#bridgic.core.automa.worker.Worker.ferry_to","title":"ferry_to","text":"<pre><code>ferry_to(key: str, /, *args, **kwargs)\n</code></pre> <p>Handoff control flow to the specified worker, passing along any arguments as needed. The specified worker will always start to run asynchronously in the next event loop, regardless of its dependencies.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>The key of the worker to run.</p> required <code>args</code> <code>optional</code> <p>Positional arguments to be passed.</p> <code>()</code> <code>kwargs</code> <code>optional</code> <p>Keyword arguments to be passed.</p> <code>{}</code> Source code in <code>bridgic/core/automa/worker/_worker.py</code> <pre><code>def ferry_to(self, key: str, /, *args, **kwargs):\n    \"\"\"\n    Handoff control flow to the specified worker, passing along any arguments as needed.\n    The specified worker will always start to run asynchronously in the next event loop, regardless of its dependencies.\n\n    Parameters\n    ----------\n    key : str\n        The key of the worker to run.\n    args : optional\n        Positional arguments to be passed.\n    kwargs : optional\n        Keyword arguments to be passed.\n    \"\"\"\n    if self.is_top_level():\n        raise WorkerRuntimeError(f\"`ferry_to` method can only be called by a worker inside an Automa\")\n    self.parent.ferry_to(key, *args, **kwargs)\n</code></pre>"},{"location":"reference/bridgic-core/bridgic/core/automa/worker/#bridgic.core.automa.worker.Worker.post_event","title":"post_event","text":"<pre><code>post_event(event: Event) -&gt; None\n</code></pre> <p>Post an event to the application layer outside the Automa.</p> <p>The event handler implemented by the application layer will be called in the same thread as the worker (maybe the main thread or a new thread from the thread pool).</p> <p>Note that <code>post_event</code> can be called in a non-async method or an async method.</p> <p>The event will be bubbled up to the top-level Automa, where it will be processed by the event handler registered with the event type.</p> <p>Parameters:</p> Name Type Description Default <code>event</code> <code>Event</code> <p>The event to be posted.</p> required Source code in <code>bridgic/core/automa/worker/_worker.py</code> <pre><code>def post_event(self, event: Event) -&gt; None:\n    \"\"\"\n    Post an event to the application layer outside the Automa.\n\n    The event handler implemented by the application layer will be called in the same thread as the worker (maybe the main thread or a new thread from the thread pool).\n\n    Note that `post_event` can be called in a non-async method or an async method.\n\n    The event will be bubbled up to the top-level Automa, where it will be processed by the event handler registered with the event type.\n\n    Parameters\n    ----------\n    event: Event\n        The event to be posted.\n    \"\"\"\n    if self.is_top_level():\n        raise WorkerRuntimeError(f\"`post_event` method can only be called by a worker inside an Automa\")\n    self.parent.post_event(event)\n</code></pre>"},{"location":"reference/bridgic-core/bridgic/core/automa/worker/#bridgic.core.automa.worker.Worker.request_feedback","title":"request_feedback","text":"<pre><code>request_feedback(\n    event: Event, timeout: Optional[float] = None\n) -&gt; Feedback\n</code></pre> <p>Request feedback for the specified event from the application layer outside the Automa. This method blocks the caller until the feedback is received.</p> <p>Note that <code>post_event</code> should only be called from within a non-async method running in the new thread of the Automa thread pool.</p> <p>Parameters:</p> Name Type Description Default <code>event</code> <code>Event</code> <p>The event to be posted to the event handler implemented by the application layer.</p> required <code>timeout</code> <code>Optional[float]</code> <p>A float or int number of seconds to wait for if the feedback is not received. If None, then there is no limit on the wait time.</p> <code>None</code> <p>Returns:</p> Type Description <code>Feedback</code> <p>The feedback received from the application layer.</p> <p>Raises:</p> Type Description <code>TimeoutError</code> <p>If the feedback is not received before the timeout. Note that the raised exception is the built-in <code>TimeoutError</code> exception, instead of asyncio.TimeoutError or concurrent.futures.TimeoutError!</p> Source code in <code>bridgic/core/automa/worker/_worker.py</code> <pre><code>def request_feedback(\n    self, \n    event: Event,\n    timeout: Optional[float] = None\n) -&gt; Feedback:\n    \"\"\"\n    Request feedback for the specified event from the application layer outside the Automa. This method blocks the caller until the feedback is received.\n\n    Note that `post_event` should only be called from within a non-async method running in the new thread of the Automa thread pool.\n\n    Parameters\n    ----------\n    event: Event\n        The event to be posted to the event handler implemented by the application layer.\n    timeout: Optional[float]\n        A float or int number of seconds to wait for if the feedback is not received. If None, then there is no limit on the wait time.\n\n    Returns\n    -------\n    Feedback\n        The feedback received from the application layer.\n\n    Raises\n    ------\n    TimeoutError\n        If the feedback is not received before the timeout. Note that the raised exception is the built-in `TimeoutError` exception, instead of asyncio.TimeoutError or concurrent.futures.TimeoutError!\n    \"\"\"\n    if self.is_top_level():\n        raise WorkerRuntimeError(f\"`request_feedback` method can only be called by a worker inside an Automa\")\n    return self.parent.request_feedback(event, timeout)\n</code></pre>"},{"location":"reference/bridgic-core/bridgic/core/automa/worker/#bridgic.core.automa.worker.Worker.request_feedback_async","title":"request_feedback_async","text":"<code>async</code> <pre><code>request_feedback_async(\n    event: Event, timeout: Optional[float] = None\n) -&gt; Feedback\n</code></pre> <p>Request feedback for the specified event from the application layer outside the Automa. This method blocks the caller until the feedback is received.</p> <p>The event handler implemented by the application layer will be called in the next event loop, in the main thread.</p> <p>Note that <code>post_event</code> should only be called from within an asynchronous method running in the main event loop of the top-level Automa.</p> <p>Parameters:</p> Name Type Description Default <code>event</code> <code>Event</code> <p>The event to be posted to the event handler implemented by the application layer.</p> required <code>timeout</code> <code>Optional[float]</code> <p>A float or int number of seconds to wait for if the feedback is not received. If None, then there is no limit on the wait time.</p> <code>None</code> <p>Returns:</p> Type Description <code>Feedback</code> <p>The feedback received from the application layer.</p> <p>Raises:</p> Type Description <code>TimeoutError</code> <p>If the feedback is not received before the timeout. Note that the raised exception is the built-in <code>TimeoutError</code> exception, instead of asyncio.TimeoutError!</p> Source code in <code>bridgic/core/automa/worker/_worker.py</code> <pre><code>async def request_feedback_async(\n    self, \n    event: Event,\n    timeout: Optional[float] = None\n) -&gt; Feedback:\n    \"\"\"\n    Request feedback for the specified event from the application layer outside the Automa. This method blocks the caller until the feedback is received.\n\n    The event handler implemented by the application layer will be called in the next event loop, in the main thread.\n\n    Note that `post_event` should only be called from within an asynchronous method running in the main event loop of the top-level Automa.\n\n    Parameters\n    ----------\n    event: Event\n        The event to be posted to the event handler implemented by the application layer.\n    timeout: Optional[float]\n        A float or int number of seconds to wait for if the feedback is not received. If None, then there is no limit on the wait time.\n\n    Returns\n    -------\n    Feedback\n        The feedback received from the application layer.\n\n    Raises\n    ------\n    TimeoutError\n        If the feedback is not received before the timeout. Note that the raised exception is the built-in `TimeoutError` exception, instead of asyncio.TimeoutError!\n    \"\"\"\n    if self.is_top_level():\n        raise WorkerRuntimeError(f\"`request_feedback_async` method can only be called by a worker inside an Automa\")\n    return await self.parent.request_feedback_async(event, timeout)\n</code></pre>"},{"location":"reference/bridgic-core/bridgic/core/automa/worker/#bridgic.core.automa.worker.CallableWorker","title":"CallableWorker","text":"<p>               Bases: <code>Worker</code></p> <p>This class is a worker that wraps a callable object, such as functions or methods.</p> <p>Parameters:</p> Name Type Description Default <code>func_or_method</code> <code>Optional[Callable]</code> <p>The callable to be wrapped by the worker. If <code>func_or_method</code> is None,  <code>state_dict</code> must be provided.</p> <code>None</code> Source code in <code>bridgic/core/automa/worker/_callable_worker.py</code> <pre><code>class CallableWorker(Worker):\n    \"\"\"\n    This class is a worker that wraps a callable object, such as functions or methods.\n\n    Parameters\n    ----------\n    func_or_method : Optional[Callable]\n        The callable to be wrapped by the worker. If `func_or_method` is None, \n        `state_dict` must be provided.\n    \"\"\"\n    _is_async: bool\n    _callable: Callable\n    # Used to deserialization.\n    _expected_bound_parent: bool\n\n    # Cached method signatures, with no need for serialization.\n    __cached_param_names_of_callable: Dict[_ParameterKind, List[str]]\n\n    def __init__(\n        self, \n        func_or_method: Optional[Callable] = None,\n    ):\n        \"\"\"\n        Parameters\n        ----------\n        func_or_method : Optional[Callable]\n            The callable to be wrapped by the worker. If `func_or_method` is None, \n            `state_dict` must be provided.\n        \"\"\"\n        super().__init__()\n        self._is_async = inspect.iscoroutinefunction(func_or_method)\n        self._callable = func_or_method\n        self._expected_bound_parent = False\n\n        # Cached method signatures, with no need for serialization.\n        self.__cached_param_names_of_callable = None\n\n    async def arun(self, *args: Tuple[Any, ...], **kwargs: Dict[str, Any]) -&gt; Any:\n        if self._expected_bound_parent:\n            raise WorkerRuntimeError(\n                f\"The callable is expected to be bound to the parent, \"\n                f\"but not bounded yet: {self._callable}\"\n            )\n        if self._is_async:\n            return await self._callable(*args, **kwargs)\n        return await super().arun(*args, **kwargs)\n\n    def run(self, *args: Tuple[Any, ...], **kwargs: Dict[str, Any]) -&gt; Any:\n        assert self._is_async is False\n        return self._callable(*args, **kwargs)\n\n    @override\n    def get_input_param_names(self) -&gt; Dict[_ParameterKind, List[str]]:\n        \"\"\"\n        Get the names of input parameters of this callable worker.\n        Use cached result if available in order to improve performance.\n\n        Returns\n        -------\n        Dict[_ParameterKind, List[str]]\n            A dictionary of input parameter names by the kind of the parameter.\n            The key is the kind of the parameter, which is one of five possible values:\n\n            - inspect.Parameter.POSITIONAL_ONLY\n            - inspect.Parameter.POSITIONAL_OR_KEYWORD\n            - inspect.Parameter.VAR_POSITIONAL\n            - inspect.Parameter.KEYWORD_ONLY\n            - inspect.Parameter.VAR_KEYWORD\n        \"\"\"\n        if self.__cached_param_names_of_callable is None:\n            self.__cached_param_names_of_callable = get_param_names_all_kinds(self._callable)\n        return self.__cached_param_names_of_callable\n\n    @override\n    def dump_to_dict(self) -&gt; Dict[str, Any]:\n        state_dict = super().dump_to_dict()\n        state_dict[\"is_async\"] = self._is_async\n        # Note: Not to use pickle to serialize the callable here.\n        # We customize the serialization method of the callable to avoid creating instance multiple times and to minimize side effects.\n        bounded = isinstance(self._callable, MethodType)\n        state_dict[\"bounded\"] = bounded\n        if bounded:\n            if self._callable.__self__ is self.parent:\n                state_dict[\"callable_name\"] = self._callable.__module__ + \".\" + self._callable.__qualname__\n            else:\n                state_dict[\"pickled_callable\"] = pickle.dumps(self._callable)\n        else:\n            state_dict[\"callable_name\"] = self._callable.__module__ + \".\" + self._callable.__qualname__\n        return state_dict\n\n    @override\n    def load_from_dict(self, state_dict: Dict[str, Any]) -&gt; None:\n        super().load_from_dict(state_dict)\n        # Deserialize from the state_dict.\n        self._is_async = state_dict[\"is_async\"]\n        bounded = state_dict[\"bounded\"]\n        if bounded:\n            pickled_callable = state_dict.get(\"pickled_callable\", None)\n            if pickled_callable is None:\n                self._callable = load_qualified_class_or_func(state_dict[\"callable_name\"])\n                # Partially deserialized, need to be bound to the parent.\n                self._expected_bound_parent = True\n            else:\n                self._callable = pickle.loads(pickled_callable)\n                self._expected_bound_parent = False\n        else:\n            self._callable = load_qualified_class_or_func(state_dict[\"callable_name\"])\n            self._expected_bound_parent = False\n\n        # Cached method signatures, with no need for serialization.\n        self.__cached_param_names_of_callable = None\n\n    @property\n    def callable(self):\n        return self._callable\n\n    @property\n    def parent(self) -&gt; \"Automa\":\n        return super().parent\n\n    @parent.setter\n    def parent(self, value: \"Automa\"):\n        if self._expected_bound_parent:\n            self._callable = MethodType(self._callable, value)\n            self._expected_bound_parent = False\n        Worker.parent.fset(self, value)\n\n    @override\n    def __str__(self) -&gt; str:\n        return f\"CallableWorker(callable={self._callable.__name__})\"\n</code></pre>"},{"location":"reference/bridgic-core/bridgic/core/automa/worker/#bridgic.core.automa.worker.CallableWorker.get_input_param_names","title":"get_input_param_names","text":"<pre><code>get_input_param_names() -&gt; Dict[_ParameterKind, List[str]]\n</code></pre> <p>Get the names of input parameters of this callable worker. Use cached result if available in order to improve performance.</p> <p>Returns:</p> Type Description <code>Dict[_ParameterKind, List[str]]</code> <p>A dictionary of input parameter names by the kind of the parameter. The key is the kind of the parameter, which is one of five possible values:</p> <ul> <li>inspect.Parameter.POSITIONAL_ONLY</li> <li>inspect.Parameter.POSITIONAL_OR_KEYWORD</li> <li>inspect.Parameter.VAR_POSITIONAL</li> <li>inspect.Parameter.KEYWORD_ONLY</li> <li>inspect.Parameter.VAR_KEYWORD</li> </ul> Source code in <code>bridgic/core/automa/worker/_callable_worker.py</code> <pre><code>@override\ndef get_input_param_names(self) -&gt; Dict[_ParameterKind, List[str]]:\n    \"\"\"\n    Get the names of input parameters of this callable worker.\n    Use cached result if available in order to improve performance.\n\n    Returns\n    -------\n    Dict[_ParameterKind, List[str]]\n        A dictionary of input parameter names by the kind of the parameter.\n        The key is the kind of the parameter, which is one of five possible values:\n\n        - inspect.Parameter.POSITIONAL_ONLY\n        - inspect.Parameter.POSITIONAL_OR_KEYWORD\n        - inspect.Parameter.VAR_POSITIONAL\n        - inspect.Parameter.KEYWORD_ONLY\n        - inspect.Parameter.VAR_KEYWORD\n    \"\"\"\n    if self.__cached_param_names_of_callable is None:\n        self.__cached_param_names_of_callable = get_param_names_all_kinds(self._callable)\n    return self.__cached_param_names_of_callable\n</code></pre>"},{"location":"reference/bridgic-core/bridgic/core/automa/worker/#bridgic.core.automa.worker.WorkerCallback","title":"WorkerCallback","text":"<p>               Bases: <code>Serializable</code></p> <p>Callback for the execution of a worker instance during the running  of a prebult automa.</p> <p>This class defines the interfaces that will be called before or after  the execution of the corresponding worker. Callbacks are typically used  for validating input, monitoring execution, and collecting logs, etc.</p> <p>Methods:</p> Name Description <code>on_worker_start</code> <p>Hook invoked before worker execution.</p> <code>on_worker_end</code> <p>Hook invoked after worker execution.</p> <code>on_worker_error</code> <p>Hook invoked when worker execution raises an exception.</p> Source code in <code>bridgic/core/automa/worker/_worker_callback.py</code> <pre><code>class WorkerCallback(Serializable):\n    \"\"\"\n    Callback for the execution of a worker instance during the running \n    of a prebult automa.\n\n    This class defines the interfaces that will be called before or after \n    the execution of the corresponding worker. Callbacks are typically used \n    for validating input, monitoring execution, and collecting logs, etc.\n\n    Methods\n    -------\n    on_worker_start(key, is_top_level, parent, arguments)\n        Hook invoked before worker execution.\n    on_worker_end(key, is_top_level, parent, arguments, result)\n        Hook invoked after worker execution.\n    on_worker_error(key, is_top_level, parent, arguments, error)\n        Hook invoked when worker execution raises an exception.\n    \"\"\"\n    async def on_worker_start(\n        self, \n        key: str,\n        is_top_level: bool = False,\n        parent: Optional[\"Automa\"] = None,\n        arguments: Dict[str, Any] = None,\n    ) -&gt; None:\n        \"\"\"\n        Hook invoked before worker execution.\n\n        Called immediately before the worker runs. Use for arguments\n        validation, logging, or monitoring. Cannot modify execution\n        arguments or logic.\n\n        Parameters\n        ----------\n        key : str\n            Worker identifier.\n        is_top_level: bool = False\n            Whether the worker is the top-level automa. When True, parent will be the automa itself (parent is self).\n        parent : Optional[Automa] = None\n            Parent automa instance containing this worker. For top-level automa, parent is the automa itself.\n        arguments : Dict[str, Any] = None\n            Execution parameters with keys \"args\" and \"kwargs\".\n        \"\"\"\n        pass\n\n    async def on_worker_end(\n        self,\n        key: str,\n        is_top_level: bool = False,\n        parent: Optional[\"Automa\"] = None,\n        arguments: Dict[str, Any] = None,\n        result: Any = None,\n    ) -&gt; None:\n        \"\"\"\n        Hook invoked after worker execution.\n\n        Called immediately after the worker completes. Use for result\n        monitoring, logging, event publishing, or validation. Cannot\n        modify execution results or logic.\n\n        Parameters\n        ----------\n        key : str\n            Worker identifier.\n        is_top_level: bool = False\n            Whether the worker is the top-level automa. When True, parent will be the automa itself (parent is self).\n        parent : Optional[Automa] = None\n            Parent automa instance containing this worker. For top-level automa, parent is the automa itself.\n        arguments : Dict[str, Any] = None\n            Execution arguments with keys \"args\" and \"kwargs\".\n        result : Any = None\n            Worker execution result.\n        \"\"\"\n        pass\n\n    async def on_worker_error(\n        self,\n        key: str,\n        is_top_level: bool = False,\n        parent: Optional[\"Automa\"] = None,\n        arguments: Dict[str, Any] = None,\n        error: Exception = None,\n    ) -&gt; bool:\n        \"\"\"\n        Hook invoked when worker execution raises an exception.\n\n        Called when the worker execution raises an exception. Use for error handling, logging, \n        or event publishing. Cannot modify execution logic or arguments.\n\n        **Exception Matching Mechanism: How to Handle a Specific Exception**\n\n        The framework enable your callback to handle a given exception based on the \n        **type annotation** of the `error` parameter in your `on_worker_error` method.\n        The matching follows these rules:\n\n        - The parameter name MUST be `error` and the type annotation is critical for the \n          matching mechanism.\n        - If you annotate `error: ValueError`, it will match `ValueError` and all its \n          subclasses (e.g., `UnicodeDecodeError`).\n        - If you annotate `error: Exception`, it will match all exceptions (since all exceptions \n          inherit from Exception).\n        - If you want to match multiple exception types, you can use `Union[Type1, Type2, ...]`.\n\n\n        **Return Value: Whether to Suppress the Exception**\n\n        - If `on_worker_error` returns `True`, the framework will suppress the exception. \n          The framework will then proceed as if there was no error, and the worker result \n          will be set to None.\n        - If `on_worker_error` returns `False`, the framework will simply observe the error; \n          after all matching callbacks are called, the framework will re-raise the exception.\n\n        **Special Case: Interaction Exceptions Cannot Be Suppressed**\n\n        To ensure human-interaction mechanisms work correctly, exceptions of type\n        `_InteractionEventException` or `InteractionException` (including their subclasses) \n        **CANNOT** be suppressed by any callback. Even if your callback returns `True`, the \n        framework will forcibly re-raise the exception. This ensures these exceptions always \n        propagate correctly through the automa hierarchy to trigger necessary human interactions.\n\n        Parameters\n        ----------\n        key : str\n            Worker identifier.\n        is_top_level: bool = False\n            Whether the worker is the top-level automa. When True, parent will be the automa itself (parent is self).\n        parent : Optional[Automa] = None\n            Parent automa instance containing this worker. For top-level automa, parent is the automa itself.\n        arguments : Dict[str, Any] = None\n            Execution arguments with keys \"args\" and \"kwargs\".\n        error : Exception = None\n            The exception raised during worker execution. The type annotation of this\n            parameter determines which exceptions this callback will handle. The matching\n            is based on inheritance relationship (using isinstance), so a callback with\n            `error: ValueError` will match ValueError and all its subclasses.\n\n        Returns\n        -------\n        bool\n            True if the automa should suppress the exception (not re-raise it); False otherwise.\n        \"\"\"\n        return False\n\n    @override\n    def dump_to_dict(self) -&gt; Dict[str, Any]:\n        return {\n            \"callback_cls\": self.__class__.__module__ + \".\" + self.__class__.__qualname__,\n        }\n\n    @override\n    def load_from_dict(self, state_dict: Dict[str, Any]) -&gt; None:\n        pass\n</code></pre>"},{"location":"reference/bridgic-core/bridgic/core/automa/worker/#bridgic.core.automa.worker.WorkerCallback.on_worker_start","title":"on_worker_start","text":"<code>async</code> <pre><code>on_worker_start(\n    key: str,\n    is_top_level: bool = False,\n    parent: Optional[Automa] = None,\n    arguments: Dict[str, Any] = None,\n) -&gt; None\n</code></pre> <p>Hook invoked before worker execution.</p> <p>Called immediately before the worker runs. Use for arguments validation, logging, or monitoring. Cannot modify execution arguments or logic.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>Worker identifier.</p> required <code>is_top_level</code> <code>bool</code> <p>Whether the worker is the top-level automa. When True, parent will be the automa itself (parent is self).</p> <code>False</code> <code>parent</code> <code>Optional[Automa] = None</code> <p>Parent automa instance containing this worker. For top-level automa, parent is the automa itself.</p> <code>None</code> <code>arguments</code> <code>Dict[str, Any] = None</code> <p>Execution parameters with keys \"args\" and \"kwargs\".</p> <code>None</code> Source code in <code>bridgic/core/automa/worker/_worker_callback.py</code> <pre><code>async def on_worker_start(\n    self, \n    key: str,\n    is_top_level: bool = False,\n    parent: Optional[\"Automa\"] = None,\n    arguments: Dict[str, Any] = None,\n) -&gt; None:\n    \"\"\"\n    Hook invoked before worker execution.\n\n    Called immediately before the worker runs. Use for arguments\n    validation, logging, or monitoring. Cannot modify execution\n    arguments or logic.\n\n    Parameters\n    ----------\n    key : str\n        Worker identifier.\n    is_top_level: bool = False\n        Whether the worker is the top-level automa. When True, parent will be the automa itself (parent is self).\n    parent : Optional[Automa] = None\n        Parent automa instance containing this worker. For top-level automa, parent is the automa itself.\n    arguments : Dict[str, Any] = None\n        Execution parameters with keys \"args\" and \"kwargs\".\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/bridgic-core/bridgic/core/automa/worker/#bridgic.core.automa.worker.WorkerCallback.on_worker_end","title":"on_worker_end","text":"<code>async</code> <pre><code>on_worker_end(\n    key: str,\n    is_top_level: bool = False,\n    parent: Optional[Automa] = None,\n    arguments: Dict[str, Any] = None,\n    result: Any = None,\n) -&gt; None\n</code></pre> <p>Hook invoked after worker execution.</p> <p>Called immediately after the worker completes. Use for result monitoring, logging, event publishing, or validation. Cannot modify execution results or logic.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>Worker identifier.</p> required <code>is_top_level</code> <code>bool</code> <p>Whether the worker is the top-level automa. When True, parent will be the automa itself (parent is self).</p> <code>False</code> <code>parent</code> <code>Optional[Automa] = None</code> <p>Parent automa instance containing this worker. For top-level automa, parent is the automa itself.</p> <code>None</code> <code>arguments</code> <code>Dict[str, Any] = None</code> <p>Execution arguments with keys \"args\" and \"kwargs\".</p> <code>None</code> <code>result</code> <code>Any = None</code> <p>Worker execution result.</p> <code>None</code> Source code in <code>bridgic/core/automa/worker/_worker_callback.py</code> <pre><code>async def on_worker_end(\n    self,\n    key: str,\n    is_top_level: bool = False,\n    parent: Optional[\"Automa\"] = None,\n    arguments: Dict[str, Any] = None,\n    result: Any = None,\n) -&gt; None:\n    \"\"\"\n    Hook invoked after worker execution.\n\n    Called immediately after the worker completes. Use for result\n    monitoring, logging, event publishing, or validation. Cannot\n    modify execution results or logic.\n\n    Parameters\n    ----------\n    key : str\n        Worker identifier.\n    is_top_level: bool = False\n        Whether the worker is the top-level automa. When True, parent will be the automa itself (parent is self).\n    parent : Optional[Automa] = None\n        Parent automa instance containing this worker. For top-level automa, parent is the automa itself.\n    arguments : Dict[str, Any] = None\n        Execution arguments with keys \"args\" and \"kwargs\".\n    result : Any = None\n        Worker execution result.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/bridgic-core/bridgic/core/automa/worker/#bridgic.core.automa.worker.WorkerCallback.on_worker_error","title":"on_worker_error","text":"<code>async</code> <pre><code>on_worker_error(\n    key: str,\n    is_top_level: bool = False,\n    parent: Optional[Automa] = None,\n    arguments: Dict[str, Any] = None,\n    error: Exception = None,\n) -&gt; bool\n</code></pre> <p>Hook invoked when worker execution raises an exception.</p> <p>Called when the worker execution raises an exception. Use for error handling, logging,  or event publishing. Cannot modify execution logic or arguments.</p> <p>Exception Matching Mechanism: How to Handle a Specific Exception</p> <p>The framework enable your callback to handle a given exception based on the  type annotation of the <code>error</code> parameter in your <code>on_worker_error</code> method. The matching follows these rules:</p> <ul> <li>The parameter name MUST be <code>error</code> and the type annotation is critical for the    matching mechanism.</li> <li>If you annotate <code>error: ValueError</code>, it will match <code>ValueError</code> and all its    subclasses (e.g., <code>UnicodeDecodeError</code>).</li> <li>If you annotate <code>error: Exception</code>, it will match all exceptions (since all exceptions    inherit from Exception).</li> <li>If you want to match multiple exception types, you can use <code>Union[Type1, Type2, ...]</code>.</li> </ul> <p>Return Value: Whether to Suppress the Exception</p> <ul> <li>If <code>on_worker_error</code> returns <code>True</code>, the framework will suppress the exception.    The framework will then proceed as if there was no error, and the worker result    will be set to None.</li> <li>If <code>on_worker_error</code> returns <code>False</code>, the framework will simply observe the error;    after all matching callbacks are called, the framework will re-raise the exception.</li> </ul> <p>Special Case: Interaction Exceptions Cannot Be Suppressed</p> <p>To ensure human-interaction mechanisms work correctly, exceptions of type <code>_InteractionEventException</code> or <code>InteractionException</code> (including their subclasses)  CANNOT be suppressed by any callback. Even if your callback returns <code>True</code>, the  framework will forcibly re-raise the exception. This ensures these exceptions always  propagate correctly through the automa hierarchy to trigger necessary human interactions.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>Worker identifier.</p> required <code>is_top_level</code> <code>bool</code> <p>Whether the worker is the top-level automa. When True, parent will be the automa itself (parent is self).</p> <code>False</code> <code>parent</code> <code>Optional[Automa] = None</code> <p>Parent automa instance containing this worker. For top-level automa, parent is the automa itself.</p> <code>None</code> <code>arguments</code> <code>Dict[str, Any] = None</code> <p>Execution arguments with keys \"args\" and \"kwargs\".</p> <code>None</code> <code>error</code> <code>Exception = None</code> <p>The exception raised during worker execution. The type annotation of this parameter determines which exceptions this callback will handle. The matching is based on inheritance relationship (using isinstance), so a callback with <code>error: ValueError</code> will match ValueError and all its subclasses.</p> <code>None</code> <p>Returns:</p> Type Description <code>bool</code> <p>True if the automa should suppress the exception (not re-raise it); False otherwise.</p> Source code in <code>bridgic/core/automa/worker/_worker_callback.py</code> <pre><code>async def on_worker_error(\n    self,\n    key: str,\n    is_top_level: bool = False,\n    parent: Optional[\"Automa\"] = None,\n    arguments: Dict[str, Any] = None,\n    error: Exception = None,\n) -&gt; bool:\n    \"\"\"\n    Hook invoked when worker execution raises an exception.\n\n    Called when the worker execution raises an exception. Use for error handling, logging, \n    or event publishing. Cannot modify execution logic or arguments.\n\n    **Exception Matching Mechanism: How to Handle a Specific Exception**\n\n    The framework enable your callback to handle a given exception based on the \n    **type annotation** of the `error` parameter in your `on_worker_error` method.\n    The matching follows these rules:\n\n    - The parameter name MUST be `error` and the type annotation is critical for the \n      matching mechanism.\n    - If you annotate `error: ValueError`, it will match `ValueError` and all its \n      subclasses (e.g., `UnicodeDecodeError`).\n    - If you annotate `error: Exception`, it will match all exceptions (since all exceptions \n      inherit from Exception).\n    - If you want to match multiple exception types, you can use `Union[Type1, Type2, ...]`.\n\n\n    **Return Value: Whether to Suppress the Exception**\n\n    - If `on_worker_error` returns `True`, the framework will suppress the exception. \n      The framework will then proceed as if there was no error, and the worker result \n      will be set to None.\n    - If `on_worker_error` returns `False`, the framework will simply observe the error; \n      after all matching callbacks are called, the framework will re-raise the exception.\n\n    **Special Case: Interaction Exceptions Cannot Be Suppressed**\n\n    To ensure human-interaction mechanisms work correctly, exceptions of type\n    `_InteractionEventException` or `InteractionException` (including their subclasses) \n    **CANNOT** be suppressed by any callback. Even if your callback returns `True`, the \n    framework will forcibly re-raise the exception. This ensures these exceptions always \n    propagate correctly through the automa hierarchy to trigger necessary human interactions.\n\n    Parameters\n    ----------\n    key : str\n        Worker identifier.\n    is_top_level: bool = False\n        Whether the worker is the top-level automa. When True, parent will be the automa itself (parent is self).\n    parent : Optional[Automa] = None\n        Parent automa instance containing this worker. For top-level automa, parent is the automa itself.\n    arguments : Dict[str, Any] = None\n        Execution arguments with keys \"args\" and \"kwargs\".\n    error : Exception = None\n        The exception raised during worker execution. The type annotation of this\n        parameter determines which exceptions this callback will handle. The matching\n        is based on inheritance relationship (using isinstance), so a callback with\n        `error: ValueError` will match ValueError and all its subclasses.\n\n    Returns\n    -------\n    bool\n        True if the automa should suppress the exception (not re-raise it); False otherwise.\n    \"\"\"\n    return False\n</code></pre>"},{"location":"reference/bridgic-core/bridgic/core/automa/worker/#bridgic.core.automa.worker.WorkerCallbackBuilder","title":"WorkerCallbackBuilder","text":"<p>               Bases: <code>Generic[T_WorkerCallback]</code></p> <p>Builder class for creating instances of <code>WorkerCallback</code> subclasses.</p> <p>This builder is designed to construct instances of subclasses of <code>WorkerCallback</code>. The <code>_callback_type</code> parameter should be a subclass of <code>WorkerCallback</code>, and <code>build()</code>  will return an instance of that specific subclass. There is no need to call <code>build()</code>  directly. Instead, the framework calls the <code>build</code> method automatically to create  its own <code>WorkerCallback</code> instance for each worker instance.</p> Notes <p>Register a Callback in Different Scope</p> <p>There are three ways to register a callback for three levels of customization:</p> <ul> <li>Case 1: Use in worker decorator to register the callback for a specific worker.</li> <li>Case 2: Use in RunningOptions to register the callback for a specific Automa instance.</li> <li>Case 3: Use in GlobalSetting to register the callback for all workers.</li> </ul> Notes <p>Shared Instance Mode</p> <ul> <li>When <code>is_shared=True</code> (default), all workers within the same scope will share the same    callback instance. This is useful for scenarios where a single callback instance is needed    to maintain some state across workers within the same scope, such as the connection to    an external service. The scope is determined by where the builder is declared:</li> <li>If declared in GlobalSetting: shared across all workers globally</li> <li>If declared in RunningOptions: shared across all workers within that Automa instance</li> <li>When <code>is_shared=False</code>, each worker will get its own callback instance. This is useful for    scenarios where a independent callback instance is needed for each worker.</li> </ul> <p>Examples:</p> <p>There are three ways to use the builder, for different levels of customization:</p> <pre><code>&gt;&gt;&gt; # Define a custom callback class:\n&gt;&gt;&gt; class MyEmptyCallback(WorkerCallback):\n...     pass\n...\n&gt;&gt;&gt; # Case 1: Use in worker decorator to register the callback for a specific worker:\n&gt;&gt;&gt; class MyGraphAutoma(GraphAutoma):\n...     @worker(callback_builders=[WorkerCallbackBuilder(MyEmptyCallback)])\n...     async def my_worker(self, x: int) -&gt; int:\n...         return x + 1\n...\n&gt;&gt;&gt; # Case 2: Use in RunningOptions to register the callback for a specific Automa instance:\n...     running_options = RunningOptions(callback_builders=[WorkerCallbackBuilder(MyEmptyCallback)])\n...     graph = MyGraphAutoma(running_options=running_options)\n...\n&gt;&gt;&gt; # Case 3: Use in GlobalSetting to register the callback for all workers:\n&gt;&gt;&gt; GlobalSetting.set(callback_builders=[WorkerCallbackBuilder(MyEmptyCallback)])\n</code></pre> Source code in <code>bridgic/core/automa/worker/_worker_callback.py</code> <pre><code>class WorkerCallbackBuilder(Generic[T_WorkerCallback]):\n    \"\"\"\n    Builder class for creating instances of `WorkerCallback` subclasses.\n\n    This builder is designed to construct instances of subclasses of `WorkerCallback`.\n    The `_callback_type` parameter should be a subclass of `WorkerCallback`, and `build()` \n    will return an instance of that specific subclass. There is no need to call `build()` \n    directly. Instead, the framework calls the `build` method automatically to create \n    its own `WorkerCallback` instance for each worker instance.\n\n    Notes\n    -----\n    **Register a Callback in Different Scope**\n\n    There are three ways to register a callback for three levels of customization:\n\n    - Case 1: Use in worker decorator to register the callback for a specific worker.\n    - Case 2: Use in RunningOptions to register the callback for a specific Automa instance.\n    - Case 3: Use in GlobalSetting to register the callback for all workers.\n\n    Notes\n    -----\n    **Shared Instance Mode**\n\n    - When `is_shared=True` (default), all workers within the same scope will share the same \n      callback instance. This is useful for scenarios where a single callback instance is needed \n      to maintain some state across workers within the same scope, such as the connection to \n      an external service. The scope is determined by where the builder is declared:\n      - If declared in GlobalSetting: shared across all workers globally\n      - If declared in RunningOptions: shared across all workers within that Automa instance\n    - When `is_shared=False`, each worker will get its own callback instance. This is useful for \n      scenarios where a independent callback instance is needed for each worker.\n\n    Examples\n    --------\n    There are three ways to use the builder, for different levels of customization:\n\n    &gt;&gt;&gt; # Define a custom callback class:\n    &gt;&gt;&gt; class MyEmptyCallback(WorkerCallback):\n    ...     pass\n    ...\n    &gt;&gt;&gt; # Case 1: Use in worker decorator to register the callback for a specific worker:\n    &gt;&gt;&gt; class MyGraphAutoma(GraphAutoma):\n    ...     @worker(callback_builders=[WorkerCallbackBuilder(MyEmptyCallback)])\n    ...     async def my_worker(self, x: int) -&gt; int:\n    ...         return x + 1\n    ...\n    &gt;&gt;&gt; # Case 2: Use in RunningOptions to register the callback for a specific Automa instance:\n    ...     running_options = RunningOptions(callback_builders=[WorkerCallbackBuilder(MyEmptyCallback)])\n    ...     graph = MyGraphAutoma(running_options=running_options)\n    ...\n    &gt;&gt;&gt; # Case 3: Use in GlobalSetting to register the callback for all workers:\n    &gt;&gt;&gt; GlobalSetting.set(callback_builders=[WorkerCallbackBuilder(MyEmptyCallback)])\n    \"\"\"\n    _callback_type: Type[T_WorkerCallback]\n    \"\"\"The specific subclass of `WorkerCallback` to instantiate.\"\"\"\n    _init_kwargs: Dict[str, Any]\n    \"\"\"The initialization arguments for the instance.\"\"\"\n    _is_shared: bool\n    \"\"\"Whether to use shared instance mode (reuse the same instance within the declaration scope).\"\"\"\n\n    _shared_instance: Optional[T_WorkerCallback] = None\n    \"\"\"Shared instance of the callback within the declaration scope.\"\"\"\n    _shared_lock: Lock = Lock()\n    \"\"\"Lock for thread-safe shared instance creation.\"\"\"\n\n    def __init__(\n        self,\n        callback_type: Type[T_WorkerCallback],\n        init_kwargs: Optional[Dict[str, Any]] = None,\n        is_shared: bool = True,\n    ):\n        \"\"\"\n        Initialize the builder with a `WorkerCallback` subclass type.\n\n        Parameters\n        ----------\n        callback_type : Type[T_WorkerCallback]\n            A subclass of `WorkerCallback` to be instantiated.\n        init_kwargs : Optional[Dict[str, Any]]\n            Keyword arguments to pass to the subclass constructor.\n        is_shared : bool, default True\n            If True, the callback instance will be shared within the declaration scope:\n            If False, each worker will get its own callback instance.\n        \"\"\"\n        self._callback_type = callback_type\n        self._init_kwargs = init_kwargs or {}\n        self._is_shared = is_shared\n\n    def build(self) -&gt; T_WorkerCallback:\n        \"\"\"\n        Build and return an instance of the specified `WorkerCallback` subclass.\n\n        Returns\n        -------\n        T_WorkerCallback\n            An instance of the `WorkerCallback` subclass specified during initialization.\n        \"\"\"\n        if self._is_shared:\n            if self._shared_instance is None:\n                with self._shared_lock:\n                    if self._shared_instance is None:\n                        self._shared_instance = self._callback_type(**self._init_kwargs)\n            return self._shared_instance\n        else:\n            return self._callback_type(**self._init_kwargs)\n\n    @override\n    def dump_to_dict(self) -&gt; Dict[str, Any]:\n        return {\n            \"callback_type\": self._callback_type.__module__ + \".\" + self._callback_type.__qualname__,\n            \"init_kwargs\": self._init_kwargs,\n            \"is_shared\": self._is_shared,\n        }\n\n    @override\n    def load_from_dict(self, state_dict: Dict[str, Any]) -&gt; None:\n        # Load the callback type from its fully qualified name\n        callback_type_name = state_dict[\"callback_type\"]\n        self._callback_type = load_qualified_class_or_func(callback_type_name)\n\n        # Load init_kwargs (default to empty dict if not present or None)\n        init_kwargs = state_dict.get(\"init_kwargs\")\n        self._init_kwargs = init_kwargs if init_kwargs is not None else {}\n\n        # Load is_shared (default to True if not present for backward compatibility)\n        self._is_shared = state_dict.get(\"is_shared\", True)\n\n        # Reset shared instance and lock (they will be recreated when needed)\n        self._shared_instance = None\n        self._shared_lock = Lock()\n</code></pre>"},{"location":"reference/bridgic-core/bridgic/core/automa/worker/#bridgic.core.automa.worker.WorkerCallbackBuilder.build","title":"build","text":"<pre><code>build() -&gt; T_WorkerCallback\n</code></pre> <p>Build and return an instance of the specified <code>WorkerCallback</code> subclass.</p> <p>Returns:</p> Type Description <code>T_WorkerCallback</code> <p>An instance of the <code>WorkerCallback</code> subclass specified during initialization.</p> Source code in <code>bridgic/core/automa/worker/_worker_callback.py</code> <pre><code>def build(self) -&gt; T_WorkerCallback:\n    \"\"\"\n    Build and return an instance of the specified `WorkerCallback` subclass.\n\n    Returns\n    -------\n    T_WorkerCallback\n        An instance of the `WorkerCallback` subclass specified during initialization.\n    \"\"\"\n    if self._is_shared:\n        if self._shared_instance is None:\n            with self._shared_lock:\n                if self._shared_instance is None:\n                    self._shared_instance = self._callback_type(**self._init_kwargs)\n        return self._shared_instance\n    else:\n        return self._callback_type(**self._init_kwargs)\n</code></pre>"},{"location":"reference/bridgic-core/bridgic/core/config/","title":"config","text":"<p>Configuration management module for Bridgic.</p>"},{"location":"reference/bridgic-core/bridgic/core/config/#bridgic.core.config.GlobalSetting","title":"GlobalSetting","text":"<p>               Bases: <code>BaseModel</code></p> <p>Global configuration settings for the Bridgic framework.</p> <p>This class implements a singleton pattern to provide centralized configuration that applies across all Automa instances. The main methods are:</p> <ul> <li><code>GlobalSetting.read()</code>: Get the singleton global setting instance.</li> <li><code>GlobalSetting.set()</code>: Set the specific fields of the global setting instance.</li> </ul> <p>Attributes:</p> Name Type Description <code>callback_builders</code> <code>List[WorkerCallbackBuilder]</code> <p>Callback builders that will be automatically applied to all workers across all Automa instances.</p> Source code in <code>bridgic/core/config/_global_setting.py</code> <pre><code>class GlobalSetting(BaseModel):\n    \"\"\"\n    Global configuration settings for the Bridgic framework.\n\n    This class implements a singleton pattern to provide centralized configuration\n    that applies across all Automa instances. The main methods are:\n\n    - `GlobalSetting.read()`: Get the singleton global setting instance.\n    - `GlobalSetting.set()`: Set the specific fields of the global setting instance.\n\n    Attributes\n    ----------\n    callback_builders : List[WorkerCallbackBuilder]\n        Callback builders that will be automatically applied to all workers\n        across all Automa instances.\n    \"\"\"\n    model_config = {\"arbitrary_types_allowed\": True}\n\n    callback_builders: List[\"WorkerCallbackBuilder\"] = []\n    \"\"\"Global callback builders that will be applied to all workers.\"\"\"\n\n    # Singleton instance\n    _instance: ClassVar[Optional[\"GlobalSetting\"]] = None\n    _lock: ClassVar[Lock] = Lock()\n\n    @classmethod\n    def read(cls) -&gt; \"GlobalSetting\":\n        \"\"\"\n        Get the singleton global setting instance.\n\n        Returns\n        -------\n        GlobalSetting\n            The singleton global setting instance.\n        \"\"\"\n        if cls._instance is None:\n            with cls._lock:\n                if cls._instance is None:\n                    cls._instance = cls()\n        return cls._instance\n\n    @classmethod\n    def set(\n        cls,\n        callback_builders: Optional[List[\"WorkerCallbackBuilder\"]] = None,\n    ) -&gt; None:\n        \"\"\"\n        Set global setting fields.\n\n        This method allows you to update specific fields of the global setting\n        without needing to create a complete GlobalSetting object.\n\n        Parameters\n        ----------\n        callback_builders : Optional[List[WorkerCallbackBuilder]], optional\n            Global callback builders that will be applied to all workers.\n            If None, the current callback_builders are not changed.\n        \"\"\"\n        instance = cls.read()\n        with cls._lock:\n            if callback_builders is not None:\n                instance.callback_builders = callback_builders\n\n    @classmethod\n    def add(cls, callback_builder: Optional[\"WorkerCallbackBuilder\"] = None) -&gt; None:\n        \"\"\"\n        Add new element to the existing field(s) of the `GlobalSetting`.\n\n        Parameters\n        ----------\n        callback_builder : Optional[WorkerCallbackBuilder]\n            The callback builder to add to the global setting callback builders. If None is passed in, nothing will be done.\n        \"\"\"\n        instance = cls.read()\n        with cls._lock:\n            if callback_builder is not None:\n                instance.callback_builders.append(callback_builder)\n</code></pre>"},{"location":"reference/bridgic-core/bridgic/core/config/#bridgic.core.config.GlobalSetting.callback_builders","title":"callback_builders  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>callback_builders: List[WorkerCallbackBuilder] = []\n</code></pre> <p>Global callback builders that will be applied to all workers.</p>"},{"location":"reference/bridgic-core/bridgic/core/config/#bridgic.core.config.GlobalSetting.read","title":"read","text":"<code>classmethod</code> <pre><code>read() -&gt; GlobalSetting\n</code></pre> <p>Get the singleton global setting instance.</p> <p>Returns:</p> Type Description <code>GlobalSetting</code> <p>The singleton global setting instance.</p> Source code in <code>bridgic/core/config/_global_setting.py</code> <pre><code>@classmethod\ndef read(cls) -&gt; \"GlobalSetting\":\n    \"\"\"\n    Get the singleton global setting instance.\n\n    Returns\n    -------\n    GlobalSetting\n        The singleton global setting instance.\n    \"\"\"\n    if cls._instance is None:\n        with cls._lock:\n            if cls._instance is None:\n                cls._instance = cls()\n    return cls._instance\n</code></pre>"},{"location":"reference/bridgic-core/bridgic/core/config/#bridgic.core.config.GlobalSetting.set","title":"set","text":"<code>classmethod</code> <pre><code>set(\n    callback_builders: Optional[\n        List[WorkerCallbackBuilder]\n    ] = None,\n) -&gt; None\n</code></pre> <p>Set global setting fields.</p> <p>This method allows you to update specific fields of the global setting without needing to create a complete GlobalSetting object.</p> <p>Parameters:</p> Name Type Description Default <code>callback_builders</code> <code>Optional[List[WorkerCallbackBuilder]]</code> <p>Global callback builders that will be applied to all workers. If None, the current callback_builders are not changed.</p> <code>None</code> Source code in <code>bridgic/core/config/_global_setting.py</code> <pre><code>@classmethod\ndef set(\n    cls,\n    callback_builders: Optional[List[\"WorkerCallbackBuilder\"]] = None,\n) -&gt; None:\n    \"\"\"\n    Set global setting fields.\n\n    This method allows you to update specific fields of the global setting\n    without needing to create a complete GlobalSetting object.\n\n    Parameters\n    ----------\n    callback_builders : Optional[List[WorkerCallbackBuilder]], optional\n        Global callback builders that will be applied to all workers.\n        If None, the current callback_builders are not changed.\n    \"\"\"\n    instance = cls.read()\n    with cls._lock:\n        if callback_builders is not None:\n            instance.callback_builders = callback_builders\n</code></pre>"},{"location":"reference/bridgic-core/bridgic/core/config/#bridgic.core.config.GlobalSetting.add","title":"add","text":"<code>classmethod</code> <pre><code>add(\n    callback_builder: Optional[\n        WorkerCallbackBuilder\n    ] = None,\n) -&gt; None\n</code></pre> <p>Add new element to the existing field(s) of the <code>GlobalSetting</code>.</p> <p>Parameters:</p> Name Type Description Default <code>callback_builder</code> <code>Optional[WorkerCallbackBuilder]</code> <p>The callback builder to add to the global setting callback builders. If None is passed in, nothing will be done.</p> <code>None</code> Source code in <code>bridgic/core/config/_global_setting.py</code> <pre><code>@classmethod\ndef add(cls, callback_builder: Optional[\"WorkerCallbackBuilder\"] = None) -&gt; None:\n    \"\"\"\n    Add new element to the existing field(s) of the `GlobalSetting`.\n\n    Parameters\n    ----------\n    callback_builder : Optional[WorkerCallbackBuilder]\n        The callback builder to add to the global setting callback builders. If None is passed in, nothing will be done.\n    \"\"\"\n    instance = cls.read()\n    with cls._lock:\n        if callback_builder is not None:\n            instance.callback_builders.append(callback_builder)\n</code></pre>"},{"location":"reference/bridgic-core/bridgic/core/model/","title":"model","text":"<p>The Model module provides core abstraction entities for LLMs (Large Language Models).</p> <p>This module defines core abstraction entities for interacting with models, providing  foundational type abstractions for different model implementations.</p>"},{"location":"reference/bridgic-core/bridgic/core/model/#bridgic.core.model.BaseLlm","title":"BaseLlm","text":"<p>               Bases: <code>ABC</code>, <code>Serializable</code></p> <p>Base class for Large Language Model implementations.</p> Source code in <code>bridgic/core/model/_base_llm.py</code> <pre><code>class BaseLlm(ABC, Serializable):\n    \"\"\"\n    Base class for Large Language Model implementations.\n    \"\"\"\n\n    @abstractmethod\n    def chat(self, messages: List[Message], **kwargs) -&gt; Response:\n        ...\n\n    @abstractmethod\n    def stream(self, messages: List[Message], **kwargs) -&gt; StreamResponse:\n        ...\n\n    @abstractmethod\n    async def achat(self, messages: List[Message], **kwargs) -&gt; Response:\n        ...\n\n    @abstractmethod\n    async def astream(self, messages: List[Message], **kwargs) -&gt; AsyncStreamResponse:\n        ...\n</code></pre>"},{"location":"reference/bridgic-core/bridgic/core/model/protocols/","title":"protocols","text":"<p>The Model Protocols module defines high-level interface protocols for model interaction.</p> <p>This module contains several important interface protocol definitions to provide  capabilities needed in real-world application development, such as tool selection and  structured output. These interfaces have clear input and output definitions and are  \"model-neutral\", aiming to reduce the details developers need to consider when  implementing features, thereby improving development efficiency.</p>"},{"location":"reference/bridgic-core/bridgic/core/model/protocols/#bridgic.core.model.protocols.Constraint","title":"Constraint  <code>module-attribute</code>","text":"<pre><code>Constraint = Union[\n    PydanticModel,\n    JsonSchema,\n    EbnfGrammar,\n    LarkGrammar,\n    Regex,\n    Choice,\n]\n</code></pre> <p>The constraint type for structured LLM output.</p>"},{"location":"reference/bridgic-core/bridgic/core/model/protocols/#bridgic.core.model.protocols.ToolSelection","title":"ToolSelection","text":"<p>               Bases: <code>Protocol</code></p> <p>Protocol for LLM providers that support tool selection and parameter determination.</p> <p>ToolSelection defines the interface for language models that can intelligently  select appropriate tools from a given tools and determine the specific parameters  needed for tool execution.</p> <p>Methods:</p> Name Description <code>select_tool</code> <p>Synchronous method for tool selection based on conversation context.</p> <code>aselect_tool</code> <p>Asynchronous method for tool selection based on conversation context.</p> Notes <ol> <li>Both synchronous and asynchronous methods must be implemented</li> <li>Tool selection should be based on conversation context and available tools</li> <li>Return value includes both selected tool calls and optional response text</li> </ol> Source code in <code>bridgic/core/model/protocols/_tool_selection.py</code> <pre><code>class ToolSelection(Protocol):\n    \"\"\"\n    Protocol for LLM providers that support tool selection and parameter determination.\n\n    ToolSelection defines the interface for language models that can intelligently \n    select appropriate tools from a given tools and determine the specific parameters \n    needed for tool execution.\n\n    Methods\n    -------\n    select_tool\n        Synchronous method for tool selection based on conversation context.\n    aselect_tool\n        Asynchronous method for tool selection based on conversation context.\n\n    Notes\n    ----\n    1. Both synchronous and asynchronous methods must be implemented\n    2. Tool selection should be based on conversation context and available tools\n    3. Return value includes both selected tool calls and optional response text\n    \"\"\"\n\n    def select_tool(\n        self,\n        messages: List[Message],\n        tools: List[Tool],\n        **kwargs,\n    ) -&gt; Tuple[List[ToolCall], Optional[str]]:\n        \"\"\"\n        Select appropriate tools and determine their parameters based on conversation context.\n\n        Parameters\n        ----------\n        messages : List[Message]\n            The conversation history and current context.\n        tools : List[Tool]\n            Available tools that can be selected for use.\n        **kwargs\n            Additional keyword arguments for tool selection configuration.\n\n        Returns\n        -------\n        Tuple[List[ToolCall], Optional[str]]\n            A tuple containing:\n            - List of selected tool calls with determined parameters\n            - Optional response text from the LLM\n        \"\"\"\n        ...\n\n    async def aselect_tool(\n        self,\n        messages: List[Message],\n        tools: List[Tool],\n        **kwargs,\n    ) -&gt; Tuple[List[ToolCall], Optional[str]]:\n        \"\"\"\n        Asynchronously select appropriate tools and determine their parameters.\n\n        Parameters\n        ----------\n        messages : List[Message]\n            The conversation history and current context.\n        tools : List[Tool]\n            Available tools that can be selected for use.\n        **kwargs\n            Additional keyword arguments for tool selection configuration.\n\n        Returns\n        -------\n        Tuple[List[ToolCall], Optional[str]]\n            A tuple containing:\n            - List of selected tool calls with determined parameters\n            - Optional response text from the LLM\n        \"\"\"\n        ...\n</code></pre>"},{"location":"reference/bridgic-core/bridgic/core/model/protocols/#bridgic.core.model.protocols.ToolSelection.select_tool","title":"select_tool","text":"<pre><code>select_tool(\n    messages: List[Message], tools: List[Tool], **kwargs\n) -&gt; Tuple[List[ToolCall], Optional[str]]\n</code></pre> <p>Select appropriate tools and determine their parameters based on conversation context.</p> <p>Parameters:</p> Name Type Description Default <code>messages</code> <code>List[Message]</code> <p>The conversation history and current context.</p> required <code>tools</code> <code>List[Tool]</code> <p>Available tools that can be selected for use.</p> required <code>**kwargs</code> <p>Additional keyword arguments for tool selection configuration.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Tuple[List[ToolCall], Optional[str]]</code> <p>A tuple containing: - List of selected tool calls with determined parameters - Optional response text from the LLM</p> Source code in <code>bridgic/core/model/protocols/_tool_selection.py</code> <pre><code>def select_tool(\n    self,\n    messages: List[Message],\n    tools: List[Tool],\n    **kwargs,\n) -&gt; Tuple[List[ToolCall], Optional[str]]:\n    \"\"\"\n    Select appropriate tools and determine their parameters based on conversation context.\n\n    Parameters\n    ----------\n    messages : List[Message]\n        The conversation history and current context.\n    tools : List[Tool]\n        Available tools that can be selected for use.\n    **kwargs\n        Additional keyword arguments for tool selection configuration.\n\n    Returns\n    -------\n    Tuple[List[ToolCall], Optional[str]]\n        A tuple containing:\n        - List of selected tool calls with determined parameters\n        - Optional response text from the LLM\n    \"\"\"\n    ...\n</code></pre>"},{"location":"reference/bridgic-core/bridgic/core/model/protocols/#bridgic.core.model.protocols.ToolSelection.aselect_tool","title":"aselect_tool","text":"<code>async</code> <pre><code>aselect_tool(\n    messages: List[Message], tools: List[Tool], **kwargs\n) -&gt; Tuple[List[ToolCall], Optional[str]]\n</code></pre> <p>Asynchronously select appropriate tools and determine their parameters.</p> <p>Parameters:</p> Name Type Description Default <code>messages</code> <code>List[Message]</code> <p>The conversation history and current context.</p> required <code>tools</code> <code>List[Tool]</code> <p>Available tools that can be selected for use.</p> required <code>**kwargs</code> <p>Additional keyword arguments for tool selection configuration.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Tuple[List[ToolCall], Optional[str]]</code> <p>A tuple containing: - List of selected tool calls with determined parameters - Optional response text from the LLM</p> Source code in <code>bridgic/core/model/protocols/_tool_selection.py</code> <pre><code>async def aselect_tool(\n    self,\n    messages: List[Message],\n    tools: List[Tool],\n    **kwargs,\n) -&gt; Tuple[List[ToolCall], Optional[str]]:\n    \"\"\"\n    Asynchronously select appropriate tools and determine their parameters.\n\n    Parameters\n    ----------\n    messages : List[Message]\n        The conversation history and current context.\n    tools : List[Tool]\n        Available tools that can be selected for use.\n    **kwargs\n        Additional keyword arguments for tool selection configuration.\n\n    Returns\n    -------\n    Tuple[List[ToolCall], Optional[str]]\n        A tuple containing:\n        - List of selected tool calls with determined parameters\n        - Optional response text from the LLM\n    \"\"\"\n    ...\n</code></pre>"},{"location":"reference/bridgic-core/bridgic/core/model/protocols/#bridgic.core.model.protocols.PydanticModel","title":"PydanticModel","text":"<p>               Bases: <code>BaseModel</code></p> <p>A constraint defined as a Pydantic model for structured LLM output.</p> Source code in <code>bridgic/core/model/protocols/_structured_output.py</code> <pre><code>class PydanticModel(BaseModel):\n    \"\"\"\n    A constraint defined as a Pydantic model for structured LLM output.\n    \"\"\"\n    constraint_type: Literal[\"pydantic_model\"] = \"pydantic_model\"\n    \"\"\"The type of the constraint, in this case `pydantic_model`.\"\"\"\n    model: Type[BaseModel] = Field(..., description=\"Model type of the PydanticModel constraint.\")\n    \"\"\"The Pydantic model type of the constraint.\"\"\"\n</code></pre>"},{"location":"reference/bridgic-core/bridgic/core/model/protocols/#bridgic.core.model.protocols.PydanticModel.constraint_type","title":"constraint_type  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>constraint_type: Literal[\"pydantic_model\"] = (\n    \"pydantic_model\"\n)\n</code></pre> <p>The type of the constraint, in this case <code>pydantic_model</code>.</p>"},{"location":"reference/bridgic-core/bridgic/core/model/protocols/#bridgic.core.model.protocols.PydanticModel.model","title":"model  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>model: Type[BaseModel] = Field(\n    ...,\n    description=\"Model type of the PydanticModel constraint.\",\n)\n</code></pre> <p>The Pydantic model type of the constraint.</p>"},{"location":"reference/bridgic-core/bridgic/core/model/protocols/#bridgic.core.model.protocols.JsonSchema","title":"JsonSchema","text":"<p>               Bases: <code>BaseModel</code></p> <p>A constraint defined as a JSON schema for structured LLM output.</p> Source code in <code>bridgic/core/model/protocols/_structured_output.py</code> <pre><code>class JsonSchema(BaseModel):\n    \"\"\"\n    A constraint defined as a JSON schema for structured LLM output.\n    \"\"\"\n    constraint_type: Literal[\"json_schema\"] = \"json_schema\"\n    \"\"\"The type of the constraint, in this case `json_schema`.\"\"\"\n    schema_dict: Dict[str, Any] = Field(..., description=\"Schema of the JsonSchema constraint.\")\n    \"\"\"The JSON schema of the constraint.\"\"\"\n</code></pre>"},{"location":"reference/bridgic-core/bridgic/core/model/protocols/#bridgic.core.model.protocols.JsonSchema.constraint_type","title":"constraint_type  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>constraint_type: Literal['json_schema'] = 'json_schema'\n</code></pre> <p>The type of the constraint, in this case <code>json_schema</code>.</p>"},{"location":"reference/bridgic-core/bridgic/core/model/protocols/#bridgic.core.model.protocols.JsonSchema.schema_dict","title":"schema_dict  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>schema_dict: Dict[str, Any] = Field(\n    ..., description=\"Schema of the JsonSchema constraint.\"\n)\n</code></pre> <p>The JSON schema of the constraint.</p>"},{"location":"reference/bridgic-core/bridgic/core/model/protocols/#bridgic.core.model.protocols.EbnfGrammar","title":"EbnfGrammar","text":"<p>               Bases: <code>BaseModel</code></p> <p>A constraint defined as an EBNF grammar for structured LLM output.</p> Source code in <code>bridgic/core/model/protocols/_structured_output.py</code> <pre><code>class EbnfGrammar(BaseModel):\n    \"\"\"\n    A constraint defined as an EBNF grammar for structured LLM output.\n    \"\"\"\n    constraint_type: Literal[\"ebnf_grammar\"] = \"ebnf_grammar\"\n    \"\"\"The type of the constraint, in this case `ebnf_grammar`.\"\"\"\n    syntax: str = Field(..., description=\"Syntax of the EBNF grammar constraint.\")\n    \"\"\"The syntax of the EBNF grammar constraint.\"\"\"\n</code></pre>"},{"location":"reference/bridgic-core/bridgic/core/model/protocols/#bridgic.core.model.protocols.EbnfGrammar.constraint_type","title":"constraint_type  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>constraint_type: Literal['ebnf_grammar'] = 'ebnf_grammar'\n</code></pre> <p>The type of the constraint, in this case <code>ebnf_grammar</code>.</p>"},{"location":"reference/bridgic-core/bridgic/core/model/protocols/#bridgic.core.model.protocols.EbnfGrammar.syntax","title":"syntax  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>syntax: str = Field(\n    ...,\n    description=\"Syntax of the EBNF grammar constraint.\",\n)\n</code></pre> <p>The syntax of the EBNF grammar constraint.</p>"},{"location":"reference/bridgic-core/bridgic/core/model/protocols/#bridgic.core.model.protocols.LarkGrammar","title":"LarkGrammar","text":"<p>               Bases: <code>BaseModel</code></p> <p>A constraint defined as a Lark grammar for structured LLM output.</p> Source code in <code>bridgic/core/model/protocols/_structured_output.py</code> <pre><code>class LarkGrammar(BaseModel):\n    \"\"\"\n    A constraint defined as a Lark grammar for structured LLM output.\n    \"\"\"\n    constraint_type: Literal[\"lark_grammar\"] = \"lark_grammar\"\n    \"\"\"The type of the constraint, in this case `lark_grammar`.\"\"\"\n    syntax: str = Field(..., description=\"Syntax of the Lark grammar constraint.\")\n    \"\"\"The syntax of the Lark grammar constraint.\"\"\"\n</code></pre>"},{"location":"reference/bridgic-core/bridgic/core/model/protocols/#bridgic.core.model.protocols.LarkGrammar.constraint_type","title":"constraint_type  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>constraint_type: Literal['lark_grammar'] = 'lark_grammar'\n</code></pre> <p>The type of the constraint, in this case <code>lark_grammar</code>.</p>"},{"location":"reference/bridgic-core/bridgic/core/model/protocols/#bridgic.core.model.protocols.LarkGrammar.syntax","title":"syntax  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>syntax: str = Field(\n    ...,\n    description=\"Syntax of the Lark grammar constraint.\",\n)\n</code></pre> <p>The syntax of the Lark grammar constraint.</p>"},{"location":"reference/bridgic-core/bridgic/core/model/protocols/#bridgic.core.model.protocols.Regex","title":"Regex","text":"<p>               Bases: <code>BaseModel</code></p> <p>A constraint defined as a regular expression for structured LLM output.</p> Source code in <code>bridgic/core/model/protocols/_structured_output.py</code> <pre><code>class Regex(BaseModel):\n    \"\"\"\n    A constraint defined as a regular expression for structured LLM output.\n    \"\"\"\n    constraint_type: Literal[\"regex\"] = \"regex\"\n    \"\"\"The type of the constraint, in this case `regex`.\"\"\"\n    pattern: str = Field(..., description=\"Pattern of the Regex constraint.\")\n    \"\"\"The regular expression of the constraint.\"\"\"\n</code></pre>"},{"location":"reference/bridgic-core/bridgic/core/model/protocols/#bridgic.core.model.protocols.Regex.constraint_type","title":"constraint_type  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>constraint_type: Literal['regex'] = 'regex'\n</code></pre> <p>The type of the constraint, in this case <code>regex</code>.</p>"},{"location":"reference/bridgic-core/bridgic/core/model/protocols/#bridgic.core.model.protocols.Regex.pattern","title":"pattern  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>pattern: str = Field(\n    ..., description=\"Pattern of the Regex constraint.\"\n)\n</code></pre> <p>The regular expression of the constraint.</p>"},{"location":"reference/bridgic-core/bridgic/core/model/protocols/#bridgic.core.model.protocols.Choice","title":"Choice","text":"<p>               Bases: <code>BaseModel</code></p> <p>A constraint defined as a predefined set of choices for structured LLM output.</p> Source code in <code>bridgic/core/model/protocols/_structured_output.py</code> <pre><code>class Choice(BaseModel):\n    \"\"\"\n    A constraint defined as a predefined set of choices for structured LLM output.\n    \"\"\"\n    constraint_type: Literal[\"choice\"] = \"choice\"\n    \"\"\"The type of the constraint, in this case `choice`.\"\"\"\n    choices: List[str] = Field(..., description=\"Choices of the choice constraint.\")\n    \"\"\"The choices of the constraint.\"\"\"\n</code></pre>"},{"location":"reference/bridgic-core/bridgic/core/model/protocols/#bridgic.core.model.protocols.Choice.constraint_type","title":"constraint_type  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>constraint_type: Literal['choice'] = 'choice'\n</code></pre> <p>The type of the constraint, in this case <code>choice</code>.</p>"},{"location":"reference/bridgic-core/bridgic/core/model/protocols/#bridgic.core.model.protocols.Choice.choices","title":"choices  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>choices: List[str] = Field(\n    ..., description=\"Choices of the choice constraint.\"\n)\n</code></pre> <p>The choices of the constraint.</p>"},{"location":"reference/bridgic-core/bridgic/core/model/protocols/#bridgic.core.model.protocols.RegexPattern","title":"RegexPattern","text":"<p>Constants that define some common regular expressions for structured LLM output.</p> Source code in <code>bridgic/core/model/protocols/_structured_output.py</code> <pre><code>class RegexPattern:\n    \"\"\"Constants that define some common regular expressions for structured LLM output.\"\"\"\n    INTEGER: ClassVar[Regex] = Regex(pattern=r\"-?\\d+\")\n    \"\"\"A regular expression for integers.\"\"\"\n    FLOAT = Regex(pattern=r\"-?(?:\\d+\\.\\d+|\\d+\\.|\\.\\d+|\\d+)([eE][-+]?\\d+)?\")\n    \"\"\"A regular expression for floats.\"\"\"\n    DATE: ClassVar[Regex] = Regex(pattern=r\"\\d{4}-\\d{2}-\\d{2}\")\n    \"\"\"A regular expression for dates.\"\"\"\n    TIME: ClassVar[Regex] = Regex(pattern=r\"(?:[01]\\d|2[0-3]):[0-5]\\d:[0-5]\\d(?:\\.\\d+)?\")\n    \"\"\"A regular expression for times.\"\"\"\n    DATE_TIME_ISO_8601: ClassVar[Regex] = Regex(pattern=rf\"{DATE.pattern}T{TIME.pattern}(?:Z|[+-](?:[01]\\d|2[0-3]):[0-5]\\d)?\")\n    \"\"\"A regular expression for date-time in ISO 8601 format.\"\"\"\n    IP_V4_ADDRESS: ClassVar[Regex] = Regex(pattern=r\"(?:(?:25[0-5]|2[0-4]\\d|1\\d{2}|[1-9]?\\d)\\.){3}(?:25[0-5]|2[0-4]\\d|1\\d{2}|[1-9]?\\d)\")\n    \"\"\"A regular expression for IPv4 addresses.\"\"\"\n    IP_V6_ADDRESS: ClassVar[Regex] = Regex(pattern=r\"([0-9a-fA-F]{1,4}:){7}[0-9a-fA-F]{1,4}\")\n    \"\"\"A regular expression for IPv6 addresses.\"\"\"\n    EMAIL: ClassVar[Regex] = Regex(pattern=r\"[a-zA-Z0-9_.+-]+@[a-zA-Z0-9-]+\\.[a-zA-Z0-9-.]+\")\n    \"\"\"A regular expression for email addresses.\"\"\"\n</code></pre>"},{"location":"reference/bridgic-core/bridgic/core/model/protocols/#bridgic.core.model.protocols.RegexPattern.INTEGER","title":"INTEGER  <code>class-attribute</code>","text":"<pre><code>INTEGER: Regex = Regex(pattern='-?\\\\d+')\n</code></pre> <p>A regular expression for integers.</p>"},{"location":"reference/bridgic-core/bridgic/core/model/protocols/#bridgic.core.model.protocols.RegexPattern.FLOAT","title":"FLOAT  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>FLOAT = Regex(\n    pattern=\"-?(?:\\\\d+\\\\.\\\\d+|\\\\d+\\\\.|\\\\.\\\\d+|\\\\d+)([eE][-+]?\\\\d+)?\"\n)\n</code></pre> <p>A regular expression for floats.</p>"},{"location":"reference/bridgic-core/bridgic/core/model/protocols/#bridgic.core.model.protocols.RegexPattern.DATE","title":"DATE  <code>class-attribute</code>","text":"<pre><code>DATE: Regex = Regex(pattern='\\\\d{4}-\\\\d{2}-\\\\d{2}')\n</code></pre> <p>A regular expression for dates.</p>"},{"location":"reference/bridgic-core/bridgic/core/model/protocols/#bridgic.core.model.protocols.RegexPattern.TIME","title":"TIME  <code>class-attribute</code>","text":"<pre><code>TIME: Regex = Regex(\n    pattern=\"(?:[01]\\\\d|2[0-3]):[0-5]\\\\d:[0-5]\\\\d(?:\\\\.\\\\d+)?\"\n)\n</code></pre> <p>A regular expression for times.</p>"},{"location":"reference/bridgic-core/bridgic/core/model/protocols/#bridgic.core.model.protocols.RegexPattern.DATE_TIME_ISO_8601","title":"DATE_TIME_ISO_8601  <code>class-attribute</code>","text":"<pre><code>DATE_TIME_ISO_8601: Regex = Regex(\n    pattern=f\"{pattern}T{pattern}(?:Z|[+-](?:[01]\\d|2[0-3]):[0-5]\\d)?\"\n)\n</code></pre> <p>A regular expression for date-time in ISO 8601 format.</p>"},{"location":"reference/bridgic-core/bridgic/core/model/protocols/#bridgic.core.model.protocols.RegexPattern.IP_V4_ADDRESS","title":"IP_V4_ADDRESS  <code>class-attribute</code>","text":"<pre><code>IP_V4_ADDRESS: Regex = Regex(\n    pattern=\"(?:(?:25[0-5]|2[0-4]\\\\d|1\\\\d{2}|[1-9]?\\\\d)\\\\.){3}(?:25[0-5]|2[0-4]\\\\d|1\\\\d{2}|[1-9]?\\\\d)\"\n)\n</code></pre> <p>A regular expression for IPv4 addresses.</p>"},{"location":"reference/bridgic-core/bridgic/core/model/protocols/#bridgic.core.model.protocols.RegexPattern.IP_V6_ADDRESS","title":"IP_V6_ADDRESS  <code>class-attribute</code>","text":"<pre><code>IP_V6_ADDRESS: Regex = Regex(\n    pattern=\"([0-9a-fA-F]{1,4}:){7}[0-9a-fA-F]{1,4}\"\n)\n</code></pre> <p>A regular expression for IPv6 addresses.</p>"},{"location":"reference/bridgic-core/bridgic/core/model/protocols/#bridgic.core.model.protocols.RegexPattern.EMAIL","title":"EMAIL  <code>class-attribute</code>","text":"<pre><code>EMAIL: Regex = Regex(\n    pattern=\"[a-zA-Z0-9_.+-]+@[a-zA-Z0-9-]+\\\\.[a-zA-Z0-9-.]+\"\n)\n</code></pre> <p>A regular expression for email addresses.</p>"},{"location":"reference/bridgic-core/bridgic/core/model/protocols/#bridgic.core.model.protocols.StructuredOutput","title":"StructuredOutput","text":"<p>               Bases: <code>Protocol</code></p> <p>Protocol for LLM providers that support structured output generation.</p> <p>StructuredOutput defines the interface for language models that can generate  responses in specific formats according to given constraints. This protocol  enables controlled output generation for various data structures and formats.</p> <p>Methods:</p> Name Description <code>structured_output</code> <p>Synchronous method for generating structured output based on constraints.</p> <code>astructured_output</code> <p>Asynchronous method for generating structured output based on constraints.</p> Notes <ol> <li>Both synchronous and asynchronous methods must be implemented</li> <li>Supported constraint types depend on the specific LLM provider implementation</li> <li>Output format is determined by the constraint type provided</li> <li>Common constraint types include PydanticModel, JsonSchema, Regex, Choice, etc.</li> </ol> Source code in <code>bridgic/core/model/protocols/_structured_output.py</code> <pre><code>class StructuredOutput(Protocol):\n    \"\"\"\n    Protocol for LLM providers that support structured output generation.\n\n    StructuredOutput defines the interface for language models that can generate \n    responses in specific formats according to given constraints. This protocol \n    enables controlled output generation for various data structures and formats.\n\n    Methods\n    -------\n    structured_output\n        Synchronous method for generating structured output based on constraints.\n    astructured_output\n        Asynchronous method for generating structured output based on constraints.\n\n    Notes\n    ----\n    1. Both synchronous and asynchronous methods must be implemented\n    2. Supported constraint types depend on the specific LLM provider implementation\n    3. Output format is determined by the constraint type provided\n    4. Common constraint types include PydanticModel, JsonSchema, Regex, Choice, etc.\n    \"\"\"\n\n    def structured_output(\n        self,\n        messages: List[Message],\n        constraint: Constraint,\n        **kwargs,\n    ) -&gt; Any:\n        \"\"\"\n        Generate structured output based on conversation context and constraints.\n\n        Parameters\n        ----------\n        messages : List[Message]\n            The conversation history and current context.\n        constraint : Constraint\n            The output format constraint. Supported types:\n\n            - PydanticModel: Output as Pydantic model instance\n            - JsonSchema: Output as JSON matching the schema\n            - Regex: Output matching the regex pattern\n            - Choice: Output from predefined choices\n            - EbnfGrammar: Output following EBNF grammar rules\n            - LarkGrammar: Output following Lark grammar rules\n        **kwargs\n            Additional keyword arguments for output generation configuration.\n\n        Returns\n        -------\n        Any\n            The structured output matching the specified constraint format.\n        \"\"\"\n        ...\n\n    async def astructured_output(\n        self,\n        messages: List[Message],\n        constraint: Constraint,\n        **kwargs,\n    ) -&gt; Any:\n        \"\"\"\n        Asynchronously generate structured output based on conversation context and constraints.\n\n        Parameters\n        ----------\n        messages : List[Message]\n            The conversation history and current context.\n        constraint : Constraint\n            The output format constraint. Supported types:\n\n            - PydanticModel: Output as Pydantic model instance\n            - JsonSchema: Output as JSON matching the schema\n            - Regex: Output matching the regex pattern\n            - Choice: Output from predefined choices\n            - EbnfGrammar: Output following EBNF grammar rules\n            - LarkGrammar: Output following Lark grammar rules\n        **kwargs\n            Additional keyword arguments for output generation configuration.\n\n        Returns\n        -------\n        Any\n            The structured output matching the specified constraint format.\n        \"\"\"\n        ...\n</code></pre>"},{"location":"reference/bridgic-core/bridgic/core/model/protocols/#bridgic.core.model.protocols.StructuredOutput.structured_output","title":"structured_output","text":"<pre><code>structured_output(\n    messages: List[Message],\n    constraint: Constraint,\n    **kwargs\n) -&gt; Any\n</code></pre> <p>Generate structured output based on conversation context and constraints.</p> <p>Parameters:</p> Name Type Description Default <code>messages</code> <code>List[Message]</code> <p>The conversation history and current context.</p> required <code>constraint</code> <code>Constraint</code> <p>The output format constraint. Supported types:</p> <ul> <li>PydanticModel: Output as Pydantic model instance</li> <li>JsonSchema: Output as JSON matching the schema</li> <li>Regex: Output matching the regex pattern</li> <li>Choice: Output from predefined choices</li> <li>EbnfGrammar: Output following EBNF grammar rules</li> <li>LarkGrammar: Output following Lark grammar rules</li> </ul> required <code>**kwargs</code> <p>Additional keyword arguments for output generation configuration.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Any</code> <p>The structured output matching the specified constraint format.</p> Source code in <code>bridgic/core/model/protocols/_structured_output.py</code> <pre><code>def structured_output(\n    self,\n    messages: List[Message],\n    constraint: Constraint,\n    **kwargs,\n) -&gt; Any:\n    \"\"\"\n    Generate structured output based on conversation context and constraints.\n\n    Parameters\n    ----------\n    messages : List[Message]\n        The conversation history and current context.\n    constraint : Constraint\n        The output format constraint. Supported types:\n\n        - PydanticModel: Output as Pydantic model instance\n        - JsonSchema: Output as JSON matching the schema\n        - Regex: Output matching the regex pattern\n        - Choice: Output from predefined choices\n        - EbnfGrammar: Output following EBNF grammar rules\n        - LarkGrammar: Output following Lark grammar rules\n    **kwargs\n        Additional keyword arguments for output generation configuration.\n\n    Returns\n    -------\n    Any\n        The structured output matching the specified constraint format.\n    \"\"\"\n    ...\n</code></pre>"},{"location":"reference/bridgic-core/bridgic/core/model/protocols/#bridgic.core.model.protocols.StructuredOutput.astructured_output","title":"astructured_output","text":"<code>async</code> <pre><code>astructured_output(\n    messages: List[Message],\n    constraint: Constraint,\n    **kwargs\n) -&gt; Any\n</code></pre> <p>Asynchronously generate structured output based on conversation context and constraints.</p> <p>Parameters:</p> Name Type Description Default <code>messages</code> <code>List[Message]</code> <p>The conversation history and current context.</p> required <code>constraint</code> <code>Constraint</code> <p>The output format constraint. Supported types:</p> <ul> <li>PydanticModel: Output as Pydantic model instance</li> <li>JsonSchema: Output as JSON matching the schema</li> <li>Regex: Output matching the regex pattern</li> <li>Choice: Output from predefined choices</li> <li>EbnfGrammar: Output following EBNF grammar rules</li> <li>LarkGrammar: Output following Lark grammar rules</li> </ul> required <code>**kwargs</code> <p>Additional keyword arguments for output generation configuration.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Any</code> <p>The structured output matching the specified constraint format.</p> Source code in <code>bridgic/core/model/protocols/_structured_output.py</code> <pre><code>async def astructured_output(\n    self,\n    messages: List[Message],\n    constraint: Constraint,\n    **kwargs,\n) -&gt; Any:\n    \"\"\"\n    Asynchronously generate structured output based on conversation context and constraints.\n\n    Parameters\n    ----------\n    messages : List[Message]\n        The conversation history and current context.\n    constraint : Constraint\n        The output format constraint. Supported types:\n\n        - PydanticModel: Output as Pydantic model instance\n        - JsonSchema: Output as JSON matching the schema\n        - Regex: Output matching the regex pattern\n        - Choice: Output from predefined choices\n        - EbnfGrammar: Output following EBNF grammar rules\n        - LarkGrammar: Output following Lark grammar rules\n    **kwargs\n        Additional keyword arguments for output generation configuration.\n\n    Returns\n    -------\n    Any\n        The structured output matching the specified constraint format.\n    \"\"\"\n    ...\n</code></pre>"},{"location":"reference/bridgic-core/bridgic/core/model/types/","title":"types","text":"<p>The Model Types module defines core data types for interacting with models.</p> <p>This module contains type definitions for messages, content blocks, tool calls,  responses, and more, providing a unified data structure representation for model  input and output.</p>"},{"location":"reference/bridgic-core/bridgic/core/model/types/#bridgic.core.model.types.Tool","title":"Tool","text":"<p>               Bases: <code>BaseModel</code></p> <p>A description of a tool that can be used by the LLM. Generally, a list of tools is provided  to the LLM for selection.</p> Source code in <code>bridgic/core/model/types/_tool_use.py</code> <pre><code>class Tool(BaseModel):\n    \"\"\"\n    A description of a tool that can be used by the LLM. Generally, a list of tools is provided \n    to the LLM for selection.\n    \"\"\"\n\n    name: str = Field(..., description=\"Name of the tool.\")\n    \"\"\"The name of the tool to be called.\"\"\"\n    description: str = Field(..., description=\"Description of the tool.\")\n    \"\"\"\n    A description of what the tool does, used by the LLM to choose when and how to call the tool.\n    \"\"\"\n    parameters: Dict[str, Any] = Field(..., description=\"JSON schema object that describes the parameters of the tool.\")\n    \"\"\"\n    A JSON schema dictionary that describes the parameters of the tool.\n    \"\"\"\n</code></pre>"},{"location":"reference/bridgic-core/bridgic/core/model/types/#bridgic.core.model.types.Tool.name","title":"name  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>name: str = Field(..., description='Name of the tool.')\n</code></pre> <p>The name of the tool to be called.</p>"},{"location":"reference/bridgic-core/bridgic/core/model/types/#bridgic.core.model.types.Tool.description","title":"description  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>description: str = Field(\n    ..., description=\"Description of the tool.\"\n)\n</code></pre> <p>A description of what the tool does, used by the LLM to choose when and how to call the tool.</p>"},{"location":"reference/bridgic-core/bridgic/core/model/types/#bridgic.core.model.types.Tool.parameters","title":"parameters  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>parameters: Dict[str, Any] = Field(\n    ...,\n    description=\"JSON schema object that describes the parameters of the tool.\",\n)\n</code></pre> <p>A JSON schema dictionary that describes the parameters of the tool.</p>"},{"location":"reference/bridgic-core/bridgic/core/model/types/#bridgic.core.model.types.ToolCall","title":"ToolCall","text":"<p>               Bases: <code>BaseModel</code></p> <p>A description of a tool call that is generated by the LLM. The LLM can return one or multiple  tool calls at once.</p> Source code in <code>bridgic/core/model/types/_tool_use.py</code> <pre><code>class ToolCall(BaseModel):\n    \"\"\"\n    A description of a tool call that is generated by the LLM. The LLM can return one or multiple \n    tool calls at once.\n    \"\"\"\n\n    id: Optional[str] = Field(..., description=\"ID of the tool call.\")\n    \"\"\"The ID of the tool call.\"\"\"\n    name: str = Field(..., description=\"Name of the tool.\")\n    \"\"\"The name of the tool to be called.\"\"\"\n    arguments: Dict[str, Any] = Field(..., default_factory=dict, description=\"Arguments that are used to call the tool.\")\n    \"\"\"The arguments to call the tool with, as generated by the LLM in JSON format. \"\"\"\n</code></pre>"},{"location":"reference/bridgic-core/bridgic/core/model/types/#bridgic.core.model.types.ToolCall.id","title":"id  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>id: Optional[str] = Field(\n    ..., description=\"ID of the tool call.\"\n)\n</code></pre> <p>The ID of the tool call.</p>"},{"location":"reference/bridgic-core/bridgic/core/model/types/#bridgic.core.model.types.ToolCall.name","title":"name  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>name: str = Field(..., description='Name of the tool.')\n</code></pre> <p>The name of the tool to be called.</p>"},{"location":"reference/bridgic-core/bridgic/core/model/types/#bridgic.core.model.types.ToolCall.arguments","title":"arguments  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>arguments: Dict[str, Any] = Field(\n    ...,\n    default_factory=dict,\n    description=\"Arguments that are used to call the tool.\",\n)\n</code></pre> <p>The arguments to call the tool with, as generated by the LLM in JSON format.</p>"},{"location":"reference/bridgic-core/bridgic/core/model/types/#bridgic.core.model.types.ToolCallDict","title":"ToolCallDict","text":"<p>               Bases: <code>TypedDict</code></p> <p>A dictionary that describes a tool call that is generated by the LLM. The LLM can return one or multiple  tool calls at once. This <code>ToolCallDict</code> is another format of <code>ToolCall</code> that may be easier for developers to use.</p> Source code in <code>bridgic/core/model/types/_tool_use.py</code> <pre><code>class ToolCallDict(TypedDict):\n    \"\"\"\n    A dictionary that describes a tool call that is generated by the LLM. The LLM can return one or multiple \n    tool calls at once. This `ToolCallDict` is another format of `ToolCall` that may be easier for developers to use.\n    \"\"\"\n\n    id: str\n    \"\"\"The ID of the tool call.\"\"\"\n    name: str\n    \"\"\"The name of the tool to be called.\"\"\"\n    arguments: Dict[str, Any]\n    \"\"\"The arguments to call the tool with, as generated by the LLM in JSON format. \"\"\"\n</code></pre>"},{"location":"reference/bridgic-core/bridgic/core/model/types/#bridgic.core.model.types.ToolCallDict.id","title":"id  <code>instance-attribute</code>","text":"<pre><code>id: str\n</code></pre> <p>The ID of the tool call.</p>"},{"location":"reference/bridgic-core/bridgic/core/model/types/#bridgic.core.model.types.ToolCallDict.name","title":"name  <code>instance-attribute</code>","text":"<pre><code>name: str\n</code></pre> <p>The name of the tool to be called.</p>"},{"location":"reference/bridgic-core/bridgic/core/model/types/#bridgic.core.model.types.ToolCallDict.arguments","title":"arguments  <code>instance-attribute</code>","text":"<pre><code>arguments: Dict[str, Any]\n</code></pre> <p>The arguments to call the tool with, as generated by the LLM in JSON format.</p>"},{"location":"reference/bridgic-core/bridgic/core/model/types/#bridgic.core.model.types.TextBlock","title":"TextBlock","text":"<p>               Bases: <code>BaseModel</code></p> <p>Encapsulates plain text data that is passed to or received from language models.</p> <p>Attributes:</p> Name Type Description <code>block_type</code> <code>Literal['text']</code> <p>The type identifier for this content block.</p> <code>text</code> <code>str</code> <p>The actual text content.</p> Source code in <code>bridgic/core/model/types/_content_block.py</code> <pre><code>class TextBlock(BaseModel):\n    \"\"\"\n    Encapsulates plain text data that is passed to or received from language models.\n\n    Attributes\n    ----------\n    block_type : Literal[\"text\"]\n        The type identifier for this content block.\n    text : str\n        The actual text content.\n    \"\"\"\n    block_type: Literal[\"text\"] = Field(default=\"text\")\n    text: str\n</code></pre>"},{"location":"reference/bridgic-core/bridgic/core/model/types/#bridgic.core.model.types.ToolCallBlock","title":"ToolCallBlock","text":"<p>               Bases: <code>BaseModel</code></p> <p>Encapsulates tool invocation data that is received from language models.</p> <p>Attributes:</p> Name Type Description <code>block_type</code> <code>Literal['tool_call']</code> <p>The type identifier for this content block.</p> <code>id</code> <code>str</code> <p>Unique identifier for the tool call instance.</p> <code>name</code> <code>str</code> <p>Name of the tool to be called.</p> <code>arguments</code> <code>Dict[str, Any]</code> <p>Parameters to be passed to the tool function.</p> Source code in <code>bridgic/core/model/types/_content_block.py</code> <pre><code>class ToolCallBlock(BaseModel):\n    \"\"\"\n    Encapsulates tool invocation data that is received from language models.\n\n    Attributes\n    ----------\n    block_type : Literal[\"tool_call\"]\n        The type identifier for this content block.\n    id : str\n        Unique identifier for the tool call instance.\n    name : str\n        Name of the tool to be called.\n    arguments : Dict[str, Any]\n        Parameters to be passed to the tool function.\n    \"\"\"\n    block_type: Literal[\"tool_call\"] = Field(default=\"tool_call\")\n    id: str = Field(..., description=\"The ID of the tool call.\")\n    name: str = Field(..., description=\"The name of the tool call.\")\n    arguments: Dict[str, Any] = Field(..., description=\"The arguments of the tool call.\")\n</code></pre>"},{"location":"reference/bridgic-core/bridgic/core/model/types/#bridgic.core.model.types.ToolResultBlock","title":"ToolResultBlock","text":"<p>               Bases: <code>BaseModel</code></p> <p>Encapsulates the results returned from tool executions.</p> <p>Attributes:</p> Name Type Description <code>block_type</code> <code>Literal['tool_result']</code> <p>The type identifier for this content block.</p> <code>id</code> <code>str</code> <p>Unique identifier matching the corresponding tool call.</p> <code>content</code> <code>str</code> <p>The result content returned from the tool execution.</p> Source code in <code>bridgic/core/model/types/_content_block.py</code> <pre><code>class ToolResultBlock(BaseModel):\n    \"\"\"\n    Encapsulates the results returned from tool executions.\n\n    Attributes\n    ----------\n    block_type : Literal[\"tool_result\"]\n        The type identifier for this content block.\n    id : str\n        Unique identifier matching the corresponding tool call.\n    content : str\n        The result content returned from the tool execution.\n    \"\"\"\n    block_type: Literal[\"tool_result\"] = Field(default=\"tool_result\")\n    id: str = Field(..., description=\"The ID of the tool call.\")\n    content: str = Field(..., description=\"The result content of the tool call.\")\n</code></pre>"},{"location":"reference/bridgic-core/bridgic/core/model/types/#bridgic.core.model.types.Role","title":"Role","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p> <p>Message role enumeration for LLM conversations.</p> <p>Defines the different roles that can be assigned to messages in a conversation with language models, following standard chat completion formats.</p> <p>Attributes:</p> Name Type Description <code>SYSTEM</code> <code>str</code> <p>System role for providing instructions or context to the model.</p> <code>USER</code> <code>str</code> <p>User role for human input and queries.</p> <code>AI</code> <code>str</code> <p>Assistant role for model responses and outputs.</p> <code>TOOL</code> <code>str</code> <p>Tool role for tool execution results and responses.</p> Source code in <code>bridgic/core/model/types/_message.py</code> <pre><code>class Role(str, Enum):\n    \"\"\"\n    Message role enumeration for LLM conversations.\n\n    Defines the different roles that can be assigned to messages in a conversation\n    with language models, following standard chat completion formats.\n\n    Attributes\n    ----------\n    SYSTEM : str\n        System role for providing instructions or context to the model.\n    USER : str\n        User role for human input and queries.\n    AI : str\n        Assistant role for model responses and outputs.\n    TOOL : str\n        Tool role for tool execution results and responses.\n    \"\"\"\n    SYSTEM = \"system\"\n    USER = \"user\"\n    AI = \"assistant\"\n    TOOL = \"tool\"\n\n    @classmethod\n    def get_all_roles(cls) -&gt; List[str]:\n        return [role.value for role in Role]\n</code></pre>"},{"location":"reference/bridgic-core/bridgic/core/model/types/#bridgic.core.model.types.Message","title":"Message","text":"<p>               Bases: <code>BaseModel</code></p> <p>LLM message container for conversation exchanges.</p> <p>Represents a single message in a conversation with language models, containing role information, content blocks, and optional metadata. Supports various content types including text, tool calls, and tool results.</p> <p>Attributes:</p> Name Type Description <code>role</code> <code>Role</code> <p>The role of the message sender (system, user, assistant, or tool).</p> <code>blocks</code> <code>List[ContentBlock]</code> <p>List of content blocks containing the actual message data.</p> <code>extras</code> <code>Dict[str, Any]</code> <p>Additional metadata and custom fields for the message.</p> Source code in <code>bridgic/core/model/types/_message.py</code> <pre><code>class Message(BaseModel):\n    \"\"\"\n    LLM message container for conversation exchanges.\n\n    Represents a single message in a conversation with language models, containing\n    role information, content blocks, and optional metadata. Supports various\n    content types including text, tool calls, and tool results.\n\n    Attributes\n    ----------\n    role : Role\n        The role of the message sender (system, user, assistant, or tool).\n    blocks : List[ContentBlock]\n        List of content blocks containing the actual message data.\n    extras : Dict[str, Any]\n        Additional metadata and custom fields for the message.\n    \"\"\"\n    role: Role = Field(default=Role.USER)\n    blocks: List[ContentBlock] = Field(default=[])\n    extras: Dict[str, Any] = Field(default={})\n\n    @classmethod\n    def from_text(\n        cls,\n        text: str,\n        role: Union[Role, str] = Role.USER,\n        extras: Optional[Dict[str, Any]] = {},\n    ) -&gt; \"Message\":\n        if isinstance(role, str):\n            role = Role(role)\n        return cls(role=role, blocks=[TextBlock(text=text)], extras=extras)\n\n    @classmethod\n    def from_tool_call(\n        cls,\n        tool_calls: Union[\n            \"ToolCallDict\", \n            List[\"ToolCallDict\"], \n            \"ToolCall\",\n            List[\"ToolCall\"]\n        ],\n        text: Optional[str] = None,\n        extras: Optional[Dict[str, Any]] = {},\n    ) -&gt; \"Message\":\n        \"\"\"\n        Create a message with tool call blocks and optional text content.\n\n        Parameters\n        ----------\n        tool_calls : Union[ToolCallDict, List[ToolCallDict], ToolCall, List[ToolCall]]\n            Tool call data in various formats:\n            - Single tool call dict: {\"id\": \"call_123\", \"name\": \"get_weather\", \"arguments\": {...}}\n            - List of tool call dicts: [{\"id\": \"call_123\", ...}, {\"id\": \"call_124\", ...}]\n            - Single ToolCall instance\n            - List of ToolCall instances\n        text : Optional[str], optional\n            Optional text content to include in the message\n\n        extras : Optional[Dict[str, Any]], optional\n            Additional metadata for the message\n\n        Returns\n        -------\n        Message\n            A message containing the tool call blocks and optional text\n\n        Examples\n        --------\n        &gt;&gt;&gt; # Build from single tool call dict.\n        ... message = Message.from_tool_call(\n        ...     tool_calls={\n        ...         \"id\": \"call_id_123\",\n        ...         \"name\": \"get_weather\",\n        ...         \"arguments\": {\"city\": \"Tokyo\", \"unit\": \"celsius\"}\n        ...     },\n        ...     text=\"I will check the weather for you.\"\n        ... )\n\n        &gt;&gt;&gt; # Build from multiple tool call dicts.\n        ... message = Message.from_tool_call(\n        ...     tool_calls=[\n        ...         {\"id\": \"call_id_123\", \"name\": \"get_weather\", \"arguments\": {\"city\": \"Tokyo\"}},\n        ...         {\"id\": \"call_id_456\", \"name\": \"get_news\", \"arguments\": {\"topic\": \"weather\"}},\n        ...     ],\n        ...     text=\"I will get weather and news for you.\"\n        ... )\n\n        &gt;&gt;&gt; # Build from single ToolCall object.\n        ... tool_call = ToolCall(id=\"call_123\", name=\"get_weather\", arguments={\"city\": \"Tokyo\"})\n        ... message = Message.from_tool_call(tool_calls=tool_call, text=\"I will check the weather.\")\n\n        &gt;&gt;&gt; # Build from multiple ToolCall objects.\n        ... tool_calls = [\n        ...     ToolCall(id=\"call_id_123\", name=\"get_weather\", arguments={\"city\": \"Tokyo\"}),\n        ...     ToolCall(id=\"call_id_456\", name=\"get_news\", arguments={\"topic\": \"weather\"}),\n        ... ]\n        ... message = Message.from_tool_call(tool_calls=tool_calls, text=\"I will get weather and news.\")\n        \"\"\"\n        role = Role(Role.AI)\n        blocks = []\n\n        # Add text content if provided\n        if text:\n            blocks.append(TextBlock(text=text))\n\n        # Handle different tool_calls formats\n        if isinstance(tool_calls, dict):\n            # Single tool call dict\n            tool_calls = [tool_calls]\n        if isinstance(tool_calls, list):\n            # List of tool calls (dicts or ToolCall)\n            for tool_call in tool_calls:\n                if isinstance(tool_call, dict):\n                    # Tool call dict\n                    blocks.append(ToolCallBlock(\n                        id=tool_call[\"id\"],\n                        name=tool_call[\"name\"],\n                        arguments=tool_call[\"arguments\"]\n                    ))\n                elif hasattr(tool_call, 'id') and hasattr(tool_call, 'name') and hasattr(tool_call, 'arguments'):\n                    blocks.append(ToolCallBlock(\n                        id=tool_call.id,\n                        name=tool_call.name,\n                        arguments=tool_call.arguments\n                    ))\n                else:\n                    raise ValueError(f\"Invalid tool call format: {tool_call}\")\n        elif hasattr(tool_calls, 'id') and hasattr(tool_calls, 'name') and hasattr(tool_calls, 'arguments'):\n            blocks.append(ToolCallBlock(\n                id=tool_calls.id,\n                name=tool_calls.name,\n                arguments=tool_calls.arguments\n            ))\n        else:\n            raise ValueError(f\"Invalid tool_calls format: {type(tool_calls)}\")\n\n        return cls(role=role, blocks=blocks, extras=extras)\n\n    @classmethod\n    def from_tool_result(\n        cls,\n        tool_id: str,\n        content: str,\n        extras: Optional[Dict[str, Any]] = {},\n    ) -&gt; \"Message\":\n        \"\"\"\n        Create a message with a tool result block.\n\n        Parameters\n        ----------\n        tool_id : str\n            The ID of the tool call that this result corresponds to\n        content : str\n            The result content from the tool execution\n        extras : Optional[Dict[str, Any]], optional\n            Additional metadata for the message\n\n        Returns\n        -------\n        Message\n            A message containing the tool result block\n\n        Examples\n        --------\n        &gt;&gt;&gt; message = Message.from_tool_result(\n        ...     tool_id=\"call_id_123\",\n        ...     content=\"The weather in Tokyo is 22\u00b0C and sunny.\"\n        ... )\n        \"\"\"\n        role = Role(Role.TOOL)\n        return cls(\n            role=role, \n            blocks=[ToolResultBlock(id=tool_id, content=content)], \n            extras=extras\n        )\n\n    @property\n    def content(self) -&gt; str:\n        return \"\\n\\n\".join([block.text for block in self.blocks if isinstance(block, TextBlock)])\n\n    @content.setter\n    def content(self, text: str):\n        if not self.blocks:\n            self.blocks = [TextBlock(text=text)]\n        elif len(self.blocks) == 1 and isinstance(self.blocks[0], TextBlock):\n            self.blocks = [TextBlock(text=text)]\n        else:\n            raise ValueError(\n                \"Message contains multiple blocks or contains a non-text block, thus it could not be \"\n                \"easily set by the property \\\"Message.content\\\". Use \\\"Message.blocks\\\" instead.\"\n            )\n</code></pre>"},{"location":"reference/bridgic-core/bridgic/core/model/types/#bridgic.core.model.types.Message.from_tool_call","title":"from_tool_call","text":"<code>classmethod</code> <pre><code>from_tool_call(\n    tool_calls: Union[\n        ToolCallDict,\n        List[ToolCallDict],\n        ToolCall,\n        List[ToolCall],\n    ],\n    text: Optional[str] = None,\n    extras: Optional[Dict[str, Any]] = {},\n) -&gt; Message\n</code></pre> <p>Create a message with tool call blocks and optional text content.</p> <p>Parameters:</p> Name Type Description Default <code>tool_calls</code> <code>Union[ToolCallDict, List[ToolCallDict], ToolCall, List[ToolCall]]</code> <p>Tool call data in various formats: - Single tool call dict: {\"id\": \"call_123\", \"name\": \"get_weather\", \"arguments\": {...}} - List of tool call dicts: [{\"id\": \"call_123\", ...}, {\"id\": \"call_124\", ...}] - Single ToolCall instance - List of ToolCall instances</p> required <code>text</code> <code>Optional[str]</code> <p>Optional text content to include in the message</p> <code>None</code> <code>extras</code> <code>Optional[Dict[str, Any]]</code> <p>Additional metadata for the message</p> <code>{}</code> <p>Returns:</p> Type Description <code>Message</code> <p>A message containing the tool call blocks and optional text</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; # Build from single tool call dict.\n... message = Message.from_tool_call(\n...     tool_calls={\n...         \"id\": \"call_id_123\",\n...         \"name\": \"get_weather\",\n...         \"arguments\": {\"city\": \"Tokyo\", \"unit\": \"celsius\"}\n...     },\n...     text=\"I will check the weather for you.\"\n... )\n</code></pre> <pre><code>&gt;&gt;&gt; # Build from multiple tool call dicts.\n... message = Message.from_tool_call(\n...     tool_calls=[\n...         {\"id\": \"call_id_123\", \"name\": \"get_weather\", \"arguments\": {\"city\": \"Tokyo\"}},\n...         {\"id\": \"call_id_456\", \"name\": \"get_news\", \"arguments\": {\"topic\": \"weather\"}},\n...     ],\n...     text=\"I will get weather and news for you.\"\n... )\n</code></pre> <pre><code>&gt;&gt;&gt; # Build from single ToolCall object.\n... tool_call = ToolCall(id=\"call_123\", name=\"get_weather\", arguments={\"city\": \"Tokyo\"})\n... message = Message.from_tool_call(tool_calls=tool_call, text=\"I will check the weather.\")\n</code></pre> <pre><code>&gt;&gt;&gt; # Build from multiple ToolCall objects.\n... tool_calls = [\n...     ToolCall(id=\"call_id_123\", name=\"get_weather\", arguments={\"city\": \"Tokyo\"}),\n...     ToolCall(id=\"call_id_456\", name=\"get_news\", arguments={\"topic\": \"weather\"}),\n... ]\n... message = Message.from_tool_call(tool_calls=tool_calls, text=\"I will get weather and news.\")\n</code></pre> Source code in <code>bridgic/core/model/types/_message.py</code> <pre><code>@classmethod\ndef from_tool_call(\n    cls,\n    tool_calls: Union[\n        \"ToolCallDict\", \n        List[\"ToolCallDict\"], \n        \"ToolCall\",\n        List[\"ToolCall\"]\n    ],\n    text: Optional[str] = None,\n    extras: Optional[Dict[str, Any]] = {},\n) -&gt; \"Message\":\n    \"\"\"\n    Create a message with tool call blocks and optional text content.\n\n    Parameters\n    ----------\n    tool_calls : Union[ToolCallDict, List[ToolCallDict], ToolCall, List[ToolCall]]\n        Tool call data in various formats:\n        - Single tool call dict: {\"id\": \"call_123\", \"name\": \"get_weather\", \"arguments\": {...}}\n        - List of tool call dicts: [{\"id\": \"call_123\", ...}, {\"id\": \"call_124\", ...}]\n        - Single ToolCall instance\n        - List of ToolCall instances\n    text : Optional[str], optional\n        Optional text content to include in the message\n\n    extras : Optional[Dict[str, Any]], optional\n        Additional metadata for the message\n\n    Returns\n    -------\n    Message\n        A message containing the tool call blocks and optional text\n\n    Examples\n    --------\n    &gt;&gt;&gt; # Build from single tool call dict.\n    ... message = Message.from_tool_call(\n    ...     tool_calls={\n    ...         \"id\": \"call_id_123\",\n    ...         \"name\": \"get_weather\",\n    ...         \"arguments\": {\"city\": \"Tokyo\", \"unit\": \"celsius\"}\n    ...     },\n    ...     text=\"I will check the weather for you.\"\n    ... )\n\n    &gt;&gt;&gt; # Build from multiple tool call dicts.\n    ... message = Message.from_tool_call(\n    ...     tool_calls=[\n    ...         {\"id\": \"call_id_123\", \"name\": \"get_weather\", \"arguments\": {\"city\": \"Tokyo\"}},\n    ...         {\"id\": \"call_id_456\", \"name\": \"get_news\", \"arguments\": {\"topic\": \"weather\"}},\n    ...     ],\n    ...     text=\"I will get weather and news for you.\"\n    ... )\n\n    &gt;&gt;&gt; # Build from single ToolCall object.\n    ... tool_call = ToolCall(id=\"call_123\", name=\"get_weather\", arguments={\"city\": \"Tokyo\"})\n    ... message = Message.from_tool_call(tool_calls=tool_call, text=\"I will check the weather.\")\n\n    &gt;&gt;&gt; # Build from multiple ToolCall objects.\n    ... tool_calls = [\n    ...     ToolCall(id=\"call_id_123\", name=\"get_weather\", arguments={\"city\": \"Tokyo\"}),\n    ...     ToolCall(id=\"call_id_456\", name=\"get_news\", arguments={\"topic\": \"weather\"}),\n    ... ]\n    ... message = Message.from_tool_call(tool_calls=tool_calls, text=\"I will get weather and news.\")\n    \"\"\"\n    role = Role(Role.AI)\n    blocks = []\n\n    # Add text content if provided\n    if text:\n        blocks.append(TextBlock(text=text))\n\n    # Handle different tool_calls formats\n    if isinstance(tool_calls, dict):\n        # Single tool call dict\n        tool_calls = [tool_calls]\n    if isinstance(tool_calls, list):\n        # List of tool calls (dicts or ToolCall)\n        for tool_call in tool_calls:\n            if isinstance(tool_call, dict):\n                # Tool call dict\n                blocks.append(ToolCallBlock(\n                    id=tool_call[\"id\"],\n                    name=tool_call[\"name\"],\n                    arguments=tool_call[\"arguments\"]\n                ))\n            elif hasattr(tool_call, 'id') and hasattr(tool_call, 'name') and hasattr(tool_call, 'arguments'):\n                blocks.append(ToolCallBlock(\n                    id=tool_call.id,\n                    name=tool_call.name,\n                    arguments=tool_call.arguments\n                ))\n            else:\n                raise ValueError(f\"Invalid tool call format: {tool_call}\")\n    elif hasattr(tool_calls, 'id') and hasattr(tool_calls, 'name') and hasattr(tool_calls, 'arguments'):\n        blocks.append(ToolCallBlock(\n            id=tool_calls.id,\n            name=tool_calls.name,\n            arguments=tool_calls.arguments\n        ))\n    else:\n        raise ValueError(f\"Invalid tool_calls format: {type(tool_calls)}\")\n\n    return cls(role=role, blocks=blocks, extras=extras)\n</code></pre>"},{"location":"reference/bridgic-core/bridgic/core/model/types/#bridgic.core.model.types.Message.from_tool_result","title":"from_tool_result","text":"<code>classmethod</code> <pre><code>from_tool_result(\n    tool_id: str,\n    content: str,\n    extras: Optional[Dict[str, Any]] = {},\n) -&gt; Message\n</code></pre> <p>Create a message with a tool result block.</p> <p>Parameters:</p> Name Type Description Default <code>tool_id</code> <code>str</code> <p>The ID of the tool call that this result corresponds to</p> required <code>content</code> <code>str</code> <p>The result content from the tool execution</p> required <code>extras</code> <code>Optional[Dict[str, Any]]</code> <p>Additional metadata for the message</p> <code>{}</code> <p>Returns:</p> Type Description <code>Message</code> <p>A message containing the tool result block</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; message = Message.from_tool_result(\n...     tool_id=\"call_id_123\",\n...     content=\"The weather in Tokyo is 22\u00b0C and sunny.\"\n... )\n</code></pre> Source code in <code>bridgic/core/model/types/_message.py</code> <pre><code>@classmethod\ndef from_tool_result(\n    cls,\n    tool_id: str,\n    content: str,\n    extras: Optional[Dict[str, Any]] = {},\n) -&gt; \"Message\":\n    \"\"\"\n    Create a message with a tool result block.\n\n    Parameters\n    ----------\n    tool_id : str\n        The ID of the tool call that this result corresponds to\n    content : str\n        The result content from the tool execution\n    extras : Optional[Dict[str, Any]], optional\n        Additional metadata for the message\n\n    Returns\n    -------\n    Message\n        A message containing the tool result block\n\n    Examples\n    --------\n    &gt;&gt;&gt; message = Message.from_tool_result(\n    ...     tool_id=\"call_id_123\",\n    ...     content=\"The weather in Tokyo is 22\u00b0C and sunny.\"\n    ... )\n    \"\"\"\n    role = Role(Role.TOOL)\n    return cls(\n        role=role, \n        blocks=[ToolResultBlock(id=tool_id, content=content)], \n        extras=extras\n    )\n</code></pre>"},{"location":"reference/bridgic-core/bridgic/core/model/types/#bridgic.core.model.types.MessageChunk","title":"MessageChunk","text":"<p>               Bases: <code>BaseModel</code></p> <p>Streaming message chunk for real-time LLM responses.</p> <p>Represents a partial message chunk received during streaming responses from language models, allowing for real-time processing of incremental content.</p> <p>Attributes:</p> Name Type Description <code>delta</code> <code>Optional[str]</code> <p>The incremental text content of this chunk.</p> <code>raw</code> <code>Optional[Any]</code> <p>Raw response data from the LLM provider.</p> Source code in <code>bridgic/core/model/types/_message.py</code> <pre><code>class MessageChunk(BaseModel):\n    \"\"\"\n    Streaming message chunk for real-time LLM responses.\n\n    Represents a partial message chunk received during streaming responses from\n    language models, allowing for real-time processing of incremental content.\n\n    Attributes\n    ----------\n    delta : Optional[str]\n        The incremental text content of this chunk.\n    raw : Optional[Any]\n        Raw response data from the LLM provider.\n    \"\"\"\n    delta: Optional[str] = None\n    raw: Optional[Any] = None\n</code></pre>"},{"location":"reference/bridgic-core/bridgic/core/model/types/#bridgic.core.model.types.Response","title":"Response","text":"<p>               Bases: <code>BaseModel</code></p> <p>LLM response container for model outputs.</p> <p>Represents the complete response from a language model, containing both the message content and the raw response data from the underlying model  provider.</p> <p>Attributes:</p> Name Type Description <code>message</code> <code>Optional[Message]</code> <p>The structured message containing the model's response content.</p> <code>raw</code> <code>Optional[Any]</code> <p>Raw response data from the LLM provider for debugging or custom processing.</p> Source code in <code>bridgic/core/model/types/_response.py</code> <pre><code>class Response(BaseModel):\n    \"\"\"\n    LLM response container for model outputs.\n\n    Represents the complete response from a language model, containing both\n    the message content and the raw response data from the underlying model \n    provider.\n\n    Attributes\n    ----------\n    message : Optional[Message]\n        The structured message containing the model's response content.\n    raw : Optional[Any]\n        Raw response data from the LLM provider for debugging or custom processing.\n    \"\"\"\n    message: Optional[Message] = None\n    raw: Optional[Any] = None\n</code></pre>"},{"location":"reference/bridgic-core/bridgic/core/prompt/","title":"prompt","text":"<p>The Prompt module provides core functionality for managing and rendering prompt templates.</p> <p>This module contains multiple prompt template implementations for more convenient  construction of dynamic LLM prompt content.</p>"},{"location":"reference/bridgic-core/bridgic/core/prompt/#bridgic.core.prompt.BasePromptTemplate","title":"BasePromptTemplate","text":"<p>               Bases: <code>BaseModel</code></p> <p>Abstract base class for prompt templates.</p> <p>This class provides a common interface for messages from template strings with variable substitutions.    </p> <p>Attributes:</p> Name Type Description <code>template_str</code> <code>str</code> <p>The template string containing placeholders for variable substitution. The specific placeholder syntax depends on the concrete implementation (e.g., f-string, Jinja2, etc.).</p> <p>Methods:</p> Name Description <code>format_message</code> <p>Format a single message from the template.</p> <code>format_messages</code> <p>Format multiple messages from the template.</p> Notes <p>This is an abstract base class that must be subclassed to provide concrete implementations. Subclasses should implement the <code>format_message</code> and <code>format_messages</code> methods according to their specific template formatting requirements.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; class MyTemplate(BasePromptTemplate):\n...     def format_message(self, role=Role.USER, **kwargs):\n...         # Implementation here\n...         pass\n...     \n...     def format_messages(self, **kwargs):\n...         # Implementation here\n...         pass\n&gt;&gt;&gt; \n&gt;&gt;&gt; template = MyTemplate(template_str=\"Hello {name}!\")\n&gt;&gt;&gt; message = template.format_message(name=\"World\")\n</code></pre> Source code in <code>bridgic/core/prompt/_base_template.py</code> <pre><code>class BasePromptTemplate(BaseModel):\n    \"\"\"\n    Abstract base class for prompt templates.\n\n    This class provides a common interface for messages from template strings with variable substitutions.    \n\n    Attributes\n    ----------\n    template_str : str\n        The template string containing placeholders for variable substitution.\n        The specific placeholder syntax depends on the concrete implementation\n        (e.g., f-string, Jinja2, etc.).\n\n    Methods\n    -------\n    format_message(role, **kwargs)\n        Format a single message from the template.\n    format_messages(**kwargs)\n        Format multiple messages from the template.\n\n    Notes\n    -----\n    This is an abstract base class that must be subclassed to provide\n    concrete implementations. Subclasses should implement the `format_message`\n    and `format_messages` methods according to their specific template\n    formatting requirements.\n\n    Examples\n    --------\n    &gt;&gt;&gt; class MyTemplate(BasePromptTemplate):\n    ...     def format_message(self, role=Role.USER, **kwargs):\n    ...         # Implementation here\n    ...         pass\n    ...     \n    ...     def format_messages(self, **kwargs):\n    ...         # Implementation here\n    ...         pass\n    &gt;&gt;&gt; \n    &gt;&gt;&gt; template = MyTemplate(template_str=\"Hello {name}!\")\n    &gt;&gt;&gt; message = template.format_message(name=\"World\")\n    \"\"\"\n\n    template_str: str\n\n    def format_message(self, role: Union[Role, str] = Role.USER, **kwargs) -&gt; Message:\n        \"\"\"\n        Format a single message from the template.\n\n        Parameters\n        ----------\n        role : Union[Role, str], default=Role.USER\n            The role of the message (e.g., 'user', 'assistant', 'system').\n        **kwargs\n            Additional keyword arguments to be substituted into the template.\n\n        Returns\n        -------\n        Message\n            A formatted message object.\n\n        Raises\n        ------\n        NotImplementedError\n            This method must be implemented by subclasses.\n        \"\"\"\n        raise NotImplementedError(f\"format_message is not implemented in class {self.__class__.__name__}\")\n\n    def format_messages(self, **kwargs) -&gt; List[Message]:\n        \"\"\"\n        Format multiple messages from the template.\n\n        Parameters\n        ----------\n        **kwargs\n            Additional keyword arguments to be substituted into the template.\n\n        Returns\n        -------\n        List[Message]\n            A list of formatted message objects.\n\n        Raises\n        ------\n        NotImplementedError\n            This method must be implemented by subclasses.\n        \"\"\"\n        raise NotImplementedError(f\"format_messages is not implemented in class {self.__class__.__name__}\")\n</code></pre>"},{"location":"reference/bridgic-core/bridgic/core/prompt/#bridgic.core.prompt.BasePromptTemplate.format_message","title":"format_message","text":"<pre><code>format_message(\n    role: Union[Role, str] = USER, **kwargs\n) -&gt; Message\n</code></pre> <p>Format a single message from the template.</p> <p>Parameters:</p> Name Type Description Default <code>role</code> <code>Union[Role, str]</code> <p>The role of the message (e.g., 'user', 'assistant', 'system').</p> <code>Role.USER</code> <code>**kwargs</code> <p>Additional keyword arguments to be substituted into the template.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Message</code> <p>A formatted message object.</p> <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>This method must be implemented by subclasses.</p> Source code in <code>bridgic/core/prompt/_base_template.py</code> <pre><code>def format_message(self, role: Union[Role, str] = Role.USER, **kwargs) -&gt; Message:\n    \"\"\"\n    Format a single message from the template.\n\n    Parameters\n    ----------\n    role : Union[Role, str], default=Role.USER\n        The role of the message (e.g., 'user', 'assistant', 'system').\n    **kwargs\n        Additional keyword arguments to be substituted into the template.\n\n    Returns\n    -------\n    Message\n        A formatted message object.\n\n    Raises\n    ------\n    NotImplementedError\n        This method must be implemented by subclasses.\n    \"\"\"\n    raise NotImplementedError(f\"format_message is not implemented in class {self.__class__.__name__}\")\n</code></pre>"},{"location":"reference/bridgic-core/bridgic/core/prompt/#bridgic.core.prompt.BasePromptTemplate.format_messages","title":"format_messages","text":"<pre><code>format_messages(**kwargs) -&gt; List[Message]\n</code></pre> <p>Format multiple messages from the template.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <p>Additional keyword arguments to be substituted into the template.</p> <code>{}</code> <p>Returns:</p> Type Description <code>List[Message]</code> <p>A list of formatted message objects.</p> <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>This method must be implemented by subclasses.</p> Source code in <code>bridgic/core/prompt/_base_template.py</code> <pre><code>def format_messages(self, **kwargs) -&gt; List[Message]:\n    \"\"\"\n    Format multiple messages from the template.\n\n    Parameters\n    ----------\n    **kwargs\n        Additional keyword arguments to be substituted into the template.\n\n    Returns\n    -------\n    List[Message]\n        A list of formatted message objects.\n\n    Raises\n    ------\n    NotImplementedError\n        This method must be implemented by subclasses.\n    \"\"\"\n    raise NotImplementedError(f\"format_messages is not implemented in class {self.__class__.__name__}\")\n</code></pre>"},{"location":"reference/bridgic-core/bridgic/core/prompt/#bridgic.core.prompt.FstringPromptTemplate","title":"FstringPromptTemplate","text":"<p>               Bases: <code>BasePromptTemplate</code></p> <p>This template implementation uses Python's f-string syntax (braces <code>{}</code>).</p> <p>Methods:</p> Name Description <code>format_message</code> <p>Format a single message from the template.</p> Notes <p>This template supports single message rendering via <code>format_message()</code>. The template uses Python's built-in <code>str.format()</code> method for variable substitution, which provides basic formatting capabilities.</p> <p>Examples:</p> <p>Basic usage:</p> <pre><code>&gt;&gt;&gt; template = FstringPromptTemplate(\"Hello {name}, you are {age} years old.\")\n&gt;&gt;&gt; message = template.format_message(role=\"user\", name=\"Alice\", age=25)\n</code></pre> <p>With context:</p> <pre><code>&gt;&gt;&gt; template = FstringPromptTemplate('''\n... Context: {context}\n... Question: {question}\n... Please provide a helpful answer.\n... ''')\n&gt;&gt;&gt; message = template.format_message(\n...     role=\"system\", \n...     context=\"Python programming\", \n...     question=\"What is a decorator?\"\n... )\n</code></pre> <p>Multiple variables:</p> <pre><code>&gt;&gt;&gt; template = FstringPromptTemplate(\"{greeting} {name}! Today is {date}.\")\n&gt;&gt;&gt; message = template.format_message(\n...     role=\"assistant\",\n...     greeting=\"Good morning\",\n...     name=\"Bob\", \n...     date=\"Monday\"\n... )\n</code></pre> Source code in <code>bridgic/core/prompt/_fstring_template.py</code> <pre><code>class FstringPromptTemplate(BasePromptTemplate):\n    \"\"\"    \n    This template implementation uses Python's f-string syntax (braces `{}`).\n\n    Methods\n    -------\n    format_message(role, **kwargs)\n        Format a single message from the template.\n\n    Notes\n    -----\n    This template supports single message rendering via `format_message()`.\n    The template uses Python's built-in `str.format()` method for variable\n    substitution, which provides basic formatting capabilities.\n\n    Examples\n    --------\n    Basic usage:\n    &gt;&gt;&gt; template = FstringPromptTemplate(\"Hello {name}, you are {age} years old.\")\n    &gt;&gt;&gt; message = template.format_message(role=\"user\", name=\"Alice\", age=25)\n\n    With context:\n    &gt;&gt;&gt; template = FstringPromptTemplate('''\n    ... Context: {context}\n    ... Question: {question}\n    ... Please provide a helpful answer.\n    ... ''')\n    &gt;&gt;&gt; message = template.format_message(\n    ...     role=\"system\", \n    ...     context=\"Python programming\", \n    ...     question=\"What is a decorator?\"\n    ... )\n\n    Multiple variables:\n    &gt;&gt;&gt; template = FstringPromptTemplate(\"{greeting} {name}! Today is {date}.\")\n    &gt;&gt;&gt; message = template.format_message(\n    ...     role=\"assistant\",\n    ...     greeting=\"Good morning\",\n    ...     name=\"Bob\", \n    ...     date=\"Monday\"\n    ... )\n    \"\"\"\n\n    def format_message(self, role: Union[Role, str], **kwargs) -&gt; Message:\n        \"\"\"\n        Format a single message from the template.\n\n        Parameters\n        ----------\n        role : Union[Role, str]\n            The role of the message (e.g., 'user', 'assistant', 'system').\n            Required parameter for this template implementation.\n        **kwargs\n            Keyword arguments containing values for all variables referenced\n            in the template string. All variables must be provided.\n\n        Returns\n        -------\n        Message\n            A formatted message object with the specified role and rendered content.\n\n        Raises\n        ------\n        PromptRenderError\n            If any variables referenced in the template are missing from\n            the provided keyword arguments.\n        \"\"\"\n        if isinstance(role, str):\n            role = Role(role)\n\n        all_vars = self._find_variables()\n        missing_vars = set(all_vars) - set(kwargs.keys())\n        if missing_vars:\n            raise PromptRenderError(f\"Missing variables that are required to render the prompt template: {', '.join(missing_vars)}\")\n\n        rendered = self.template_str.format(**kwargs)\n        return Message.from_text(text=rendered, role=role)\n\n    def _find_variables(self) -&gt; List[str]:\n        \"\"\"\n        Extract variable names from the template string.\n\n        Returns\n        -------\n        List[str]\n            A list of unique variable names found in the template string,\n            in the order they first appear. Variable names are extracted\n            from curly brace syntax `{variable_name}`.\n        \"\"\"\n        var_list = re.findall(r'{([^}]+)}', self.template_str)\n        var_list = [var.strip() for var in var_list]\n        return unique_list_in_order(var_list)\n</code></pre>"},{"location":"reference/bridgic-core/bridgic/core/prompt/#bridgic.core.prompt.FstringPromptTemplate.format_message","title":"format_message","text":"<pre><code>format_message(role: Union[Role, str], **kwargs) -&gt; Message\n</code></pre> <p>Format a single message from the template.</p> <p>Parameters:</p> Name Type Description Default <code>role</code> <code>Union[Role, str]</code> <p>The role of the message (e.g., 'user', 'assistant', 'system'). Required parameter for this template implementation.</p> required <code>**kwargs</code> <p>Keyword arguments containing values for all variables referenced in the template string. All variables must be provided.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Message</code> <p>A formatted message object with the specified role and rendered content.</p> <p>Raises:</p> Type Description <code>PromptRenderError</code> <p>If any variables referenced in the template are missing from the provided keyword arguments.</p> Source code in <code>bridgic/core/prompt/_fstring_template.py</code> <pre><code>def format_message(self, role: Union[Role, str], **kwargs) -&gt; Message:\n    \"\"\"\n    Format a single message from the template.\n\n    Parameters\n    ----------\n    role : Union[Role, str]\n        The role of the message (e.g., 'user', 'assistant', 'system').\n        Required parameter for this template implementation.\n    **kwargs\n        Keyword arguments containing values for all variables referenced\n        in the template string. All variables must be provided.\n\n    Returns\n    -------\n    Message\n        A formatted message object with the specified role and rendered content.\n\n    Raises\n    ------\n    PromptRenderError\n        If any variables referenced in the template are missing from\n        the provided keyword arguments.\n    \"\"\"\n    if isinstance(role, str):\n        role = Role(role)\n\n    all_vars = self._find_variables()\n    missing_vars = set(all_vars) - set(kwargs.keys())\n    if missing_vars:\n        raise PromptRenderError(f\"Missing variables that are required to render the prompt template: {', '.join(missing_vars)}\")\n\n    rendered = self.template_str.format(**kwargs)\n    return Message.from_text(text=rendered, role=role)\n</code></pre>"},{"location":"reference/bridgic-core/bridgic/core/prompt/#bridgic.core.prompt.EjinjaPromptTemplate","title":"EjinjaPromptTemplate","text":"<p>               Bases: <code>BasePromptTemplate</code></p> <p>Extended Jinja2-based prompt template with custom message blocks.</p> <p>This template implementation extends the standard Jinja2 syntax with custom <code>{% msg %}</code> blocks to create structured Message objects. It supports both single message and multiple message rendering with variable substitution and content block parsing.</p> <p>Attributes:</p> Name Type Description <code>_env_template</code> <code>Template</code> <p>The compiled Jinja2 template object.</p> <code>_render_cache</code> <code>MemoryCache</code> <p>Cache for rendered template results to improve performance.</p> <p>Methods:</p> Name Description <code>format_message</code> <p>Format a single message from the template.</p> <code>format_messages</code> <p>Format multiple messages from the template.</p> Notes <p>This template supports two rendering modes:</p> <ol> <li>Single Message Mode: Use <code>format_message()</code> to render one message.    </li> <li>Multiple Messages Mode: Use <code>format_messages()</code> to render multiple messages.</li> </ol> <p>Examples:</p> <p>Single message with role in template:</p> <pre><code>&gt;&gt;&gt; template = EjinjaPromptTemplate('''\n... {% msg role=\"system\" %}\n... You are a helpful assistant. User name: {{ name }}\n... {% endmsg %}\n... ''')\n&gt;&gt;&gt; message = template.format_message(name=\"Alice\")\n</code></pre> <p>Single message with role as parameter:</p> <pre><code>&gt;&gt;&gt; template = EjinjaPromptTemplate(\"Hello {{ name }}, how are you?\")\n&gt;&gt;&gt; message = template.format_message(role=\"user\", name=\"Bob\")\n</code></pre> <p>Multiple messages:</p> <pre><code>&gt;&gt;&gt; template = EjinjaPromptTemplate('''\n... {% msg role=\"system\" %}You are helpful{% endmsg %}\n... {% msg role=\"user\" %}Hello {{ name }}{% endmsg %}\n... ''')\n&gt;&gt;&gt; messages = template.format_messages(name=\"Charlie\")\n</code></pre> Source code in <code>bridgic/core/prompt/_ejinja_template.py</code> <pre><code>class EjinjaPromptTemplate(BasePromptTemplate):\n    \"\"\"\n    Extended Jinja2-based prompt template with custom message blocks.\n\n    This template implementation extends the standard Jinja2 syntax with custom\n    `{% msg %}` blocks to create structured Message objects. It supports both\n    single message and multiple message rendering with variable substitution\n    and content block parsing.\n\n    Attributes\n    ----------\n    _env_template : Template\n        The compiled Jinja2 template object.\n    _render_cache : MemoryCache\n        Cache for rendered template results to improve performance.\n\n    Methods\n    -------\n    format_message(role, **kwargs)\n        Format a single message from the template.\n    format_messages(**kwargs)\n        Format multiple messages from the template.\n\n    Notes\n    -----\n    This template supports two rendering modes:\n\n    1. **Single Message Mode**: Use `format_message()` to render one message.    \n    2. **Multiple Messages Mode**: Use `format_messages()` to render multiple messages.\n\n    Examples\n    --------\n    Single message with role in template:\n    &gt;&gt;&gt; template = EjinjaPromptTemplate('''\n    ... {% msg role=\"system\" %}\n    ... You are a helpful assistant. User name: {{ name }}\n    ... {% endmsg %}\n    ... ''')\n    &gt;&gt;&gt; message = template.format_message(name=\"Alice\")\n\n    Single message with role as parameter:\n    &gt;&gt;&gt; template = EjinjaPromptTemplate(\"Hello {{ name }}, how are you?\")\n    &gt;&gt;&gt; message = template.format_message(role=\"user\", name=\"Bob\")\n\n    Multiple messages:\n    &gt;&gt;&gt; template = EjinjaPromptTemplate('''\n    ... {% msg role=\"system\" %}You are helpful{% endmsg %}\n    ... {% msg role=\"user\" %}Hello {{ name }}{% endmsg %}\n    ... ''')\n    &gt;&gt;&gt; messages = template.format_messages(name=\"Charlie\")\n    \"\"\"\n\n    _env_template: Template\n    _render_cache: MemoryCache\n\n    def __init__(self, template_str: str):\n        \"\"\"\n        Initialize the EjinjaPromptTemplate.\n\n        Parameters\n        ----------\n        template_str : str\n            The Jinja2 template string with optional `{% msg %}` blocks.\n        \"\"\"\n        super().__init__(template_str=template_str)\n        self._env_template = env.from_string(template_str)\n        self._render_cache = MemoryCache()\n\n    def format_message(self, role: Union[Role, str] = None, **kwargs) -&gt; Message:\n        \"\"\"\n        Format a single message from the template.\n\n        Parameters\n        ----------\n        role : Union[Role, str], optional\n            The role of the message. If the template contains a `{% msg %}` block,\n            this parameter should be None as the role will be extracted from\n            the template. If no `{% msg %}` block exists, this parameter is required.\n        **kwargs\n            Additional keyword arguments to be substituted into the template.\n\n        Returns\n        -------\n        Message\n            A formatted message object with the specified role and content.\n\n        Raises\n        ------\n        PromptSyntaxError\n            If the template contains more than one `{% msg %}` block.\n        PromptRenderError\n            If role parameter conflicts with template-defined role, or if\n            no role is specified when template has no `{% msg %}` block.\n        \"\"\"\n        if isinstance(role, str):\n            role = Role(role)\n\n        rendered = self._env_template.render(**kwargs)\n        match_list = re.findall(r\"{%\\s*msg\\s*role=\\\"(.*?)\\\"\\s*%}(.*?){%\\s*endmsg\\s*%}\", rendered)\n        if len(match_list) &gt; 1:\n            raise PromptSyntaxError(\n                f\"It is required to just have one {{% msg %}} block in the template, \"\n                f\"but got {len(match_list)}\"\n            )\n        elif len(match_list) == 1:\n            if role is not None:\n                raise PromptRenderError(\n                    f\"If you want to render a single message, the role has to be only specified in the template \"\n                    f\"and not be passed as an argument to the \\\"format_message\\\" method in {type(self).__name__}\"\n                )\n            role, content = match_list[0][0], match_list[0][1]\n        else:\n            if role is None:\n                raise PromptRenderError(\n                    f\"If you want to render a template without {{% msg %}} blocks, the role has to be specified \"\n                    f\"as an argument to the \\\"format_message\\\" method in {type(self).__name__}\"\n                )\n            role, content = role, rendered\n        return Message.from_text(text=content, role=role)\n\n    def format_messages(self, **kwargs) -&gt; List[Message]:\n        \"\"\"\n        Format multiple messages from the template.\n\n        Parameters\n        ----------\n        **kwargs\n            Additional keyword arguments to be substituted into the template.\n\n        Returns\n        -------\n        List[Message]\n            A list of formatted message objects. Each line of the rendered\n            template should be a valid JSON representation of a Message object.\n            If no valid messages are found but content exists, a default user\n            message is created.\n\n        Raises\n        ------\n        PromptRenderError\n            If any line in the rendered template is not a valid JSON\n            representation of a Message object.\n\n        Notes\n        -----\n        This method uses caching to improve performance for repeated calls\n        with the same parameters. The rendered template is cached based on\n        the provided keyword arguments.\n        \"\"\"\n        rendered = self._render_cache.get(kwargs)\n        if not rendered:\n            rendered = self._env_template.render(kwargs)\n            self._render_cache.set(kwargs, rendered)\n\n        messages: List[Message] = []\n        for line in rendered.strip().split(\"\\n\"):\n            try:\n                messages.append(Message.model_validate_json(line))\n            except Exception:\n                raise PromptRenderError(\n                    f\"It is required to wrap each content in a {{% msg %}} block when calling the \"\n                    f\"\\\"format_messages\\\" method of {type(self).__name__}, but got: {line}\"\n                )\n\n        if not messages and rendered.strip():\n            messages.append(_chat_message_from_text(role=\"user\", content=rendered))\n        return messages\n</code></pre>"},{"location":"reference/bridgic-core/bridgic/core/prompt/#bridgic.core.prompt.EjinjaPromptTemplate.format_message","title":"format_message","text":"<pre><code>format_message(\n    role: Union[Role, str] = None, **kwargs\n) -&gt; Message\n</code></pre> <p>Format a single message from the template.</p> <p>Parameters:</p> Name Type Description Default <code>role</code> <code>Union[Role, str]</code> <p>The role of the message. If the template contains a <code>{% msg %}</code> block, this parameter should be None as the role will be extracted from the template. If no <code>{% msg %}</code> block exists, this parameter is required.</p> <code>None</code> <code>**kwargs</code> <p>Additional keyword arguments to be substituted into the template.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Message</code> <p>A formatted message object with the specified role and content.</p> <p>Raises:</p> Type Description <code>PromptSyntaxError</code> <p>If the template contains more than one <code>{% msg %}</code> block.</p> <code>PromptRenderError</code> <p>If role parameter conflicts with template-defined role, or if no role is specified when template has no <code>{% msg %}</code> block.</p> Source code in <code>bridgic/core/prompt/_ejinja_template.py</code> <pre><code>def format_message(self, role: Union[Role, str] = None, **kwargs) -&gt; Message:\n    \"\"\"\n    Format a single message from the template.\n\n    Parameters\n    ----------\n    role : Union[Role, str], optional\n        The role of the message. If the template contains a `{% msg %}` block,\n        this parameter should be None as the role will be extracted from\n        the template. If no `{% msg %}` block exists, this parameter is required.\n    **kwargs\n        Additional keyword arguments to be substituted into the template.\n\n    Returns\n    -------\n    Message\n        A formatted message object with the specified role and content.\n\n    Raises\n    ------\n    PromptSyntaxError\n        If the template contains more than one `{% msg %}` block.\n    PromptRenderError\n        If role parameter conflicts with template-defined role, or if\n        no role is specified when template has no `{% msg %}` block.\n    \"\"\"\n    if isinstance(role, str):\n        role = Role(role)\n\n    rendered = self._env_template.render(**kwargs)\n    match_list = re.findall(r\"{%\\s*msg\\s*role=\\\"(.*?)\\\"\\s*%}(.*?){%\\s*endmsg\\s*%}\", rendered)\n    if len(match_list) &gt; 1:\n        raise PromptSyntaxError(\n            f\"It is required to just have one {{% msg %}} block in the template, \"\n            f\"but got {len(match_list)}\"\n        )\n    elif len(match_list) == 1:\n        if role is not None:\n            raise PromptRenderError(\n                f\"If you want to render a single message, the role has to be only specified in the template \"\n                f\"and not be passed as an argument to the \\\"format_message\\\" method in {type(self).__name__}\"\n            )\n        role, content = match_list[0][0], match_list[0][1]\n    else:\n        if role is None:\n            raise PromptRenderError(\n                f\"If you want to render a template without {{% msg %}} blocks, the role has to be specified \"\n                f\"as an argument to the \\\"format_message\\\" method in {type(self).__name__}\"\n            )\n        role, content = role, rendered\n    return Message.from_text(text=content, role=role)\n</code></pre>"},{"location":"reference/bridgic-core/bridgic/core/prompt/#bridgic.core.prompt.EjinjaPromptTemplate.format_messages","title":"format_messages","text":"<pre><code>format_messages(**kwargs) -&gt; List[Message]\n</code></pre> <p>Format multiple messages from the template.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <p>Additional keyword arguments to be substituted into the template.</p> <code>{}</code> <p>Returns:</p> Type Description <code>List[Message]</code> <p>A list of formatted message objects. Each line of the rendered template should be a valid JSON representation of a Message object. If no valid messages are found but content exists, a default user message is created.</p> <p>Raises:</p> Type Description <code>PromptRenderError</code> <p>If any line in the rendered template is not a valid JSON representation of a Message object.</p> Notes <p>This method uses caching to improve performance for repeated calls with the same parameters. The rendered template is cached based on the provided keyword arguments.</p> Source code in <code>bridgic/core/prompt/_ejinja_template.py</code> <pre><code>def format_messages(self, **kwargs) -&gt; List[Message]:\n    \"\"\"\n    Format multiple messages from the template.\n\n    Parameters\n    ----------\n    **kwargs\n        Additional keyword arguments to be substituted into the template.\n\n    Returns\n    -------\n    List[Message]\n        A list of formatted message objects. Each line of the rendered\n        template should be a valid JSON representation of a Message object.\n        If no valid messages are found but content exists, a default user\n        message is created.\n\n    Raises\n    ------\n    PromptRenderError\n        If any line in the rendered template is not a valid JSON\n        representation of a Message object.\n\n    Notes\n    -----\n    This method uses caching to improve performance for repeated calls\n    with the same parameters. The rendered template is cached based on\n    the provided keyword arguments.\n    \"\"\"\n    rendered = self._render_cache.get(kwargs)\n    if not rendered:\n        rendered = self._env_template.render(kwargs)\n        self._render_cache.set(kwargs, rendered)\n\n    messages: List[Message] = []\n    for line in rendered.strip().split(\"\\n\"):\n        try:\n            messages.append(Message.model_validate_json(line))\n        except Exception:\n            raise PromptRenderError(\n                f\"It is required to wrap each content in a {{% msg %}} block when calling the \"\n                f\"\\\"format_messages\\\" method of {type(self).__name__}, but got: {line}\"\n            )\n\n    if not messages and rendered.strip():\n        messages.append(_chat_message_from_text(role=\"user\", content=rendered))\n    return messages\n</code></pre>"},{"location":"reference/bridgic-core/bridgic/core/types/","title":"types","text":"<p>The Types module defines several basic data types for the framework.</p>"},{"location":"reference/bridgic-core/bridgic/core/types/#bridgic.core.types.Serializable","title":"Serializable","text":"<p>               Bases: <code>Protocol</code></p> <p>Serializable is a protocol that defines the interfaces that customizes serialization.</p> Source code in <code>bridgic/core/types/_serialization.py</code> <pre><code>@runtime_checkable\nclass Serializable(Protocol):\n    \"\"\"\n    Serializable is a protocol that defines the interfaces that customizes serialization.\n    \"\"\"\n    @abstractmethod\n    def dump_to_dict(self) -&gt; Dict[str, Any]:\n        \"\"\"\n        Dump the object to a dictionary, which will finally be serialized to bytes.\n        \"\"\"\n        ...\n\n    @abstractmethod\n    def load_from_dict(self, state_dict: Dict[str, Any]) -&gt; None:\n        \"\"\"\n        Load the object state from a dictionary previously obtained by deserializing from bytes.\n        \"\"\"\n        ...\n</code></pre>"},{"location":"reference/bridgic-core/bridgic/core/types/#bridgic.core.types.Serializable.dump_to_dict","title":"dump_to_dict","text":"<code>abstractmethod</code> <pre><code>dump_to_dict() -&gt; Dict[str, Any]\n</code></pre> <p>Dump the object to a dictionary, which will finally be serialized to bytes.</p> Source code in <code>bridgic/core/types/_serialization.py</code> <pre><code>@abstractmethod\ndef dump_to_dict(self) -&gt; Dict[str, Any]:\n    \"\"\"\n    Dump the object to a dictionary, which will finally be serialized to bytes.\n    \"\"\"\n    ...\n</code></pre>"},{"location":"reference/bridgic-core/bridgic/core/types/#bridgic.core.types.Serializable.load_from_dict","title":"load_from_dict","text":"<code>abstractmethod</code> <pre><code>load_from_dict(state_dict: Dict[str, Any]) -&gt; None\n</code></pre> <p>Load the object state from a dictionary previously obtained by deserializing from bytes.</p> Source code in <code>bridgic/core/types/_serialization.py</code> <pre><code>@abstractmethod\ndef load_from_dict(self, state_dict: Dict[str, Any]) -&gt; None:\n    \"\"\"\n    Load the object state from a dictionary previously obtained by deserializing from bytes.\n    \"\"\"\n    ...\n</code></pre>"},{"location":"reference/bridgic-core/bridgic/core/types/#bridgic.core.types.Picklable","title":"Picklable","text":"<p>               Bases: <code>Protocol</code></p> <p>Picklable is a protocol that defines the interfaces that customizes serialization using pickle.</p> Notes <p>If a class implements both Serializable and Picklable, the object of the class will be  serialized using the implementation provided by Serializable instead of using pickle.</p> Source code in <code>bridgic/core/types/_serialization.py</code> <pre><code>@runtime_checkable\nclass Picklable(Protocol):\n    \"\"\"\n    Picklable is a protocol that defines the interfaces that customizes serialization using pickle.\n\n    Notes\n    -----\n    If a class implements both Serializable and Picklable, the object of the class will be \n    serialized using the implementation provided by Serializable instead of using pickle.\n    \"\"\"\n\n    def __picklable_marker__(self) -&gt; None:\n        \"\"\"\n        This is just a marker method to distinguish Picklable objects from other objects.\n        Since it is not necessary to implement this method in the subclass, thus no \n        @abstractmethod is used here.\n        \"\"\"\n        ...\n</code></pre>"},{"location":"reference/bridgic-llms-openai/bridgic/llms/openai/","title":"openai","text":"<p>The OpenAI integration module provides support for the OpenAI API.</p> <p>This module implements integration interfaces with OpenAI language models, supporting  calls to large language models provided by OpenAI such as the GPT series, and provides  several wrappers for advanced functionality.</p> <p>You can install the OpenAI integration package for Bridgic by running:</p> <pre><code>pip install bridgic-llms-openai\n</code></pre>"},{"location":"reference/bridgic-llms-openai/bridgic/llms/openai/#bridgic.llms.openai.OpenAIConfiguration","title":"OpenAIConfiguration","text":"<p>               Bases: <code>OpenAILikeConfiguration</code></p> <p>Configuration for OpenAI chat completions.</p> Source code in <code>bridgic/llms/openai/_openai_llm.py</code> <pre><code>class OpenAIConfiguration(OpenAILikeConfiguration):\n    \"\"\"\n    Configuration for OpenAI chat completions.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/bridgic-llms-openai/bridgic/llms/openai/#bridgic.llms.openai.OpenAILlm","title":"OpenAILlm","text":"<p>               Bases: <code>BaseLlm</code>, <code>StructuredOutput</code>, <code>ToolSelection</code></p> <p>Wrapper class for OpenAI, providing common chat and stream calling interfaces for OpenAI model and implementing the common protocols in the Bridgic framework.</p> <p>Parameters:</p> Name Type Description Default <code>api_key</code> <code>str</code> <p>The API key for OpenAI services. Required for authentication.</p> required <code>api_base</code> <code>Optional[str]</code> <p>The base URL for the OpenAI API. If None, uses the default OpenAI endpoint.</p> <code>None</code> <code>configuration</code> <code>Optional[OpenAIConfiguration]</code> <p>The configuration for the OpenAI API. If None, uses the default configuration.</p> <code>OpenAIConfiguration()</code> <code>timeout</code> <code>Optional[float]</code> <p>Request timeout in seconds. If None, no timeout is applied.</p> <code>None</code> <code>http_client</code> <code>Optional[Client]</code> <p>Custom synchronous HTTP client for requests. If None, creates a default client.</p> <code>None</code> <code>http_async_client</code> <code>Optional[AsyncClient]</code> <p>Custom asynchronous HTTP client for requests. If None, creates a default client.</p> <code>None</code> <p>Examples:</p> <p>Basic usage for chat completion:</p> <pre><code>llm = OpenAILlm(api_key=\"your-api-key\")\nmessages = [Message.from_text(\"Hello!\", role=Role.USER)]\nresponse = llm.chat(messages=messages, model=\"gpt-4\")\n</code></pre> <p>Structured output with Pydantic model:</p> <pre><code>class Answer(BaseModel):\n    reasoning: str\n    result: int\n\nconstraint = PydanticModel(model=Answer)\nstructured_response = llm.structured_output(\n    messages=messages,\n    constraint=constraint,\n    model=\"gpt-4\"\n)\n</code></pre> <p>Tool calling:</p> <pre><code>tools = [Tool(name=\"calculator\", description=\"Calculate math\", parameters={})]\ntool_calls, tool_call_response = llm.select_tool(messages=messages, tools=tools, model=\"gpt-4\")\n</code></pre> Source code in <code>bridgic/llms/openai/_openai_llm.py</code> <pre><code>class OpenAILlm(BaseLlm, StructuredOutput, ToolSelection):\n    \"\"\"\n    Wrapper class for OpenAI, providing common chat and stream calling interfaces for OpenAI model\n    and implementing the common protocols in the Bridgic framework.\n\n    Parameters\n    ----------\n    api_key : str\n        The API key for OpenAI services. Required for authentication.\n    api_base : Optional[str]\n        The base URL for the OpenAI API. If None, uses the default OpenAI endpoint.\n    configuration : Optional[OpenAIConfiguration]\n        The configuration for the OpenAI API. If None, uses the default configuration.\n    timeout : Optional[float]\n        Request timeout in seconds. If None, no timeout is applied.\n    http_client : Optional[httpx.Client]\n        Custom synchronous HTTP client for requests. If None, creates a default client.\n    http_async_client : Optional[httpx.AsyncClient]\n        Custom asynchronous HTTP client for requests. If None, creates a default client.\n\n    Examples\n    --------\n    Basic usage for chat completion:\n\n    ```python\n    llm = OpenAILlm(api_key=\"your-api-key\")\n    messages = [Message.from_text(\"Hello!\", role=Role.USER)]\n    response = llm.chat(messages=messages, model=\"gpt-4\")\n    ```\n\n    Structured output with Pydantic model:\n\n    ```python\n    class Answer(BaseModel):\n        reasoning: str\n        result: int\n\n    constraint = PydanticModel(model=Answer)\n    structured_response = llm.structured_output(\n        messages=messages,\n        constraint=constraint,\n        model=\"gpt-4\"\n    )\n    ```\n\n    Tool calling:\n\n    ```python\n    tools = [Tool(name=\"calculator\", description=\"Calculate math\", parameters={})]\n    tool_calls, tool_call_response = llm.select_tool(messages=messages, tools=tools, model=\"gpt-4\")\n    ```\n    \"\"\"\n\n    api_base: str\n    api_key: str\n    configuration: OpenAIConfiguration\n    timeout: float\n    http_client: httpx.Client\n    http_async_client: httpx.AsyncClient\n\n    client: OpenAI\n    async_client: AsyncOpenAI\n\n    def __init__(\n        self,\n        api_key: str,\n        api_base: Optional[str] = None,\n        configuration: Optional[OpenAIConfiguration] = OpenAIConfiguration(),\n        timeout: Optional[float] = None,\n        http_client: Optional[httpx.Client] = None,\n        http_async_client: Optional[httpx.AsyncClient] = None,\n    ):\n        \"\"\"\n        Initialize the OpenAI LLM client with configuration parameters.\n\n        Parameters\n        ----------\n        api_key : str\n            The API key for OpenAI services. Required for authentication.\n        api_base : Optional[str]\n            The base URL for the OpenAI API. If None, uses the default OpenAI endpoint.\n        configuration : Optional[OpenAIConfiguration]\n            The configuration for the OpenAI API. If None, uses the default configuration.\n        timeout : Optional[float]\n            Request timeout in seconds. If None, no timeout is applied.\n        http_client : Optional[httpx.Client]\n            Custom synchronous HTTP client for requests. If None, creates a default client.\n        http_async_client : Optional[httpx.AsyncClient]\n            Custom asynchronous HTTP client for requests. If None, creates a default client.\n        \"\"\"\n        # Record for serialization / deserialization.\n        self.api_base = api_base\n        self.api_key = api_key\n        self.configuration = configuration\n        self.timeout = timeout\n        self.http_client = http_client\n        self.http_async_client = http_async_client\n\n        # Initialize clients.\n        self.client = OpenAI(base_url=api_base, api_key=api_key, timeout=timeout, http_client=http_client)\n        self.async_client = AsyncOpenAI(base_url=api_base, api_key=api_key, timeout=timeout, http_client=http_async_client)\n\n    def chat(\n        self,\n        messages: List[Message],\n        model: Optional[str] = None,\n        temperature: Optional[float] = None,\n        top_p: Optional[float] = None,\n        presence_penalty: Optional[float] = None,\n        frequency_penalty: Optional[float] = None,\n        max_tokens: Optional[int] = None,\n        stop: Optional[List[str]] = None,\n        tools: Optional[List[Tool]] = None,\n        extra_body: Optional[Dict[str, Any]] = None,\n        **kwargs,\n    ) -&gt; Response:\n        \"\"\"\n        Send a synchronous chat completion request to OpenAI.\n\n        Parameters\n        ----------\n        messages : List[Message]\n            A list of messages comprising the conversation so far.\n        model : str\n            Model ID used to generate the response, like `gpt-4o` or `gpt-4`.\n        temperature : Optional[float]\n            What sampling temperature to use, between 0 and 2. Higher values like 0.8 will\n            make the output more random, while lower values like 0.2 will make it more\n            focused and deterministic.\n        top_p : Optional[float]\n            An alternative to sampling with temperature, called nucleus sampling, where the\n            model considers the results of the tokens with top_p probability mass.\n        presence_penalty : Optional[float]\n            Number between -2.0 and 2.0. Positive values penalize new tokens based on\n            whether they appear in the text so far, increasing the model's likelihood to\n            talk about new topics.\n        frequency_penalty : Optional[float]\n            Number between -2.0 and 2.0. Positive values penalize new tokens based on their\n            existing frequency in the text so far, decreasing the model's likelihood to\n            repeat the same line verbatim.\n        max_tokens : Optional[int]\n            The maximum number of tokens that can be generated in the chat completion.\n            This value is now deprecated in favor of `max_completion_tokens`.\n        stop : Optional[List[str]]\n            Up to 4 sequences where the API will stop generating further tokens.\n            Not supported with latest reasoning models `o3` and `o3-mini`.\n        tools : Optional[List[Tool]]\n            A list of tools to use in the chat completion.\n        extra_body : Optional[Dict[str, Any]]\n            Add additional JSON properties to the request.\n        **kwargs\n            Additional keyword arguments passed to the OpenAI API.\n\n        Returns\n        -------\n        Response\n            A response object containing the generated message and raw API response.\n        \"\"\"\n        params = self._build_parameters(\n            messages=messages,\n            model=model,\n            temperature=temperature,\n            top_p=top_p,\n            presence_penalty=presence_penalty,\n            frequency_penalty=frequency_penalty,\n            max_tokens=max_tokens,\n            stop=stop,\n            extra_body=extra_body,\n            **kwargs,\n        )\n        # Validate required parameters for non-streaming chat completion\n        validate_required_params(params, [\"messages\", \"model\"])\n\n        response: ChatCompletion = self.client.chat.completions.create(**params)\n        return self._handle_chat_response(response)\n\n    def stream(\n        self,\n        messages: List[Message],\n        model: Optional[str] = None,\n        temperature: Optional[float] = None,\n        top_p: Optional[float] = None,\n        presence_penalty: Optional[float] = None,\n        frequency_penalty: Optional[float] = None,\n        max_tokens: Optional[int] = None,\n        stop: Optional[List[str]] = None,\n        extra_body: Optional[Dict[str, Any]] = None,\n        **kwargs,\n    ) -&gt; StreamResponse:\n        \"\"\"\n        Send a streaming chat completion request to OpenAI.\n\n        Parameters\n        ----------\n        messages : List[Message]\n            A list of messages comprising the conversation so far.\n        model : str\n            Model ID used to generate the response, like `gpt-4o` or `gpt-4`.\n        temperature : Optional[float]\n            What sampling temperature to use, between 0 and 2. Higher values like 0.8 will\n            make the output more random, while lower values like 0.2 will make it more\n            focused and deterministic.\n        top_p : Optional[float]\n            An alternative to sampling with temperature, called nucleus sampling, where the\n            model considers the results of the tokens with top_p probability mass.\n        presence_penalty : Optional[float]\n            Number between -2.0 and 2.0. Positive values penalize new tokens based on\n            whether they appear in the text so far, increasing the model's likelihood to\n            talk about new topics.\n        frequency_penalty : Optional[float]\n            Number between -2.0 and 2.0. Positive values penalize new tokens based on their\n            existing frequency in the text so far, decreasing the model's likelihood to\n            repeat the same line verbatim.\n        max_tokens : Optional[int]\n            The maximum number of tokens that can be generated in the chat completion.\n            This value is now deprecated in favor of `max_completion_tokens`.\n        stop : Optional[List[str]]\n            Up to 4 sequences where the API will stop generating further tokens.\n            Not supported with latest reasoning models `o3` and `o3-mini`.\n        extra_body : Optional[Dict[str, Any]]\n            Add additional JSON properties to the request.\n        **kwargs\n            Additional keyword arguments passed to the OpenAI API.\n\n        Yields\n        ------\n        MessageChunk\n            Individual chunks of the response as they are received from the API.\n            Each chunk contains a delta (partial content) and the raw response.\n\n        Notes\n        -----\n        This method enables real-time streaming of the model's response,\n        useful for providing incremental updates to users as the response is generated.\n        \"\"\"\n        params = self._build_parameters(\n            messages=messages,\n            model=model,\n            temperature=temperature,\n            top_p=top_p,\n            presence_penalty=presence_penalty,\n            frequency_penalty=frequency_penalty,\n            max_tokens=max_tokens,\n            stop=stop,\n            extra_body=extra_body,\n            stream=True,\n            **kwargs,\n        )\n        # Validate required parameters for streaming chat completion\n        validate_required_params(params, [\"messages\", \"model\", \"stream\"])\n\n        response: Stream[ChatCompletionChunk] = self.client.chat.completions.create(**params)\n        for chunk in response:\n            if chunk.choices and chunk.choices[0].delta.content:\n                delta_content = chunk.choices[0].delta.content\n                delta_content = delta_content if delta_content else \"\"\n                yield MessageChunk(delta=delta_content, raw=chunk)\n\n    async def achat(\n        self,\n        messages: List[Message],\n        model: Optional[str] = None,\n        temperature: Optional[float] = None,\n        top_p: Optional[float] = None,\n        presence_penalty: Optional[float] = None,\n        frequency_penalty: Optional[float] = None,\n        max_tokens: Optional[int] = None,\n        stop: Optional[List[str]] = None,\n        tools: Optional[List[Tool]] = None,\n        extra_body: Optional[Dict[str, Any]] = None,\n        **kwargs,\n    ) -&gt; Response:\n        \"\"\"\n        Send an asynchronous chat completion request to OpenAI.\n\n        Parameters\n        ----------\n        messages : List[Message]\n            A list of messages comprising the conversation so far.\n        model : str\n            Model ID used to generate the response, like `gpt-4o` or `gpt-4`.\n        temperature : Optional[float]\n            What sampling temperature to use, between 0 and 2. Higher values like 0.8 will\n            make the output more random, while lower values like 0.2 will make it more\n            focused and deterministic.\n        top_p : Optional[float]\n            An alternative to sampling with temperature, called nucleus sampling, where the\n            model considers the results of the tokens with top_p probability mass.\n        presence_penalty : Optional[float]\n            Number between -2.0 and 2.0. Positive values penalize new tokens based on\n            whether they appear in the text so far, increasing the model's likelihood to\n            talk about new topics.\n        frequency_penalty : Optional[float]\n            Number between -2.0 and 2.0. Positive values penalize new tokens based on their\n            existing frequency in the text so far, decreasing the model's likelihood to\n            repeat the same line verbatim.\n        max_tokens : Optional[int]\n            The maximum number of tokens that can be generated in the chat completion.\n            This value is now deprecated in favor of `max_completion_tokens`.\n        stop : Optional[List[str]]\n            Up to 4 sequences where the API will stop generating further tokens.\n            Not supported with latest reasoning models `o3` and `o3-mini`.\n        tools : Optional[List[Tool]]\n            A list of tools to use in the chat completion.\n        extra_body : Optional[Dict[str, Any]]\n            Add additional JSON properties to the request.\n        **kwargs\n            Additional keyword arguments passed to the OpenAI API.\n\n        Returns\n        -------\n        Response\n            A response object containing the generated message and raw API response.\n\n        Notes\n        -----\n        This is the asynchronous version of the chat method, suitable for\n        concurrent processing and non-blocking I/O operations.\n        \"\"\"\n        params = self._build_parameters(\n            messages=messages,\n            model=model,\n            temperature=temperature,\n            top_p=top_p,\n            presence_penalty=presence_penalty,\n            frequency_penalty=frequency_penalty,\n            max_tokens=max_tokens,\n            stop=stop,\n            extra_body=extra_body,\n            **kwargs,\n        )\n        # Validate required parameters for non-streaming chat completion\n        validate_required_params(params, [\"messages\", \"model\"])\n\n        response = await self.async_client.chat.completions.create(**params)\n        return self._handle_chat_response(response)\n\n    async def astream(\n        self,\n        messages: List[Message],\n        model: Optional[str] = None,\n        temperature: Optional[float] = None,\n        top_p: Optional[float] = None,\n        presence_penalty: Optional[float] = None,\n        frequency_penalty: Optional[float] = None,\n        max_tokens: Optional[int] = None,\n        stop: Optional[List[str]] = None,\n        extra_body: Optional[Dict[str, Any]] = None,\n        **kwargs,\n    ) -&gt; AsyncStreamResponse:\n        \"\"\"\n        Send an asynchronous streaming chat completion request to OpenAI.\n\n        Parameters\n        ----------\n        messages : List[Message]\n            A list of messages comprising the conversation so far.\n        model : str\n            Model ID used to generate the response, like `gpt-4o` or `gpt-4`.\n        temperature : Optional[float]\n            What sampling temperature to use, between 0 and 2. Higher values like 0.8 will\n            make the output more random, while lower values like 0.2 will make it more\n            focused and deterministic.\n        top_p : Optional[float]\n            An alternative to sampling with temperature, called nucleus sampling, where the\n            model considers the results of the tokens with top_p probability mass.\n        presence_penalty : Optional[float]\n            Number between -2.0 and 2.0. Positive values penalize new tokens based on\n            whether they appear in the text so far, increasing the model's likelihood to\n            talk about new topics.\n        frequency_penalty : Optional[float]\n            Number between -2.0 and 2.0. Positive values penalize new tokens based on their\n            existing frequency in the text so far, decreasing the model's likelihood to\n            repeat the same line verbatim.\n        max_tokens : Optional[int]\n            The maximum number of tokens that can be generated in the chat completion.\n            This value is now deprecated in favor of `max_completion_tokens`.\n        stop : Optional[List[str]]\n            Up to 4 sequences where the API will stop generating further tokens.\n            Not supported with latest reasoning models `o3` and `o3-mini`.\n        extra_body : Optional[Dict[str, Any]]\n            Add additional JSON properties to the request.\n        **kwargs\n            Additional keyword arguments passed to the OpenAI API.\n\n        Yields\n        ------\n        MessageChunk\n            Individual chunks of the response as they are received from the API.\n            Each chunk contains a delta (partial content) and the raw response.\n\n        Notes\n        -----\n        This is the asynchronous version of the stream method, suitable for\n        concurrent processing and non-blocking streaming operations.\n        \"\"\"\n        params = self._build_parameters(\n            messages=messages,\n            model=model,\n            temperature=temperature,\n            top_p=top_p,\n            presence_penalty=presence_penalty,\n            frequency_penalty=frequency_penalty,\n            max_tokens=max_tokens,\n            stop=stop,\n            extra_body=extra_body,\n            stream=True,\n            **kwargs,\n        )\n        # Validate required parameters for streaming chat completion\n        validate_required_params(params, [\"messages\", \"model\", \"stream\"])\n\n        response = await self.async_client.chat.completions.create(**params)\n        async for chunk in response:\n            if chunk.choices and chunk.choices[0].delta.content:\n                delta_content = chunk.choices[0].delta.content\n                delta_content = delta_content if delta_content else \"\"\n                yield MessageChunk(delta=delta_content, raw=chunk)\n\n    def _build_parameters(\n        self,\n        messages: List[Message],\n        model: Optional[str] = None,\n        temperature: Optional[float] = None,\n        top_p: Optional[float] = None,\n        presence_penalty: Optional[float] = None,\n        frequency_penalty: Optional[float] = None,\n        max_tokens: Optional[int] = None,\n        stop: Optional[List[str]] = None,\n        tools: Optional[List[Tool]] = None,\n        extra_body: Optional[Dict[str, Any]] = None,\n        stream: Optional[bool] = None,\n        response_format: Optional[Dict[str, Any]] = None,\n        tool_choice: Optional[ChatCompletionToolChoiceOptionParam] = None,\n        parallel_tool_calls: Optional[bool] = None,\n        **kwargs,\n    ) -&gt; Dict[str, Any]:\n        msgs: List[ChatCompletionMessageParam] = [self._convert_chat_completions_message(msg) for msg in messages]\n\n        # Handle tools parameter - convert to list if provided, otherwise use empty list\n        json_desc_tools = [self._convert_tool_to_json(tool) for tool in tools] if tools is not None else None\n\n        # Build parameters dictionary and filter out None values\n        # The priority order is as follows: configuration passed through the interface &gt; configuration of the instance itself.\n        merge_params = merge_dict(self.configuration.model_dump(), {\n            \"messages\": msgs,\n            \"model\": model,\n            \"temperature\": temperature,\n            \"top_p\": top_p,\n            \"presence_penalty\": presence_penalty,\n            \"frequency_penalty\": frequency_penalty,\n            \"max_tokens\": max_tokens,\n            \"stop\": stop,\n            \"tools\": json_desc_tools,\n            \"extra_body\": extra_body,\n            \"stream\": stream,\n            \"response_format\": response_format,\n            \"tool_choice\": tool_choice,\n            \"parallel_tool_calls\": parallel_tool_calls,\n            **kwargs,\n        })\n\n        params = filter_dict(merge_params, exclude_none=True)\n        return params\n\n    def _handle_chat_response(self, response: ChatCompletion) -&gt; Response:\n        openai_message = response.choices[0].message\n        text = openai_message.content if openai_message.content else \"\"\n\n        if openai_message.refusal:\n            warnings.warn(openai_message.refusal, RuntimeWarning)\n\n        # Handle tool calls in the response\n        # if openai_message.tool_calls:\n        #     # Create a message with both text content and tool calls\n        #     blocks = []\n        #     if text:\n        #         blocks.append(TextBlock(text=text))\n        #     else:\n        #         # Ensure there's always some text content, even if empty\n        #         blocks.append(TextBlock(text=\"\"))\n\n        #     for tool_call in openai_message.tool_calls:\n        #         tool_call_block = ToolCallBlock(\n        #             id=tool_call.id,\n        #             name=tool_call.function.name,\n        #             arguments=json.loads(tool_call.function.arguments)\n        #         )\n        #         blocks.append(tool_call_block)\n\n        #     message = Message(role=Role.AI, blocks=blocks)\n        # else:\n        #     # Regular text response\n        #     message = Message.from_text(text, role=Role.AI)\n\n        return Response(\n            message=Message.from_text(text, role=Role.AI),\n            raw=response,\n        )\n\n    def _convert_chat_completions_message(self, message: Message) -&gt; ChatCompletionMessageParam:\n        \"\"\"\n        Convert a Bridgic Message to OpenAI ChatCompletionMessageParam.\n\n        This method handles different message types including:\n        - Text messages\n        - Messages with tool calls (ToolCallBlock)\n        - Messages with tool results (ToolResultBlock)\n\n        Parameters\n        ----\n        message : Message\n            The Bridgic message to convert\n\n        Returns\n        ----\n        ChatCompletionMessageParam\n            The converted OpenAI message parameter\n        \"\"\"\n        # Extract text content from TextBlocks and ToolResultBlocks\n        content_list = []\n        for block in message.blocks:\n            if isinstance(block, TextBlock):\n                content_list.append(block.text)\n            elif isinstance(block, ToolResultBlock):\n                content_list.append(block.content)\n        content_txt = \"\\n\\n\".join(content_list) if content_list else \"\"\n\n        # Extract tool calls from ToolCallBlocks\n        tool_calls = []\n        for block in message.blocks:\n            if isinstance(block, ToolCallBlock):\n                tool_call = ChatCompletionMessageToolCallParam(\n                    id=block.id,\n                    type=\"function\",\n                    function=Function(\n                        name=block.name,\n                        arguments=json.dumps(block.arguments)\n                    )\n                )\n                tool_calls.append(tool_call)\n\n        # Handle different message roles\n        if message.role == Role.SYSTEM:\n            return ChatCompletionSystemMessageParam(content=content_txt, role=\"system\", **message.extras)\n        elif message.role == Role.USER:\n            return ChatCompletionUserMessageParam(content=content_txt, role=\"user\", **message.extras)\n        elif message.role == Role.AI:\n            # For AI messages, include tool calls if present\n            if tool_calls:\n                return ChatCompletionAssistantMessageParam(\n                    content=content_txt, \n                    role=\"assistant\", \n                    tool_calls=tool_calls,\n                    **message.extras\n                )\n            else:\n                return ChatCompletionAssistantMessageParam(content=content_txt, role=\"assistant\", **message.extras)\n        elif message.role == Role.TOOL:\n            # For tool messages, extract tool_call_id from ToolResultBlock\n            tool_call_id = None\n            for block in message.blocks:\n                if isinstance(block, ToolResultBlock):\n                    tool_call_id = block.id\n                    break\n\n            if tool_call_id is None:\n                raise ValueError(\"Tool message must contain a ToolResultBlock with an ID\")\n\n            return ChatCompletionToolMessageParam(\n                content=content_txt, \n                role=\"tool\", \n                tool_call_id=tool_call_id,\n                **message.extras\n            )\n        else:\n            raise ValueError(f\"Invalid role: {message.role}\")\n\n    @overload\n    def structured_output(\n        self,\n        messages: List[Message],\n        constraint: PydanticModel,\n        model: Optional[str] = None,\n        temperature: Optional[float] = ...,\n        top_p: Optional[float] = ...,\n        presence_penalty: Optional[float] = ...,\n        frequency_penalty: Optional[float] = ...,\n        extra_body: Optional[Dict[str, Any]] = ...,\n        **kwargs,\n    ) -&gt; BaseModel: ...\n\n    @overload\n    def structured_output(\n        self,\n        messages: List[Message],\n        constraint: JsonSchema,\n        model: Optional[str] = None,\n        temperature: Optional[float] = ...,\n        top_p: Optional[float] = ...,\n        presence_penalty: Optional[float] = ...,\n        frequency_penalty: Optional[float] = ...,\n        extra_body: Optional[Dict[str, Any]] = ...,\n        **kwargs,\n    ) -&gt; Dict[str, Any]: ...\n\n\n    def structured_output(\n        self,\n        messages: List[Message],\n        constraint: Union[PydanticModel, JsonSchema],\n        model: Optional[str] = None,\n        temperature: Optional[float] = None,\n        top_p: Optional[float] = None,\n        presence_penalty: Optional[float] = None,\n        frequency_penalty: Optional[float] = None,\n        extra_body: Optional[Dict[str, Any]] = None,\n        **kwargs,\n    ) -&gt; Union[BaseModel, Dict[str, Any]]:\n        \"\"\"\n        Generate structured output in a specified format using OpenAI's structured output API.\n\n        This method leverages OpenAI's structured output capabilities to ensure the model\n        response conforms to a specified schema. Recommended for use with GPT-4o and later models.\n\n        Parameters\n        ----------\n        messages : List[Message]\n            A list of messages comprising the conversation so far.\n        constraint : Constraint\n            The constraint defining the desired output format (PydanticModel or JsonSchema).\n        model : str\n            Model ID used to generate the response. Structured outputs work best with GPT-4o and later.\n        temperature : Optional[float]\n            What sampling temperature to use, between 0 and 2. Higher values like 0.8 will\n            make the output more random, while lower values like 0.2 will make it more\n            focused and deterministic.\n        top_p : Optional[float]\n            An alternative to sampling with temperature, called nucleus sampling, where the\n            model considers the results of the tokens with top_p probability mass.\n        presence_penalty : Optional[float]\n            Number between -2.0 and 2.0. Positive values penalize new tokens based on\n            whether they appear in the text so far, increasing the model's likelihood to\n            talk about new topics.\n        frequency_penalty : Optional[float]\n            Number between -2.0 and 2.0. Positive values penalize new tokens based on their\n            existing frequency in the text so far, decreasing the model's likelihood to\n            repeat the same line verbatim.\n        extra_body : Optional[Dict[str, Any]]\n            Add additional JSON properties to the request.\n        **kwargs\n            Additional keyword arguments passed to the OpenAI API.\n\n        Returns\n        -------\n        Union[BaseModel, Dict[str, Any]]\n            The structured response in the format specified by the constraint:\n            - BaseModel instance if constraint is PydanticModel\n            - Dict[str, Any] if constraint is JsonSchema\n\n        Examples\n        --------\n        Using a Pydantic model constraint:\n\n        ```python\n        class Answer(BaseModel):\n            reasoning: str\n            result: int\n\n        constraint = PydanticModel(model=Answer)\n        response = llm.structured_output(\n            messages=[Message.from_text(\"What is 2+2?\", role=Role.USER)],\n            constraint=constraint,\n            model=\"gpt-4o\"\n        )\n        print(response.reasoning, response.result)\n        ```\n\n        Using a JSON schema constraint:\n\n        ```python\n        schema = {\"type\": \"object\", \"properties\": {\"answer\": {\"type\": \"string\"}}}\n        constraint = JsonSchema(schema=schema)\n        response = llm.structured_output(\n            messages=[Message.from_text(\"Hello\", role=Role.USER)],\n            constraint=constraint,\n            model=\"gpt-4o\"\n        )\n        print(response[\"answer\"])\n        ```\n\n        Notes\n        -----\n        - Utilizes OpenAI's native structured output API with strict schema validation\n        - All schemas automatically have additionalProperties set to False\n        - Best performance achieved with GPT-4o and later models (gpt-4o-mini, gpt-4o-2024-08-06, and later)\n        \"\"\"\n        params = self._build_parameters(\n            messages=messages,\n            model=model,\n            temperature=temperature,\n            top_p=top_p,\n            presence_penalty=presence_penalty,\n            frequency_penalty=frequency_penalty,\n            extra_body=extra_body,\n            response_format=self._get_response_format(constraint),\n            **kwargs,\n        )\n        # Validate required parameters for structured output\n        validate_required_params(params, [\"messages\", \"model\"])\n\n        response = self.client.chat.completions.parse(**params)\n        return self._convert_response(constraint, response.choices[0].message.content)\n\n    async def astructured_output(\n        self,\n        messages: List[Message],\n        constraint: Union[PydanticModel, JsonSchema],\n        model: Optional[str] = None,\n        temperature: Optional[float] = None,\n        top_p: Optional[float] = None,\n        presence_penalty: Optional[float] = None,\n        frequency_penalty: Optional[float] = None,\n        extra_body: Optional[Dict[str, Any]] = None,\n        **kwargs,\n    ) -&gt; Union[BaseModel, Dict[str, Any]]:\n        \"\"\"\n        Asynchronously generate structured output in a specified format using OpenAI's API.\n\n        This is the asynchronous version of structured_output, suitable for concurrent\n        processing and non-blocking operations. It leverages OpenAI's structured output\n        capabilities to ensure the model response conforms to a specified schema.\n\n        Parameters\n        ----------\n        messages : List[Message]\n            A list of messages comprising the conversation so far.\n        constraint : Constraint\n            The constraint defining the desired output format (PydanticModel or JsonSchema).\n        model : str\n            Model ID used to generate the response. Structured outputs work best with GPT-4o and later.\n        temperature : Optional[float]\n            What sampling temperature to use, between 0 and 2. Higher values like 0.8 will\n            make the output more random, while lower values like 0.2 will make it more\n            focused and deterministic.\n        top_p : Optional[float]\n            An alternative to sampling with temperature, called nucleus sampling, where the\n            model considers the results of the tokens with top_p probability mass.\n        presence_penalty : Optional[float]\n            Number between -2.0 and 2.0. Positive values penalize new tokens based on\n            whether they appear in the text so far, increasing the model's likelihood to\n            talk about new topics.\n        frequency_penalty : Optional[float]\n            Number between -2.0 and 2.0. Positive values penalize new tokens based on their\n            existing frequency in the text so far, decreasing the model's likelihood to\n            repeat the same line verbatim.\n        extra_body : Optional[Dict[str, Any]]\n            Add additional JSON properties to the request.\n        **kwargs\n            Additional keyword arguments passed to the OpenAI API.\n\n        Returns\n        -------\n        Union[BaseModel, Dict[str, Any]]\n            The structured response in the format specified by the constraint:\n            - BaseModel instance if constraint is PydanticModel\n            - Dict[str, Any] if constraint is JsonSchema\n\n        Examples\n        --------\n        Using asynchronous structured output:\n\n        ```python\n        async def get_structured_response():\n            llm = OpenAILlm(api_key=\"your-key\")\n            constraint = PydanticModel(model=Answer)\n            response = await llm.astructured_output(\n                messages=[Message.from_text(\"Calculate 5+3\", role=Role.USER)],\n                constraint=constraint,\n                model=\"gpt-4o\"\n            )\n            return response\n        ```\n\n        Notes\n        -----\n        - This is the asynchronous version of structured_output\n        - Utilizes OpenAI's native structured output API with strict schema validation\n        - Suitable for concurrent processing and high-throughput applications\n        - Best performance achieved with GPT-4o and later models (gpt-4o-mini, gpt-4o-2024-08-06, and later)\n        \"\"\"\n        params = self._build_parameters(\n            messages=messages,\n            model=model,\n            temperature=temperature,\n            top_p=top_p,\n            presence_penalty=presence_penalty,\n            frequency_penalty=frequency_penalty,\n            extra_body=extra_body,\n            response_format=self._get_response_format(constraint),\n            **kwargs,\n        )\n        # Validate required parameters for structured output\n        validate_required_params(params, [\"messages\", \"model\"])\n\n        response = await self.async_client.chat.completions.parse(**params)\n        return self._convert_response(constraint, response.choices[0].message.content)\n\n    def _add_schema_properties(self, schema: Dict[str, Any]) -&gt; Dict[str, Any]:\n        \"\"\"\n        OpenAI requires additionalProperties to be set to False for all objects\n        in structured output schemas. See:\n        [AdditionalProperties False Must Always Be Set in Objects](https://platform.openai.com/docs/guides/structured-outputs?example=moderation#additionalproperties-false-must-always-be-set-in-objects)\n        \"\"\"\n        schema[\"additionalProperties\"] = False\n        return schema\n\n    def _get_response_format(self, constraint: Union[PydanticModel, JsonSchema]) -&gt; Dict[str, Any]:\n        if isinstance(constraint, PydanticModel):\n            result = {\n                \"type\": \"json_schema\",\n                \"json_schema\": {\n                    \"schema\": self._add_schema_properties(constraint.model.model_json_schema()),\n                    \"name\": constraint.model.__name__,\n                    \"strict\": True,\n                },\n            }\n            return result\n        elif isinstance(constraint, JsonSchema):\n            return {\n                \"type\": \"json_schema\",\n                \"json_schema\": {\n                    \"schema\": self._add_schema_properties(constraint.schema_dict),\n                    # default name for schema\n                    \"name\": \"schema\",\n                    \"strict\": True,\n                },\n            }\n        else:\n            raise ValueError(f\"Unsupported constraint type '{constraint.constraint_type}'. More info about OpenAI structured output: https://platform.openai.com/docs/guides/structured-outputs\")\n\n    def _convert_response(\n        self,\n        constraint: Union[PydanticModel, JsonSchema],\n        content: str,\n    ) -&gt; Union[BaseModel, Dict[str, Any]]:\n        if isinstance(constraint, PydanticModel):\n            return constraint.model.model_validate_json(content)\n        elif isinstance(constraint, JsonSchema):\n            return json.loads(content)\n        else:\n            raise ValueError(f\"Unsupported constraint type '{constraint.constraint_type}'. More info about OpenAI structured output: https://platform.openai.com/docs/guides/structured-outputs\")\n\n    def select_tool(\n        self,\n        messages: List[Message],\n        tools: List[Tool],\n        model: Optional[str] = None,\n        temperature: Optional[float] = None,\n        top_p: Optional[float] = None,\n        presence_penalty: Optional[float] = None,\n        frequency_penalty: Optional[float] = None,\n        extra_body: Optional[Dict[str, Any]] = None,\n        parallel_tool_calls: Optional[bool] = None,\n        tool_choice: Union[Literal[\"auto\", \"required\", \"none\"], ChatCompletionNamedToolChoiceParam] = None,\n        **kwargs,\n    ) -&gt; Tuple[List[ToolCall], Optional[str]]:\n        \"\"\"\n        Select and invoke tools from a list based on conversation context.\n\n        This method enables the model to intelligently select and call appropriate tools\n        from a provided list based on the conversation context. It supports OpenAI's\n        function calling capabilities with parallel execution and various control options.\n\n        More OpenAI information: [function-calling](https://platform.openai.com/docs/guides/function-calling)\n\n        Parameters\n        ----------\n        messages : List[Message]\n            A list of messages comprising the conversation so far providing context for tool selection.\n        tools : List[Tool]\n            A list of tools the model may call.\n        model : str\n            Model ID used to generate the response. Function calling requires compatible models.\n        temperature : Optional[float]\n            What sampling temperature to use, between 0 and 2. Higher values like 0.8 will\n            make the output more random, while lower values like 0.2 will make it more\n            focused and deterministic.\n        top_p : Optional[float]\n            An alternative to sampling with temperature, called nucleus sampling, where the\n            model considers the results of the tokens with top_p probability mass.\n        presence_penalty : Optional[float]\n            Number between -2.0 and 2.0. Positive values penalize new tokens based on\n            whether they appear in the text so far, increasing the model's likelihood to\n            talk about new topics.\n        frequency_penalty : Optional[float]\n            Number between -2.0 and 2.0. Positive values penalize new tokens based on their\n            existing frequency in the text so far, decreasing the model's likelihood to\n            repeat the same line verbatim.\n        extra_body : Optional[Dict[str, Any]]\n            Add additional JSON properties to the request.\n        parallel_tool_calls : Optional[bool]\n            Whether to enable parallel function calling during tool use.\n        tool_choice : Union[Literal[\"auto\", \"required\", \"none\"], ChatCompletionNamedToolChoiceParam]\n            Controls which tool, if any, the model may call.\n            - `none`: The model will not call any tool and will instead generate a message. This is the default when no tools are provided.\n            - `auto`: The model may choose to generate a message or call one or more tools. This is the default when tools are provided.\n            - `required`: The model must call one or more tools.\n            - To force a specific tool, pass `{\"type\": \"function\", \"function\": {\"name\": \"my_function\"}}`.\n        **kwargs\n            Additional keyword arguments passed to the OpenAI API.\n\n        Returns\n        -------\n        List[ToolCall]\n            List of selected tool calls with their IDs, names, and parsed arguments.\n        Union[str, None]\n            The content of the message from the model.\n        \"\"\"\n        params = self._build_parameters(\n            messages=messages,\n            model=model,\n            temperature=temperature,\n            top_p=top_p,\n            presence_penalty=presence_penalty,\n            frequency_penalty=frequency_penalty,\n            tools=tools,\n            tool_choice=tool_choice,\n            parallel_tool_calls=parallel_tool_calls,\n            extra_body=extra_body,\n            **kwargs,\n        )\n        # Validate required parameters for tool selection\n        validate_required_params(params, [\"messages\", \"model\"])\n\n        response: ChatCompletion = self.client.chat.completions.create(**params)\n        tool_calls = response.choices[0].message.tool_calls\n        content = response.choices[0].message.content\n        return (self._convert_tool_calls(tool_calls), content)\n\n    async def aselect_tool(\n        self,\n        messages: List[Message],\n        tools: List[Tool],\n        model: Optional[str] = None,\n        temperature: Optional[float] = None,\n        top_p: Optional[float] = None,\n        presence_penalty: Optional[float] = None,\n        frequency_penalty: Optional[float] = None,\n        extra_body: Optional[Dict[str, Any]] = None,\n        parallel_tool_calls: Optional[bool] = None,\n        tool_choice: Union[Literal[\"auto\", \"required\", \"none\"], ChatCompletionNamedToolChoiceParam] = None,\n        **kwargs,\n    )-&gt; Tuple[List[ToolCall], Optional[str]]:\n        \"\"\"\n        Select and invoke tools from a list based on conversation context.\n\n        This method enables the model to intelligently select and call appropriate tools\n        from a provided list based on the conversation context. It supports OpenAI's\n        function calling capabilities with parallel execution and various control options.\n\n        More OpenAI information: [function-calling](https://platform.openai.com/docs/guides/function-calling)\n\n        Parameters\n        ----------\n        messages : List[Message]\n            A list of messages comprising the conversation so far providing context for tool selection.\n        tools : List[Tool]\n            A list of tools the model may call.\n        model : str\n            Model ID used to generate the response. Function calling requires compatible models.\n        temperature : Optional[float]\n            What sampling temperature to use, between 0 and 2. Higher values like 0.8 will\n            make the output more random, while lower values like 0.2 will make it more\n            focused and deterministic.\n        top_p : Optional[float]\n            An alternative to sampling with temperature, called nucleus sampling, where the\n            model considers the results of the tokens with top_p probability mass.\n        presence_penalty : Optional[float]\n            Number between -2.0 and 2.0. Positive values penalize new tokens based on\n            whether they appear in the text so far, increasing the model's likelihood to\n            talk about new topics.\n        frequency_penalty : Optional[float]\n            Number between -2.0 and 2.0. Positive values penalize new tokens based on their\n            existing frequency in the text so far, decreasing the model's likelihood to\n            repeat the same line verbatim.\n        extra_body : Optional[Dict[str, Any]]\n            Add additional JSON properties to the request.\n        parallel_tool_calls : Optional[bool]\n            Whether to enable parallel function calling during tool use.\n        tool_choice : Union[Literal[\"auto\", \"required\", \"none\"], ChatCompletionNamedToolChoiceParam]\n            Controls which tool, if any, the model may call.\n            - `none`: The model will not call any tool and will instead generate a message. This is the default when no tools are provided.\n            - `auto`: The model may choose to generate a message or call one or more tools. This is the default when tools are provided.\n            - `required`: The model must call one or more tools.\n            - To force a specific tool, pass `{\"type\": \"function\", \"function\": {\"name\": \"my_function\"}}`.\n\n        **kwargs\n            Additional keyword arguments passed to the OpenAI API.\n\n        Returns\n        -------\n        List[ToolCall]\n            List of selected tool calls with their IDs, names, and parsed arguments.\n        Union[str, None]\n            The content of the message from the model.\n        \"\"\"\n        params = self._build_parameters(\n            messages=messages,\n            model=model,\n            temperature=temperature,\n            top_p=top_p,\n            presence_penalty=presence_penalty,\n            frequency_penalty=frequency_penalty,\n            tools=tools,\n            tool_choice=tool_choice,\n            parallel_tool_calls=parallel_tool_calls,\n            extra_body=extra_body,\n            **kwargs,\n        )\n        # Validate required parameters for tool selection\n        validate_required_params(params, [\"messages\", \"model\"])\n\n        response: ChatCompletion = await self.async_client.chat.completions.create(**params)\n        tool_calls = response.choices[0].message.tool_calls\n        content = response.choices[0].message.content\n        return (self._convert_tool_calls(tool_calls), content)\n\n    def _convert_parameters(self, parameters: Dict[str, Any]) -&gt; Dict[str, Any]:\n        return {\n            \"type\": \"object\",\n            \"properties\": parameters.get(\"properties\", {}),\n            \"required\": parameters.get(\"required\", []),\n            \"additionalProperties\": False\n        }\n\n    def _convert_tool_to_json(self, tool: Tool) -&gt; Dict[str, Any]:\n        return {\n            \"type\": \"function\",\n            \"function\": {\n                \"name\": tool.name,\n                \"description\": tool.description,\n                \"parameters\": self._convert_parameters(tool.parameters),\n            }\n        }\n\n    def _convert_tool_calls(self, tool_calls: List[ChatCompletionMessageFunctionToolCall]) -&gt; List[ToolCall]:\n        return [] if tool_calls is None else [\n            ToolCall(\n                id=tool_call.id,\n                name=tool_call.function.name,\n                arguments=json.loads(tool_call.function.arguments),\n            ) for tool_call in tool_calls\n        ]\n\n    @override\n    def dump_to_dict(self) -&gt; Dict[str, Any]:\n        state_dict = {\n            \"api_base\": self.api_base,\n            \"api_key\": self.api_key,\n            \"timeout\": self.timeout,\n            \"configuration\": self.configuration.model_dump(),\n        }\n        if self.http_client:\n            warnings.warn(\n                \"httpx.Client is not serializable, so it will be set to None in the deserialization.\",\n                RuntimeWarning,\n            )\n        if self.http_async_client:\n            warnings.warn(\n                \"httpx.AsyncClient is not serializable, so it will be set to None in the deserialization.\",\n                RuntimeWarning,\n            )\n        return state_dict\n\n    @override\n    def load_from_dict(self, state_dict: Dict[str, Any]) -&gt; None:\n        self.api_base = state_dict[\"api_base\"]\n        self.api_key = state_dict[\"api_key\"]\n        self.timeout = state_dict[\"timeout\"]\n        self.configuration = OpenAIConfiguration(**state_dict.get(\"configuration\", {}))\n        self.http_client = None\n        self.http_async_client = None\n\n        self.client = OpenAI(\n            base_url=self.api_base,\n            api_key=self.api_key,\n            timeout=self.timeout,\n            http_client=self.http_client,\n        )\n        self.async_client = AsyncOpenAI(\n            base_url=self.api_base,\n            api_key=self.api_key,\n            timeout=self.timeout,\n            http_client=self.http_async_client,\n        )\n</code></pre>"},{"location":"reference/bridgic-llms-openai/bridgic/llms/openai/#bridgic.llms.openai.OpenAILlm.chat","title":"chat","text":"<pre><code>chat(\n    messages: List[Message],\n    model: Optional[str] = None,\n    temperature: Optional[float] = None,\n    top_p: Optional[float] = None,\n    presence_penalty: Optional[float] = None,\n    frequency_penalty: Optional[float] = None,\n    max_tokens: Optional[int] = None,\n    stop: Optional[List[str]] = None,\n    tools: Optional[List[Tool]] = None,\n    extra_body: Optional[Dict[str, Any]] = None,\n    **kwargs\n) -&gt; Response\n</code></pre> <p>Send a synchronous chat completion request to OpenAI.</p> <p>Parameters:</p> Name Type Description Default <code>messages</code> <code>List[Message]</code> <p>A list of messages comprising the conversation so far.</p> required <code>model</code> <code>str</code> <p>Model ID used to generate the response, like <code>gpt-4o</code> or <code>gpt-4</code>.</p> <code>None</code> <code>temperature</code> <code>Optional[float]</code> <p>What sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic.</p> <code>None</code> <code>top_p</code> <code>Optional[float]</code> <p>An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass.</p> <code>None</code> <code>presence_penalty</code> <code>Optional[float]</code> <p>Number between -2.0 and 2.0. Positive values penalize new tokens based on whether they appear in the text so far, increasing the model's likelihood to talk about new topics.</p> <code>None</code> <code>frequency_penalty</code> <code>Optional[float]</code> <p>Number between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model's likelihood to repeat the same line verbatim.</p> <code>None</code> <code>max_tokens</code> <code>Optional[int]</code> <p>The maximum number of tokens that can be generated in the chat completion. This value is now deprecated in favor of <code>max_completion_tokens</code>.</p> <code>None</code> <code>stop</code> <code>Optional[List[str]]</code> <p>Up to 4 sequences where the API will stop generating further tokens. Not supported with latest reasoning models <code>o3</code> and <code>o3-mini</code>.</p> <code>None</code> <code>tools</code> <code>Optional[List[Tool]]</code> <p>A list of tools to use in the chat completion.</p> <code>None</code> <code>extra_body</code> <code>Optional[Dict[str, Any]]</code> <p>Add additional JSON properties to the request.</p> <code>None</code> <code>**kwargs</code> <p>Additional keyword arguments passed to the OpenAI API.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Response</code> <p>A response object containing the generated message and raw API response.</p> Source code in <code>bridgic/llms/openai/_openai_llm.py</code> <pre><code>def chat(\n    self,\n    messages: List[Message],\n    model: Optional[str] = None,\n    temperature: Optional[float] = None,\n    top_p: Optional[float] = None,\n    presence_penalty: Optional[float] = None,\n    frequency_penalty: Optional[float] = None,\n    max_tokens: Optional[int] = None,\n    stop: Optional[List[str]] = None,\n    tools: Optional[List[Tool]] = None,\n    extra_body: Optional[Dict[str, Any]] = None,\n    **kwargs,\n) -&gt; Response:\n    \"\"\"\n    Send a synchronous chat completion request to OpenAI.\n\n    Parameters\n    ----------\n    messages : List[Message]\n        A list of messages comprising the conversation so far.\n    model : str\n        Model ID used to generate the response, like `gpt-4o` or `gpt-4`.\n    temperature : Optional[float]\n        What sampling temperature to use, between 0 and 2. Higher values like 0.8 will\n        make the output more random, while lower values like 0.2 will make it more\n        focused and deterministic.\n    top_p : Optional[float]\n        An alternative to sampling with temperature, called nucleus sampling, where the\n        model considers the results of the tokens with top_p probability mass.\n    presence_penalty : Optional[float]\n        Number between -2.0 and 2.0. Positive values penalize new tokens based on\n        whether they appear in the text so far, increasing the model's likelihood to\n        talk about new topics.\n    frequency_penalty : Optional[float]\n        Number between -2.0 and 2.0. Positive values penalize new tokens based on their\n        existing frequency in the text so far, decreasing the model's likelihood to\n        repeat the same line verbatim.\n    max_tokens : Optional[int]\n        The maximum number of tokens that can be generated in the chat completion.\n        This value is now deprecated in favor of `max_completion_tokens`.\n    stop : Optional[List[str]]\n        Up to 4 sequences where the API will stop generating further tokens.\n        Not supported with latest reasoning models `o3` and `o3-mini`.\n    tools : Optional[List[Tool]]\n        A list of tools to use in the chat completion.\n    extra_body : Optional[Dict[str, Any]]\n        Add additional JSON properties to the request.\n    **kwargs\n        Additional keyword arguments passed to the OpenAI API.\n\n    Returns\n    -------\n    Response\n        A response object containing the generated message and raw API response.\n    \"\"\"\n    params = self._build_parameters(\n        messages=messages,\n        model=model,\n        temperature=temperature,\n        top_p=top_p,\n        presence_penalty=presence_penalty,\n        frequency_penalty=frequency_penalty,\n        max_tokens=max_tokens,\n        stop=stop,\n        extra_body=extra_body,\n        **kwargs,\n    )\n    # Validate required parameters for non-streaming chat completion\n    validate_required_params(params, [\"messages\", \"model\"])\n\n    response: ChatCompletion = self.client.chat.completions.create(**params)\n    return self._handle_chat_response(response)\n</code></pre>"},{"location":"reference/bridgic-llms-openai/bridgic/llms/openai/#bridgic.llms.openai.OpenAILlm.stream","title":"stream","text":"<pre><code>stream(\n    messages: List[Message],\n    model: Optional[str] = None,\n    temperature: Optional[float] = None,\n    top_p: Optional[float] = None,\n    presence_penalty: Optional[float] = None,\n    frequency_penalty: Optional[float] = None,\n    max_tokens: Optional[int] = None,\n    stop: Optional[List[str]] = None,\n    extra_body: Optional[Dict[str, Any]] = None,\n    **kwargs\n) -&gt; StreamResponse\n</code></pre> <p>Send a streaming chat completion request to OpenAI.</p> <p>Parameters:</p> Name Type Description Default <code>messages</code> <code>List[Message]</code> <p>A list of messages comprising the conversation so far.</p> required <code>model</code> <code>str</code> <p>Model ID used to generate the response, like <code>gpt-4o</code> or <code>gpt-4</code>.</p> <code>None</code> <code>temperature</code> <code>Optional[float]</code> <p>What sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic.</p> <code>None</code> <code>top_p</code> <code>Optional[float]</code> <p>An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass.</p> <code>None</code> <code>presence_penalty</code> <code>Optional[float]</code> <p>Number between -2.0 and 2.0. Positive values penalize new tokens based on whether they appear in the text so far, increasing the model's likelihood to talk about new topics.</p> <code>None</code> <code>frequency_penalty</code> <code>Optional[float]</code> <p>Number between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model's likelihood to repeat the same line verbatim.</p> <code>None</code> <code>max_tokens</code> <code>Optional[int]</code> <p>The maximum number of tokens that can be generated in the chat completion. This value is now deprecated in favor of <code>max_completion_tokens</code>.</p> <code>None</code> <code>stop</code> <code>Optional[List[str]]</code> <p>Up to 4 sequences where the API will stop generating further tokens. Not supported with latest reasoning models <code>o3</code> and <code>o3-mini</code>.</p> <code>None</code> <code>extra_body</code> <code>Optional[Dict[str, Any]]</code> <p>Add additional JSON properties to the request.</p> <code>None</code> <code>**kwargs</code> <p>Additional keyword arguments passed to the OpenAI API.</p> <code>{}</code> <p>Yields:</p> Type Description <code>MessageChunk</code> <p>Individual chunks of the response as they are received from the API. Each chunk contains a delta (partial content) and the raw response.</p> Notes <p>This method enables real-time streaming of the model's response, useful for providing incremental updates to users as the response is generated.</p> Source code in <code>bridgic/llms/openai/_openai_llm.py</code> <pre><code>def stream(\n    self,\n    messages: List[Message],\n    model: Optional[str] = None,\n    temperature: Optional[float] = None,\n    top_p: Optional[float] = None,\n    presence_penalty: Optional[float] = None,\n    frequency_penalty: Optional[float] = None,\n    max_tokens: Optional[int] = None,\n    stop: Optional[List[str]] = None,\n    extra_body: Optional[Dict[str, Any]] = None,\n    **kwargs,\n) -&gt; StreamResponse:\n    \"\"\"\n    Send a streaming chat completion request to OpenAI.\n\n    Parameters\n    ----------\n    messages : List[Message]\n        A list of messages comprising the conversation so far.\n    model : str\n        Model ID used to generate the response, like `gpt-4o` or `gpt-4`.\n    temperature : Optional[float]\n        What sampling temperature to use, between 0 and 2. Higher values like 0.8 will\n        make the output more random, while lower values like 0.2 will make it more\n        focused and deterministic.\n    top_p : Optional[float]\n        An alternative to sampling with temperature, called nucleus sampling, where the\n        model considers the results of the tokens with top_p probability mass.\n    presence_penalty : Optional[float]\n        Number between -2.0 and 2.0. Positive values penalize new tokens based on\n        whether they appear in the text so far, increasing the model's likelihood to\n        talk about new topics.\n    frequency_penalty : Optional[float]\n        Number between -2.0 and 2.0. Positive values penalize new tokens based on their\n        existing frequency in the text so far, decreasing the model's likelihood to\n        repeat the same line verbatim.\n    max_tokens : Optional[int]\n        The maximum number of tokens that can be generated in the chat completion.\n        This value is now deprecated in favor of `max_completion_tokens`.\n    stop : Optional[List[str]]\n        Up to 4 sequences where the API will stop generating further tokens.\n        Not supported with latest reasoning models `o3` and `o3-mini`.\n    extra_body : Optional[Dict[str, Any]]\n        Add additional JSON properties to the request.\n    **kwargs\n        Additional keyword arguments passed to the OpenAI API.\n\n    Yields\n    ------\n    MessageChunk\n        Individual chunks of the response as they are received from the API.\n        Each chunk contains a delta (partial content) and the raw response.\n\n    Notes\n    -----\n    This method enables real-time streaming of the model's response,\n    useful for providing incremental updates to users as the response is generated.\n    \"\"\"\n    params = self._build_parameters(\n        messages=messages,\n        model=model,\n        temperature=temperature,\n        top_p=top_p,\n        presence_penalty=presence_penalty,\n        frequency_penalty=frequency_penalty,\n        max_tokens=max_tokens,\n        stop=stop,\n        extra_body=extra_body,\n        stream=True,\n        **kwargs,\n    )\n    # Validate required parameters for streaming chat completion\n    validate_required_params(params, [\"messages\", \"model\", \"stream\"])\n\n    response: Stream[ChatCompletionChunk] = self.client.chat.completions.create(**params)\n    for chunk in response:\n        if chunk.choices and chunk.choices[0].delta.content:\n            delta_content = chunk.choices[0].delta.content\n            delta_content = delta_content if delta_content else \"\"\n            yield MessageChunk(delta=delta_content, raw=chunk)\n</code></pre>"},{"location":"reference/bridgic-llms-openai/bridgic/llms/openai/#bridgic.llms.openai.OpenAILlm.achat","title":"achat","text":"<code>async</code> <pre><code>achat(\n    messages: List[Message],\n    model: Optional[str] = None,\n    temperature: Optional[float] = None,\n    top_p: Optional[float] = None,\n    presence_penalty: Optional[float] = None,\n    frequency_penalty: Optional[float] = None,\n    max_tokens: Optional[int] = None,\n    stop: Optional[List[str]] = None,\n    tools: Optional[List[Tool]] = None,\n    extra_body: Optional[Dict[str, Any]] = None,\n    **kwargs\n) -&gt; Response\n</code></pre> <p>Send an asynchronous chat completion request to OpenAI.</p> <p>Parameters:</p> Name Type Description Default <code>messages</code> <code>List[Message]</code> <p>A list of messages comprising the conversation so far.</p> required <code>model</code> <code>str</code> <p>Model ID used to generate the response, like <code>gpt-4o</code> or <code>gpt-4</code>.</p> <code>None</code> <code>temperature</code> <code>Optional[float]</code> <p>What sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic.</p> <code>None</code> <code>top_p</code> <code>Optional[float]</code> <p>An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass.</p> <code>None</code> <code>presence_penalty</code> <code>Optional[float]</code> <p>Number between -2.0 and 2.0. Positive values penalize new tokens based on whether they appear in the text so far, increasing the model's likelihood to talk about new topics.</p> <code>None</code> <code>frequency_penalty</code> <code>Optional[float]</code> <p>Number between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model's likelihood to repeat the same line verbatim.</p> <code>None</code> <code>max_tokens</code> <code>Optional[int]</code> <p>The maximum number of tokens that can be generated in the chat completion. This value is now deprecated in favor of <code>max_completion_tokens</code>.</p> <code>None</code> <code>stop</code> <code>Optional[List[str]]</code> <p>Up to 4 sequences where the API will stop generating further tokens. Not supported with latest reasoning models <code>o3</code> and <code>o3-mini</code>.</p> <code>None</code> <code>tools</code> <code>Optional[List[Tool]]</code> <p>A list of tools to use in the chat completion.</p> <code>None</code> <code>extra_body</code> <code>Optional[Dict[str, Any]]</code> <p>Add additional JSON properties to the request.</p> <code>None</code> <code>**kwargs</code> <p>Additional keyword arguments passed to the OpenAI API.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Response</code> <p>A response object containing the generated message and raw API response.</p> Notes <p>This is the asynchronous version of the chat method, suitable for concurrent processing and non-blocking I/O operations.</p> Source code in <code>bridgic/llms/openai/_openai_llm.py</code> <pre><code>async def achat(\n    self,\n    messages: List[Message],\n    model: Optional[str] = None,\n    temperature: Optional[float] = None,\n    top_p: Optional[float] = None,\n    presence_penalty: Optional[float] = None,\n    frequency_penalty: Optional[float] = None,\n    max_tokens: Optional[int] = None,\n    stop: Optional[List[str]] = None,\n    tools: Optional[List[Tool]] = None,\n    extra_body: Optional[Dict[str, Any]] = None,\n    **kwargs,\n) -&gt; Response:\n    \"\"\"\n    Send an asynchronous chat completion request to OpenAI.\n\n    Parameters\n    ----------\n    messages : List[Message]\n        A list of messages comprising the conversation so far.\n    model : str\n        Model ID used to generate the response, like `gpt-4o` or `gpt-4`.\n    temperature : Optional[float]\n        What sampling temperature to use, between 0 and 2. Higher values like 0.8 will\n        make the output more random, while lower values like 0.2 will make it more\n        focused and deterministic.\n    top_p : Optional[float]\n        An alternative to sampling with temperature, called nucleus sampling, where the\n        model considers the results of the tokens with top_p probability mass.\n    presence_penalty : Optional[float]\n        Number between -2.0 and 2.0. Positive values penalize new tokens based on\n        whether they appear in the text so far, increasing the model's likelihood to\n        talk about new topics.\n    frequency_penalty : Optional[float]\n        Number between -2.0 and 2.0. Positive values penalize new tokens based on their\n        existing frequency in the text so far, decreasing the model's likelihood to\n        repeat the same line verbatim.\n    max_tokens : Optional[int]\n        The maximum number of tokens that can be generated in the chat completion.\n        This value is now deprecated in favor of `max_completion_tokens`.\n    stop : Optional[List[str]]\n        Up to 4 sequences where the API will stop generating further tokens.\n        Not supported with latest reasoning models `o3` and `o3-mini`.\n    tools : Optional[List[Tool]]\n        A list of tools to use in the chat completion.\n    extra_body : Optional[Dict[str, Any]]\n        Add additional JSON properties to the request.\n    **kwargs\n        Additional keyword arguments passed to the OpenAI API.\n\n    Returns\n    -------\n    Response\n        A response object containing the generated message and raw API response.\n\n    Notes\n    -----\n    This is the asynchronous version of the chat method, suitable for\n    concurrent processing and non-blocking I/O operations.\n    \"\"\"\n    params = self._build_parameters(\n        messages=messages,\n        model=model,\n        temperature=temperature,\n        top_p=top_p,\n        presence_penalty=presence_penalty,\n        frequency_penalty=frequency_penalty,\n        max_tokens=max_tokens,\n        stop=stop,\n        extra_body=extra_body,\n        **kwargs,\n    )\n    # Validate required parameters for non-streaming chat completion\n    validate_required_params(params, [\"messages\", \"model\"])\n\n    response = await self.async_client.chat.completions.create(**params)\n    return self._handle_chat_response(response)\n</code></pre>"},{"location":"reference/bridgic-llms-openai/bridgic/llms/openai/#bridgic.llms.openai.OpenAILlm.astream","title":"astream","text":"<code>async</code> <pre><code>astream(\n    messages: List[Message],\n    model: Optional[str] = None,\n    temperature: Optional[float] = None,\n    top_p: Optional[float] = None,\n    presence_penalty: Optional[float] = None,\n    frequency_penalty: Optional[float] = None,\n    max_tokens: Optional[int] = None,\n    stop: Optional[List[str]] = None,\n    extra_body: Optional[Dict[str, Any]] = None,\n    **kwargs\n) -&gt; AsyncStreamResponse\n</code></pre> <p>Send an asynchronous streaming chat completion request to OpenAI.</p> <p>Parameters:</p> Name Type Description Default <code>messages</code> <code>List[Message]</code> <p>A list of messages comprising the conversation so far.</p> required <code>model</code> <code>str</code> <p>Model ID used to generate the response, like <code>gpt-4o</code> or <code>gpt-4</code>.</p> <code>None</code> <code>temperature</code> <code>Optional[float]</code> <p>What sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic.</p> <code>None</code> <code>top_p</code> <code>Optional[float]</code> <p>An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass.</p> <code>None</code> <code>presence_penalty</code> <code>Optional[float]</code> <p>Number between -2.0 and 2.0. Positive values penalize new tokens based on whether they appear in the text so far, increasing the model's likelihood to talk about new topics.</p> <code>None</code> <code>frequency_penalty</code> <code>Optional[float]</code> <p>Number between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model's likelihood to repeat the same line verbatim.</p> <code>None</code> <code>max_tokens</code> <code>Optional[int]</code> <p>The maximum number of tokens that can be generated in the chat completion. This value is now deprecated in favor of <code>max_completion_tokens</code>.</p> <code>None</code> <code>stop</code> <code>Optional[List[str]]</code> <p>Up to 4 sequences where the API will stop generating further tokens. Not supported with latest reasoning models <code>o3</code> and <code>o3-mini</code>.</p> <code>None</code> <code>extra_body</code> <code>Optional[Dict[str, Any]]</code> <p>Add additional JSON properties to the request.</p> <code>None</code> <code>**kwargs</code> <p>Additional keyword arguments passed to the OpenAI API.</p> <code>{}</code> <p>Yields:</p> Type Description <code>MessageChunk</code> <p>Individual chunks of the response as they are received from the API. Each chunk contains a delta (partial content) and the raw response.</p> Notes <p>This is the asynchronous version of the stream method, suitable for concurrent processing and non-blocking streaming operations.</p> Source code in <code>bridgic/llms/openai/_openai_llm.py</code> <pre><code>async def astream(\n    self,\n    messages: List[Message],\n    model: Optional[str] = None,\n    temperature: Optional[float] = None,\n    top_p: Optional[float] = None,\n    presence_penalty: Optional[float] = None,\n    frequency_penalty: Optional[float] = None,\n    max_tokens: Optional[int] = None,\n    stop: Optional[List[str]] = None,\n    extra_body: Optional[Dict[str, Any]] = None,\n    **kwargs,\n) -&gt; AsyncStreamResponse:\n    \"\"\"\n    Send an asynchronous streaming chat completion request to OpenAI.\n\n    Parameters\n    ----------\n    messages : List[Message]\n        A list of messages comprising the conversation so far.\n    model : str\n        Model ID used to generate the response, like `gpt-4o` or `gpt-4`.\n    temperature : Optional[float]\n        What sampling temperature to use, between 0 and 2. Higher values like 0.8 will\n        make the output more random, while lower values like 0.2 will make it more\n        focused and deterministic.\n    top_p : Optional[float]\n        An alternative to sampling with temperature, called nucleus sampling, where the\n        model considers the results of the tokens with top_p probability mass.\n    presence_penalty : Optional[float]\n        Number between -2.0 and 2.0. Positive values penalize new tokens based on\n        whether they appear in the text so far, increasing the model's likelihood to\n        talk about new topics.\n    frequency_penalty : Optional[float]\n        Number between -2.0 and 2.0. Positive values penalize new tokens based on their\n        existing frequency in the text so far, decreasing the model's likelihood to\n        repeat the same line verbatim.\n    max_tokens : Optional[int]\n        The maximum number of tokens that can be generated in the chat completion.\n        This value is now deprecated in favor of `max_completion_tokens`.\n    stop : Optional[List[str]]\n        Up to 4 sequences where the API will stop generating further tokens.\n        Not supported with latest reasoning models `o3` and `o3-mini`.\n    extra_body : Optional[Dict[str, Any]]\n        Add additional JSON properties to the request.\n    **kwargs\n        Additional keyword arguments passed to the OpenAI API.\n\n    Yields\n    ------\n    MessageChunk\n        Individual chunks of the response as they are received from the API.\n        Each chunk contains a delta (partial content) and the raw response.\n\n    Notes\n    -----\n    This is the asynchronous version of the stream method, suitable for\n    concurrent processing and non-blocking streaming operations.\n    \"\"\"\n    params = self._build_parameters(\n        messages=messages,\n        model=model,\n        temperature=temperature,\n        top_p=top_p,\n        presence_penalty=presence_penalty,\n        frequency_penalty=frequency_penalty,\n        max_tokens=max_tokens,\n        stop=stop,\n        extra_body=extra_body,\n        stream=True,\n        **kwargs,\n    )\n    # Validate required parameters for streaming chat completion\n    validate_required_params(params, [\"messages\", \"model\", \"stream\"])\n\n    response = await self.async_client.chat.completions.create(**params)\n    async for chunk in response:\n        if chunk.choices and chunk.choices[0].delta.content:\n            delta_content = chunk.choices[0].delta.content\n            delta_content = delta_content if delta_content else \"\"\n            yield MessageChunk(delta=delta_content, raw=chunk)\n</code></pre>"},{"location":"reference/bridgic-llms-openai/bridgic/llms/openai/#bridgic.llms.openai.OpenAILlm.structured_output","title":"structured_output","text":"<pre><code>structured_output(\n    messages: List[Message],\n    constraint: Union[PydanticModel, JsonSchema],\n    model: Optional[str] = None,\n    temperature: Optional[float] = None,\n    top_p: Optional[float] = None,\n    presence_penalty: Optional[float] = None,\n    frequency_penalty: Optional[float] = None,\n    extra_body: Optional[Dict[str, Any]] = None,\n    **kwargs\n) -&gt; Union[BaseModel, Dict[str, Any]]\n</code></pre> <p>Generate structured output in a specified format using OpenAI's structured output API.</p> <p>This method leverages OpenAI's structured output capabilities to ensure the model response conforms to a specified schema. Recommended for use with GPT-4o and later models.</p> <p>Parameters:</p> Name Type Description Default <code>messages</code> <code>List[Message]</code> <p>A list of messages comprising the conversation so far.</p> required <code>constraint</code> <code>Constraint</code> <p>The constraint defining the desired output format (PydanticModel or JsonSchema).</p> required <code>model</code> <code>str</code> <p>Model ID used to generate the response. Structured outputs work best with GPT-4o and later.</p> <code>None</code> <code>temperature</code> <code>Optional[float]</code> <p>What sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic.</p> <code>None</code> <code>top_p</code> <code>Optional[float]</code> <p>An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass.</p> <code>None</code> <code>presence_penalty</code> <code>Optional[float]</code> <p>Number between -2.0 and 2.0. Positive values penalize new tokens based on whether they appear in the text so far, increasing the model's likelihood to talk about new topics.</p> <code>None</code> <code>frequency_penalty</code> <code>Optional[float]</code> <p>Number between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model's likelihood to repeat the same line verbatim.</p> <code>None</code> <code>extra_body</code> <code>Optional[Dict[str, Any]]</code> <p>Add additional JSON properties to the request.</p> <code>None</code> <code>**kwargs</code> <p>Additional keyword arguments passed to the OpenAI API.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Union[BaseModel, Dict[str, Any]]</code> <p>The structured response in the format specified by the constraint: - BaseModel instance if constraint is PydanticModel - Dict[str, Any] if constraint is JsonSchema</p> <p>Examples:</p> <p>Using a Pydantic model constraint:</p> <pre><code>class Answer(BaseModel):\n    reasoning: str\n    result: int\n\nconstraint = PydanticModel(model=Answer)\nresponse = llm.structured_output(\n    messages=[Message.from_text(\"What is 2+2?\", role=Role.USER)],\n    constraint=constraint,\n    model=\"gpt-4o\"\n)\nprint(response.reasoning, response.result)\n</code></pre> <p>Using a JSON schema constraint:</p> <pre><code>schema = {\"type\": \"object\", \"properties\": {\"answer\": {\"type\": \"string\"}}}\nconstraint = JsonSchema(schema=schema)\nresponse = llm.structured_output(\n    messages=[Message.from_text(\"Hello\", role=Role.USER)],\n    constraint=constraint,\n    model=\"gpt-4o\"\n)\nprint(response[\"answer\"])\n</code></pre> Notes <ul> <li>Utilizes OpenAI's native structured output API with strict schema validation</li> <li>All schemas automatically have additionalProperties set to False</li> <li>Best performance achieved with GPT-4o and later models (gpt-4o-mini, gpt-4o-2024-08-06, and later)</li> </ul> Source code in <code>bridgic/llms/openai/_openai_llm.py</code> <pre><code>def structured_output(\n    self,\n    messages: List[Message],\n    constraint: Union[PydanticModel, JsonSchema],\n    model: Optional[str] = None,\n    temperature: Optional[float] = None,\n    top_p: Optional[float] = None,\n    presence_penalty: Optional[float] = None,\n    frequency_penalty: Optional[float] = None,\n    extra_body: Optional[Dict[str, Any]] = None,\n    **kwargs,\n) -&gt; Union[BaseModel, Dict[str, Any]]:\n    \"\"\"\n    Generate structured output in a specified format using OpenAI's structured output API.\n\n    This method leverages OpenAI's structured output capabilities to ensure the model\n    response conforms to a specified schema. Recommended for use with GPT-4o and later models.\n\n    Parameters\n    ----------\n    messages : List[Message]\n        A list of messages comprising the conversation so far.\n    constraint : Constraint\n        The constraint defining the desired output format (PydanticModel or JsonSchema).\n    model : str\n        Model ID used to generate the response. Structured outputs work best with GPT-4o and later.\n    temperature : Optional[float]\n        What sampling temperature to use, between 0 and 2. Higher values like 0.8 will\n        make the output more random, while lower values like 0.2 will make it more\n        focused and deterministic.\n    top_p : Optional[float]\n        An alternative to sampling with temperature, called nucleus sampling, where the\n        model considers the results of the tokens with top_p probability mass.\n    presence_penalty : Optional[float]\n        Number between -2.0 and 2.0. Positive values penalize new tokens based on\n        whether they appear in the text so far, increasing the model's likelihood to\n        talk about new topics.\n    frequency_penalty : Optional[float]\n        Number between -2.0 and 2.0. Positive values penalize new tokens based on their\n        existing frequency in the text so far, decreasing the model's likelihood to\n        repeat the same line verbatim.\n    extra_body : Optional[Dict[str, Any]]\n        Add additional JSON properties to the request.\n    **kwargs\n        Additional keyword arguments passed to the OpenAI API.\n\n    Returns\n    -------\n    Union[BaseModel, Dict[str, Any]]\n        The structured response in the format specified by the constraint:\n        - BaseModel instance if constraint is PydanticModel\n        - Dict[str, Any] if constraint is JsonSchema\n\n    Examples\n    --------\n    Using a Pydantic model constraint:\n\n    ```python\n    class Answer(BaseModel):\n        reasoning: str\n        result: int\n\n    constraint = PydanticModel(model=Answer)\n    response = llm.structured_output(\n        messages=[Message.from_text(\"What is 2+2?\", role=Role.USER)],\n        constraint=constraint,\n        model=\"gpt-4o\"\n    )\n    print(response.reasoning, response.result)\n    ```\n\n    Using a JSON schema constraint:\n\n    ```python\n    schema = {\"type\": \"object\", \"properties\": {\"answer\": {\"type\": \"string\"}}}\n    constraint = JsonSchema(schema=schema)\n    response = llm.structured_output(\n        messages=[Message.from_text(\"Hello\", role=Role.USER)],\n        constraint=constraint,\n        model=\"gpt-4o\"\n    )\n    print(response[\"answer\"])\n    ```\n\n    Notes\n    -----\n    - Utilizes OpenAI's native structured output API with strict schema validation\n    - All schemas automatically have additionalProperties set to False\n    - Best performance achieved with GPT-4o and later models (gpt-4o-mini, gpt-4o-2024-08-06, and later)\n    \"\"\"\n    params = self._build_parameters(\n        messages=messages,\n        model=model,\n        temperature=temperature,\n        top_p=top_p,\n        presence_penalty=presence_penalty,\n        frequency_penalty=frequency_penalty,\n        extra_body=extra_body,\n        response_format=self._get_response_format(constraint),\n        **kwargs,\n    )\n    # Validate required parameters for structured output\n    validate_required_params(params, [\"messages\", \"model\"])\n\n    response = self.client.chat.completions.parse(**params)\n    return self._convert_response(constraint, response.choices[0].message.content)\n</code></pre>"},{"location":"reference/bridgic-llms-openai/bridgic/llms/openai/#bridgic.llms.openai.OpenAILlm.astructured_output","title":"astructured_output","text":"<code>async</code> <pre><code>astructured_output(\n    messages: List[Message],\n    constraint: Union[PydanticModel, JsonSchema],\n    model: Optional[str] = None,\n    temperature: Optional[float] = None,\n    top_p: Optional[float] = None,\n    presence_penalty: Optional[float] = None,\n    frequency_penalty: Optional[float] = None,\n    extra_body: Optional[Dict[str, Any]] = None,\n    **kwargs\n) -&gt; Union[BaseModel, Dict[str, Any]]\n</code></pre> <p>Asynchronously generate structured output in a specified format using OpenAI's API.</p> <p>This is the asynchronous version of structured_output, suitable for concurrent processing and non-blocking operations. It leverages OpenAI's structured output capabilities to ensure the model response conforms to a specified schema.</p> <p>Parameters:</p> Name Type Description Default <code>messages</code> <code>List[Message]</code> <p>A list of messages comprising the conversation so far.</p> required <code>constraint</code> <code>Constraint</code> <p>The constraint defining the desired output format (PydanticModel or JsonSchema).</p> required <code>model</code> <code>str</code> <p>Model ID used to generate the response. Structured outputs work best with GPT-4o and later.</p> <code>None</code> <code>temperature</code> <code>Optional[float]</code> <p>What sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic.</p> <code>None</code> <code>top_p</code> <code>Optional[float]</code> <p>An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass.</p> <code>None</code> <code>presence_penalty</code> <code>Optional[float]</code> <p>Number between -2.0 and 2.0. Positive values penalize new tokens based on whether they appear in the text so far, increasing the model's likelihood to talk about new topics.</p> <code>None</code> <code>frequency_penalty</code> <code>Optional[float]</code> <p>Number between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model's likelihood to repeat the same line verbatim.</p> <code>None</code> <code>extra_body</code> <code>Optional[Dict[str, Any]]</code> <p>Add additional JSON properties to the request.</p> <code>None</code> <code>**kwargs</code> <p>Additional keyword arguments passed to the OpenAI API.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Union[BaseModel, Dict[str, Any]]</code> <p>The structured response in the format specified by the constraint: - BaseModel instance if constraint is PydanticModel - Dict[str, Any] if constraint is JsonSchema</p> <p>Examples:</p> <p>Using asynchronous structured output:</p> <pre><code>async def get_structured_response():\n    llm = OpenAILlm(api_key=\"your-key\")\n    constraint = PydanticModel(model=Answer)\n    response = await llm.astructured_output(\n        messages=[Message.from_text(\"Calculate 5+3\", role=Role.USER)],\n        constraint=constraint,\n        model=\"gpt-4o\"\n    )\n    return response\n</code></pre> Notes <ul> <li>This is the asynchronous version of structured_output</li> <li>Utilizes OpenAI's native structured output API with strict schema validation</li> <li>Suitable for concurrent processing and high-throughput applications</li> <li>Best performance achieved with GPT-4o and later models (gpt-4o-mini, gpt-4o-2024-08-06, and later)</li> </ul> Source code in <code>bridgic/llms/openai/_openai_llm.py</code> <pre><code>async def astructured_output(\n    self,\n    messages: List[Message],\n    constraint: Union[PydanticModel, JsonSchema],\n    model: Optional[str] = None,\n    temperature: Optional[float] = None,\n    top_p: Optional[float] = None,\n    presence_penalty: Optional[float] = None,\n    frequency_penalty: Optional[float] = None,\n    extra_body: Optional[Dict[str, Any]] = None,\n    **kwargs,\n) -&gt; Union[BaseModel, Dict[str, Any]]:\n    \"\"\"\n    Asynchronously generate structured output in a specified format using OpenAI's API.\n\n    This is the asynchronous version of structured_output, suitable for concurrent\n    processing and non-blocking operations. It leverages OpenAI's structured output\n    capabilities to ensure the model response conforms to a specified schema.\n\n    Parameters\n    ----------\n    messages : List[Message]\n        A list of messages comprising the conversation so far.\n    constraint : Constraint\n        The constraint defining the desired output format (PydanticModel or JsonSchema).\n    model : str\n        Model ID used to generate the response. Structured outputs work best with GPT-4o and later.\n    temperature : Optional[float]\n        What sampling temperature to use, between 0 and 2. Higher values like 0.8 will\n        make the output more random, while lower values like 0.2 will make it more\n        focused and deterministic.\n    top_p : Optional[float]\n        An alternative to sampling with temperature, called nucleus sampling, where the\n        model considers the results of the tokens with top_p probability mass.\n    presence_penalty : Optional[float]\n        Number between -2.0 and 2.0. Positive values penalize new tokens based on\n        whether they appear in the text so far, increasing the model's likelihood to\n        talk about new topics.\n    frequency_penalty : Optional[float]\n        Number between -2.0 and 2.0. Positive values penalize new tokens based on their\n        existing frequency in the text so far, decreasing the model's likelihood to\n        repeat the same line verbatim.\n    extra_body : Optional[Dict[str, Any]]\n        Add additional JSON properties to the request.\n    **kwargs\n        Additional keyword arguments passed to the OpenAI API.\n\n    Returns\n    -------\n    Union[BaseModel, Dict[str, Any]]\n        The structured response in the format specified by the constraint:\n        - BaseModel instance if constraint is PydanticModel\n        - Dict[str, Any] if constraint is JsonSchema\n\n    Examples\n    --------\n    Using asynchronous structured output:\n\n    ```python\n    async def get_structured_response():\n        llm = OpenAILlm(api_key=\"your-key\")\n        constraint = PydanticModel(model=Answer)\n        response = await llm.astructured_output(\n            messages=[Message.from_text(\"Calculate 5+3\", role=Role.USER)],\n            constraint=constraint,\n            model=\"gpt-4o\"\n        )\n        return response\n    ```\n\n    Notes\n    -----\n    - This is the asynchronous version of structured_output\n    - Utilizes OpenAI's native structured output API with strict schema validation\n    - Suitable for concurrent processing and high-throughput applications\n    - Best performance achieved with GPT-4o and later models (gpt-4o-mini, gpt-4o-2024-08-06, and later)\n    \"\"\"\n    params = self._build_parameters(\n        messages=messages,\n        model=model,\n        temperature=temperature,\n        top_p=top_p,\n        presence_penalty=presence_penalty,\n        frequency_penalty=frequency_penalty,\n        extra_body=extra_body,\n        response_format=self._get_response_format(constraint),\n        **kwargs,\n    )\n    # Validate required parameters for structured output\n    validate_required_params(params, [\"messages\", \"model\"])\n\n    response = await self.async_client.chat.completions.parse(**params)\n    return self._convert_response(constraint, response.choices[0].message.content)\n</code></pre>"},{"location":"reference/bridgic-llms-openai/bridgic/llms/openai/#bridgic.llms.openai.OpenAILlm.select_tool","title":"select_tool","text":"<pre><code>select_tool(\n    messages: List[Message],\n    tools: List[Tool],\n    model: Optional[str] = None,\n    temperature: Optional[float] = None,\n    top_p: Optional[float] = None,\n    presence_penalty: Optional[float] = None,\n    frequency_penalty: Optional[float] = None,\n    extra_body: Optional[Dict[str, Any]] = None,\n    parallel_tool_calls: Optional[bool] = None,\n    tool_choice: Union[\n        Literal[\"auto\", \"required\", \"none\"],\n        ChatCompletionNamedToolChoiceParam,\n    ] = None,\n    **kwargs\n) -&gt; Tuple[List[ToolCall], Optional[str]]\n</code></pre> <p>Select and invoke tools from a list based on conversation context.</p> <p>This method enables the model to intelligently select and call appropriate tools from a provided list based on the conversation context. It supports OpenAI's function calling capabilities with parallel execution and various control options.</p> <p>More OpenAI information: function-calling</p> <p>Parameters:</p> Name Type Description Default <code>messages</code> <code>List[Message]</code> <p>A list of messages comprising the conversation so far providing context for tool selection.</p> required <code>tools</code> <code>List[Tool]</code> <p>A list of tools the model may call.</p> required <code>model</code> <code>str</code> <p>Model ID used to generate the response. Function calling requires compatible models.</p> <code>None</code> <code>temperature</code> <code>Optional[float]</code> <p>What sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic.</p> <code>None</code> <code>top_p</code> <code>Optional[float]</code> <p>An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass.</p> <code>None</code> <code>presence_penalty</code> <code>Optional[float]</code> <p>Number between -2.0 and 2.0. Positive values penalize new tokens based on whether they appear in the text so far, increasing the model's likelihood to talk about new topics.</p> <code>None</code> <code>frequency_penalty</code> <code>Optional[float]</code> <p>Number between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model's likelihood to repeat the same line verbatim.</p> <code>None</code> <code>extra_body</code> <code>Optional[Dict[str, Any]]</code> <p>Add additional JSON properties to the request.</p> <code>None</code> <code>parallel_tool_calls</code> <code>Optional[bool]</code> <p>Whether to enable parallel function calling during tool use.</p> <code>None</code> <code>tool_choice</code> <code>Union[Literal['auto', 'required', 'none'], ChatCompletionNamedToolChoiceParam]</code> <p>Controls which tool, if any, the model may call. - <code>none</code>: The model will not call any tool and will instead generate a message. This is the default when no tools are provided. - <code>auto</code>: The model may choose to generate a message or call one or more tools. This is the default when tools are provided. - <code>required</code>: The model must call one or more tools. - To force a specific tool, pass <code>{\"type\": \"function\", \"function\": {\"name\": \"my_function\"}}</code>.</p> <code>None</code> <code>**kwargs</code> <p>Additional keyword arguments passed to the OpenAI API.</p> <code>{}</code> <p>Returns:</p> Type Description <code>List[ToolCall]</code> <p>List of selected tool calls with their IDs, names, and parsed arguments.</p> <code>Union[str, None]</code> <p>The content of the message from the model.</p> Source code in <code>bridgic/llms/openai/_openai_llm.py</code> <pre><code>def select_tool(\n    self,\n    messages: List[Message],\n    tools: List[Tool],\n    model: Optional[str] = None,\n    temperature: Optional[float] = None,\n    top_p: Optional[float] = None,\n    presence_penalty: Optional[float] = None,\n    frequency_penalty: Optional[float] = None,\n    extra_body: Optional[Dict[str, Any]] = None,\n    parallel_tool_calls: Optional[bool] = None,\n    tool_choice: Union[Literal[\"auto\", \"required\", \"none\"], ChatCompletionNamedToolChoiceParam] = None,\n    **kwargs,\n) -&gt; Tuple[List[ToolCall], Optional[str]]:\n    \"\"\"\n    Select and invoke tools from a list based on conversation context.\n\n    This method enables the model to intelligently select and call appropriate tools\n    from a provided list based on the conversation context. It supports OpenAI's\n    function calling capabilities with parallel execution and various control options.\n\n    More OpenAI information: [function-calling](https://platform.openai.com/docs/guides/function-calling)\n\n    Parameters\n    ----------\n    messages : List[Message]\n        A list of messages comprising the conversation so far providing context for tool selection.\n    tools : List[Tool]\n        A list of tools the model may call.\n    model : str\n        Model ID used to generate the response. Function calling requires compatible models.\n    temperature : Optional[float]\n        What sampling temperature to use, between 0 and 2. Higher values like 0.8 will\n        make the output more random, while lower values like 0.2 will make it more\n        focused and deterministic.\n    top_p : Optional[float]\n        An alternative to sampling with temperature, called nucleus sampling, where the\n        model considers the results of the tokens with top_p probability mass.\n    presence_penalty : Optional[float]\n        Number between -2.0 and 2.0. Positive values penalize new tokens based on\n        whether they appear in the text so far, increasing the model's likelihood to\n        talk about new topics.\n    frequency_penalty : Optional[float]\n        Number between -2.0 and 2.0. Positive values penalize new tokens based on their\n        existing frequency in the text so far, decreasing the model's likelihood to\n        repeat the same line verbatim.\n    extra_body : Optional[Dict[str, Any]]\n        Add additional JSON properties to the request.\n    parallel_tool_calls : Optional[bool]\n        Whether to enable parallel function calling during tool use.\n    tool_choice : Union[Literal[\"auto\", \"required\", \"none\"], ChatCompletionNamedToolChoiceParam]\n        Controls which tool, if any, the model may call.\n        - `none`: The model will not call any tool and will instead generate a message. This is the default when no tools are provided.\n        - `auto`: The model may choose to generate a message or call one or more tools. This is the default when tools are provided.\n        - `required`: The model must call one or more tools.\n        - To force a specific tool, pass `{\"type\": \"function\", \"function\": {\"name\": \"my_function\"}}`.\n    **kwargs\n        Additional keyword arguments passed to the OpenAI API.\n\n    Returns\n    -------\n    List[ToolCall]\n        List of selected tool calls with their IDs, names, and parsed arguments.\n    Union[str, None]\n        The content of the message from the model.\n    \"\"\"\n    params = self._build_parameters(\n        messages=messages,\n        model=model,\n        temperature=temperature,\n        top_p=top_p,\n        presence_penalty=presence_penalty,\n        frequency_penalty=frequency_penalty,\n        tools=tools,\n        tool_choice=tool_choice,\n        parallel_tool_calls=parallel_tool_calls,\n        extra_body=extra_body,\n        **kwargs,\n    )\n    # Validate required parameters for tool selection\n    validate_required_params(params, [\"messages\", \"model\"])\n\n    response: ChatCompletion = self.client.chat.completions.create(**params)\n    tool_calls = response.choices[0].message.tool_calls\n    content = response.choices[0].message.content\n    return (self._convert_tool_calls(tool_calls), content)\n</code></pre>"},{"location":"reference/bridgic-llms-openai/bridgic/llms/openai/#bridgic.llms.openai.OpenAILlm.aselect_tool","title":"aselect_tool","text":"<code>async</code> <pre><code>aselect_tool(\n    messages: List[Message],\n    tools: List[Tool],\n    model: Optional[str] = None,\n    temperature: Optional[float] = None,\n    top_p: Optional[float] = None,\n    presence_penalty: Optional[float] = None,\n    frequency_penalty: Optional[float] = None,\n    extra_body: Optional[Dict[str, Any]] = None,\n    parallel_tool_calls: Optional[bool] = None,\n    tool_choice: Union[\n        Literal[\"auto\", \"required\", \"none\"],\n        ChatCompletionNamedToolChoiceParam,\n    ] = None,\n    **kwargs\n) -&gt; Tuple[List[ToolCall], Optional[str]]\n</code></pre> <p>Select and invoke tools from a list based on conversation context.</p> <p>This method enables the model to intelligently select and call appropriate tools from a provided list based on the conversation context. It supports OpenAI's function calling capabilities with parallel execution and various control options.</p> <p>More OpenAI information: function-calling</p> <p>Parameters:</p> Name Type Description Default <code>messages</code> <code>List[Message]</code> <p>A list of messages comprising the conversation so far providing context for tool selection.</p> required <code>tools</code> <code>List[Tool]</code> <p>A list of tools the model may call.</p> required <code>model</code> <code>str</code> <p>Model ID used to generate the response. Function calling requires compatible models.</p> <code>None</code> <code>temperature</code> <code>Optional[float]</code> <p>What sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic.</p> <code>None</code> <code>top_p</code> <code>Optional[float]</code> <p>An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass.</p> <code>None</code> <code>presence_penalty</code> <code>Optional[float]</code> <p>Number between -2.0 and 2.0. Positive values penalize new tokens based on whether they appear in the text so far, increasing the model's likelihood to talk about new topics.</p> <code>None</code> <code>frequency_penalty</code> <code>Optional[float]</code> <p>Number between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model's likelihood to repeat the same line verbatim.</p> <code>None</code> <code>extra_body</code> <code>Optional[Dict[str, Any]]</code> <p>Add additional JSON properties to the request.</p> <code>None</code> <code>parallel_tool_calls</code> <code>Optional[bool]</code> <p>Whether to enable parallel function calling during tool use.</p> <code>None</code> <code>tool_choice</code> <code>Union[Literal['auto', 'required', 'none'], ChatCompletionNamedToolChoiceParam]</code> <p>Controls which tool, if any, the model may call. - <code>none</code>: The model will not call any tool and will instead generate a message. This is the default when no tools are provided. - <code>auto</code>: The model may choose to generate a message or call one or more tools. This is the default when tools are provided. - <code>required</code>: The model must call one or more tools. - To force a specific tool, pass <code>{\"type\": \"function\", \"function\": {\"name\": \"my_function\"}}</code>.</p> <code>None</code> <code>**kwargs</code> <p>Additional keyword arguments passed to the OpenAI API.</p> <code>{}</code> <p>Returns:</p> Type Description <code>List[ToolCall]</code> <p>List of selected tool calls with their IDs, names, and parsed arguments.</p> <code>Union[str, None]</code> <p>The content of the message from the model.</p> Source code in <code>bridgic/llms/openai/_openai_llm.py</code> <pre><code>async def aselect_tool(\n    self,\n    messages: List[Message],\n    tools: List[Tool],\n    model: Optional[str] = None,\n    temperature: Optional[float] = None,\n    top_p: Optional[float] = None,\n    presence_penalty: Optional[float] = None,\n    frequency_penalty: Optional[float] = None,\n    extra_body: Optional[Dict[str, Any]] = None,\n    parallel_tool_calls: Optional[bool] = None,\n    tool_choice: Union[Literal[\"auto\", \"required\", \"none\"], ChatCompletionNamedToolChoiceParam] = None,\n    **kwargs,\n)-&gt; Tuple[List[ToolCall], Optional[str]]:\n    \"\"\"\n    Select and invoke tools from a list based on conversation context.\n\n    This method enables the model to intelligently select and call appropriate tools\n    from a provided list based on the conversation context. It supports OpenAI's\n    function calling capabilities with parallel execution and various control options.\n\n    More OpenAI information: [function-calling](https://platform.openai.com/docs/guides/function-calling)\n\n    Parameters\n    ----------\n    messages : List[Message]\n        A list of messages comprising the conversation so far providing context for tool selection.\n    tools : List[Tool]\n        A list of tools the model may call.\n    model : str\n        Model ID used to generate the response. Function calling requires compatible models.\n    temperature : Optional[float]\n        What sampling temperature to use, between 0 and 2. Higher values like 0.8 will\n        make the output more random, while lower values like 0.2 will make it more\n        focused and deterministic.\n    top_p : Optional[float]\n        An alternative to sampling with temperature, called nucleus sampling, where the\n        model considers the results of the tokens with top_p probability mass.\n    presence_penalty : Optional[float]\n        Number between -2.0 and 2.0. Positive values penalize new tokens based on\n        whether they appear in the text so far, increasing the model's likelihood to\n        talk about new topics.\n    frequency_penalty : Optional[float]\n        Number between -2.0 and 2.0. Positive values penalize new tokens based on their\n        existing frequency in the text so far, decreasing the model's likelihood to\n        repeat the same line verbatim.\n    extra_body : Optional[Dict[str, Any]]\n        Add additional JSON properties to the request.\n    parallel_tool_calls : Optional[bool]\n        Whether to enable parallel function calling during tool use.\n    tool_choice : Union[Literal[\"auto\", \"required\", \"none\"], ChatCompletionNamedToolChoiceParam]\n        Controls which tool, if any, the model may call.\n        - `none`: The model will not call any tool and will instead generate a message. This is the default when no tools are provided.\n        - `auto`: The model may choose to generate a message or call one or more tools. This is the default when tools are provided.\n        - `required`: The model must call one or more tools.\n        - To force a specific tool, pass `{\"type\": \"function\", \"function\": {\"name\": \"my_function\"}}`.\n\n    **kwargs\n        Additional keyword arguments passed to the OpenAI API.\n\n    Returns\n    -------\n    List[ToolCall]\n        List of selected tool calls with their IDs, names, and parsed arguments.\n    Union[str, None]\n        The content of the message from the model.\n    \"\"\"\n    params = self._build_parameters(\n        messages=messages,\n        model=model,\n        temperature=temperature,\n        top_p=top_p,\n        presence_penalty=presence_penalty,\n        frequency_penalty=frequency_penalty,\n        tools=tools,\n        tool_choice=tool_choice,\n        parallel_tool_calls=parallel_tool_calls,\n        extra_body=extra_body,\n        **kwargs,\n    )\n    # Validate required parameters for tool selection\n    validate_required_params(params, [\"messages\", \"model\"])\n\n    response: ChatCompletion = await self.async_client.chat.completions.create(**params)\n    tool_calls = response.choices[0].message.tool_calls\n    content = response.choices[0].message.content\n    return (self._convert_tool_calls(tool_calls), content)\n</code></pre>"},{"location":"reference/bridgic-llms-openai-like/bridgic/llms/openai_like/","title":"openai_like","text":"<p>The OpenAI-Like integration module provides support for third-party services  compatible with the OpenAI API.</p> <p>This package is a thin wrapper for the OpenAI API, designed to meet the needs  for calling third-party model services compatible with the OpenAI API.</p> <p>Note that this integration does not adapt to specific model providers, but  provides general-purpose interfaces. Therefore, it is not fully comprehensive  in functionality and only supports basic chat/stream operations and their  corresponding async interfaces.</p> <p>You can install the OpenAI-Like integration package for Bridgic by running:</p> <pre><code>pip install bridgic-llms-openai-like\n</code></pre>"},{"location":"reference/bridgic-llms-openai-like/bridgic/llms/openai_like/#bridgic.llms.openai_like.OpenAILikeConfiguration","title":"OpenAILikeConfiguration","text":"<p>               Bases: <code>BaseModel</code></p> <p>Default configuration for OpenAI-compatible chat completions.</p> Source code in <code>bridgic/llms/openai_like/_openai_like_llm.py</code> <pre><code>class OpenAILikeConfiguration(BaseModel):\n    \"\"\"\n    Default configuration for OpenAI-compatible chat completions.\n    \"\"\"\n    model: Optional[str] = None\n    \"\"\"Default model to use when a call-time `model` is not provided.\"\"\"\n    temperature: Optional[float] = None\n    \"\"\"Sampling temperature in [0, 2]. Higher is more random, lower is more deterministic.\"\"\"\n    top_p: Optional[float] = None\n    \"\"\"Nucleus sampling probability mass in (0, 1]. Alternative to temperature.\"\"\"\n    presence_penalty: Optional[float] = None\n    \"\"\"Penalize new tokens based on whether they appear so far. [-2.0, 2.0].\"\"\"\n    frequency_penalty: Optional[float] = None\n    \"\"\"Penalize new tokens based on their frequency so far. [-2.0, 2.0].\"\"\"\n    max_tokens: Optional[int] = None\n    \"\"\"Maximum number of tokens to generate for the completion.\"\"\"\n    stop: Optional[List[str]] = None\n    \"\"\"Up to 4 sequences where generation will stop.\"\"\"\n</code></pre>"},{"location":"reference/bridgic-llms-openai-like/bridgic/llms/openai_like/#bridgic.llms.openai_like.OpenAILikeConfiguration.model","title":"model  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>model: Optional[str] = None\n</code></pre> <p>Default model to use when a call-time <code>model</code> is not provided.</p>"},{"location":"reference/bridgic-llms-openai-like/bridgic/llms/openai_like/#bridgic.llms.openai_like.OpenAILikeConfiguration.temperature","title":"temperature  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>temperature: Optional[float] = None\n</code></pre> <p>Sampling temperature in [0, 2]. Higher is more random, lower is more deterministic.</p>"},{"location":"reference/bridgic-llms-openai-like/bridgic/llms/openai_like/#bridgic.llms.openai_like.OpenAILikeConfiguration.top_p","title":"top_p  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>top_p: Optional[float] = None\n</code></pre> <p>Nucleus sampling probability mass in (0, 1]. Alternative to temperature.</p>"},{"location":"reference/bridgic-llms-openai-like/bridgic/llms/openai_like/#bridgic.llms.openai_like.OpenAILikeConfiguration.presence_penalty","title":"presence_penalty  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>presence_penalty: Optional[float] = None\n</code></pre> <p>Penalize new tokens based on whether they appear so far. [-2.0, 2.0].</p>"},{"location":"reference/bridgic-llms-openai-like/bridgic/llms/openai_like/#bridgic.llms.openai_like.OpenAILikeConfiguration.frequency_penalty","title":"frequency_penalty  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>frequency_penalty: Optional[float] = None\n</code></pre> <p>Penalize new tokens based on their frequency so far. [-2.0, 2.0].</p>"},{"location":"reference/bridgic-llms-openai-like/bridgic/llms/openai_like/#bridgic.llms.openai_like.OpenAILikeConfiguration.max_tokens","title":"max_tokens  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>max_tokens: Optional[int] = None\n</code></pre> <p>Maximum number of tokens to generate for the completion.</p>"},{"location":"reference/bridgic-llms-openai-like/bridgic/llms/openai_like/#bridgic.llms.openai_like.OpenAILikeConfiguration.stop","title":"stop  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>stop: Optional[List[str]] = None\n</code></pre> <p>Up to 4 sequences where generation will stop.</p>"},{"location":"reference/bridgic-llms-openai-like/bridgic/llms/openai_like/#bridgic.llms.openai_like.OpenAILikeLlm","title":"OpenAILikeLlm","text":"<p>               Bases: <code>BaseLlm</code></p> <p>OpenAILikeLlm is a thin wrapper around the LLM providers that makes it compatible with the  services that provide OpenAI compatible API. To support the widest range of model providers,  this wrapper only supports text-modal usage.</p> <p>Parameters:</p> Name Type Description Default <code>api_base</code> <code>str</code> <p>The base URL of the LLM provider.</p> required <code>api_key</code> <code>str</code> <p>The API key of the LLM provider.</p> required <code>configuration</code> <code>Optional[OpenAILikeConfiguration]</code> <p>The configuration for the OpenAI-compatible API. If None, uses the default configuration.</p> <code>OpenAILikeConfiguration()</code> <code>timeout</code> <code>Optional[float]</code> <p>The timeout in seconds. If None, no timeout is applied.</p> <code>None</code> <code>http_client</code> <code>Optional[Client]</code> <p>Custom synchronous HTTP client for requests. If None, creates a default client.</p> <code>None</code> <code>http_async_client</code> <code>Optional[AsyncClient]</code> <p>Custom asynchronous HTTP client for requests. If None, creates a default client.</p> <code>None</code> Source code in <code>bridgic/llms/openai_like/_openai_like_llm.py</code> <pre><code>class OpenAILikeLlm(BaseLlm):\n    \"\"\"\n    OpenAILikeLlm is a thin wrapper around the LLM providers that makes it compatible with the \n    services that provide OpenAI compatible API. To support the widest range of model providers, \n    this wrapper only supports text-modal usage.\n\n    Parameters\n    ----------\n    api_base: str\n        The base URL of the LLM provider.\n    api_key: str\n        The API key of the LLM provider.\n    configuration: Optional[OpenAILikeConfiguration]\n        The configuration for the OpenAI-compatible API. If None, uses the default configuration.\n    timeout: Optional[float]\n        The timeout in seconds. If None, no timeout is applied.\n    http_client : Optional[httpx.Client]\n        Custom synchronous HTTP client for requests. If None, creates a default client.\n    http_async_client : Optional[httpx.AsyncClient]\n        Custom asynchronous HTTP client for requests. If None, creates a default client.\n    \"\"\"\n\n    api_base: str\n    api_key: str\n    configuration: OpenAILikeConfiguration\n    timeout: float\n    http_client: httpx.Client\n    http_async_client: httpx.AsyncClient\n\n    client: OpenAI\n    async_client: AsyncOpenAI\n\n    def __init__(\n        self,\n        api_base: str,\n        api_key: str,\n        configuration: Optional[OpenAILikeConfiguration] = OpenAILikeConfiguration(),\n        timeout: Optional[float] = None,\n        http_client: Optional[httpx.Client] = None,\n        http_async_client: Optional[httpx.AsyncClient] = None,\n    ):\n        # Record for serialization / deserialization.\n        self.api_base = api_base\n        self.api_key = api_key\n        self.configuration = configuration\n        self.timeout = timeout\n        self.http_client = http_client\n        self.http_async_client = http_async_client\n\n        # Initialize clients.\n        self.client = OpenAI(base_url=api_base, api_key=api_key, timeout=timeout, http_client=http_client)\n        self.async_client = AsyncOpenAI(base_url=api_base, api_key=api_key, timeout=timeout, http_client=http_async_client)\n\n    def chat(\n        self,\n        messages: List[Message],\n        model: Optional[str] = None,\n        temperature: Optional[float] = None,\n        top_p: Optional[float] = None,\n        presence_penalty: Optional[float] = None,\n        frequency_penalty: Optional[float] = None,\n        max_tokens: Optional[int] = None,\n        stop: Optional[List[str]] = None,\n        extra_body: Optional[Dict[str, Any]] = None,\n        **kwargs,\n    ) -&gt; Response:\n        \"\"\"\n        Send a synchronous chat completion request to an OpenAI-compatible provider.\n\n        Parameters\n        ----------\n        messages : list[Message]\n            Conversation messages.\n        model : str, optional\n            Model ID to use. Required unless provided in `configuration.model`.\n        temperature : float, optional\n            Sampling temperature in [0, 2]. Higher is more random, lower is more deterministic.\n        top_p : float, optional\n            Nucleus sampling probability mass in (0, 1]. Alternative to temperature.\n        presence_penalty : float, optional\n            Penalize new tokens based on whether they appear so far. [-2.0, 2.0].\n        frequency_penalty : float, optional\n            Penalize new tokens based on their frequency so far. [-2.0, 2.0].\n        max_tokens : int, optional\n            Maximum tokens to generate for completion.\n        stop : list[str], optional\n            Up to 4 sequences where generation will stop.\n        extra_body : dict, optional\n            Extra JSON payload sent to the provider.\n        **kwargs\n            Additional provider-specific arguments.\n\n        Returns\n        -------\n        Response\n            Bridgic response containing the generated message and raw API response.\n\n        Notes\n        -----\n        - Required parameter validation ensures `messages` and final `model` are present\n          (from either the call or `configuration`).\n        \"\"\"\n        params = self._build_parameters(\n            messages=messages,\n            model=model,\n            temperature=temperature,\n            top_p=top_p,\n            presence_penalty=presence_penalty,\n            frequency_penalty=frequency_penalty,\n            max_tokens=max_tokens,\n            stop=stop,\n            extra_body=extra_body,\n            **kwargs,\n        )\n        validate_required_params(params, [\"messages\", \"model\"])\n        response = self.client.chat.completions.create(**params)\n        openai_message: ChatCompletionMessage = response.choices[0].message\n        text: str = openai_message.content if openai_message.content else \"\"\n\n        if openai_message.refusal:\n            warnings.warn(openai_message.refusal, RuntimeWarning)\n\n        return Response(\n            message=Message.from_text(text, role=Role.AI),\n            raw=response,\n        )\n\n    def stream(\n        self,\n        messages: List[Message],\n        model: Optional[str] = None,\n        temperature: Optional[float] = None,\n        top_p: Optional[float] = None,\n        presence_penalty: Optional[float] = None,\n        frequency_penalty: Optional[float] = None,\n        max_tokens: Optional[int] = None,\n        stop: Optional[List[str]] = None,\n        extra_body: Optional[Dict[str, Any]] = None,\n        **kwargs,\n    ) -&gt; StreamResponse:\n        \"\"\"\n        Stream a chat completion response incrementally.\n\n        Parameters\n        ----------\n        messages : list[Message]\n            Conversation messages.\n        model : str, optional\n            Model ID to use. Required unless provided in `configuration.model`.\n        temperature, top_p, presence_penalty, frequency_penalty, max_tokens, stop, extra_body\n            See `chat` for details.\n        **kwargs\n            Additional provider-specific arguments.\n\n        Yields\n        ------\n        MessageChunk\n            Delta chunks as they arrive from the provider.\n\n        Notes\n        -----\n        - Validates `messages`, final `model`, and `stream=True`.\n        \"\"\"\n        params = self._build_parameters(\n            messages=messages,\n            model=model,\n            temperature=temperature,\n            top_p=top_p,\n            presence_penalty=presence_penalty,\n            frequency_penalty=frequency_penalty,\n            max_tokens=max_tokens,\n            stop=stop,\n            extra_body=extra_body,\n            stream=True,\n            **kwargs,\n        )\n        validate_required_params(params, [\"messages\", \"model\", \"stream\"])\n        response = self.client.chat.completions.create(**params)\n        for chunk in response:\n            delta_content = chunk.choices[0].delta.content\n            delta_content = delta_content if delta_content else \"\"\n            yield MessageChunk(delta=delta_content, raw=chunk)\n\n    async def achat(\n        self,\n        messages: List[Message],\n        model: Optional[str] = None,\n        temperature: Optional[float] = None,\n        top_p: Optional[float] = None,\n        presence_penalty: Optional[float] = None,\n        frequency_penalty: Optional[float] = None,\n        max_tokens: Optional[int] = None,\n        stop: Optional[List[str]] = None,\n        extra_body: Optional[Dict[str, Any]] = None,\n        **kwargs,\n    ) -&gt; Response:\n        \"\"\"\n        Asynchronously send a chat completion request to an OpenAI-compatible provider.\n\n        Parameters\n        ----------\n        messages : list[Message]\n            Conversation messages.\n        model : str, optional\n            Model ID to use. Required unless provided in `configuration.model`.\n        temperature, top_p, presence_penalty, frequency_penalty, max_tokens, stop, extra_body\n            See `chat` for details.\n        **kwargs\n            Additional provider-specific arguments.\n\n        Returns\n        -------\n        Response\n            Bridgic response containing the generated message and raw API response.\n        \"\"\"\n        params = self._build_parameters(\n            messages=messages,\n            model=model,\n            temperature=temperature,\n            top_p=top_p,\n            presence_penalty=presence_penalty,\n            frequency_penalty=frequency_penalty,\n            max_tokens=max_tokens,\n            stop=stop,\n            extra_body=extra_body,\n            **kwargs,\n        )\n        validate_required_params(params, [\"messages\", \"model\"])\n        response = await self.async_client.chat.completions.create(**params)\n        openai_message: ChatCompletionMessage = response.choices[0].message\n        text: str = openai_message.content if openai_message.content else \"\"\n\n        if openai_message.refusal:\n            warnings.warn(openai_message.refusal, RuntimeWarning)\n\n        return Response(\n            message=Message.from_text(text, role=Role.AI),\n            raw=response,\n        )\n\n    async def astream(\n        self,\n        messages: List[Message],\n        model: Optional[str] = None,\n        temperature: Optional[float] = None,\n        top_p: Optional[float] = None,\n        presence_penalty: Optional[float] = None,\n        frequency_penalty: Optional[float] = None,\n        max_tokens: Optional[int] = None,\n        stop: Optional[List[str]] = None,\n        extra_body: Optional[Dict[str, Any]] = None,\n        **kwargs,\n    ) -&gt; AsyncStreamResponse:\n        \"\"\"\n        Asynchronously stream a chat completion response incrementally.\n\n        Parameters\n        ----------\n        messages : list[Message]\n            Conversation messages.\n        model : str, optional\n            Model ID to use. Required unless provided in `configuration.model`.\n        temperature, top_p, presence_penalty, frequency_penalty, max_tokens, stop, extra_body\n            See `chat` for details.\n        **kwargs\n            Additional provider-specific arguments.\n\n        Yields\n        ------\n        MessageChunk\n            Delta chunks as they arrive from the provider.\n\n        Notes\n        -----\n        - Validates `messages`, final `model`, and `stream=True`.\n        \"\"\"\n        params = self._build_parameters(\n            messages=messages,\n            model=model,\n            temperature=temperature,\n            top_p=top_p,\n            presence_penalty=presence_penalty,\n            frequency_penalty=frequency_penalty,\n            max_tokens=max_tokens,\n            stop=stop,\n            extra_body=extra_body,\n            stream=True,\n            **kwargs,\n        )\n        validate_required_params(params, [\"messages\", \"model\", \"stream\"])\n        response = await self.async_client.chat.completions.create(**params)\n        async for chunk in response:\n            delta_content = chunk.choices[0].delta.content\n            delta_content = delta_content if delta_content else \"\"\n            yield MessageChunk(delta=delta_content, raw=chunk)\n\n    def _build_parameters(\n        self,\n        messages: List[Message],\n        model: Optional[str] = None,\n        temperature: Optional[float] = None,\n        top_p: Optional[float] = None,\n        presence_penalty: Optional[float] = None,\n        frequency_penalty: Optional[float] = None,\n        max_tokens: Optional[int] = None,\n        stop: Optional[List[str]] = None,\n        extra_body: Optional[Dict[str, Any]] = None,\n        stream: Optional[bool] = None,\n        **kwargs,\n    ) -&gt; Dict[str, Any]:\n        \"\"\"\n        Merge configuration defaults with per-call parameters and remove None values.\n\n        Parameters\n        ----------\n        messages : list[Message]\n            Conversation messages to send.\n        model : str, optional\n            Model identifier. May be omitted if `configuration.model` is set.\n        temperature, top_p, presence_penalty, frequency_penalty, max_tokens, stop, extra_body, stream\n            Standard OpenAI chat parameters.\n        **kwargs\n            Additional provider-specific parameters.\n\n        Returns\n        -------\n        dict\n            Final parameter dictionary for the OpenAI-compatible API.\n        \"\"\"\n        msgs: List[ChatCompletionMessageParam] = [self._convert_message(msg) for msg in messages]\n        merge_params = merge_dict(self.configuration.model_dump(), {\n            \"messages\": msgs,\n            \"model\": model,\n            \"temperature\": temperature,\n            \"top_p\": top_p,\n            \"presence_penalty\": presence_penalty,\n            \"frequency_penalty\": frequency_penalty,\n            \"max_tokens\": max_tokens,\n            \"stop\": stop,\n            \"extra_body\": extra_body,\n            \"stream\": stream,\n            **kwargs,\n        })\n        return filter_dict(merge_params, exclude_none=True)\n\n    def _convert_message(self, message: Message, strict: bool = False) -&gt; ChatCompletionMessageParam:\n        if strict:\n            return self._convert_message_strict(message)\n        else:\n            return self._convert_message_normal(message)\n\n    def _convert_message_normal(self, message: Message) -&gt; ChatCompletionMessageParam:\n        content_list = []\n        for block in message.blocks:\n            if isinstance(block, TextBlock):\n                content_list.append(block.text)\n            if isinstance(block, ToolCallBlock):\n                content_list.append(\n                    f\"Tool call:\\n\"\n                    f\"- id: {block.id}\\n\"\n                    f\"- name: {block.name}\\n\"\n                    f\"- arguments: {block.arguments}\"\n                )\n            if isinstance(block, ToolResultBlock):\n                content_list.append(f\"Tool result: {block.content}\")\n        content_txt = \"\\n\\n\".join(content_list)\n\n        if message.role == Role.SYSTEM:\n            return ChatCompletionSystemMessageParam(content=content_txt, role=\"system\")\n        elif message.role == Role.USER:\n            return ChatCompletionUserMessageParam(content=content_txt, role=\"user\")\n        elif message.role == Role.AI:\n            return ChatCompletionAssistantMessageParam(content=content_txt, role=\"assistant\")\n        elif message.role == Role.TOOL:\n            return ChatCompletionToolMessageParam(content=content_txt, role=\"tool\")\n        else:\n            raise ValueError(f\"Invalid role: {message.role}\")\n\n    def _convert_message_strict(self, message: Message) -&gt; ChatCompletionMessageParam:\n        content_list = []\n        tool_call_list = []\n        tool_result = \"\"\n        tool_result_call_id = None\n\n        for block in message.blocks:\n            if isinstance(block, TextBlock):\n                content_list.append(block.text)\n            if isinstance(block, ToolCallBlock):\n                tool_call: ChatCompletionMessageFunctionToolCallParam = {\n                    \"type\": \"function\",\n                    \"id\": block.id,\n                    \"function\": {\n                        \"name\": block.name,\n                        \"arguments\": json.dumps(block.arguments),\n                    },\n                }\n                tool_call_list.append(tool_call)\n            if isinstance(block, ToolResultBlock):\n                tool_result = block.content\n                tool_result_call_id = block.id\n\n        content_txt = \"\\n\\n\".join(content_list)\n\n        if message.role == Role.SYSTEM:\n            return ChatCompletionSystemMessageParam(content=content_txt, role=\"system\")\n        elif message.role == Role.USER:\n            return ChatCompletionUserMessageParam(content=content_txt, role=\"user\")\n        elif message.role == Role.AI:\n            return ChatCompletionAssistantMessageParam(content=content_txt, tool_calls=tool_call_list, role=\"assistant\")\n        elif message.role == Role.TOOL:\n            content_txt = \"\\n\\n\".join([content_txt, tool_result])\n            return ChatCompletionToolMessageParam(content=content_txt, tool_call_id=tool_result_call_id, role=\"tool\")\n        else:\n            raise ValueError(f\"Invalid role: {message.role}\")\n\n    @override\n    def dump_to_dict(self) -&gt; Dict[str, Any]:\n        state_dict = {\n            \"api_base\": self.api_base,\n            \"api_key\": self.api_key,\n            \"timeout\": self.timeout,\n            \"configuration\": self.configuration.model_dump(),\n        }\n        if self.http_client:\n            warnings.warn(\n                \"httpx.Client is not serializable, so it will be set to None in the deserialization.\",\n                RuntimeWarning,\n            )\n        if self.http_async_client:\n            warnings.warn(\n                \"httpx.AsyncClient is not serializable, so it will be set to None in the deserialization.\",\n                RuntimeWarning,\n            )\n        return state_dict\n\n    @override\n    def load_from_dict(self, state_dict: Dict[str, Any]) -&gt; None:\n        self.api_base = state_dict[\"api_base\"]\n        self.api_key = state_dict[\"api_key\"]\n        self.timeout = state_dict[\"timeout\"]\n        self.configuration = OpenAILikeConfiguration(**state_dict.get(\"configuration\", {}))\n\n        self.http_client = None\n        self.http_async_client = None\n\n        self.client = OpenAI(\n            base_url=self.api_base,\n            api_key=self.api_key,\n            timeout=self.timeout,\n            http_client=self.http_client,\n        )\n        self.async_client = AsyncOpenAI(\n            base_url=self.api_base,\n            api_key=self.api_key,\n            timeout=self.timeout,\n            http_client=self.http_async_client,\n        )\n</code></pre>"},{"location":"reference/bridgic-llms-openai-like/bridgic/llms/openai_like/#bridgic.llms.openai_like.OpenAILikeLlm.chat","title":"chat","text":"<pre><code>chat(\n    messages: List[Message],\n    model: Optional[str] = None,\n    temperature: Optional[float] = None,\n    top_p: Optional[float] = None,\n    presence_penalty: Optional[float] = None,\n    frequency_penalty: Optional[float] = None,\n    max_tokens: Optional[int] = None,\n    stop: Optional[List[str]] = None,\n    extra_body: Optional[Dict[str, Any]] = None,\n    **kwargs\n) -&gt; Response\n</code></pre> <p>Send a synchronous chat completion request to an OpenAI-compatible provider.</p> <p>Parameters:</p> Name Type Description Default <code>messages</code> <code>list[Message]</code> <p>Conversation messages.</p> required <code>model</code> <code>str</code> <p>Model ID to use. Required unless provided in <code>configuration.model</code>.</p> <code>None</code> <code>temperature</code> <code>float</code> <p>Sampling temperature in [0, 2]. Higher is more random, lower is more deterministic.</p> <code>None</code> <code>top_p</code> <code>float</code> <p>Nucleus sampling probability mass in (0, 1]. Alternative to temperature.</p> <code>None</code> <code>presence_penalty</code> <code>float</code> <p>Penalize new tokens based on whether they appear so far. [-2.0, 2.0].</p> <code>None</code> <code>frequency_penalty</code> <code>float</code> <p>Penalize new tokens based on their frequency so far. [-2.0, 2.0].</p> <code>None</code> <code>max_tokens</code> <code>int</code> <p>Maximum tokens to generate for completion.</p> <code>None</code> <code>stop</code> <code>list[str]</code> <p>Up to 4 sequences where generation will stop.</p> <code>None</code> <code>extra_body</code> <code>dict</code> <p>Extra JSON payload sent to the provider.</p> <code>None</code> <code>**kwargs</code> <p>Additional provider-specific arguments.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Response</code> <p>Bridgic response containing the generated message and raw API response.</p> Notes <ul> <li>Required parameter validation ensures <code>messages</code> and final <code>model</code> are present   (from either the call or <code>configuration</code>).</li> </ul> Source code in <code>bridgic/llms/openai_like/_openai_like_llm.py</code> <pre><code>def chat(\n    self,\n    messages: List[Message],\n    model: Optional[str] = None,\n    temperature: Optional[float] = None,\n    top_p: Optional[float] = None,\n    presence_penalty: Optional[float] = None,\n    frequency_penalty: Optional[float] = None,\n    max_tokens: Optional[int] = None,\n    stop: Optional[List[str]] = None,\n    extra_body: Optional[Dict[str, Any]] = None,\n    **kwargs,\n) -&gt; Response:\n    \"\"\"\n    Send a synchronous chat completion request to an OpenAI-compatible provider.\n\n    Parameters\n    ----------\n    messages : list[Message]\n        Conversation messages.\n    model : str, optional\n        Model ID to use. Required unless provided in `configuration.model`.\n    temperature : float, optional\n        Sampling temperature in [0, 2]. Higher is more random, lower is more deterministic.\n    top_p : float, optional\n        Nucleus sampling probability mass in (0, 1]. Alternative to temperature.\n    presence_penalty : float, optional\n        Penalize new tokens based on whether they appear so far. [-2.0, 2.0].\n    frequency_penalty : float, optional\n        Penalize new tokens based on their frequency so far. [-2.0, 2.0].\n    max_tokens : int, optional\n        Maximum tokens to generate for completion.\n    stop : list[str], optional\n        Up to 4 sequences where generation will stop.\n    extra_body : dict, optional\n        Extra JSON payload sent to the provider.\n    **kwargs\n        Additional provider-specific arguments.\n\n    Returns\n    -------\n    Response\n        Bridgic response containing the generated message and raw API response.\n\n    Notes\n    -----\n    - Required parameter validation ensures `messages` and final `model` are present\n      (from either the call or `configuration`).\n    \"\"\"\n    params = self._build_parameters(\n        messages=messages,\n        model=model,\n        temperature=temperature,\n        top_p=top_p,\n        presence_penalty=presence_penalty,\n        frequency_penalty=frequency_penalty,\n        max_tokens=max_tokens,\n        stop=stop,\n        extra_body=extra_body,\n        **kwargs,\n    )\n    validate_required_params(params, [\"messages\", \"model\"])\n    response = self.client.chat.completions.create(**params)\n    openai_message: ChatCompletionMessage = response.choices[0].message\n    text: str = openai_message.content if openai_message.content else \"\"\n\n    if openai_message.refusal:\n        warnings.warn(openai_message.refusal, RuntimeWarning)\n\n    return Response(\n        message=Message.from_text(text, role=Role.AI),\n        raw=response,\n    )\n</code></pre>"},{"location":"reference/bridgic-llms-openai-like/bridgic/llms/openai_like/#bridgic.llms.openai_like.OpenAILikeLlm.stream","title":"stream","text":"<pre><code>stream(\n    messages: List[Message],\n    model: Optional[str] = None,\n    temperature: Optional[float] = None,\n    top_p: Optional[float] = None,\n    presence_penalty: Optional[float] = None,\n    frequency_penalty: Optional[float] = None,\n    max_tokens: Optional[int] = None,\n    stop: Optional[List[str]] = None,\n    extra_body: Optional[Dict[str, Any]] = None,\n    **kwargs\n) -&gt; StreamResponse\n</code></pre> <p>Stream a chat completion response incrementally.</p> <p>Parameters:</p> Name Type Description Default <code>messages</code> <code>list[Message]</code> <p>Conversation messages.</p> required <code>model</code> <code>str</code> <p>Model ID to use. Required unless provided in <code>configuration.model</code>.</p> <code>None</code> <code>temperature</code> <code>Optional[float]</code> <p>See <code>chat</code> for details.</p> <code>None</code> <code>top_p</code> <code>Optional[float]</code> <p>See <code>chat</code> for details.</p> <code>None</code> <code>presence_penalty</code> <code>Optional[float]</code> <p>See <code>chat</code> for details.</p> <code>None</code> <code>frequency_penalty</code> <code>Optional[float]</code> <p>See <code>chat</code> for details.</p> <code>None</code> <code>max_tokens</code> <code>Optional[float]</code> <p>See <code>chat</code> for details.</p> <code>None</code> <code>stop</code> <code>Optional[float]</code> <p>See <code>chat</code> for details.</p> <code>None</code> <code>extra_body</code> <code>Optional[float]</code> <p>See <code>chat</code> for details.</p> <code>None</code> <code>**kwargs</code> <p>Additional provider-specific arguments.</p> <code>{}</code> <p>Yields:</p> Type Description <code>MessageChunk</code> <p>Delta chunks as they arrive from the provider.</p> Notes <ul> <li>Validates <code>messages</code>, final <code>model</code>, and <code>stream=True</code>.</li> </ul> Source code in <code>bridgic/llms/openai_like/_openai_like_llm.py</code> <pre><code>def stream(\n    self,\n    messages: List[Message],\n    model: Optional[str] = None,\n    temperature: Optional[float] = None,\n    top_p: Optional[float] = None,\n    presence_penalty: Optional[float] = None,\n    frequency_penalty: Optional[float] = None,\n    max_tokens: Optional[int] = None,\n    stop: Optional[List[str]] = None,\n    extra_body: Optional[Dict[str, Any]] = None,\n    **kwargs,\n) -&gt; StreamResponse:\n    \"\"\"\n    Stream a chat completion response incrementally.\n\n    Parameters\n    ----------\n    messages : list[Message]\n        Conversation messages.\n    model : str, optional\n        Model ID to use. Required unless provided in `configuration.model`.\n    temperature, top_p, presence_penalty, frequency_penalty, max_tokens, stop, extra_body\n        See `chat` for details.\n    **kwargs\n        Additional provider-specific arguments.\n\n    Yields\n    ------\n    MessageChunk\n        Delta chunks as they arrive from the provider.\n\n    Notes\n    -----\n    - Validates `messages`, final `model`, and `stream=True`.\n    \"\"\"\n    params = self._build_parameters(\n        messages=messages,\n        model=model,\n        temperature=temperature,\n        top_p=top_p,\n        presence_penalty=presence_penalty,\n        frequency_penalty=frequency_penalty,\n        max_tokens=max_tokens,\n        stop=stop,\n        extra_body=extra_body,\n        stream=True,\n        **kwargs,\n    )\n    validate_required_params(params, [\"messages\", \"model\", \"stream\"])\n    response = self.client.chat.completions.create(**params)\n    for chunk in response:\n        delta_content = chunk.choices[0].delta.content\n        delta_content = delta_content if delta_content else \"\"\n        yield MessageChunk(delta=delta_content, raw=chunk)\n</code></pre>"},{"location":"reference/bridgic-llms-openai-like/bridgic/llms/openai_like/#bridgic.llms.openai_like.OpenAILikeLlm.achat","title":"achat","text":"<code>async</code> <pre><code>achat(\n    messages: List[Message],\n    model: Optional[str] = None,\n    temperature: Optional[float] = None,\n    top_p: Optional[float] = None,\n    presence_penalty: Optional[float] = None,\n    frequency_penalty: Optional[float] = None,\n    max_tokens: Optional[int] = None,\n    stop: Optional[List[str]] = None,\n    extra_body: Optional[Dict[str, Any]] = None,\n    **kwargs\n) -&gt; Response\n</code></pre> <p>Asynchronously send a chat completion request to an OpenAI-compatible provider.</p> <p>Parameters:</p> Name Type Description Default <code>messages</code> <code>list[Message]</code> <p>Conversation messages.</p> required <code>model</code> <code>str</code> <p>Model ID to use. Required unless provided in <code>configuration.model</code>.</p> <code>None</code> <code>temperature</code> <code>Optional[float]</code> <p>See <code>chat</code> for details.</p> <code>None</code> <code>top_p</code> <code>Optional[float]</code> <p>See <code>chat</code> for details.</p> <code>None</code> <code>presence_penalty</code> <code>Optional[float]</code> <p>See <code>chat</code> for details.</p> <code>None</code> <code>frequency_penalty</code> <code>Optional[float]</code> <p>See <code>chat</code> for details.</p> <code>None</code> <code>max_tokens</code> <code>Optional[float]</code> <p>See <code>chat</code> for details.</p> <code>None</code> <code>stop</code> <code>Optional[float]</code> <p>See <code>chat</code> for details.</p> <code>None</code> <code>extra_body</code> <code>Optional[float]</code> <p>See <code>chat</code> for details.</p> <code>None</code> <code>**kwargs</code> <p>Additional provider-specific arguments.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Response</code> <p>Bridgic response containing the generated message and raw API response.</p> Source code in <code>bridgic/llms/openai_like/_openai_like_llm.py</code> <pre><code>async def achat(\n    self,\n    messages: List[Message],\n    model: Optional[str] = None,\n    temperature: Optional[float] = None,\n    top_p: Optional[float] = None,\n    presence_penalty: Optional[float] = None,\n    frequency_penalty: Optional[float] = None,\n    max_tokens: Optional[int] = None,\n    stop: Optional[List[str]] = None,\n    extra_body: Optional[Dict[str, Any]] = None,\n    **kwargs,\n) -&gt; Response:\n    \"\"\"\n    Asynchronously send a chat completion request to an OpenAI-compatible provider.\n\n    Parameters\n    ----------\n    messages : list[Message]\n        Conversation messages.\n    model : str, optional\n        Model ID to use. Required unless provided in `configuration.model`.\n    temperature, top_p, presence_penalty, frequency_penalty, max_tokens, stop, extra_body\n        See `chat` for details.\n    **kwargs\n        Additional provider-specific arguments.\n\n    Returns\n    -------\n    Response\n        Bridgic response containing the generated message and raw API response.\n    \"\"\"\n    params = self._build_parameters(\n        messages=messages,\n        model=model,\n        temperature=temperature,\n        top_p=top_p,\n        presence_penalty=presence_penalty,\n        frequency_penalty=frequency_penalty,\n        max_tokens=max_tokens,\n        stop=stop,\n        extra_body=extra_body,\n        **kwargs,\n    )\n    validate_required_params(params, [\"messages\", \"model\"])\n    response = await self.async_client.chat.completions.create(**params)\n    openai_message: ChatCompletionMessage = response.choices[0].message\n    text: str = openai_message.content if openai_message.content else \"\"\n\n    if openai_message.refusal:\n        warnings.warn(openai_message.refusal, RuntimeWarning)\n\n    return Response(\n        message=Message.from_text(text, role=Role.AI),\n        raw=response,\n    )\n</code></pre>"},{"location":"reference/bridgic-llms-openai-like/bridgic/llms/openai_like/#bridgic.llms.openai_like.OpenAILikeLlm.astream","title":"astream","text":"<code>async</code> <pre><code>astream(\n    messages: List[Message],\n    model: Optional[str] = None,\n    temperature: Optional[float] = None,\n    top_p: Optional[float] = None,\n    presence_penalty: Optional[float] = None,\n    frequency_penalty: Optional[float] = None,\n    max_tokens: Optional[int] = None,\n    stop: Optional[List[str]] = None,\n    extra_body: Optional[Dict[str, Any]] = None,\n    **kwargs\n) -&gt; AsyncStreamResponse\n</code></pre> <p>Asynchronously stream a chat completion response incrementally.</p> <p>Parameters:</p> Name Type Description Default <code>messages</code> <code>list[Message]</code> <p>Conversation messages.</p> required <code>model</code> <code>str</code> <p>Model ID to use. Required unless provided in <code>configuration.model</code>.</p> <code>None</code> <code>temperature</code> <code>Optional[float]</code> <p>See <code>chat</code> for details.</p> <code>None</code> <code>top_p</code> <code>Optional[float]</code> <p>See <code>chat</code> for details.</p> <code>None</code> <code>presence_penalty</code> <code>Optional[float]</code> <p>See <code>chat</code> for details.</p> <code>None</code> <code>frequency_penalty</code> <code>Optional[float]</code> <p>See <code>chat</code> for details.</p> <code>None</code> <code>max_tokens</code> <code>Optional[float]</code> <p>See <code>chat</code> for details.</p> <code>None</code> <code>stop</code> <code>Optional[float]</code> <p>See <code>chat</code> for details.</p> <code>None</code> <code>extra_body</code> <code>Optional[float]</code> <p>See <code>chat</code> for details.</p> <code>None</code> <code>**kwargs</code> <p>Additional provider-specific arguments.</p> <code>{}</code> <p>Yields:</p> Type Description <code>MessageChunk</code> <p>Delta chunks as they arrive from the provider.</p> Notes <ul> <li>Validates <code>messages</code>, final <code>model</code>, and <code>stream=True</code>.</li> </ul> Source code in <code>bridgic/llms/openai_like/_openai_like_llm.py</code> <pre><code>async def astream(\n    self,\n    messages: List[Message],\n    model: Optional[str] = None,\n    temperature: Optional[float] = None,\n    top_p: Optional[float] = None,\n    presence_penalty: Optional[float] = None,\n    frequency_penalty: Optional[float] = None,\n    max_tokens: Optional[int] = None,\n    stop: Optional[List[str]] = None,\n    extra_body: Optional[Dict[str, Any]] = None,\n    **kwargs,\n) -&gt; AsyncStreamResponse:\n    \"\"\"\n    Asynchronously stream a chat completion response incrementally.\n\n    Parameters\n    ----------\n    messages : list[Message]\n        Conversation messages.\n    model : str, optional\n        Model ID to use. Required unless provided in `configuration.model`.\n    temperature, top_p, presence_penalty, frequency_penalty, max_tokens, stop, extra_body\n        See `chat` for details.\n    **kwargs\n        Additional provider-specific arguments.\n\n    Yields\n    ------\n    MessageChunk\n        Delta chunks as they arrive from the provider.\n\n    Notes\n    -----\n    - Validates `messages`, final `model`, and `stream=True`.\n    \"\"\"\n    params = self._build_parameters(\n        messages=messages,\n        model=model,\n        temperature=temperature,\n        top_p=top_p,\n        presence_penalty=presence_penalty,\n        frequency_penalty=frequency_penalty,\n        max_tokens=max_tokens,\n        stop=stop,\n        extra_body=extra_body,\n        stream=True,\n        **kwargs,\n    )\n    validate_required_params(params, [\"messages\", \"model\", \"stream\"])\n    response = await self.async_client.chat.completions.create(**params)\n    async for chunk in response:\n        delta_content = chunk.choices[0].delta.content\n        delta_content = delta_content if delta_content else \"\"\n        yield MessageChunk(delta=delta_content, raw=chunk)\n</code></pre>"},{"location":"reference/bridgic-llms-vllm/bridgic/llms/vllm/","title":"vllm","text":"<p>The vLLM integration module provides support for the vLLM inference engine.</p> <p>This module implements communication interfaces with vLLM inference services, supporting  highly reliable calls to large language models deployed via vLLM, and provides several  encapsulations for common seen high-level functionality.</p> <p>You can install the vLLM integration package for Bridgic by running:</p> <pre><code>pip install bridgic-llms-vllm\n</code></pre>"},{"location":"reference/bridgic-llms-vllm/bridgic/llms/vllm/#bridgic.llms.vllm.VllmServerLlm","title":"VllmServerLlm","text":"<p>               Bases: <code>OpenAILikeLlm</code>, <code>StructuredOutput</code>, <code>ToolSelection</code></p> <p>VllmServerLlm is a wrapper around the vLLM server, providing common calling interfaces for  self-hosted LLM service, such as chat, stream, as well as with encapsulation of common  seen high-level functionality.</p> <p>Parameters:</p> Name Type Description Default <code>api_base</code> <code>str</code> <p>The base URL of the LLM provider.</p> required <code>api_key</code> <code>str</code> <p>The API key of the LLM provider.</p> required <code>configuration</code> <code>Optional[VllmServerConfiguration]</code> <p>The configuration for the OpenAI-compatible API. If None, uses the default configuration.</p> <code>VllmServerConfiguration()</code> <code>timeout</code> <code>Optional[float]</code> <p>The timeout in seconds. If None, no timeout is applied.</p> <code>None</code> <code>http_client</code> <code>Optional[Client]</code> <p>Custom synchronous HTTP client for requests. If None, creates a default client.</p> <code>None</code> <code>http_async_client</code> <code>Optional[AsyncClient]</code> <p>Custom asynchronous HTTP client for requests. If None, creates a default client.</p> <code>None</code> Source code in <code>bridgic/llms/vllm/_vllm_server_llm.py</code> <pre><code>class VllmServerLlm(OpenAILikeLlm, StructuredOutput, ToolSelection):\n    \"\"\"\n    VllmServerLlm is a wrapper around the vLLM server, providing common calling interfaces for \n    self-hosted LLM service, such as chat, stream, as well as with encapsulation of common \n    seen high-level functionality.\n\n    Parameters\n    ----------\n    api_base: str\n        The base URL of the LLM provider.\n    api_key: str\n        The API key of the LLM provider.\n    configuration: Optional[VllmServerConfiguration]\n        The configuration for the OpenAI-compatible API. If None, uses the default configuration.\n    timeout: Optional[float]\n        The timeout in seconds. If None, no timeout is applied.\n    http_client : Optional[httpx.Client]\n        Custom synchronous HTTP client for requests. If None, creates a default client.\n    http_async_client : Optional[httpx.AsyncClient]\n        Custom asynchronous HTTP client for requests. If None, creates a default client.\n    \"\"\"\n\n    def __init__(\n        self,\n        api_base: str,\n        api_key: str,\n        configuration: Optional[VllmServerConfiguration] = VllmServerConfiguration(),\n        timeout: Optional[float] = None,\n        http_client: Optional[httpx.Client] = None,\n        http_async_client: Optional[httpx.AsyncClient] = None,\n    ):\n        super().__init__(\n            api_base=api_base,\n            api_key=api_key,\n            configuration=configuration,\n            timeout=timeout,\n            http_client=http_client,\n            http_async_client=http_async_client,\n        )\n\n    @override\n    def dump_to_dict(self) -&gt; Dict[str, Any]:\n        return super().dump_to_dict()\n\n    @override\n    def load_from_dict(self, state_dict: Dict[str, Any]) -&gt; None:\n        super().load_from_dict(state_dict)\n\n    @overload\n    def structured_output(\n        self,\n        messages: List[Message],\n        constraint: PydanticModel,\n        model: Optional[str] = None,\n        temperature: Optional[float] = ...,\n        top_p: Optional[float] = ...,\n        presence_penalty: Optional[float] = ...,\n        frequency_penalty: Optional[float] = ...,\n        extra_body: Optional[Dict[str, Any]] = ...,\n        **kwargs,\n    ) -&gt; BaseModel: ...\n\n    @overload\n    def structured_output(\n        self,\n        messages: List[Message],\n        constraint: JsonSchema,\n        model: Optional[str] = None,\n        temperature: Optional[float] = ...,\n        top_p: Optional[float] = ...,\n        presence_penalty: Optional[float] = ...,\n        frequency_penalty: Optional[float] = ...,\n        extra_body: Optional[Dict[str, Any]] = ...,\n        **kwargs,\n    ) -&gt; Dict[str, Any]: ...\n\n    @overload\n    def structured_output(\n        self,\n        messages: List[Message],\n        constraint: Choice,\n        model: Optional[str] = None,\n        temperature: Optional[float] = ...,\n        top_p: Optional[float] = ...,\n        presence_penalty: Optional[float] = ...,\n        frequency_penalty: Optional[float] = ...,\n        extra_body: Optional[Dict[str, Any]] = ...,\n        **kwargs,\n    ) -&gt; str: ...\n\n    def structured_output(\n        self,\n        messages: List[Message],\n        constraint: Constraint,\n        model: Optional[str] = None,\n        temperature: Optional[float] = None,\n        top_p: Optional[float] = None,\n        presence_penalty: Optional[float] = None,\n        frequency_penalty: Optional[float] = None,\n        extra_body: Optional[Dict[str, Any]] = None,\n        **kwargs,\n    ) -&gt; Union[BaseModel, Dict[str, Any], str]:\n        '''\n        Structured output in a specified format. This part of the functionality is provided based on the \n        capabilities of [vLLM Structured Output](https://docs.vllm.ai/en/latest/features/structured_outputs.html).\n\n        Parameters\n        ----------\n        messages: List[Message]\n            The messages to send to the LLM.\n        constraint: Constraint\n            The constraint to use for the structured output.\n        model: Optional[str]\n            The model to use for the structured output.\n        temperature: Optional[float]\n            The temperature to use for the structured output.\n        top_p: Optional[float]\n            The top_p to use for the structured output.\n        presence_penalty: Optional[float]\n            The presence_penalty to use for the structured output.\n        frequency_penalty: Optional[float]\n            The frequency_penalty to use for the structured output.\n        extra_body: Optional[Dict[str, Any]]\n            The extra_body to use for the structured output.\n        **kwargs: Any\n            The kwargs to use for the structured output.\n\n        Returns\n        -------\n        Union[BaseModel, Dict[str, Any], str]\n            The return type is based on the constraint type:\n            * If the constraint is PydanticModel, return an instance of the corresponding Pydantic model.\n            * If the constraint is JsonSchema, return a Dict[str, Any] that is the parsed JSON.\n            * Otherwise, return a str.\n        '''\n        response = self.chat(\n            messages=messages,\n            model=model,\n            temperature=temperature,\n            top_p=top_p,\n            presence_penalty=presence_penalty,\n            frequency_penalty=frequency_penalty,\n            extra_body=self._convert_constraint(constraint, extra_body),\n            **kwargs,\n        )\n        return self._convert_response(constraint, response)\n\n    async def astructured_output(\n        self,\n        messages: List[Message],\n        constraint: Constraint,\n        model: Optional[str] = None,\n        temperature: Optional[float] = None,\n        top_p: Optional[float] = None,\n        presence_penalty: Optional[float] = None,\n        frequency_penalty: Optional[float] = None,\n        extra_body: Optional[Dict[str, Any]] = None,\n        **kwargs,\n    ) -&gt; Union[BaseModel, Dict[str, Any], str]:\n        '''\n        Structured output in a specified format. This part of the functionality is provided based on the \n        capabilities of [vLLM Structured Output](https://docs.vllm.ai/en/latest/features/structured_outputs.html).\n\n        Parameters\n        ----------\n        messages: List[Message]\n            The messages to send to the LLM.\n        constraint: Constraint\n            The constraint to use for the structured output.\n        model: Optional[str]\n            The model to use for the structured output.\n        temperature: Optional[float]\n            The temperature to use for the structured output.\n        top_p: Optional[float]\n            The top_p to use for the structured output.\n        presence_penalty: Optional[float]\n            The presence_penalty to use for the structured output.\n        frequency_penalty: Optional[float]\n            The frequency_penalty to use for the structured output.\n        extra_body: Optional[Dict[str, Any]]\n            The extra_body to use for the structured output.\n        **kwargs: Any\n            The kwargs to use for the structured output.\n\n        Returns\n        -------\n        Union[BaseModel, Dict[str, Any], str]\n            The return type is based on the constraint type:\n            * If the constraint is PydanticModel, return an instance of the corresponding Pydantic model.\n            * If the constraint is JsonSchema, return a Dict[str, Any] that is the parsed JSON.\n            * Otherwise, return a str.\n        '''\n        response = await self.achat(\n            messages=messages,\n            model=model,\n            temperature=temperature,\n            top_p=top_p,\n            presence_penalty=presence_penalty,\n            frequency_penalty=frequency_penalty,\n            extra_body=self._convert_constraint(constraint, extra_body),\n            **kwargs,\n        )\n        return self._convert_response(constraint, response)\n\n    def _convert_constraint(\n        self,\n        constraint: Constraint,\n        extra_body: Optional[Dict[str, Any]] = None,\n    ) -&gt; Dict[str, Any]:\n        extra_body = {} if extra_body is None else extra_body\n\n        if isinstance(constraint, PydanticModel):\n            extra_body[\"guided_json\"] = constraint.model.model_json_schema()\n        elif isinstance(constraint, JsonSchema):\n            extra_body[\"guided_json\"] = constraint.schema_dict\n        elif isinstance(constraint, Regex):\n            extra_body[\"guided_regex\"] = constraint.pattern\n        elif isinstance(constraint, Choice):\n            extra_body[\"guided_choice\"] = constraint.choices\n        elif isinstance(constraint, EbnfGrammar):\n            extra_body[\"guided_grammar\"] = constraint.syntax\n        else:\n            raise ValueError(f\"Invalid constraint: {constraint}\")\n\n        return extra_body\n\n    def _convert_response(\n        self,\n        constraint: Constraint,\n        response: Response,\n    ) -&gt; Union[BaseModel, Dict[str, Any], str]:\n        content = response.message.content\n\n        if isinstance(constraint, PydanticModel):\n            return constraint.model.model_validate_json(content)\n        elif isinstance(constraint, JsonSchema):\n            return json.loads(content)\n        return content\n\n    def select_tool(\n        self,\n        messages: List[Message],\n        tools: List[Tool],\n        model: Optional[str] = None,\n        temperature: Optional[float] = None,\n        top_p: Optional[float] = None,\n        presence_penalty: Optional[float] = None,\n        frequency_penalty: Optional[float] = None,\n        extra_body: Optional[Dict[str, Any]] = None,\n        tool_choice: Union[Literal[\"auto\", \"required\", \"none\"], ChatCompletionNamedToolChoiceParam] = \"auto\",\n        **kwargs,\n    ) -&gt; Tuple[List[ToolCall], Optional[Dict]]:\n        \"\"\"\n        Select tools from a specified list of tools.\n\n        Parameters\n        ----------\n        messages: List[Message]\n            The messages to send to the LLM.\n        tools: List[Tool]\n            The tools to use for the tool select.\n        model: Optional[str]\n            The model to use for the tool select.\n        temperature: Optional[float]\n            The temperature to use for the tool select.\n        top_p: Optional[float]\n            The top_p to use for the tool select.\n        presence_penalty: Optional[float]\n            The presence_penalty to use for the tool select.\n        frequency_penalty: Optional[float]\n            The frequency_penalty to use for the tool select.\n        extra_body: Optional[Dict[str, Any]]\n            The extra_body to use for the tool select.\n        tool_choice : Union[Literal[\"auto\", \"required\", \"none\"], ChatCompletionNamedToolChoiceParam]\n            Tool choice mode for tool calling. There are 4 choices that are supported:\n            - `auto` means the model can pick between generating a message or calling one or more tools.\n            To enable this feature, you should set the tags `--enable-auto-tool-choice` and `--tool-call-parser` \n            when starting the vLLM server.\n            - `required` means the model must generate one or more tool calls based on the specified tool list \n            in the `tools` parameter. The number of tool calls depends on the user's query.\n            - `none` means the model will not call any tool and instead generates a message. When tools are \n            specified in the request, vLLM includes tool definitions in the prompt by default, regardless \n            of the tool_choice setting. To exclude tool definitions when tool_choice='none', use the \n            `--exclude-tools-when-tool-choice-none` option when starting the vLLM server.\n            - You can also specify a particular function using named function calling by setting `tool_choice` \n            parameter to a json object, like `tool_choice={\"type\": \"function\", \"function\": {\"name\": \"get_weather\"}}`.\n\n        **kwargs: Any\n            The kwargs to use for the tool select.\n\n        Returns\n        -------\n        Tuple[List[ToolCall], Optional[str]]\n            A list that contains the selected tools and their arguments.\n\n        Notes\n        -----\n        See more on [Tool Calling](https://docs.vllm.ai/en/stable/features/tool_calling.html).\n        \"\"\"\n        # Build parameters dictionary for validation\n        params = filter_dict(merge_dict(self.configuration.model_dump(), {\n            \"model\": model,\n            \"temperature\": temperature,\n            \"top_p\": top_p,\n            \"presence_penalty\": presence_penalty,\n            \"frequency_penalty\": frequency_penalty,\n            \"extra_body\": extra_body,\n            **kwargs,\n        }))\n\n        # Validate required parameters for tool selection\n        validate_required_params(params, [\"model\"])\n\n        input_messages = [self._convert_message(message=msg, strict=True) for msg in messages]\n        input_tools = [\n            {\n                \"type\": \"function\",\n                \"function\": {\n                    \"name\": tool.name,\n                    \"description\": tool.description,\n                    \"parameters\": tool.parameters,\n                },\n            } for tool in tools\n        ]\n\n        response = self.client.chat.completions.create(\n            model=model,\n            messages=input_messages,\n            tools=input_tools,\n            tool_choice=tool_choice,\n            **kwargs,\n        )\n        tool_calls = response.choices[0].message.tool_calls\n\n        output_content = \"\"\n        if response.choices[0].message.content:\n            output_content = response.choices[0].message.content\n\n        output_tool_calls = []\n        if tool_calls:\n            output_tool_calls = self._convert_tool_calls(tool_calls)\n\n        return (output_tool_calls, output_content)\n\n    async def aselect_tool(\n        self,\n        messages: List[Message],\n        tools: List[Tool],\n        model: Optional[str] = None,\n        temperature: Optional[float] = None,\n        top_p: Optional[float] = None,\n        presence_penalty: Optional[float] = None,\n        frequency_penalty: Optional[float] = None,\n        extra_body: Optional[Dict[str, Any]] = None,\n        tool_choice: Union[Literal[\"auto\", \"required\", \"none\"], ChatCompletionNamedToolChoiceParam] = \"auto\",\n        **kwargs,\n    ) -&gt; Tuple[List[ToolCall], Optional[str]]:\n        \"\"\"\n        Select tools from a specified list of tools.\n\n        Parameters\n        ----------\n        messages: List[Message]\n            The messages to send to the LLM.\n        tools: List[Tool]\n            The tools to use for the tool select.\n        model: Optional[str]\n            The model to use for the tool select.\n        temperature: Optional[float]\n            The temperature to use for the tool select.\n        top_p: Optional[float]\n            The top_p to use for the tool select.\n        presence_penalty: Optional[float]\n            The presence_penalty to use for the tool select.\n        frequency_penalty: Optional[float]\n            The frequency_penalty to use for the tool select.\n        extra_body: Optional[Dict[str, Any]]\n            The extra_body to use for the tool select.\n        tool_choice : Union[Literal[\"auto\", \"required\", \"none\"], ChatCompletionNamedToolChoiceParam]\n            Tool choice mode for tool calling. There are 4 choices that are supported:\n            - `auto` means the model can pick between generating a message or calling one or more tools.\n            To enable this feature, you should set the tags `--enable-auto-tool-choice` and `--tool-call-parser` \n            when starting the vLLM server.\n            - `required` means the model must generate one or more tool calls based on the specified tool list \n            in the `tools` parameter. The number of tool calls depends on the user's query.\n            - `none` means the model will not call any tool and instead generates a message. When tools are \n            specified in the request, vLLM includes tool definitions in the prompt by default, regardless \n            of the tool_choice setting. To exclude tool definitions when tool_choice='none', use the \n            `--exclude-tools-when-tool-choice-none` option when starting the vLLM server.\n            - You can also specify a particular function using named function calling by setting `tool_choice` \n            parameter to a json object, like `tool_choice={\"type\": \"function\", \"function\": {\"name\": \"get_weather\"}}`.\n\n        **kwargs: Any\n            The kwargs to use for the tool select.\n\n        Returns\n        -------\n        Tuple[List[ToolCall], Optional[str]]\n            A list that contains the selected tools and their arguments.\n\n        Notes\n        -----\n        See more on [Tool Calling](https://docs.vllm.ai/en/stable/features/tool_calling.html).\n        \"\"\"\n        # Build parameters dictionary for validation\n        params = filter_dict(merge_dict(self.configuration.model_dump(), {\n            \"model\": model,\n            \"temperature\": temperature,\n            \"top_p\": top_p,\n            \"presence_penalty\": presence_penalty,\n            \"frequency_penalty\": frequency_penalty,\n            \"extra_body\": extra_body,\n            **kwargs,\n        }))\n\n        # Validate required parameters for tool selection\n        validate_required_params(params, [\"model\"])\n\n        input_messages = [self._convert_message(message=msg, strict=True) for msg in messages]\n        input_tools = [\n            {\n                \"type\": \"function\",\n                \"function\": {\n                    \"name\": tool.name,\n                    \"description\": tool.description,\n                    \"parameters\": tool.parameters,\n                },\n            } for tool in tools\n        ]\n\n        response = self.client.chat.completions.create(\n            model=model,\n            messages=input_messages,\n            tools=input_tools,\n            tool_choice=tool_choice,\n            **kwargs,\n        )\n        tool_calls = response.choices[0].message.tool_calls\n\n        output_content = \"\"\n        if response.choices[0].message.content:\n            output_content = response.choices[0].message.content\n\n        output_tool_calls = []\n        if tool_calls:\n            output_tool_calls = self._convert_tool_calls(tool_calls)\n\n        return (output_tool_calls, output_content)\n\n    def _convert_tool_calls(self, tool_calls: List[ChatCompletionMessageFunctionToolCall]) -&gt; List[ToolCall]:\n        return [\n            ToolCall(\n                id=tool_call.id,\n                name=tool_call.function.name,\n                arguments=json.loads(tool_call.function.arguments),\n            ) for tool_call in tool_calls\n        ]\n</code></pre>"},{"location":"reference/bridgic-llms-vllm/bridgic/llms/vllm/#bridgic.llms.vllm.VllmServerLlm.structured_output","title":"structured_output","text":"<pre><code>structured_output(\n    messages: List[Message],\n    constraint: Constraint,\n    model: Optional[str] = None,\n    temperature: Optional[float] = None,\n    top_p: Optional[float] = None,\n    presence_penalty: Optional[float] = None,\n    frequency_penalty: Optional[float] = None,\n    extra_body: Optional[Dict[str, Any]] = None,\n    **kwargs\n) -&gt; Union[BaseModel, Dict[str, Any], str]\n</code></pre> <p>Structured output in a specified format. This part of the functionality is provided based on the  capabilities of vLLM Structured Output.</p> <p>Parameters:</p> Name Type Description Default <code>messages</code> <code>List[Message]</code> <p>The messages to send to the LLM.</p> required <code>constraint</code> <code>Constraint</code> <p>The constraint to use for the structured output.</p> required <code>model</code> <code>Optional[str]</code> <p>The model to use for the structured output.</p> <code>None</code> <code>temperature</code> <code>Optional[float]</code> <p>The temperature to use for the structured output.</p> <code>None</code> <code>top_p</code> <code>Optional[float]</code> <p>The top_p to use for the structured output.</p> <code>None</code> <code>presence_penalty</code> <code>Optional[float]</code> <p>The presence_penalty to use for the structured output.</p> <code>None</code> <code>frequency_penalty</code> <code>Optional[float]</code> <p>The frequency_penalty to use for the structured output.</p> <code>None</code> <code>extra_body</code> <code>Optional[Dict[str, Any]]</code> <p>The extra_body to use for the structured output.</p> <code>None</code> <code>**kwargs</code> <p>The kwargs to use for the structured output.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Union[BaseModel, Dict[str, Any], str]</code> <p>The return type is based on the constraint type: * If the constraint is PydanticModel, return an instance of the corresponding Pydantic model. * If the constraint is JsonSchema, return a Dict[str, Any] that is the parsed JSON. * Otherwise, return a str.</p> Source code in <code>bridgic/llms/vllm/_vllm_server_llm.py</code> <pre><code>def structured_output(\n    self,\n    messages: List[Message],\n    constraint: Constraint,\n    model: Optional[str] = None,\n    temperature: Optional[float] = None,\n    top_p: Optional[float] = None,\n    presence_penalty: Optional[float] = None,\n    frequency_penalty: Optional[float] = None,\n    extra_body: Optional[Dict[str, Any]] = None,\n    **kwargs,\n) -&gt; Union[BaseModel, Dict[str, Any], str]:\n    '''\n    Structured output in a specified format. This part of the functionality is provided based on the \n    capabilities of [vLLM Structured Output](https://docs.vllm.ai/en/latest/features/structured_outputs.html).\n\n    Parameters\n    ----------\n    messages: List[Message]\n        The messages to send to the LLM.\n    constraint: Constraint\n        The constraint to use for the structured output.\n    model: Optional[str]\n        The model to use for the structured output.\n    temperature: Optional[float]\n        The temperature to use for the structured output.\n    top_p: Optional[float]\n        The top_p to use for the structured output.\n    presence_penalty: Optional[float]\n        The presence_penalty to use for the structured output.\n    frequency_penalty: Optional[float]\n        The frequency_penalty to use for the structured output.\n    extra_body: Optional[Dict[str, Any]]\n        The extra_body to use for the structured output.\n    **kwargs: Any\n        The kwargs to use for the structured output.\n\n    Returns\n    -------\n    Union[BaseModel, Dict[str, Any], str]\n        The return type is based on the constraint type:\n        * If the constraint is PydanticModel, return an instance of the corresponding Pydantic model.\n        * If the constraint is JsonSchema, return a Dict[str, Any] that is the parsed JSON.\n        * Otherwise, return a str.\n    '''\n    response = self.chat(\n        messages=messages,\n        model=model,\n        temperature=temperature,\n        top_p=top_p,\n        presence_penalty=presence_penalty,\n        frequency_penalty=frequency_penalty,\n        extra_body=self._convert_constraint(constraint, extra_body),\n        **kwargs,\n    )\n    return self._convert_response(constraint, response)\n</code></pre>"},{"location":"reference/bridgic-llms-vllm/bridgic/llms/vllm/#bridgic.llms.vllm.VllmServerLlm.astructured_output","title":"astructured_output","text":"<code>async</code> <pre><code>astructured_output(\n    messages: List[Message],\n    constraint: Constraint,\n    model: Optional[str] = None,\n    temperature: Optional[float] = None,\n    top_p: Optional[float] = None,\n    presence_penalty: Optional[float] = None,\n    frequency_penalty: Optional[float] = None,\n    extra_body: Optional[Dict[str, Any]] = None,\n    **kwargs\n) -&gt; Union[BaseModel, Dict[str, Any], str]\n</code></pre> <p>Structured output in a specified format. This part of the functionality is provided based on the  capabilities of vLLM Structured Output.</p> <p>Parameters:</p> Name Type Description Default <code>messages</code> <code>List[Message]</code> <p>The messages to send to the LLM.</p> required <code>constraint</code> <code>Constraint</code> <p>The constraint to use for the structured output.</p> required <code>model</code> <code>Optional[str]</code> <p>The model to use for the structured output.</p> <code>None</code> <code>temperature</code> <code>Optional[float]</code> <p>The temperature to use for the structured output.</p> <code>None</code> <code>top_p</code> <code>Optional[float]</code> <p>The top_p to use for the structured output.</p> <code>None</code> <code>presence_penalty</code> <code>Optional[float]</code> <p>The presence_penalty to use for the structured output.</p> <code>None</code> <code>frequency_penalty</code> <code>Optional[float]</code> <p>The frequency_penalty to use for the structured output.</p> <code>None</code> <code>extra_body</code> <code>Optional[Dict[str, Any]]</code> <p>The extra_body to use for the structured output.</p> <code>None</code> <code>**kwargs</code> <p>The kwargs to use for the structured output.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Union[BaseModel, Dict[str, Any], str]</code> <p>The return type is based on the constraint type: * If the constraint is PydanticModel, return an instance of the corresponding Pydantic model. * If the constraint is JsonSchema, return a Dict[str, Any] that is the parsed JSON. * Otherwise, return a str.</p> Source code in <code>bridgic/llms/vllm/_vllm_server_llm.py</code> <pre><code>async def astructured_output(\n    self,\n    messages: List[Message],\n    constraint: Constraint,\n    model: Optional[str] = None,\n    temperature: Optional[float] = None,\n    top_p: Optional[float] = None,\n    presence_penalty: Optional[float] = None,\n    frequency_penalty: Optional[float] = None,\n    extra_body: Optional[Dict[str, Any]] = None,\n    **kwargs,\n) -&gt; Union[BaseModel, Dict[str, Any], str]:\n    '''\n    Structured output in a specified format. This part of the functionality is provided based on the \n    capabilities of [vLLM Structured Output](https://docs.vllm.ai/en/latest/features/structured_outputs.html).\n\n    Parameters\n    ----------\n    messages: List[Message]\n        The messages to send to the LLM.\n    constraint: Constraint\n        The constraint to use for the structured output.\n    model: Optional[str]\n        The model to use for the structured output.\n    temperature: Optional[float]\n        The temperature to use for the structured output.\n    top_p: Optional[float]\n        The top_p to use for the structured output.\n    presence_penalty: Optional[float]\n        The presence_penalty to use for the structured output.\n    frequency_penalty: Optional[float]\n        The frequency_penalty to use for the structured output.\n    extra_body: Optional[Dict[str, Any]]\n        The extra_body to use for the structured output.\n    **kwargs: Any\n        The kwargs to use for the structured output.\n\n    Returns\n    -------\n    Union[BaseModel, Dict[str, Any], str]\n        The return type is based on the constraint type:\n        * If the constraint is PydanticModel, return an instance of the corresponding Pydantic model.\n        * If the constraint is JsonSchema, return a Dict[str, Any] that is the parsed JSON.\n        * Otherwise, return a str.\n    '''\n    response = await self.achat(\n        messages=messages,\n        model=model,\n        temperature=temperature,\n        top_p=top_p,\n        presence_penalty=presence_penalty,\n        frequency_penalty=frequency_penalty,\n        extra_body=self._convert_constraint(constraint, extra_body),\n        **kwargs,\n    )\n    return self._convert_response(constraint, response)\n</code></pre>"},{"location":"reference/bridgic-llms-vllm/bridgic/llms/vllm/#bridgic.llms.vllm.VllmServerLlm.select_tool","title":"select_tool","text":"<pre><code>select_tool(\n    messages: List[Message],\n    tools: List[Tool],\n    model: Optional[str] = None,\n    temperature: Optional[float] = None,\n    top_p: Optional[float] = None,\n    presence_penalty: Optional[float] = None,\n    frequency_penalty: Optional[float] = None,\n    extra_body: Optional[Dict[str, Any]] = None,\n    tool_choice: Union[\n        Literal[\"auto\", \"required\", \"none\"],\n        ChatCompletionNamedToolChoiceParam,\n    ] = \"auto\",\n    **kwargs\n) -&gt; Tuple[List[ToolCall], Optional[Dict]]\n</code></pre> <p>Select tools from a specified list of tools.</p> <p>Parameters:</p> Name Type Description Default <code>messages</code> <code>List[Message]</code> <p>The messages to send to the LLM.</p> required <code>tools</code> <code>List[Tool]</code> <p>The tools to use for the tool select.</p> required <code>model</code> <code>Optional[str]</code> <p>The model to use for the tool select.</p> <code>None</code> <code>temperature</code> <code>Optional[float]</code> <p>The temperature to use for the tool select.</p> <code>None</code> <code>top_p</code> <code>Optional[float]</code> <p>The top_p to use for the tool select.</p> <code>None</code> <code>presence_penalty</code> <code>Optional[float]</code> <p>The presence_penalty to use for the tool select.</p> <code>None</code> <code>frequency_penalty</code> <code>Optional[float]</code> <p>The frequency_penalty to use for the tool select.</p> <code>None</code> <code>extra_body</code> <code>Optional[Dict[str, Any]]</code> <p>The extra_body to use for the tool select.</p> <code>None</code> <code>tool_choice</code> <code>Union[Literal['auto', 'required', 'none'], ChatCompletionNamedToolChoiceParam]</code> <p>Tool choice mode for tool calling. There are 4 choices that are supported: - <code>auto</code> means the model can pick between generating a message or calling one or more tools. To enable this feature, you should set the tags <code>--enable-auto-tool-choice</code> and <code>--tool-call-parser</code>  when starting the vLLM server. - <code>required</code> means the model must generate one or more tool calls based on the specified tool list  in the <code>tools</code> parameter. The number of tool calls depends on the user's query. - <code>none</code> means the model will not call any tool and instead generates a message. When tools are  specified in the request, vLLM includes tool definitions in the prompt by default, regardless  of the tool_choice setting. To exclude tool definitions when tool_choice='none', use the  <code>--exclude-tools-when-tool-choice-none</code> option when starting the vLLM server. - You can also specify a particular function using named function calling by setting <code>tool_choice</code>  parameter to a json object, like <code>tool_choice={\"type\": \"function\", \"function\": {\"name\": \"get_weather\"}}</code>.</p> <code>'auto'</code> <code>**kwargs</code> <p>The kwargs to use for the tool select.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Tuple[List[ToolCall], Optional[str]]</code> <p>A list that contains the selected tools and their arguments.</p> Notes <p>See more on Tool Calling.</p> Source code in <code>bridgic/llms/vllm/_vllm_server_llm.py</code> <pre><code>def select_tool(\n    self,\n    messages: List[Message],\n    tools: List[Tool],\n    model: Optional[str] = None,\n    temperature: Optional[float] = None,\n    top_p: Optional[float] = None,\n    presence_penalty: Optional[float] = None,\n    frequency_penalty: Optional[float] = None,\n    extra_body: Optional[Dict[str, Any]] = None,\n    tool_choice: Union[Literal[\"auto\", \"required\", \"none\"], ChatCompletionNamedToolChoiceParam] = \"auto\",\n    **kwargs,\n) -&gt; Tuple[List[ToolCall], Optional[Dict]]:\n    \"\"\"\n    Select tools from a specified list of tools.\n\n    Parameters\n    ----------\n    messages: List[Message]\n        The messages to send to the LLM.\n    tools: List[Tool]\n        The tools to use for the tool select.\n    model: Optional[str]\n        The model to use for the tool select.\n    temperature: Optional[float]\n        The temperature to use for the tool select.\n    top_p: Optional[float]\n        The top_p to use for the tool select.\n    presence_penalty: Optional[float]\n        The presence_penalty to use for the tool select.\n    frequency_penalty: Optional[float]\n        The frequency_penalty to use for the tool select.\n    extra_body: Optional[Dict[str, Any]]\n        The extra_body to use for the tool select.\n    tool_choice : Union[Literal[\"auto\", \"required\", \"none\"], ChatCompletionNamedToolChoiceParam]\n        Tool choice mode for tool calling. There are 4 choices that are supported:\n        - `auto` means the model can pick between generating a message or calling one or more tools.\n        To enable this feature, you should set the tags `--enable-auto-tool-choice` and `--tool-call-parser` \n        when starting the vLLM server.\n        - `required` means the model must generate one or more tool calls based on the specified tool list \n        in the `tools` parameter. The number of tool calls depends on the user's query.\n        - `none` means the model will not call any tool and instead generates a message. When tools are \n        specified in the request, vLLM includes tool definitions in the prompt by default, regardless \n        of the tool_choice setting. To exclude tool definitions when tool_choice='none', use the \n        `--exclude-tools-when-tool-choice-none` option when starting the vLLM server.\n        - You can also specify a particular function using named function calling by setting `tool_choice` \n        parameter to a json object, like `tool_choice={\"type\": \"function\", \"function\": {\"name\": \"get_weather\"}}`.\n\n    **kwargs: Any\n        The kwargs to use for the tool select.\n\n    Returns\n    -------\n    Tuple[List[ToolCall], Optional[str]]\n        A list that contains the selected tools and their arguments.\n\n    Notes\n    -----\n    See more on [Tool Calling](https://docs.vllm.ai/en/stable/features/tool_calling.html).\n    \"\"\"\n    # Build parameters dictionary for validation\n    params = filter_dict(merge_dict(self.configuration.model_dump(), {\n        \"model\": model,\n        \"temperature\": temperature,\n        \"top_p\": top_p,\n        \"presence_penalty\": presence_penalty,\n        \"frequency_penalty\": frequency_penalty,\n        \"extra_body\": extra_body,\n        **kwargs,\n    }))\n\n    # Validate required parameters for tool selection\n    validate_required_params(params, [\"model\"])\n\n    input_messages = [self._convert_message(message=msg, strict=True) for msg in messages]\n    input_tools = [\n        {\n            \"type\": \"function\",\n            \"function\": {\n                \"name\": tool.name,\n                \"description\": tool.description,\n                \"parameters\": tool.parameters,\n            },\n        } for tool in tools\n    ]\n\n    response = self.client.chat.completions.create(\n        model=model,\n        messages=input_messages,\n        tools=input_tools,\n        tool_choice=tool_choice,\n        **kwargs,\n    )\n    tool_calls = response.choices[0].message.tool_calls\n\n    output_content = \"\"\n    if response.choices[0].message.content:\n        output_content = response.choices[0].message.content\n\n    output_tool_calls = []\n    if tool_calls:\n        output_tool_calls = self._convert_tool_calls(tool_calls)\n\n    return (output_tool_calls, output_content)\n</code></pre>"},{"location":"reference/bridgic-llms-vllm/bridgic/llms/vllm/#bridgic.llms.vllm.VllmServerLlm.aselect_tool","title":"aselect_tool","text":"<code>async</code> <pre><code>aselect_tool(\n    messages: List[Message],\n    tools: List[Tool],\n    model: Optional[str] = None,\n    temperature: Optional[float] = None,\n    top_p: Optional[float] = None,\n    presence_penalty: Optional[float] = None,\n    frequency_penalty: Optional[float] = None,\n    extra_body: Optional[Dict[str, Any]] = None,\n    tool_choice: Union[\n        Literal[\"auto\", \"required\", \"none\"],\n        ChatCompletionNamedToolChoiceParam,\n    ] = \"auto\",\n    **kwargs\n) -&gt; Tuple[List[ToolCall], Optional[str]]\n</code></pre> <p>Select tools from a specified list of tools.</p> <p>Parameters:</p> Name Type Description Default <code>messages</code> <code>List[Message]</code> <p>The messages to send to the LLM.</p> required <code>tools</code> <code>List[Tool]</code> <p>The tools to use for the tool select.</p> required <code>model</code> <code>Optional[str]</code> <p>The model to use for the tool select.</p> <code>None</code> <code>temperature</code> <code>Optional[float]</code> <p>The temperature to use for the tool select.</p> <code>None</code> <code>top_p</code> <code>Optional[float]</code> <p>The top_p to use for the tool select.</p> <code>None</code> <code>presence_penalty</code> <code>Optional[float]</code> <p>The presence_penalty to use for the tool select.</p> <code>None</code> <code>frequency_penalty</code> <code>Optional[float]</code> <p>The frequency_penalty to use for the tool select.</p> <code>None</code> <code>extra_body</code> <code>Optional[Dict[str, Any]]</code> <p>The extra_body to use for the tool select.</p> <code>None</code> <code>tool_choice</code> <code>Union[Literal['auto', 'required', 'none'], ChatCompletionNamedToolChoiceParam]</code> <p>Tool choice mode for tool calling. There are 4 choices that are supported: - <code>auto</code> means the model can pick between generating a message or calling one or more tools. To enable this feature, you should set the tags <code>--enable-auto-tool-choice</code> and <code>--tool-call-parser</code>  when starting the vLLM server. - <code>required</code> means the model must generate one or more tool calls based on the specified tool list  in the <code>tools</code> parameter. The number of tool calls depends on the user's query. - <code>none</code> means the model will not call any tool and instead generates a message. When tools are  specified in the request, vLLM includes tool definitions in the prompt by default, regardless  of the tool_choice setting. To exclude tool definitions when tool_choice='none', use the  <code>--exclude-tools-when-tool-choice-none</code> option when starting the vLLM server. - You can also specify a particular function using named function calling by setting <code>tool_choice</code>  parameter to a json object, like <code>tool_choice={\"type\": \"function\", \"function\": {\"name\": \"get_weather\"}}</code>.</p> <code>'auto'</code> <code>**kwargs</code> <p>The kwargs to use for the tool select.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Tuple[List[ToolCall], Optional[str]]</code> <p>A list that contains the selected tools and their arguments.</p> Notes <p>See more on Tool Calling.</p> Source code in <code>bridgic/llms/vllm/_vllm_server_llm.py</code> <pre><code>async def aselect_tool(\n    self,\n    messages: List[Message],\n    tools: List[Tool],\n    model: Optional[str] = None,\n    temperature: Optional[float] = None,\n    top_p: Optional[float] = None,\n    presence_penalty: Optional[float] = None,\n    frequency_penalty: Optional[float] = None,\n    extra_body: Optional[Dict[str, Any]] = None,\n    tool_choice: Union[Literal[\"auto\", \"required\", \"none\"], ChatCompletionNamedToolChoiceParam] = \"auto\",\n    **kwargs,\n) -&gt; Tuple[List[ToolCall], Optional[str]]:\n    \"\"\"\n    Select tools from a specified list of tools.\n\n    Parameters\n    ----------\n    messages: List[Message]\n        The messages to send to the LLM.\n    tools: List[Tool]\n        The tools to use for the tool select.\n    model: Optional[str]\n        The model to use for the tool select.\n    temperature: Optional[float]\n        The temperature to use for the tool select.\n    top_p: Optional[float]\n        The top_p to use for the tool select.\n    presence_penalty: Optional[float]\n        The presence_penalty to use for the tool select.\n    frequency_penalty: Optional[float]\n        The frequency_penalty to use for the tool select.\n    extra_body: Optional[Dict[str, Any]]\n        The extra_body to use for the tool select.\n    tool_choice : Union[Literal[\"auto\", \"required\", \"none\"], ChatCompletionNamedToolChoiceParam]\n        Tool choice mode for tool calling. There are 4 choices that are supported:\n        - `auto` means the model can pick between generating a message or calling one or more tools.\n        To enable this feature, you should set the tags `--enable-auto-tool-choice` and `--tool-call-parser` \n        when starting the vLLM server.\n        - `required` means the model must generate one or more tool calls based on the specified tool list \n        in the `tools` parameter. The number of tool calls depends on the user's query.\n        - `none` means the model will not call any tool and instead generates a message. When tools are \n        specified in the request, vLLM includes tool definitions in the prompt by default, regardless \n        of the tool_choice setting. To exclude tool definitions when tool_choice='none', use the \n        `--exclude-tools-when-tool-choice-none` option when starting the vLLM server.\n        - You can also specify a particular function using named function calling by setting `tool_choice` \n        parameter to a json object, like `tool_choice={\"type\": \"function\", \"function\": {\"name\": \"get_weather\"}}`.\n\n    **kwargs: Any\n        The kwargs to use for the tool select.\n\n    Returns\n    -------\n    Tuple[List[ToolCall], Optional[str]]\n        A list that contains the selected tools and their arguments.\n\n    Notes\n    -----\n    See more on [Tool Calling](https://docs.vllm.ai/en/stable/features/tool_calling.html).\n    \"\"\"\n    # Build parameters dictionary for validation\n    params = filter_dict(merge_dict(self.configuration.model_dump(), {\n        \"model\": model,\n        \"temperature\": temperature,\n        \"top_p\": top_p,\n        \"presence_penalty\": presence_penalty,\n        \"frequency_penalty\": frequency_penalty,\n        \"extra_body\": extra_body,\n        **kwargs,\n    }))\n\n    # Validate required parameters for tool selection\n    validate_required_params(params, [\"model\"])\n\n    input_messages = [self._convert_message(message=msg, strict=True) for msg in messages]\n    input_tools = [\n        {\n            \"type\": \"function\",\n            \"function\": {\n                \"name\": tool.name,\n                \"description\": tool.description,\n                \"parameters\": tool.parameters,\n            },\n        } for tool in tools\n    ]\n\n    response = self.client.chat.completions.create(\n        model=model,\n        messages=input_messages,\n        tools=input_tools,\n        tool_choice=tool_choice,\n        **kwargs,\n    )\n    tool_calls = response.choices[0].message.tool_calls\n\n    output_content = \"\"\n    if response.choices[0].message.content:\n        output_content = response.choices[0].message.content\n\n    output_tool_calls = []\n    if tool_calls:\n        output_tool_calls = self._convert_tool_calls(tool_calls)\n\n    return (output_tool_calls, output_content)\n</code></pre>"},{"location":"reference/bridgic-llms-vllm/bridgic/llms/vllm/#bridgic.llms.vllm.VllmServerConfiguration","title":"VllmServerConfiguration","text":"<p>               Bases: <code>OpenAILikeConfiguration</code></p> <p>Configuration for the vLLM server.</p> Source code in <code>bridgic/llms/vllm/_vllm_server_llm.py</code> <pre><code>class VllmServerConfiguration(OpenAILikeConfiguration):\n    \"\"\"\n    Configuration for the vLLM server.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/bridgic-traces-langwatch/bridgic/traces/langwatch/","title":"langwatch","text":""},{"location":"reference/bridgic-traces-langwatch/bridgic/traces/langwatch/#bridgic.traces.langwatch.LangWatchTraceCallback","title":"LangWatchTraceCallback","text":"<p>               Bases: <code>WorkerCallback</code></p> <p>LangWatch tracing callback handler for Bridgic.</p> <p>This callback handler integrates LangWatch tracing with Bridgic framework, providing step-level tracing for worker execution and automa orchestration. It tracks worker execution, creates spans for each worker, and manages trace lifecycle for top-level automa instances.</p> <p>Parameters:</p> Name Type Description Default <code>api_key</code> <code>Optional[str]</code> <p>The API key for the LangWatch tracing service, if none is provided,  the <code>LANGWATCH_API_KEY</code> environment variable will be used.</p> <code>None</code> <code>endpoint_url</code> <code>Optional[str]</code> <p>The URL of the LangWatch tracing service, if none is provided,  the <code>LANGWATCH_ENDPOINT</code> environment variable will be used. If that is not provided, the default value will be https://app.langwatch.ai.</p> <code>None</code> <code>base_attributes</code> <code>Optional[BaseAttributes]</code> <p>The base attributes to use for the LangWatch tracing client.</p> <code>None</code> Notes <p>Since tracing requires the execution within an automa to establish the corresponding record root, only global configurations (via <code>GlobalSetting</code>) and automa-level configurations (via <code>RunningOptions</code>) will take effect.  In other words, if you set the callback by using <code>@worker</code> or <code>add_worker</code>, it will not work.</p> Source code in <code>bridgic/traces/langwatch/_langwatch_trace_callback.py</code> <pre><code>class LangWatchTraceCallback(WorkerCallback):\n    \"\"\"\n    LangWatch tracing callback handler for Bridgic.\n\n    This callback handler integrates LangWatch tracing with Bridgic framework,\n    providing step-level tracing for worker execution and automa orchestration.\n    It tracks worker execution, creates spans for each worker, and manages\n    trace lifecycle for top-level automa instances.\n\n    Parameters\n    ----------\n    api_key : Optional[str], default=None\n        The API key for the LangWatch tracing service, if none is provided, \n        the `LANGWATCH_API_KEY` environment variable will be used.\n    endpoint_url : Optional[str], default=None\n        The URL of the LangWatch tracing service, if none is provided, \n        the `LANGWATCH_ENDPOINT` environment variable will be used. If that is not provided, the default value will be https://app.langwatch.ai.\n    base_attributes : Optional[BaseAttributes], default=None\n        The base attributes to use for the LangWatch tracing client.\n\n    Notes\n    ------\n    Since tracing requires the execution within an automa to establish the corresponding record root,\n    only global configurations (via `GlobalSetting`) and automa-level configurations (via `RunningOptions`) will take effect. \n    In other words, if you set the callback by using `@worker` or `add_worker`, it will not work.\n    \"\"\"\n\n    _api_key: Optional[str]\n    _endpoint_url: Optional[str]\n    _base_attributes: BaseAttributes\n    _is_ready: bool\n    _current_trace: ContextVar[Optional[LangWatchTrace]]\n    _current_span_stack: ContextVar[Tuple[LangWatchSpan, ...]]\n\n    def __init__(\n        self,\n        api_key: Optional[str] = None,\n        endpoint_url: Optional[str] = None,\n        base_attributes: Optional[BaseAttributes] = None,\n    ):\n        super().__init__()\n        self._is_ready = False\n        self._api_key = api_key\n        self._endpoint_url = endpoint_url\n        self._base_attributes = base_attributes\n        self._current_trace = ContextVar(\n            \"langwatch_current_trace\", default=None\n        )\n        self._current_span_stack = ContextVar(\n            \"langwatch_current_span_stack\", default=()\n        )\n        self._setup_langwatch()\n\n    def _setup_langwatch(self) -&gt; None:\n        \"\"\"\n        Initialize LangWatch and mark the callback as ready if configuration succeeds.\n        \"\"\"\n        try:\n            if get_instance() is None or self._api_key != get_instance().api_key:\n                langwatch.setup(api_key=self._api_key, endpoint_url=self._endpoint_url, base_attributes=self._base_attributes)\n        except Exception as exc:\n            self._is_ready = False\n            warnings.warn(f\"LangWatch setup failed, callback disabled: {exc}\")\n        else:\n            self._is_ready = True\n\n    def _stringify_value(self, value: Any) -&gt; str:\n        \"\"\"Serialize a value into a JSON string, falling back to str() when needed.\"\"\"\n        try:\n            return json.dumps(value, default=str)\n        except TypeError:\n            return str(value)\n\n    def _normalize_attribute_value(self, value: Any) -&gt; Any:\n        \"\"\"\n        Normalize arbitrary attribute values into LangWatch-safe primitives.\n\n        Attempts serialization through `serialize_data` and gracefully falls back to\n        JSON strings when complex structures remain.\n        \"\"\"\n        primitive_types = (str, bool, int, float, bytes)\n\n        if isinstance(value, primitive_types) or value is None:\n            return value\n\n        if isinstance(value, Sequence) and not isinstance(\n            value, (str, bytes, bytearray)\n        ):\n            normalized = []\n            for item in value:\n                normalized.append(self._normalize_attribute_value(item))\n            return normalized\n\n        serialized = serialize_data(value)\n        if isinstance(serialized, primitive_types) or serialized is None:\n            return serialized\n        if isinstance(serialized, Sequence) and not isinstance(\n            serialized, (str, bytes, bytearray)\n        ):\n            normalized = []\n            for item in serialized:\n                normalized.append(self._normalize_attribute_value(item))\n            return normalized\n\n        return self._stringify_value(serialized)\n\n    async def _complete_trace(\n        self,\n        output: Dict[str, Any],\n        error: Optional[Exception] = None,\n    ) -&gt; None:\n        \"\"\"Finalize the active trace context with output and optional error.\"\"\"\n        trace_data = self._current_trace.get()\n        if trace_data is None:\n            return\n        self._current_trace.set(None)\n        try:\n            trace_data.update(output=output, error=error)\n        finally:\n            await trace_data.__aexit__(\n                type(error) if error else None,\n                error,\n                error.__traceback__ if error else None,\n            )\n\n    async def _finish_worker_span(\n        self,\n        output: Dict[str, Any],\n        error: Optional[Exception] = None,\n    ) -&gt; None:\n        \"\"\"Finalize the current worker span and propagate outputs/errors.\"\"\"\n        stack = self._current_span_stack.get()\n        if not stack:\n            warnings.warn(\n                \"No active LangWatch span context found when finishing worker span\"\n            )\n            return\n        span_data = stack[-1]\n        self._current_span_stack.set(stack[:-1])\n\n        try:\n            span_data.update(output=output, error=error)\n        except Exception:\n            warnings.warn(\"Failed to update LangWatch span when finishing worker span\")\n        await span_data.__aexit__(\n            type(error) if error else None,\n            error,\n            error.__traceback__ if error else None,\n        )\n\n    def _build_output_payload(\n        self,\n        result: Any = None,\n        error: Optional[Exception] = None,\n    ) -&gt; Dict[str, Any]:\n        \"\"\"Create a normalized payload for either successful results or errors.\"\"\"\n        if error:\n            return {\"error_type\": type(error).__name__, \"error_message\": str(error)}\n        return {\n            \"result_type\": type(result).__name__ if result is not None else None,\n            \"result\": serialize_data(result),\n        }\n\n    async def _start_worker_span(\n        self,\n        key: str,\n        worker: \"Worker\",\n        parent: \"Automa\",\n        arguments: Optional[Dict[str, Any]],\n    ) -&gt; None:\n        \"\"\"\n        Start a LangWatch span for the worker execution using normalized metadata.\n        \"\"\"\n        step_name = get_worker_tracing_step_name(key, worker)\n        worker_tracing_dict = build_worker_tracing_dict(worker, parent)\n        normalized_worker_tracing = {\n            key: self._normalize_attribute_value(value)\n            for key, value in worker_tracing_dict.items()\n        }\n        serialized_args = serialize_data(arguments)\n\n        # LangWatch refers to span metadata as \"attributes\".\n        span = langwatch.span(\n            name=step_name,\n            input=serialized_args,\n            type=\"span\",\n            attributes={\n                **normalized_worker_tracing,\n                # TODO: Investigate why LangWatch coerces integers into dict form; keep string for now.\n                \"nesting_level\": str(worker_tracing_dict[\"nesting_level\"]),\n            },\n        )\n        await span.__aenter__()\n        stack = self._current_span_stack.get()\n        self._current_span_stack.set((*stack, span))\n\n    async def _start_top_level_trace(self, key: str, arguments: Optional[Dict[str, Any]]) -&gt; None:\n        serialized_args = serialize_data(arguments)\n        trace_metadata = {\n            \"created_from\": \"bridgic\", \n            \"key\": key, \n            \"nesting_level\": \"0\",\n        }\n        trace_data = langwatch.trace(\n            name=key or \"top_level_automa\",\n            input=serialized_args,\n            metadata=trace_metadata,\n            type=\"span\",\n        )\n        await trace_data.__aenter__()\n        self._current_trace.set(trace_data)\n\n    def _get_worker_instance(self, key: str, parent: Optional[\"Automa\"]) -&gt; Worker:\n        \"\"\"\n        Get worker instance from parent automa.\n\n        Returns\n        -------\n        Worker\n            The worker instance.\n        \"\"\"\n        if parent is None:\n            raise ValueError(\"Parent automa is required to get worker instance\")\n        return parent._get_worker_instance(key)\n\n    async def on_worker_start(\n        self,\n        key: str,\n        is_top_level: bool = False,\n        parent: \"Automa\" = None,\n        arguments: Optional[Dict[str, Any]] = None,\n    ) -&gt; None:\n        \"\"\"\n        Hook invoked before worker execution.\n\n        For top-level automa, initializes a new trace. For workers, creates\n        a new span. Handles nested automa as workers by checking if the\n        decorated worker is an automa instance.\n\n        Parameters\n        ----------\n        key : str\n            Worker identifier.\n        is_top_level : bool, default=False\n            Whether the worker is the top-level automa. When True, parent will be the automa itself (parent is self).\n        parent : Optional[Automa], default=None\n            Parent automa instance containing this worker. For top-level automa, parent is the automa itself.\n        arguments : Optional[Dict[str, Any]], default=None\n            Execution arguments with keys \"args\" and \"kwargs\".\n        \"\"\"\n        if not self._is_ready:\n            return\n\n        if is_top_level:\n            await self._start_top_level_trace(key, arguments)\n            return\n\n        try:\n            worker = self._get_worker_instance(key, parent)\n        except (KeyError, ValueError) as e:\n            warnings.warn(f\"Failed to get worker instance for key '{key}': {e}\")\n            return\n\n        await self._start_worker_span(key, worker, parent, arguments)\n\n    async def _complete_worker_execution(\n        self,\n        output: Dict[str, Any],\n        is_top_level: bool,\n        error: Optional[Exception] = None,\n    ) -&gt; None:\n        if is_top_level:\n            await self._complete_trace(output, error)\n        else:\n            await self._finish_worker_span(output, error)\n\n    async def on_worker_end(\n        self,\n        key: str,\n        is_top_level: bool = False,\n        parent: Optional[\"Automa\"] = None,\n        arguments: Optional[Dict[str, Any]] = None,\n        result: Any = None,\n    ) -&gt; None:\n        \"\"\"\n        Hook invoked after worker execution.\n\n        For top-level automa, ends the trace. For workers, ends the span\n        with execution results.\n\n        Parameters\n        ----------\n        key : str\n            Worker identifier.\n        is_top_level : bool, default=False\n            Whether the worker is the top-level automa. When True, parent will be the automa itself (parent is self).\n        parent : Optional[Automa], default=None\n            Parent automa instance containing this worker. For top-level automa, parent is the automa itself.\n        arguments : Optional[Dict[str, Any]], default=None\n            Execution arguments with keys \"args\" and \"kwargs\".\n        result : Any, default=None\n            Worker execution result.\n        \"\"\"\n        if not self._is_ready:\n            return\n        output = self._build_output_payload(result=result)\n        await self._complete_worker_execution(output, is_top_level)\n\n    async def on_worker_error(\n        self,\n        key: str,\n        is_top_level: bool = False,\n        parent: Optional[\"Automa\"] = None,\n        arguments: Optional[Dict[str, Any]] = None,\n        error: Exception = None,\n    ) -&gt; bool:\n        \"\"\"\n        Hook invoked when worker execution raises an exception.\n\n        For top-level automa, ends the trace with error information.\n        For workers, ends the span with error information.\n\n        Parameters\n        ----------\n        key : str\n            Worker identifier.\n        is_top_level : bool, default=False\n            Whether the worker is the top-level automa. When True, parent will be the automa itself (parent is self).\n        parent : Optional[Automa], default=None\n            Parent automa instance containing this worker. For top-level automa, parent is the automa itself.\n        arguments : Optional[Dict[str, Any]], default=None\n            Execution arguments with keys \"args\" and \"kwargs\".\n        error : Exception, default=None\n            The exception raised during worker execution.\n\n        Returns\n        -------\n        bool\n            Always returns False, indicating the exception should not be suppressed.\n        \"\"\"\n        if not self._is_ready:\n            return False\n        output = self._build_output_payload(error=error)\n        await self._complete_worker_execution(output, is_top_level, error=error)\n        return False\n\n    def dump_to_dict(self) -&gt; Dict[str, Any]:\n        state_dict = super().dump_to_dict()\n        state_dict[\"api_key\"] = self._api_key\n        state_dict[\"endpoint_url\"] = self._endpoint_url\n        state_dict[\"base_attributes\"] = self._base_attributes\n        return state_dict\n\n    def load_from_dict(self, state_dict: Dict[str, Any]) -&gt; None:\n        super().load_from_dict(state_dict)\n        self._api_key = state_dict.get(\"api_key\")\n        self._endpoint_url = state_dict.get(\"endpoint_url\")\n        self._base_attributes = state_dict.get(\"base_attributes\")\n        self._current_trace = ContextVar(\n            \"langwatch_current_trace\", default=None\n        )\n        self._current_span_stack = ContextVar(\n            \"langwatch_current_span_stack\", default=()\n        )\n        self._setup_langwatch()\n</code></pre>"},{"location":"reference/bridgic-traces-langwatch/bridgic/traces/langwatch/#bridgic.traces.langwatch.LangWatchTraceCallback.on_worker_start","title":"on_worker_start","text":"<code>async</code> <pre><code>on_worker_start(\n    key: str,\n    is_top_level: bool = False,\n    parent: Automa = None,\n    arguments: Optional[Dict[str, Any]] = None,\n) -&gt; None\n</code></pre> <p>Hook invoked before worker execution.</p> <p>For top-level automa, initializes a new trace. For workers, creates a new span. Handles nested automa as workers by checking if the decorated worker is an automa instance.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>Worker identifier.</p> required <code>is_top_level</code> <code>bool</code> <p>Whether the worker is the top-level automa. When True, parent will be the automa itself (parent is self).</p> <code>False</code> <code>parent</code> <code>Optional[Automa]</code> <p>Parent automa instance containing this worker. For top-level automa, parent is the automa itself.</p> <code>None</code> <code>arguments</code> <code>Optional[Dict[str, Any]]</code> <p>Execution arguments with keys \"args\" and \"kwargs\".</p> <code>None</code> Source code in <code>bridgic/traces/langwatch/_langwatch_trace_callback.py</code> <pre><code>async def on_worker_start(\n    self,\n    key: str,\n    is_top_level: bool = False,\n    parent: \"Automa\" = None,\n    arguments: Optional[Dict[str, Any]] = None,\n) -&gt; None:\n    \"\"\"\n    Hook invoked before worker execution.\n\n    For top-level automa, initializes a new trace. For workers, creates\n    a new span. Handles nested automa as workers by checking if the\n    decorated worker is an automa instance.\n\n    Parameters\n    ----------\n    key : str\n        Worker identifier.\n    is_top_level : bool, default=False\n        Whether the worker is the top-level automa. When True, parent will be the automa itself (parent is self).\n    parent : Optional[Automa], default=None\n        Parent automa instance containing this worker. For top-level automa, parent is the automa itself.\n    arguments : Optional[Dict[str, Any]], default=None\n        Execution arguments with keys \"args\" and \"kwargs\".\n    \"\"\"\n    if not self._is_ready:\n        return\n\n    if is_top_level:\n        await self._start_top_level_trace(key, arguments)\n        return\n\n    try:\n        worker = self._get_worker_instance(key, parent)\n    except (KeyError, ValueError) as e:\n        warnings.warn(f\"Failed to get worker instance for key '{key}': {e}\")\n        return\n\n    await self._start_worker_span(key, worker, parent, arguments)\n</code></pre>"},{"location":"reference/bridgic-traces-langwatch/bridgic/traces/langwatch/#bridgic.traces.langwatch.LangWatchTraceCallback.on_worker_end","title":"on_worker_end","text":"<code>async</code> <pre><code>on_worker_end(\n    key: str,\n    is_top_level: bool = False,\n    parent: Optional[Automa] = None,\n    arguments: Optional[Dict[str, Any]] = None,\n    result: Any = None,\n) -&gt; None\n</code></pre> <p>Hook invoked after worker execution.</p> <p>For top-level automa, ends the trace. For workers, ends the span with execution results.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>Worker identifier.</p> required <code>is_top_level</code> <code>bool</code> <p>Whether the worker is the top-level automa. When True, parent will be the automa itself (parent is self).</p> <code>False</code> <code>parent</code> <code>Optional[Automa]</code> <p>Parent automa instance containing this worker. For top-level automa, parent is the automa itself.</p> <code>None</code> <code>arguments</code> <code>Optional[Dict[str, Any]]</code> <p>Execution arguments with keys \"args\" and \"kwargs\".</p> <code>None</code> <code>result</code> <code>Any</code> <p>Worker execution result.</p> <code>None</code> Source code in <code>bridgic/traces/langwatch/_langwatch_trace_callback.py</code> <pre><code>async def on_worker_end(\n    self,\n    key: str,\n    is_top_level: bool = False,\n    parent: Optional[\"Automa\"] = None,\n    arguments: Optional[Dict[str, Any]] = None,\n    result: Any = None,\n) -&gt; None:\n    \"\"\"\n    Hook invoked after worker execution.\n\n    For top-level automa, ends the trace. For workers, ends the span\n    with execution results.\n\n    Parameters\n    ----------\n    key : str\n        Worker identifier.\n    is_top_level : bool, default=False\n        Whether the worker is the top-level automa. When True, parent will be the automa itself (parent is self).\n    parent : Optional[Automa], default=None\n        Parent automa instance containing this worker. For top-level automa, parent is the automa itself.\n    arguments : Optional[Dict[str, Any]], default=None\n        Execution arguments with keys \"args\" and \"kwargs\".\n    result : Any, default=None\n        Worker execution result.\n    \"\"\"\n    if not self._is_ready:\n        return\n    output = self._build_output_payload(result=result)\n    await self._complete_worker_execution(output, is_top_level)\n</code></pre>"},{"location":"reference/bridgic-traces-langwatch/bridgic/traces/langwatch/#bridgic.traces.langwatch.LangWatchTraceCallback.on_worker_error","title":"on_worker_error","text":"<code>async</code> <pre><code>on_worker_error(\n    key: str,\n    is_top_level: bool = False,\n    parent: Optional[Automa] = None,\n    arguments: Optional[Dict[str, Any]] = None,\n    error: Exception = None,\n) -&gt; bool\n</code></pre> <p>Hook invoked when worker execution raises an exception.</p> <p>For top-level automa, ends the trace with error information. For workers, ends the span with error information.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>Worker identifier.</p> required <code>is_top_level</code> <code>bool</code> <p>Whether the worker is the top-level automa. When True, parent will be the automa itself (parent is self).</p> <code>False</code> <code>parent</code> <code>Optional[Automa]</code> <p>Parent automa instance containing this worker. For top-level automa, parent is the automa itself.</p> <code>None</code> <code>arguments</code> <code>Optional[Dict[str, Any]]</code> <p>Execution arguments with keys \"args\" and \"kwargs\".</p> <code>None</code> <code>error</code> <code>Exception</code> <p>The exception raised during worker execution.</p> <code>None</code> <p>Returns:</p> Type Description <code>bool</code> <p>Always returns False, indicating the exception should not be suppressed.</p> Source code in <code>bridgic/traces/langwatch/_langwatch_trace_callback.py</code> <pre><code>async def on_worker_error(\n    self,\n    key: str,\n    is_top_level: bool = False,\n    parent: Optional[\"Automa\"] = None,\n    arguments: Optional[Dict[str, Any]] = None,\n    error: Exception = None,\n) -&gt; bool:\n    \"\"\"\n    Hook invoked when worker execution raises an exception.\n\n    For top-level automa, ends the trace with error information.\n    For workers, ends the span with error information.\n\n    Parameters\n    ----------\n    key : str\n        Worker identifier.\n    is_top_level : bool, default=False\n        Whether the worker is the top-level automa. When True, parent will be the automa itself (parent is self).\n    parent : Optional[Automa], default=None\n        Parent automa instance containing this worker. For top-level automa, parent is the automa itself.\n    arguments : Optional[Dict[str, Any]], default=None\n        Execution arguments with keys \"args\" and \"kwargs\".\n    error : Exception, default=None\n        The exception raised during worker execution.\n\n    Returns\n    -------\n    bool\n        Always returns False, indicating the exception should not be suppressed.\n    \"\"\"\n    if not self._is_ready:\n        return False\n    output = self._build_output_payload(error=error)\n    await self._complete_worker_execution(output, is_top_level, error=error)\n    return False\n</code></pre>"},{"location":"reference/bridgic-traces-langwatch/bridgic/traces/langwatch/#bridgic.traces.langwatch.start_langwatch_trace","title":"start_langwatch_trace","text":"<pre><code>start_langwatch_trace(\n    api_key: Optional[str] = None,\n    endpoint_url: Optional[str] = None,\n    base_attributes: Optional[BaseAttributes] = None,\n) -&gt; None\n</code></pre> <p>Start a LangWatch trace for a given project and service.</p> <p>Parameters:</p> Name Type Description Default <code>api_key</code> <code>Optional[str]</code> <p>The API key for the LangWatch tracing service, if none is provided, the <code>LANGWATCH_API_KEY</code> environment variable will be used.</p> <code>None</code> <code>endpoint_url</code> <code>Optional[str]</code> <p>The URL of the LangWatch tracing service, if none is provided, the <code>LANGWATCH_ENDPOINT</code> environment variable will be used. If that is not provided, the default value will be <code>https://app.langwatch.ai</code>.</p> <code>None</code> <code>base_attributes</code> <code>Optional[BaseAttributes]</code> <p>The base attributes to use for the LangWatch tracing client.</p> <code>None</code> <p>Returns:</p> Type Description <code>None</code> Source code in <code>bridgic/traces/langwatch/_utils.py</code> <pre><code>def start_langwatch_trace(\n    api_key: Optional[str] = None,\n    endpoint_url: Optional[str] = None,\n    base_attributes: Optional[BaseAttributes] = None,\n) -&gt; None:\n    \"\"\"Start a LangWatch trace for a given project and service.\n\n    Parameters\n    ----------\n    api_key : Optional[str], default=None\n        The API key for the LangWatch tracing service, if none is provided, the `LANGWATCH_API_KEY` environment variable will be used.\n    endpoint_url : Optional[str], default=None\n        The URL of the LangWatch tracing service, if none is provided, the `LANGWATCH_ENDPOINT` environment variable will be used. If that is not provided, the default value will be `https://app.langwatch.ai`.\n    base_attributes : Optional[BaseAttributes], default=None\n        The base attributes to use for the LangWatch tracing client.\n\n    Returns\n    -------\n    None\n    \"\"\"\n    from bridgic.core.config import GlobalSetting\n    builder = WorkerCallbackBuilder(\n        LangWatchTraceCallback, \n        init_kwargs={\"api_key\": api_key, \"endpoint_url\": endpoint_url, \"base_attributes\": base_attributes}\n    )\n    GlobalSetting.add(callback_builder=builder)\n</code></pre>"},{"location":"reference/bridgic-traces-opik/bridgic/traces/opik/","title":"opik","text":""},{"location":"reference/bridgic-traces-opik/bridgic/traces/opik/#bridgic.traces.opik.OpikTraceCallback","title":"OpikTraceCallback","text":"<p>               Bases: <code>WorkerCallback</code></p> <p>Opik tracing callback handler for Bridgic.</p> <p>This callback handler integrates Opik tracing with Bridgic framework, providing step-level tracing for worker execution and automa orchestration. It tracks worker execution, creates spans for each worker, and manages trace lifecycle for top-level automa instances.</p> <p>Parameters:</p> Name Type Description Default <code>project_name</code> <code>Optional[str]</code> <p>The name of the project. If None, uses <code>Default Project</code> project name.</p> <code>None</code> <code>workspace</code> <code>Optional[str]</code> <p>The name of the workspace. If None, uses <code>default</code> workspace name.</p> <code>None</code> <code>host</code> <code>Optional[str]</code> <p>The host URL for the Opik server. If None, it will default to <code>https://www.comet.com/opik/api</code>.</p> <code>None</code> <code>api_key</code> <code>Optional[str]</code> <p>The API key for Opik. This parameter is ignored for local installations.</p> <code>None</code> <code>use_local</code> <code>bool</code> <p>Whether to use local Opik server.</p> <code>False</code> Notes <p>Since tracing requires the execution within an automa to establish the corresponding record root, only global configurations (via <code>GlobalSetting</code>) and automa-level configurations (via <code>RunningOptions</code>) will take effect.  In other words, if you set the callback by using <code>@worker</code> or <code>add_worker</code>, it will not work.</p> <p>Examples:</p> <p>If you want to report tracking information to the self-hosted Opik service, you can initialize the callback instance like this: <pre><code>OpikTraceCallback(project_name=\"my-project\", use_local=True)\n</code></pre></p> <p>If you want to report tracking information to the Opik Cloud service, you can initialize the callback instance like this: <pre><code>OpikTraceCallback(project_name=\"my-project\", api_key=\"my-api-key\")\n</code></pre></p> Source code in <code>bridgic/traces/opik/_opik_trace_callback.py</code> <pre><code>class OpikTraceCallback(WorkerCallback):\n    \"\"\"\n    Opik tracing callback handler for Bridgic.\n\n    This callback handler integrates Opik tracing with Bridgic framework,\n    providing step-level tracing for worker execution and automa orchestration.\n    It tracks worker execution, creates spans for each worker, and manages\n    trace lifecycle for top-level automa instances.\n\n    Parameters\n    ----------\n    project_name : Optional[str], default=None\n        The name of the project. If None, uses `Default Project` project name.\n    workspace : Optional[str], default=None\n        The name of the workspace. If None, uses `default` workspace name.\n    host : Optional[str], default=None\n        The host URL for the Opik server. If None, it will default to `https://www.comet.com/opik/api`.\n    api_key : Optional[str], default=None\n        The API key for Opik. This parameter is ignored for local installations.\n    use_local : bool, default=False\n        Whether to use local Opik server.\n\n    Notes\n    ------\n    Since tracing requires the execution within an automa to establish the corresponding record root,\n    only global configurations (via `GlobalSetting`) and automa-level configurations (via `RunningOptions`) will take effect. \n    In other words, if you set the callback by using `@worker` or `add_worker`, it will not work.\n\n    Examples\n    ------\n    If you want to report tracking information to the self-hosted Opik service, you can initialize the callback instance like this:\n    ```python\n    OpikTraceCallback(project_name=\"my-project\", use_local=True)\n    ```\n\n    If you want to report tracking information to the Opik Cloud service, you can initialize the callback instance like this:\n    ```python\n    OpikTraceCallback(project_name=\"my-project\", api_key=\"my-api-key\")\n    ```\n    \"\"\"\n\n    _project_name: Optional[str]\n    _workspace: Optional[str]\n    _is_ready: bool\n    _api_key: Optional[str]\n    _host: Optional[str]\n    _use_local: bool\n    _opik_client: opik_client.Opik\n\n    def __init__(\n        self,\n        project_name: Optional[str] = None,\n        workspace: Optional[str] = None,\n        host: Optional[str] = None,\n        api_key: Optional[str] = None,\n        use_local: bool = False,\n    ):\n        super().__init__()\n        self._project_name = project_name\n        self._workspace = workspace\n        self._api_key = api_key\n        self._host = host\n        self._use_local = use_local\n        self._is_ready = False\n        self._setup_opik()\n\n    def _setup_opik(self) -&gt; None:\n        if self._use_local:\n            opik.configure(use_local=True)\n        self._opik_client = opik_client.Opik(_use_batching=True, project_name=self._project_name, workspace=self._workspace, api_key=self._api_key, host=self._host)\n        missing_configuration, _ = self._opik_client._config.get_misconfiguration_detection_results()\n        if missing_configuration:\n            self._is_ready = False # for serialization compatibility\n            return\n        self._check_opik_auth()\n\n    def _check_opik_auth(self) -&gt; None:\n        try:\n            self._opik_client.auth_check()\n        except Exception as e:\n            self._is_ready = False # for serialization compatibility\n            warnings.warn(f\"Opik auth check failed, OpikTracer will be disabled: {e}\")\n        else:\n            self._is_ready = True\n\n    def _get_worker_instance(self, key: str, parent: Optional[\"Automa\"]) -&gt; Worker:\n        \"\"\"\n        Get worker instance from parent automa.\n\n        Returns\n        -------\n        Worker\n            The worker instance.\n        \"\"\"\n        if parent is None:\n            raise ValueError(\"Parent automa is required to get worker instance\")\n        return parent._get_worker_instance(key)\n\n    def _create_trace_data(self, trace_name: Optional[str] = None) -&gt; trace.TraceData:\n        return trace.TraceData(\n            name=trace_name, \n            metadata={\"created_from\": \"bridgic\"}, \n            project_name=self._project_name\n        )\n\n    def _get_or_create_trace_data(self, trace_name: Optional[str] = None) -&gt; trace.TraceData:\n        \"\"\"Initialize or reuse existing trace.\"\"\"\n        existing_trace = opik_context_storage.get_trace_data()\n        if existing_trace:\n            return existing_trace\n\n        # Create new trace and set in context\n        trace_data = self._create_trace_data(trace_name)\n        opik_context_storage.set_trace_data(trace_data)\n\n        if self._opik_client.config.log_start_trace_span:\n            self._opik_client.trace(**trace_data.as_start_parameters)\n        return trace_data\n\n    def _complete_trace(self, output: Optional[Dict[str, Any]], error_info: Optional[ErrorInfoDict]) -&gt; None:\n        \"\"\"Finalize and log trace we own.\"\"\"\n        trace_data = opik_context_storage.get_trace_data()\n        if trace_data is None:\n            return\n\n        trace_data.init_end_time()\n\n        # Compute execution duration from trace start_time\n        if trace_data.start_time:\n            end_time = trace_data.end_time.timestamp() if trace_data.end_time else time.time()\n            start_time = trace_data.start_time.timestamp()\n            trace_data.metadata = merge_optional_dicts(\n                trace_data.metadata,\n                {\"execution_duration\": end_time - start_time, \"end_time\": end_time}\n            )\n\n        if output:\n            trace_data.update(output=output)\n\n        if error_info:\n            trace_data.update(error_info=error_info)\n\n        self._opik_client.trace(**trace_data.as_parameters)\n        opik_context_storage.pop_trace_data(ensure_id=trace_data.id)\n        self._flush()\n\n    def _start_span(\n        self,\n        step_name: str,\n        inputs: Optional[Dict[str, Any]] = None,\n        metadata: Optional[Dict[str, Any]] = None,\n    ) -&gt; None:\n        \"\"\"Start a span for a worker execution step and push it to context.\"\"\"\n        trace_data = opik_context_storage.get_trace_data()\n\n        parent_span = opik_context_storage.top_span_data()\n\n        project_name = helpers.resolve_child_span_project_name(\n            parent_project_name=trace_data.project_name,\n            child_project_name=self._project_name,\n            show_warning=True,\n        )\n\n        span_data = span.SpanData(\n            trace_id=trace_data.id,\n            name=step_name,\n            parent_span_id=parent_span.id if parent_span else None,\n            input=inputs,\n            metadata=metadata,\n            project_name=project_name,\n        )\n        # Store start_time in metadata for later duration calculation\n        if span_data.start_time and metadata is not None:\n            metadata[\"start_time\"] = span_data.start_time.timestamp()\n            span_data.update(metadata=metadata)\n        # Add span to context stack\n        opik_context_storage.add_span_data(span_data)\n\n        if self._opik_client.config.log_start_trace_span:\n            self._opik_client.span(**span_data.as_start_parameters)\n\n    def _finish_span(self, span_data: span.SpanData, worker_metadata: Optional[Dict[str, Any]] = None) -&gt; None:\n        \"\"\"Finish a worker span with metadata and output, then pop from context.\"\"\"\n        if worker_metadata:\n            output = worker_metadata.get(\"output\")\n            # Merge all metadata except 'output' into span metadata\n            current_metadata = span_data.metadata or {}\n            current_metadata.update({k: v for k, v in worker_metadata.items() if k != \"output\"})\n            span_data.update(metadata=current_metadata)\n\n            if output is not None:\n                span_data.update(output=output)\n\n        span_data.init_end_time()\n        self._opik_client.span(**span_data.as_parameters)\n\n        # Pop span from context stack\n        opik_context_storage.pop_span_data(ensure_id=span_data.id)\n\n    def _start_top_level_trace(self, key: str, arguments: Optional[Dict[str, Any]]) -&gt; None:\n        \"\"\"Start trace initialization for top-level automa.\"\"\"\n        trace_data = self._get_or_create_trace_data(trace_name=key or \"top_level_automa\")\n\n        serialized_args = serialize_data(arguments)\n        metadata_updates = {\"key\": key, \"nesting_level\": 0}\n        if trace_data.start_time:\n            metadata_updates[\"start_time\"] = trace_data.start_time.timestamp()\n\n        trace_data.metadata = merge_optional_dicts(trace_data.metadata, metadata_updates)\n\n        if serialized_args:\n            trace_data.input = serialized_args\n\n    def _start_worker_span(self, key: str, worker: Worker, parent: \"Automa\", arguments: Optional[Dict[str, Any]]) -&gt; None:\n        \"\"\"Start a span for worker execution.\"\"\"\n        step_name = get_worker_tracing_step_name(key, worker)\n        worker_tracing_dict = build_worker_tracing_dict(worker, parent)\n        self._start_span(\n            step_name=step_name,\n            inputs=serialize_data(arguments),\n            metadata=worker_tracing_dict,\n        )\n\n    async def on_worker_start(\n        self,\n        key: str,\n        is_top_level: bool = False,\n        parent: Optional[\"Automa\"] = None,\n        arguments: Optional[Dict[str, Any]] = None,\n    ) -&gt; None:\n        \"\"\"\n        Hook invoked before worker execution.\n\n        For top-level automa, initializes a new trace. For workers, creates\n        a new span. Handles nested automa as workers by checking if the\n        decorated worker is an automa instance.\n\n        Parameters\n        ----------\n        key : str\n            Worker identifier.\n        is_top_level : bool, default=False\n            Whether the worker is the top-level automa. When True, parent will be the automa itself (parent is self).\n        parent : Optional[Automa], default=None\n            Parent automa instance containing this worker. For top-level automa, parent is the automa itself.\n        arguments : Optional[Dict[str, Any]], default=None\n            Execution arguments with keys \"args\" and \"kwargs\".\n        \"\"\"\n        if not self._is_ready:\n            return\n        if is_top_level:\n            self._start_top_level_trace(key, arguments)\n            return\n\n        try:\n            worker = self._get_worker_instance(key, parent)\n        except (KeyError, ValueError) as e:\n            warnings.warn(f\"Failed to get worker instance for key '{key}': {e}\")\n            return\n\n        self._start_worker_span(key, worker, parent, arguments)\n\n    def _finish_current_span(self, output: Dict[str, Any], error: Optional[Exception] = None) -&gt; None:\n        \"\"\"Finish the current span and pop it from context.\"\"\"\n        current_span = opik_context_storage.top_span_data()\n        if not current_span:\n            warnings.warn(\"No span found in context when finishing worker span\")\n            return\n\n        # Calculate execution timing\n        end_time = time.time()\n        start_time = current_span.start_time.timestamp() if current_span.start_time else end_time\n\n        # Build worker metadata with timing and output\n        worker_metadata = {\n            \"end_time\": end_time,\n            \"execution_duration\": end_time - start_time,\n            \"output\": serialize_data(output),\n        }\n\n        # Handle error if present\n        if error:\n            error_info = error_info_collector.collect(error)\n            if error_info:\n                current_span.update(error_info=error_info)\n\n        # Finish the span (this will merge metadata and pop from context)\n        self._finish_span(current_span, worker_metadata=worker_metadata)\n\n    def _build_output_payload(self, result: Any = None, error: Optional[Exception] = None) -&gt; Dict[str, Any]:\n        \"\"\"Build a standardized output dictionary for results or errors.\"\"\"\n        if error:\n            return {\"error_type\": type(error).__name__, \"error_message\": str(error)}\n        return {\n            \"result_type\": type(result).__name__ if result is not None else None,\n            \"result\": serialize_data(result),\n        }\n\n    def _complete_worker_execution(self, output: Dict[str, Any], is_top_level: bool, error: Optional[Exception] = None) -&gt; None:\n        \"\"\"Complete worker or trace execution.\"\"\"\n        if is_top_level:\n            trace_data = opik_context_storage.get_trace_data()\n            if trace_data:\n                execution_status = \"failed\" if error else \"completed\"\n                trace_data.metadata = merge_optional_dicts(\n                    trace_data.metadata, {\"execution_status\": execution_status}\n                )\n\n            error_info = error_info_collector.collect(error) if error else None\n            self._complete_trace(output, error_info)\n        else:\n            self._finish_current_span(output=output, error=error)\n\n    async def on_worker_end(\n        self,\n        key: str,\n        is_top_level: bool = False,\n        parent: Optional[\"Automa\"] = None,\n        arguments: Optional[Dict[str, Any]] = None,\n        result: Any = None,\n    ) -&gt; None:\n        \"\"\"\n        Hook invoked after worker execution.\n\n        For top-level automa, ends the trace. For workers, ends the span\n        with execution results.\n\n        Parameters\n        ----------\n        key : str\n            Worker identifier.\n        is_top_level : bool, default=False\n            Whether the worker is the top-level automa. When True, parent will be the automa itself (parent is self).\n        parent : Optional[Automa], default=None\n            Parent automa instance containing this worker. For top-level automa, parent is the automa itself.\n        arguments : Optional[Dict[str, Any]], default=None\n            Execution arguments with keys \"args\" and \"kwargs\".\n        result : Any, default=None\n            Worker execution result.\n        \"\"\"\n        if not self._is_ready:\n            return\n        output = self._build_output_payload(result=result)\n        self._complete_worker_execution(output, is_top_level)\n\n    async def on_worker_error(\n        self,\n        key: str,\n        is_top_level: bool = False,\n        parent: Optional[\"Automa\"] = None,\n        arguments: Optional[Dict[str, Any]] = None,\n        error: Exception = None,\n    ) -&gt; bool:\n        \"\"\"\n        Hook invoked when worker execution raises an exception.\n\n        For top-level automa, ends the trace with error information.\n        For workers, ends the span with error information.\n\n        Parameters\n        ----------\n        key : str\n            Worker identifier.\n        is_top_level : bool, default=False\n            Whether the worker is the top-level automa. When True, parent will be the automa itself (parent is self).\n        parent : Optional[Automa], default=None\n            Parent automa instance containing this worker. For top-level automa, parent is the automa itself.\n        arguments : Optional[Dict[str, Any]], default=None\n            Execution arguments with keys \"args\" and \"kwargs\".\n        error : Exception, default=None\n            The exception raised during worker execution.\n\n        Returns\n        -------\n        bool\n            Always returns False, indicating the exception should not be suppressed.\n        \"\"\"\n        if not self._is_ready:\n            return False\n        if not is_top_level and parent:\n            try:\n                self._get_worker_instance(key, parent)\n            except (KeyError, ValueError) as e:\n                warnings.warn(f\"Failed to get worker instance for key '{key}': {e}\")\n                return False\n\n        output = self._build_output_payload(error=error)\n        self._complete_worker_execution(output, is_top_level, error=error)\n        return False\n\n    def _flush(self) -&gt; None:\n        self._opik_client.flush()\n\n    @override\n    def dump_to_dict(self) -&gt; Dict[str, Any]:\n        state_dict = super().dump_to_dict()\n        state_dict[\"project_name\"] = self._project_name\n        state_dict[\"workspace\"] = self._workspace\n        state_dict[\"api_key\"] = self._api_key\n        state_dict[\"host\"] = self._host\n        state_dict[\"use_local\"] = self._use_local\n        return state_dict\n\n    @override\n    def load_from_dict(self, state_dict: Dict[str, Any]) -&gt; None:\n        super().load_from_dict(state_dict)\n        self._project_name = state_dict[\"project_name\"]\n        self._workspace = state_dict[\"workspace\"]\n        self._api_key = state_dict[\"api_key\"]\n        self._host = state_dict[\"host\"]\n        self._use_local = state_dict[\"use_local\"]\n        self._setup_opik() # if opik is not ready, it will be set to False\n</code></pre>"},{"location":"reference/bridgic-traces-opik/bridgic/traces/opik/#bridgic.traces.opik.OpikTraceCallback.on_worker_start","title":"on_worker_start","text":"<code>async</code> <pre><code>on_worker_start(\n    key: str,\n    is_top_level: bool = False,\n    parent: Optional[Automa] = None,\n    arguments: Optional[Dict[str, Any]] = None,\n) -&gt; None\n</code></pre> <p>Hook invoked before worker execution.</p> <p>For top-level automa, initializes a new trace. For workers, creates a new span. Handles nested automa as workers by checking if the decorated worker is an automa instance.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>Worker identifier.</p> required <code>is_top_level</code> <code>bool</code> <p>Whether the worker is the top-level automa. When True, parent will be the automa itself (parent is self).</p> <code>False</code> <code>parent</code> <code>Optional[Automa]</code> <p>Parent automa instance containing this worker. For top-level automa, parent is the automa itself.</p> <code>None</code> <code>arguments</code> <code>Optional[Dict[str, Any]]</code> <p>Execution arguments with keys \"args\" and \"kwargs\".</p> <code>None</code> Source code in <code>bridgic/traces/opik/_opik_trace_callback.py</code> <pre><code>async def on_worker_start(\n    self,\n    key: str,\n    is_top_level: bool = False,\n    parent: Optional[\"Automa\"] = None,\n    arguments: Optional[Dict[str, Any]] = None,\n) -&gt; None:\n    \"\"\"\n    Hook invoked before worker execution.\n\n    For top-level automa, initializes a new trace. For workers, creates\n    a new span. Handles nested automa as workers by checking if the\n    decorated worker is an automa instance.\n\n    Parameters\n    ----------\n    key : str\n        Worker identifier.\n    is_top_level : bool, default=False\n        Whether the worker is the top-level automa. When True, parent will be the automa itself (parent is self).\n    parent : Optional[Automa], default=None\n        Parent automa instance containing this worker. For top-level automa, parent is the automa itself.\n    arguments : Optional[Dict[str, Any]], default=None\n        Execution arguments with keys \"args\" and \"kwargs\".\n    \"\"\"\n    if not self._is_ready:\n        return\n    if is_top_level:\n        self._start_top_level_trace(key, arguments)\n        return\n\n    try:\n        worker = self._get_worker_instance(key, parent)\n    except (KeyError, ValueError) as e:\n        warnings.warn(f\"Failed to get worker instance for key '{key}': {e}\")\n        return\n\n    self._start_worker_span(key, worker, parent, arguments)\n</code></pre>"},{"location":"reference/bridgic-traces-opik/bridgic/traces/opik/#bridgic.traces.opik.OpikTraceCallback.on_worker_end","title":"on_worker_end","text":"<code>async</code> <pre><code>on_worker_end(\n    key: str,\n    is_top_level: bool = False,\n    parent: Optional[Automa] = None,\n    arguments: Optional[Dict[str, Any]] = None,\n    result: Any = None,\n) -&gt; None\n</code></pre> <p>Hook invoked after worker execution.</p> <p>For top-level automa, ends the trace. For workers, ends the span with execution results.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>Worker identifier.</p> required <code>is_top_level</code> <code>bool</code> <p>Whether the worker is the top-level automa. When True, parent will be the automa itself (parent is self).</p> <code>False</code> <code>parent</code> <code>Optional[Automa]</code> <p>Parent automa instance containing this worker. For top-level automa, parent is the automa itself.</p> <code>None</code> <code>arguments</code> <code>Optional[Dict[str, Any]]</code> <p>Execution arguments with keys \"args\" and \"kwargs\".</p> <code>None</code> <code>result</code> <code>Any</code> <p>Worker execution result.</p> <code>None</code> Source code in <code>bridgic/traces/opik/_opik_trace_callback.py</code> <pre><code>async def on_worker_end(\n    self,\n    key: str,\n    is_top_level: bool = False,\n    parent: Optional[\"Automa\"] = None,\n    arguments: Optional[Dict[str, Any]] = None,\n    result: Any = None,\n) -&gt; None:\n    \"\"\"\n    Hook invoked after worker execution.\n\n    For top-level automa, ends the trace. For workers, ends the span\n    with execution results.\n\n    Parameters\n    ----------\n    key : str\n        Worker identifier.\n    is_top_level : bool, default=False\n        Whether the worker is the top-level automa. When True, parent will be the automa itself (parent is self).\n    parent : Optional[Automa], default=None\n        Parent automa instance containing this worker. For top-level automa, parent is the automa itself.\n    arguments : Optional[Dict[str, Any]], default=None\n        Execution arguments with keys \"args\" and \"kwargs\".\n    result : Any, default=None\n        Worker execution result.\n    \"\"\"\n    if not self._is_ready:\n        return\n    output = self._build_output_payload(result=result)\n    self._complete_worker_execution(output, is_top_level)\n</code></pre>"},{"location":"reference/bridgic-traces-opik/bridgic/traces/opik/#bridgic.traces.opik.OpikTraceCallback.on_worker_error","title":"on_worker_error","text":"<code>async</code> <pre><code>on_worker_error(\n    key: str,\n    is_top_level: bool = False,\n    parent: Optional[Automa] = None,\n    arguments: Optional[Dict[str, Any]] = None,\n    error: Exception = None,\n) -&gt; bool\n</code></pre> <p>Hook invoked when worker execution raises an exception.</p> <p>For top-level automa, ends the trace with error information. For workers, ends the span with error information.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>Worker identifier.</p> required <code>is_top_level</code> <code>bool</code> <p>Whether the worker is the top-level automa. When True, parent will be the automa itself (parent is self).</p> <code>False</code> <code>parent</code> <code>Optional[Automa]</code> <p>Parent automa instance containing this worker. For top-level automa, parent is the automa itself.</p> <code>None</code> <code>arguments</code> <code>Optional[Dict[str, Any]]</code> <p>Execution arguments with keys \"args\" and \"kwargs\".</p> <code>None</code> <code>error</code> <code>Exception</code> <p>The exception raised during worker execution.</p> <code>None</code> <p>Returns:</p> Type Description <code>bool</code> <p>Always returns False, indicating the exception should not be suppressed.</p> Source code in <code>bridgic/traces/opik/_opik_trace_callback.py</code> <pre><code>async def on_worker_error(\n    self,\n    key: str,\n    is_top_level: bool = False,\n    parent: Optional[\"Automa\"] = None,\n    arguments: Optional[Dict[str, Any]] = None,\n    error: Exception = None,\n) -&gt; bool:\n    \"\"\"\n    Hook invoked when worker execution raises an exception.\n\n    For top-level automa, ends the trace with error information.\n    For workers, ends the span with error information.\n\n    Parameters\n    ----------\n    key : str\n        Worker identifier.\n    is_top_level : bool, default=False\n        Whether the worker is the top-level automa. When True, parent will be the automa itself (parent is self).\n    parent : Optional[Automa], default=None\n        Parent automa instance containing this worker. For top-level automa, parent is the automa itself.\n    arguments : Optional[Dict[str, Any]], default=None\n        Execution arguments with keys \"args\" and \"kwargs\".\n    error : Exception, default=None\n        The exception raised during worker execution.\n\n    Returns\n    -------\n    bool\n        Always returns False, indicating the exception should not be suppressed.\n    \"\"\"\n    if not self._is_ready:\n        return False\n    if not is_top_level and parent:\n        try:\n            self._get_worker_instance(key, parent)\n        except (KeyError, ValueError) as e:\n            warnings.warn(f\"Failed to get worker instance for key '{key}': {e}\")\n            return False\n\n    output = self._build_output_payload(error=error)\n    self._complete_worker_execution(output, is_top_level, error=error)\n    return False\n</code></pre>"},{"location":"reference/bridgic-traces-opik/bridgic/traces/opik/#bridgic.traces.opik.start_opik_trace","title":"start_opik_trace","text":"<pre><code>start_opik_trace(\n    project_name: Optional[str] = None,\n    workspace: Optional[str] = None,\n    host: Optional[str] = None,\n    api_key: Optional[str] = None,\n    use_local: bool = False,\n) -&gt; None\n</code></pre> <p>Start a Opik trace for a given project and service.</p> <p>Parameters:</p> Name Type Description Default <code>project_name</code> <code>Optional[str]</code> <p>The name of the project. If None, uses <code>Default Project</code> project name.</p> <code>None</code> <code>workspace</code> <code>Optional[str]</code> <p>The name of the workspace. If None, uses <code>default</code> workspace name.</p> <code>None</code> <code>host</code> <code>Optional[str]</code> <p>The host URL for the Opik server. If None, it will default to <code>https://www.comet.com/opik/api</code>.</p> <code>None</code> <code>api_key</code> <code>Optional[str]</code> <p>The API key for Opik. This parameter is ignored for local installations.</p> <code>None</code> <code>use_local</code> <code>bool</code> <p>Whether to use local Opik server.</p> <code>False</code> <p>Returns:</p> Type Description <code>None</code> Source code in <code>bridgic/traces/opik/_utils.py</code> <pre><code>def start_opik_trace(\n    project_name: Optional[str] = None,\n    workspace: Optional[str] = None,\n    host: Optional[str] = None,\n    api_key: Optional[str] = None,\n    use_local: bool = False,\n) -&gt; None:\n    \"\"\"Start a Opik trace for a given project and service.\n\n    Parameters\n    ----------\n    project_name : Optional[str], default=None\n        The name of the project. If None, uses `Default Project` project name.\n    workspace : Optional[str], default=None\n        The name of the workspace. If None, uses `default` workspace name.\n    host : Optional[str], default=None\n        The host URL for the Opik server. If None, it will default to `https://www.comet.com/opik/api`.\n    api_key : Optional[str], default=None\n        The API key for Opik. This parameter is ignored for local installations.\n    use_local : bool, default=False\n        Whether to use local Opik server.\n\n    Returns\n    -------\n    None\n    \"\"\"\n    from bridgic.core.config import GlobalSetting\n    builder = WorkerCallbackBuilder(\n        OpikTraceCallback, \n        init_kwargs={\"project_name\": project_name, \"workspace\": workspace, \"host\": host, \"api_key\": api_key, \"use_local\": use_local}\n    )\n    GlobalSetting.add(callback_builder=builder)\n</code></pre>"},{"location":"tutorials/","title":"Tutorials","text":"<p>Welcome to the Bridgic tutorial!</p> <p>Here, we demonstrate the foundational concepts for developing agentic systems using the Bridgic framework, helping you quickly grasp the key ideas and best practices. For each core feature, we provide a concise example inspired by real-world development scenarios to make the concepts easier to understand and apply.</p> <p>This tutorial will walk you through these tasks:</p> <ol> <li>Quick Start: Write your first Bridgic-style program.</li> <li>Core Mechanism: Learn about the core mechanism and how Bridgic organizes program into modular building blocks.</li> <li>Model Integration: Discover how to incorporate AI models into your workflows for creating more autonomous and intelligent systems.</li> </ol> <p>Start your Bridgic journey! \ud83c\udf89</p>"},{"location":"tutorials/installation/","title":"Installation","text":"<p>Bridgic is a next-generation agent development framework that enables developers to build agentic systems. Here are the installation instructions.</p> <p>Python 3.9 or higher version is required.</p> pipuv <pre><code>pip install bridgic\n</code></pre> <pre><code>uv add bridgic\n</code></pre> <p>After installation, you can verify that the installation was successful by running:</p> <pre><code>python -c \"from bridgic.core import __version__; print(f'Bridgic version: {__version__}')\"\n</code></pre>"},{"location":"tutorials/items/core_mechanism/","title":"Core Mechanism","text":"<p>Bridgic lets you build agentic systems by breaking down your workflows into modular building blocks called worker. Each worker represents a specific task or behavior, making it easy to organize complex processes.</p> <p>Bridgic introduces clear abstractions for structuring flows, passing data between execution units, handling concurrency, and enabling dynamic control logic (such as conditional branching and routing). This design allows users to build systerm that scale efficiently from simple workflows to sophisticated agentic systems.</p> <p>Key features include:</p> <ol> <li>Concurrency Mode: Organize your concurrent execution units systematically and conveniently.</li> <li>Parameter Resolving: Explore three ways for passing data and two ways for dispatching data between execution units, including Arguments Mapping, Arguments Injection, and Inputs Propagation about parameter binding; including Input Dispatching and Result Dispatching.</li> <li>Dynamic Routing: Enable conditional branching and decision-making through an easy-to-use <code>ferry_to()</code> API.</li> <li>Dynamic Topology: Change the underlying  graph topology at runtime to support highly autonomous AI applications..</li> <li>Human-in-the-loop: Enable human interaction or external input during workflow / agent execution.</li> <li>Worker Callback: Bring non-intrusive and multi-scope callback mechanisms that allows you to observe, customize, or extend execution at different levels with minimal impact on core logic.</li> <li>Modularity: Reuse and compose Automata by embedding one inside another for scalable agentic systems.</li> <li>Model Integration: Incorporate model to building a program with more autonomous capabilities.</li> </ol> <p>This architectural foundation makes Bridgic a powerful platform for building agentic systems that are robust, adaptive, and easy to reason about, enabling you to bridge the precision of logic with the creativity potential of AI.</p>"},{"location":"tutorials/items/core_mechanism/concurrency_mode/","title":"Concurrency Mode","text":"In\u00a0[\u00a0]: Copied! <pre>url=\"http://books.toscrape.com/catalogue/a-light-in-the-attic_1000/index.html\"\n</pre> url=\"http://books.toscrape.com/catalogue/a-light-in-the-attic_1000/index.html\" <p>The page looks like this:</p> <p>Note: We use Books to Scrape, a demo website created specifically for practicing web scraping, to introduce in this tutorial. Please note that the purpose of writing a crawler here is not to build a real scraper, but to provide a simple and safe example to demonstrate how Bridgic handles both synchronous and asynchronous execution models.</p> <p>We use <code>requests</code> to obtain the web content of the given url. Use <code>pip install requests</code> to install <code>requests</code> package and crawl the page like this:</p> In\u00a0[\u00a0]: Copied! <pre>import requests\n\ndef get_web_content(url):  # will return the web content of the given url\n    response = requests.get(url)\n    return response.text\n</pre> import requests  def get_web_content(url):  # will return the web content of the given url     response = requests.get(url)     return response.text In\u00a0[\u00a0]: Copied! <pre>import os\n\n# Set the API base and key.\n_api_key = os.environ.get(\"OPENAI_API_KEY\")\n_api_base = os.environ.get(\"OPENAI_API_BASE\")\n_model_name = os.environ.get(\"OPENAI_MODEL_NAME\")\n\n# Import the necessary modules.\nfrom bridgic.core.automa import GraphAutoma, worker\nfrom bridgic.core.model.types import Message, Role\nfrom bridgic.llms.openai_like import OpenAILikeLlm\n\nllm = OpenAILikeLlm(api_base=_api_base, api_key=_api_key, timeout=30)\n</pre> import os  # Set the API base and key. _api_key = os.environ.get(\"OPENAI_API_KEY\") _api_base = os.environ.get(\"OPENAI_API_BASE\") _model_name = os.environ.get(\"OPENAI_MODEL_NAME\")  # Import the necessary modules. from bridgic.core.automa import GraphAutoma, worker from bridgic.core.model.types import Message, Role from bridgic.llms.openai_like import OpenAILikeLlm  llm = OpenAILikeLlm(api_base=_api_base, api_key=_api_key, timeout=30) <p>Let's write web content analysis assistant.</p> In\u00a0[\u00a0]: Copied! <pre>class WebContentAnalysisAgent(GraphAutoma):\n    @worker(is_start=True)\n    def crawl_web_content(self, url: str) -&gt; str:\n        response = requests.get(url)\n        return response.text\n\n    @worker(dependencies=[\"crawl_web_content\"], is_output=True)\n    async def analyze_web_content(self, content: str) -&gt; str:\n        response = await llm.achat(\n            model=_model_name,\n            messages=[\n                Message.from_text(text=\"You are a web content analysis assistant. Your task is to analyze the given web content and summarize the main content.\", role=Role.SYSTEM),\n                Message.from_text(text=content, role=Role.USER),\n            ]\n        )\n        return response.message.content\n</pre> class WebContentAnalysisAgent(GraphAutoma):     @worker(is_start=True)     def crawl_web_content(self, url: str) -&gt; str:         response = requests.get(url)         return response.text      @worker(dependencies=[\"crawl_web_content\"], is_output=True)     async def analyze_web_content(self, content: str) -&gt; str:         response = await llm.achat(             model=_model_name,             messages=[                 Message.from_text(text=\"You are a web content analysis assistant. Your task is to analyze the given web content and summarize the main content.\", role=Role.SYSTEM),                 Message.from_text(text=content, role=Role.USER),             ]         )         return response.message.content <p>Now, let's use it to help us analyze the content.</p> In\u00a0[22]: Copied! <pre>web_content_analysis_agent = WebContentAnalysisAgent()\n\n# Input the url of the web page to be analyzed.\nurl = \"http://books.toscrape.com/catalogue/a-light-in-the-attic_1000/index.html\"\n\n# Call the agent to analyze the web content.\nres = await web_content_analysis_agent.arun(url)\n\n# Print the result.\nprint(f'- - - - - result - - - - -')\nprint(res)\nprint(f'- - - - - end - - - - -')\n</pre> web_content_analysis_agent = WebContentAnalysisAgent()  # Input the url of the web page to be analyzed. url = \"http://books.toscrape.com/catalogue/a-light-in-the-attic_1000/index.html\"  # Call the agent to analyze the web content. res = await web_content_analysis_agent.arun(url)  # Print the result. print(f'- - - - - result - - - - -') print(res) print(f'- - - - - end - - - - -')  <pre>- - - - - result - - - - -\nThe provided HTML content is from a product page on **Books to Scrape**, a demo website designed for web scraping education. Here's a clear summary of the main content:\n\n---\n\n### **Main Content Summary: A Light in the Attic**\n\n- **Product Title**: *A Light in the Attic*  \n- **Author**: Shel Silverstein  \n- **Category**: Poetry  \n- **Product Type**: Book (Poetry with illustrations)  \n- **Price**: \u00a351.77 (excl. and incl. tax; tax is \u00a30.00)  \n- **Availability**: In stock (22 units available)  \n- **Rating**: 5 stars (all full stars)  \n- **Number of Reviews**: 0  \n\n---\n\n### **Product Description Highlights**\n- Celebrates its 20th anniversary with a special edition.\n- Known for humorous, creative, and rhythmic poetry that appeals to both children and adults.\n- Features classic verses such as *\"Rockabye Baby\"*:\n  &gt; *\"Rockabye baby, in the treetop / Don't you know a treetop / Is no safe place to rock?\"*\n- Described as a timeless classic that brings joy and laughter to readers of all ages.\n\n---\n\n### **Important Note**\n&gt; \u26a0\ufe0f **This is a demo website** for web scraping training.  \n&gt; - Prices and ratings are **randomly assigned** and **do not reflect real-world data**.  \n&gt; - The site is not a real marketplace and should not be used for actual purchases.\n\n---\n\n### **Navigation Path**\nHome \u2192 Books \u2192 Poetry \u2192 *A Light in the Attic*\n\n---\n\n### **Visual Elements**\n- A single image of the book displayed in a carousel.\n- Clean, responsive layout with a header, product gallery, pricing, and description sections.\n\n---\n\n\u2705 **Purpose of the Page**: To demonstrate how to extract product details (title, price, description, availability, etc.) from e-commerce-style web pages \u2014 useful for teaching web scraping techniques.  \n\n\u274c **Not for real shopping** \u2014 all data is fictional.  \n\n--- \n\nIn short: This page showcases a fictional version of a beloved children's poetry book, presented in a realistic e-commerce format, but with no real pricing or user reviews.\n- - - - - end - - - - -\n</pre>"},{"location":"tutorials/items/core_mechanism/concurrency_mode/#concurrency-mode","title":"Concurrency Mode\u00b6","text":"<p>Bridgic runs primarily on an asynchronous event loop, while seamlessly supporting I/O-bound tasks through threads. This design ensures high concurrency across diverse workloads.</p>"},{"location":"tutorials/items/core_mechanism/concurrency_mode/#web-content-analysis-assistant","title":"Web content analysis assistant\u00b6","text":"<p>To explore Bridgic\u2019s support for concurrency, let\u2019s build a web content analysis assistant to summarize and introduce the main content of a given web page. The steps are as follows:</p> <ol> <li>Crawl relevant content of the input url.</li> <li>Summarize and introduce main content</li> </ol>"},{"location":"tutorials/items/core_mechanism/concurrency_mode/#1-crawl-relevant-content","title":"1. Crawl relevant content\u00b6","text":"<p>Taking the Books to Scrape website as an example, we are given the url of a book page on the website. Like this:</p>"},{"location":"tutorials/items/core_mechanism/concurrency_mode/#2-summarize-and-introduce-main-content","title":"2. Summarize and introduce main content\u00b6","text":"<p>We create an agent, input a url and crawl the corresponding page, and then let the model summarize the main content of the web page.</p> <p>Initialize the runtime environment.</p>"},{"location":"tutorials/items/core_mechanism/concurrency_mode/#what-have-we-learnt","title":"What have we learnt?\u00b6","text":"<p>It can be seen from the example that Bridgic can seamlessly schedule both asynchronous and synchronous workers within the same automa. Although <code>crawl_web_content</code> performs a blocking network request, Bridgic automatically dispatches it to a thread so that the event loop remains unblocked. Meanwhile, <code>analyze_web_content</code> runs asynchronously within the event loop. Refer to <code>Worker</code> for details.</p> <p>In this way, Bridgic keeps an asynchronous-first design, but also provides built-in support for I/O-bound operations through its thread pool, ensuring smooth execution across different types of workloads.</p>"},{"location":"tutorials/items/core_mechanism/dynamic_routing/","title":"Dynamic Routing","text":"In\u00a0[\u00a0]: Copied! <pre># Get the environment variables.\nimport os\nfrom typing import Literal\n\n_api_key = os.environ.get(\"OPENAI_API_KEY\")\n_api_base = os.environ.get(\"OPENAI_API_BASE\")\n_model_name = os.environ.get(\"OPENAI_MODEL_NAME\")\n\n# Import the necessary packages.\nfrom pydantic import BaseModel\nfrom bridgic.core.model.protocols import PydanticModel\nfrom bridgic.core.automa import GraphAutoma, worker\nfrom bridgic.core.model.protocols import Choice\nfrom bridgic.core.model.types import Message, Role\nfrom bridgic.llms.openai import OpenAILlm, OpenAIConfiguration\n\nllm = OpenAILlm(  # the llm instance\n    api_base=_api_base,\n    api_key=_api_key,\n    configuration=OpenAIConfiguration(model=_model_name),\n    timeout=20,\n)\n</pre> # Get the environment variables. import os from typing import Literal  _api_key = os.environ.get(\"OPENAI_API_KEY\") _api_base = os.environ.get(\"OPENAI_API_BASE\") _model_name = os.environ.get(\"OPENAI_MODEL_NAME\")  # Import the necessary packages. from pydantic import BaseModel from bridgic.core.model.protocols import PydanticModel from bridgic.core.automa import GraphAutoma, worker from bridgic.core.model.protocols import Choice from bridgic.core.model.types import Message, Role from bridgic.llms.openai import OpenAILlm, OpenAIConfiguration  llm = OpenAILlm(  # the llm instance     api_base=_api_base,     api_key=_api_key,     configuration=OpenAIConfiguration(model=_model_name),     timeout=20, ) In\u00a0[\u00a0]: Copied! <pre>class Domain(BaseModel):\n    choices: Literal[\"medical\", \"legal\"]\n</pre> class Domain(BaseModel):     choices: Literal[\"medical\", \"legal\"] In\u00a0[\u00a0]: Copied! <pre>class DomainChatbot(GraphAutoma):\n    @worker(is_start=True)\n    async def pre_query(self, query: str):  # receive user input and preprocess it\n        return query\n\n    @worker(dependencies=[\"pre_query\"])\n    async def domain_recognition(self, query: str):  # domain recognition and dynamic routing\n        response: Domain = await llm.astructured_output(\n            constraint=PydanticModel(model=Domain),\n            messages=[\n                Message.from_text(text=\"Please identify the domain of the following query\", role=Role.SYSTEM),\n                Message.from_text(text=query, role=Role.USER),\n            ]\n        )\n        print(f'The query domain is: {response}')  # print the response\n        \n        # dynamic routing\n        if response.choices == \"medical\":\n            return self.ferry_to('answer_medical_query', query)\n        elif response.choices == \"legal\":\n            return self.ferry_to('answer_legal_query', query)\n    \n    @worker(is_output=True)\n    async def answer_medical_query(self, query: str):  # answer medical-related query\n        response = await llm.achat(\n            messages=[\n                Message.from_text(text=\"You are a medical expert. Answer the following medical-related query\", role=Role.SYSTEM),\n                Message.from_text(text=query, role=Role.USER),\n            ]\n        )\n        return response.message.content\n\n    @worker(is_output=True)\n    async def answer_legal_query(self, query: str):  # answer legal-related query\n        response = await llm.achat(\n            messages=[\n                Message.from_text(text=\"You are a legal expert. Answer the following legal-related query\", role=Role.SYSTEM),\n                Message.from_text(text=query, role=Role.USER),\n            ]\n        )\n        return response.message.content\n</pre> class DomainChatbot(GraphAutoma):     @worker(is_start=True)     async def pre_query(self, query: str):  # receive user input and preprocess it         return query      @worker(dependencies=[\"pre_query\"])     async def domain_recognition(self, query: str):  # domain recognition and dynamic routing         response: Domain = await llm.astructured_output(             constraint=PydanticModel(model=Domain),             messages=[                 Message.from_text(text=\"Please identify the domain of the following query\", role=Role.SYSTEM),                 Message.from_text(text=query, role=Role.USER),             ]         )         print(f'The query domain is: {response}')  # print the response                  # dynamic routing         if response.choices == \"medical\":             return self.ferry_to('answer_medical_query', query)         elif response.choices == \"legal\":             return self.ferry_to('answer_legal_query', query)          @worker(is_output=True)     async def answer_medical_query(self, query: str):  # answer medical-related query         response = await llm.achat(             messages=[                 Message.from_text(text=\"You are a medical expert. Answer the following medical-related query\", role=Role.SYSTEM),                 Message.from_text(text=query, role=Role.USER),             ]         )         return response.message.content      @worker(is_output=True)     async def answer_legal_query(self, query: str):  # answer legal-related query         response = await llm.achat(             messages=[                 Message.from_text(text=\"You are a legal expert. Answer the following legal-related query\", role=Role.SYSTEM),                 Message.from_text(text=query, role=Role.USER),             ]         )         return response.message.content  <p>Let's run it!</p> In\u00a0[5]: Copied! <pre>query = \"What is the aspirin?\"\ndomain_chatbot = DomainChatbot()\nawait domain_chatbot.arun(query)\n</pre> query = \"What is the aspirin?\" domain_chatbot = DomainChatbot() await domain_chatbot.arun(query) <pre>The query domain is: medical\n</pre> Out[5]: <pre>'Aspirin, also known as acetylsalicylic acid, is a widely used medication with several important medical properties. It is one of the oldest and most well-known over-the-counter (OTC) drugs, originally derived from willow bark.\\n\\n### Key Uses of Aspirin:\\n\\n1. **Pain Relief (Analgesic)**  \\n   Aspirin helps relieve mild to moderate pain such as headaches, toothaches, muscle aches, and menstrual cramps.\\n\\n2. **Reducing Fever (Antipyretic)**  \\n   It lowers body temperature in cases of fever, such as in infections.\\n\\n3. **Reducing Inflammation (Anti-inflammatory)**  \\n   Aspirin helps reduce inflammation, which can be beneficial in conditions like rheumatoid arthritis or other inflammatory disorders.\\n\\n4. **Preventing Blood Clots (Antiplatelet Effect)**  \\n   This is one of its most important therapeutic uses. Aspirin inhibits the formation of blood clots by blocking the action of a substance called cyclooxygenase (COX), which reduces the production of thromboxane\u2014a molecule that promotes platelet aggregation.  \\n   - **Low-dose aspirin (typically 81 mg)** is commonly prescribed for long-term use to prevent heart attacks and strokes in individuals at risk due to cardiovascular disease (e.g., history of heart attack, angina, or stroke).\\n\\n5. **Cardiovascular Protection**  \\n   Regular low-dose aspirin is recommended for secondary prevention in patients with a history of cardiovascular events to reduce the risk of recurrence.\\n\\n---\\n\\n### How It Works:\\nAspirin irreversibly inhibits the enzyme COX-1 and COX-2, which are involved in producing prostaglandins. Prostaglandins play roles in inflammation, pain, fever, and platelet aggregation. By blocking COX-1, aspirin reduces platelet activation and clot formation.\\n\\n---\\n\\n### Safety and Precautions:\\n- **Gastrointestinal Side Effects**: Aspirin can irritate the stomach lining and increase the risk of ulcers or bleeding, especially with long-term or high-dose use.\\n- **Bleeding Risk**: Due to its antiplatelet effect, it increases the risk of bleeding (e.g., gastrointestinal bleeding, bruising).\\n- **Allergic Reactions**: Some people may have a severe allergic reaction, including asthma exacerbation (especially in those with a history of aspirin sensitivity).\\n- **Reproductive Health**: Aspirin is not recommended during pregnancy, especially in the third trimester, due to risks of fetal complications.\\n- **Children and Fever**: Aspirin should be avoided in children and adolescents with viral infections (like chickenpox or flu) due to the risk of Reye\u2019s syndrome, a rare but serious condition.\\n\\n---\\n\\n### Summary:\\n**Aspirin is a versatile medication used for pain, fever, inflammation, and cardiovascular protection. Its most significant medical role today is in preventing heart attacks and strokes through long-term low-dose use. However, it should be used cautiously and only as directed by a healthcare provider.**\\n\\nAlways consult a doctor before starting or stopping aspirin therapy, especially if you have underlying health conditions or are taking other medications.'</pre> <p>If you encounter a Readtimeout error during the execution of the above process, it might be because the timeout period set during the initialization of the llm is too short, causing the model to time out before it finishes responding.</p> <p>Great! We have successfully completed the domain chatbot.</p>"},{"location":"tutorials/items/core_mechanism/dynamic_routing/#dynamic-routing","title":"Dynamic Routing\u00b6","text":"<p>Bridgic supports dynamic routing to the corresponding worker based on runtime conditions. Now let's understand it with a sample example.</p>"},{"location":"tutorials/items/core_mechanism/dynamic_routing/#domain-chatbot","title":"Domain chatbot\u00b6","text":"<p>Dynamic routing is a common requirement in chatbots. For instance, it involves analyzing the input query to determine the main domain or field it pertains to, and then dynamically routing it to the corresponding processing logic.</p> <p>The user inputs the original query, we identify whether it is a medical-related or legal-related issue, and then we answer it with specialized logic. There are three steps:</p> <ol> <li>Receive user input</li> <li>Domain recognition and dynamic routing to correct worker</li> <li>Answer query</li> </ol>"},{"location":"tutorials/items/core_mechanism/dynamic_routing/#1-initialize","title":"1. Initialize\u00b6","text":"<p>Before we start, let's prepare the running environment.</p>"},{"location":"tutorials/items/core_mechanism/dynamic_routing/#2-complete","title":"2. Complete\u00b6","text":"<p>We assume that the user query we receive is a string. Let's implement the domain chatbot to answer the user query.</p>"},{"location":"tutorials/items/core_mechanism/dynamic_routing/#what-have-we-learnt","title":"What have we learnt?\u00b6","text":"<p>We have implemented the routing mechanism using Bridgic.</p>"},{"location":"tutorials/items/core_mechanism/dynamic_routing/#ferry-to","title":"Ferry To\u00b6","text":"<p>In Bridgic, we use the <code>ferry_to</code> mechanism to achieve dynamic routing. Its first parameter is the <code>key</code> of the worker to which the route will be directed, and the second parameter is the input parameter passed to the target worker.</p>"},{"location":"tutorials/items/core_mechanism/dynamic_topology/","title":"Dynamic Topology","text":"<p>Run the following <code>pip</code> command to make sure the 'openai' integration is installed.</p> <pre>pip install -U bridgic\npip install -U bridgic-llms-openai\n</pre> In\u00a0[3]: Copied! <pre>import os\n\n# Get the API base, API key and model name.\n_api_key = os.environ.get(\"OPENAI_API_KEY\")\n_api_base = os.environ.get(\"OPENAI_API_BASE\")\n_model_name = os.environ.get(\"OPENAI_MODEL_NAME\")\n\nfrom bridgic.llms.openai import OpenAILlm, OpenAIConfiguration\n\nllm = OpenAILlm(  # the llm instance\n    api_base=_api_base,\n    api_key=_api_key,\n    configuration=OpenAIConfiguration(model=_model_name),\n    timeout=20,\n)\n</pre> import os  # Get the API base, API key and model name. _api_key = os.environ.get(\"OPENAI_API_KEY\") _api_base = os.environ.get(\"OPENAI_API_BASE\") _model_name = os.environ.get(\"OPENAI_MODEL_NAME\")  from bridgic.llms.openai import OpenAILlm, OpenAIConfiguration  llm = OpenAILlm(  # the llm instance     api_base=_api_base,     api_key=_api_key,     configuration=OpenAIConfiguration(model=_model_name),     timeout=20, ) In\u00a0[5]: Copied! <pre># Three mock tools defined as async functions.\n\nasync def get_weather(city: str, days: int):\n    \"\"\"\n    Get the weather forecast for the next few days in a specified city.\n\n    Parameters\n    ----------\n    city : str\n        The city to get the weather of, e.g. New York.\n    days : int\n        The number of days to get the weather forecast for.\n    \n    Returns\n    -------\n    str\n        The weather forecast for the next few days in the specified city.\n    \"\"\"\n    return f\"The weather in {city} will be mostly sunny for the next {days} days.\"\n\nasync def get_flight_price(origin_city: str, destination_city: str):\n    \"\"\"\n    Get the average round-trip flight price from one city to another.\n\n    Parameters\n    ----------\n    origin_city : str\n        The origin city of the flight.\n    destination_city : str\n        The destination city of the flight.\n    \n    Returns\n    -------\n    str\n        The average round-trip flight price from the origin city to the destination city.\n    \"\"\"\n    return f\"The average round-trip flight from {origin_city} to {destination_city} is about $850.\"\n\nasync def get_hotel_price(city: str, nights: int):\n    \"\"\"\n    Get the average price of a hotel stay in a specified city for a given number of nights.\n\n    Parameters\n    ----------\n    city : str\n        The city to get the hotel price of, e.g. New York.\n    nights : int\n        The number of nights to get the hotel price for.\n    \n    Returns\n    -------\n    str\n        The average price of a hotel stay in the specified city for the given number of nights.\n    \"\"\"\n    return f\"A 3-star hotel in {city} costs about $120 per night for {nights} nights.\"\n\nfrom bridgic.core.agentic.tool_specs import FunctionToolSpec\n\nfuncs = [get_weather, get_flight_price, get_hotel_price]\ntool_list = [FunctionToolSpec.from_raw(func) for func in funcs]\n</pre> # Three mock tools defined as async functions.  async def get_weather(city: str, days: int):     \"\"\"     Get the weather forecast for the next few days in a specified city.      Parameters     ----------     city : str         The city to get the weather of, e.g. New York.     days : int         The number of days to get the weather forecast for.          Returns     -------     str         The weather forecast for the next few days in the specified city.     \"\"\"     return f\"The weather in {city} will be mostly sunny for the next {days} days.\"  async def get_flight_price(origin_city: str, destination_city: str):     \"\"\"     Get the average round-trip flight price from one city to another.      Parameters     ----------     origin_city : str         The origin city of the flight.     destination_city : str         The destination city of the flight.          Returns     -------     str         The average round-trip flight price from the origin city to the destination city.     \"\"\"     return f\"The average round-trip flight from {origin_city} to {destination_city} is about $850.\"  async def get_hotel_price(city: str, nights: int):     \"\"\"     Get the average price of a hotel stay in a specified city for a given number of nights.      Parameters     ----------     city : str         The city to get the hotel price of, e.g. New York.     nights : int         The number of nights to get the hotel price for.          Returns     -------     str         The average price of a hotel stay in the specified city for the given number of nights.     \"\"\"     return f\"A 3-star hotel in {city} costs about $120 per night for {nights} nights.\"  from bridgic.core.agentic.tool_specs import FunctionToolSpec  funcs = [get_weather, get_flight_price, get_hotel_price] tool_list = [FunctionToolSpec.from_raw(func) for func in funcs] <p>In the code above, three tools are defined. The docstring of each tool provides important information, which will serve as the tool descriptions presented to the LLM. Each tool is transformed to a <code>FunctionToolSpec</code> instance, and these three tools are stored in the <code>tool_list</code> variable for later use.</p> In\u00a0[8]: Copied! <pre>from typing import List, Tuple, Any\nfrom bridgic.core.automa import GraphAutoma, worker\nfrom bridgic.core.agentic.tool_specs import ToolSpec\nfrom bridgic.core.model.types import Message, Role, ToolCall\nfrom bridgic.core.automa.args import From, ArgsMappingRule\nfrom bridgic.core.agentic.types import ToolMessage\n\nclass TravelPlanner(GraphAutoma):\n    @worker(is_start=True)\n    async def invoke_llm(self, user_input: str, tool_list: List[ToolSpec]):\n        tool_calls, _ = await llm.aselect_tool(\n            messages=[\n                Message.from_text(text=\"You are an intelligent AI assistant that can perform tasks by calling available tools.\", role=Role.SYSTEM),\n                Message.from_text(text=user_input, role=Role.USER),\n            ], \n            tools=[tool.to_tool() for tool in tool_list], \n        )\n        print(f\"[invoke_llm] - LLM returns tool_calls: {tool_calls}\")\n        return tool_calls\n    \n    @worker(dependencies=[\"invoke_llm\"])\n    async def process_tool_calls(\n        self,\n        tool_calls: List[ToolCall],\n        tool_list: List[ToolSpec],\n    ):\n        matched_list = self._match_tool_calls_and_tool_specs(tool_calls, tool_list)\n        matched_tool_calls = []\n        tool_worker_keys = []\n        for tool_call, tool_spec in matched_list:\n            matched_tool_calls.append(tool_call)\n            tool_worker = tool_spec.create_worker()\n            worker_key = f\"tool_{tool_call.name}_{tool_call.id}\"\n            print(f\"[process_tool_calls] - add worker: {worker_key}\")\n            self.add_worker(\n                key=worker_key,\n                worker=tool_worker,\n            )\n            self.ferry_to(worker_key, **tool_call.arguments)\n            tool_worker_keys.append(worker_key)\n        self.add_func_as_worker(\n            key=\"aggregate_results\",\n            func=self.aggregate_results,\n            dependencies=tool_worker_keys,\n            args_mapping_rule=ArgsMappingRule.MERGE,\n        )\n        return matched_tool_calls\n\n    async def aggregate_results(\n        self, \n        tool_results: List[Any],\n        tool_calls: List[ToolCall] = From(\"process_tool_calls\"),\n    ) -&gt; List[ToolMessage]:\n        print(f\"[aggregate_results] - tool execution results: {tool_results}\")\n        tool_messages = []\n        for tool_result, tool_call in zip(tool_results, tool_calls):\n            tool_messages.append(ToolMessage(\n                role=\"tool\", \n                content=str(tool_result), \n                tool_call_id=tool_call.id\n            ))\n        # `tool_messages` may be used as the inputs of the next LLM call...\n        print(f\"[aggregate_results] - assembled ToolMessage list: {tool_messages}\")\n        return tool_messages\n\n    def _match_tool_calls_and_tool_specs(\n        self,\n        tool_calls: List[ToolCall],\n        tool_list: List[ToolSpec],\n    ) -&gt; List[Tuple[ToolCall, ToolSpec]]:\n        matched_list: List[Tuple[ToolCall, ToolSpec]] = []\n        for tool_call in tool_calls:\n            for tool_spec in tool_list:\n                if tool_call.name == tool_spec.tool_name:\n                    matched_list.append((tool_call, tool_spec))\n        return matched_list\n</pre> from typing import List, Tuple, Any from bridgic.core.automa import GraphAutoma, worker from bridgic.core.agentic.tool_specs import ToolSpec from bridgic.core.model.types import Message, Role, ToolCall from bridgic.core.automa.args import From, ArgsMappingRule from bridgic.core.agentic.types import ToolMessage  class TravelPlanner(GraphAutoma):     @worker(is_start=True)     async def invoke_llm(self, user_input: str, tool_list: List[ToolSpec]):         tool_calls, _ = await llm.aselect_tool(             messages=[                 Message.from_text(text=\"You are an intelligent AI assistant that can perform tasks by calling available tools.\", role=Role.SYSTEM),                 Message.from_text(text=user_input, role=Role.USER),             ],              tools=[tool.to_tool() for tool in tool_list],          )         print(f\"[invoke_llm] - LLM returns tool_calls: {tool_calls}\")         return tool_calls          @worker(dependencies=[\"invoke_llm\"])     async def process_tool_calls(         self,         tool_calls: List[ToolCall],         tool_list: List[ToolSpec],     ):         matched_list = self._match_tool_calls_and_tool_specs(tool_calls, tool_list)         matched_tool_calls = []         tool_worker_keys = []         for tool_call, tool_spec in matched_list:             matched_tool_calls.append(tool_call)             tool_worker = tool_spec.create_worker()             worker_key = f\"tool_{tool_call.name}_{tool_call.id}\"             print(f\"[process_tool_calls] - add worker: {worker_key}\")             self.add_worker(                 key=worker_key,                 worker=tool_worker,             )             self.ferry_to(worker_key, **tool_call.arguments)             tool_worker_keys.append(worker_key)         self.add_func_as_worker(             key=\"aggregate_results\",             func=self.aggregate_results,             dependencies=tool_worker_keys,             args_mapping_rule=ArgsMappingRule.MERGE,         )         return matched_tool_calls      async def aggregate_results(         self,          tool_results: List[Any],         tool_calls: List[ToolCall] = From(\"process_tool_calls\"),     ) -&gt; List[ToolMessage]:         print(f\"[aggregate_results] - tool execution results: {tool_results}\")         tool_messages = []         for tool_result, tool_call in zip(tool_results, tool_calls):             tool_messages.append(ToolMessage(                 role=\"tool\",                  content=str(tool_result),                  tool_call_id=tool_call.id             ))         # `tool_messages` may be used as the inputs of the next LLM call...         print(f\"[aggregate_results] - assembled ToolMessage list: {tool_messages}\")         return tool_messages      def _match_tool_calls_and_tool_specs(         self,         tool_calls: List[ToolCall],         tool_list: List[ToolSpec],     ) -&gt; List[Tuple[ToolCall, ToolSpec]]:         matched_list: List[Tuple[ToolCall, ToolSpec]] = []         for tool_call in tool_calls:             for tool_spec in tool_list:                 if tool_call.name == tool_spec.tool_name:                     matched_list.append((tool_call, tool_spec))         return matched_list <p>In the start worker <code>invoke_llm</code>, the LLM is invoked to return a list of <code>ToolCalls</code>. Therefore, the information about the tool calls contained in this list is dynamic.</p> <p>In the second worker <code>process_tool_calls</code>, based on the dynamic list of <code>tool_calls</code>, a worker is created (through <code>tool_spec.create_worker()</code>) for each tool to be invoked and added to the DDG. Then, the <code>aggregate_results</code> worker is also dynamically added to the DDG via the <code>add_func_as_worker()</code> API, responsible for aggregating the execution results from all the tool workers.</p> <p>It is worth noting that invoking multiple tools as workers can fully leverage certain features of the Bridgic framework, such as Concurrency Mode. Here, these tools are able to execute concurrently.</p> In\u00a0[9]: Copied! <pre>agent = TravelPlanner()\nawait agent.arun(\n    user_input=\"Plan a 3-day trip to Tokyo. Check the weather forecast, estimate the flight price from San Francisco, and the hotel cost for 3 nights.\",\n    tool_list=tool_list,\n)\n</pre> agent = TravelPlanner() await agent.arun(     user_input=\"Plan a 3-day trip to Tokyo. Check the weather forecast, estimate the flight price from San Francisco, and the hotel cost for 3 nights.\",     tool_list=tool_list, ) <pre>[invoke_llm] - LLM returns tool_calls: [ToolCall(id='call_cLERxyz110tylRxgE4XQjaRQ', name='get_weather', arguments={'city': 'Tokyo', 'days': 3}), ToolCall(id='call_CqicPm6yZoyNksEl9HGVJEOQ', name='get_flight_price', arguments={'origin_city': 'San Francisco', 'destination_city': 'Tokyo'}), ToolCall(id='call_GscwR3pvHtzR2wTki1ndpHZp', name='get_hotel_price', arguments={'city': 'Tokyo', 'nights': 3})]\n[process_tool_calls] - add worker: tool_get_weather_call_cLERxyz110tylRxgE4XQjaRQ\n[process_tool_calls] - add worker: tool_get_flight_price_call_CqicPm6yZoyNksEl9HGVJEOQ\n[process_tool_calls] - add worker: tool_get_hotel_price_call_GscwR3pvHtzR2wTki1ndpHZp\n[aggregate_results] - tool execution results: ['The weather in Tokyo will be mostly sunny for the next 3 days.', 'The average round-trip flight from San Francisco to Tokyo is about $850.', 'A 3-star hotel in Tokyo costs about $120 per night for 3 nights.']\n[aggregate_results] - assembled ToolMessage list: [{'role': 'tool', 'content': 'The weather in Tokyo will be mostly sunny for the next 3 days.', 'tool_call_id': 'call_cLERxyz110tylRxgE4XQjaRQ'}, {'role': 'tool', 'content': 'The average round-trip flight from San Francisco to Tokyo is about $850.', 'tool_call_id': 'call_CqicPm6yZoyNksEl9HGVJEOQ'}, {'role': 'tool', 'content': 'A 3-star hotel in Tokyo costs about $120 per night for 3 nights.', 'tool_call_id': 'call_GscwR3pvHtzR2wTki1ndpHZp'}]\n</pre>"},{"location":"tutorials/items/core_mechanism/dynamic_topology/#dynamic-topology","title":"Dynamic Topology\u00b6","text":"<p>In the previous section of this tutorial, we learned how to use the <code>ferry_to()</code> API to implement dynamic routing. This capability allows us to create branching and looping logic, forming the foundation for handling dynamic behavior driven by runtime inputs. However, when we take into account the highly autonomous planning capabilities of LLMs, the dynamic features provided by <code>ferry_to()</code> alone are no longer sufficient.</p> <p>In order to support highly autonomous AI applications, the orchestration of workers in Bridgic is built on a Dynamic Directed Graph (DDG), whose topology can change at runtime. This DDG-based architecture is especially useful in scenarios where the execution path planned by an LLM cannot be predetermined at coding time. It provides a greater degree of flexibility than the routing mechanism described earlier.</p>"},{"location":"tutorials/items/core_mechanism/dynamic_topology/#example-tool-selection","title":"Example: Tool Selection\u00b6","text":"<p>Most LLMs support tool selection and invocation \u2014 a crucial step of a typical agent loop. In the following example, we\u2019ll demonstrate the key process of tool selection through a Travel Planning Agent, and use Bridgic\u2019s dynamic topology to implement tool calling.</p> <p>Note:</p> <p>This code example is for demonstration purposes only. It represents part of the overall execution flow within a complete agent loop. If you intend to use tool calling and the agent loop in production, please use the <code>ReActAutoma</code> class provided by the Bridgic framework.</p>"},{"location":"tutorials/items/core_mechanism/dynamic_topology/#1-initialization","title":"1. Initialization\u00b6","text":"<p>Before we start, let's initialize the OpenAI LLM instance and the running environment.</p>"},{"location":"tutorials/items/core_mechanism/dynamic_topology/#2-preparing-tools","title":"2. Preparing Tools\u00b6","text":"<p>In the travel-planning example, we need to provide several tools for the LLM to call. The following code defines these tools as functions.</p>"},{"location":"tutorials/items/core_mechanism/dynamic_topology/#3-orchestration","title":"3. Orchestration\u00b6","text":"<p>This demo consists of four steps:</p> <ol> <li>Invoke the LLM: Pass the list of available tools to the LLM and obtain its <code>tool_calls</code> output.</li> <li>Create workers dynamically: Dynamically create workers based on the <code>tool_calls</code> results.</li> <li>Invoke tools: Let the Bridgic framework automatically schedule and execute those workers that represent the tools.</li> <li>Aggregate results: Combine the execution results into a list of <code>ToolMessage</code> objects, which may later be fed into the LLM for further processing.</li> </ol> <p>We implement these steps by subclassing <code>GraphAutoma</code>:</p>"},{"location":"tutorials/items/core_mechanism/dynamic_topology/#4-lets-run-it","title":"4. Let's run it\u00b6","text":"<p>Let's create a instance of <code>TravelPlanner</code> and run it.</p>"},{"location":"tutorials/items/core_mechanism/dynamic_topology/#what-have-we-learnt","title":"What have we learnt?\u00b6","text":"<p>In this Travel Planning Agent example, we have demonstrated how to use Bridgic\u2019s dynamic topology mechanism to create workers for tools. The <code>GraphAutoma</code> class is implemented as a Dynamic Directed Graph (DDG) in Bridgic, to support topology change at runtime. The APIs that support dynamic change of topology include: <code>add_worker</code>, <code>add_func_as_worker</code>, <code>remove_worker</code>, and <code>add_dependency</code>.</p> <p>You might notice that interspersing these API calls within the worker implementation code can look a bit untidy. We plan to address this issue with a new feature in the near future.</p>"},{"location":"tutorials/items/core_mechanism/human_in_the_loop/","title":"Human-in-the-loop","text":"<p>Run the following <code>pip</code> command to make sure the 'openai' integration is installed.</p> <pre>pip install -U bridgic\npip install -U bridgic-llms-openai\n</pre> In\u00a0[\u00a0]: Copied! <pre>import os\n\n# Get the API base, API key and model name.\n_api_key = os.environ.get(\"OPENAI_API_KEY\")\n_api_base = os.environ.get(\"OPENAI_API_BASE\")\n_model_name = os.environ.get(\"OPENAI_MODEL_NAME\")\n\nfrom pydantic import BaseModel, Field\nfrom bridgic.core.automa import GraphAutoma, worker, Snapshot\nfrom bridgic.core.automa.args import From\nfrom bridgic.core.automa.interaction import Event, Feedback, FeedbackSender, InteractionFeedback, InteractionException\nfrom bridgic.core.model.types import Message, Role\nfrom bridgic.core.model.protocols import PydanticModel\nfrom bridgic.llms.openai import OpenAILlm\n</pre> import os  # Get the API base, API key and model name. _api_key = os.environ.get(\"OPENAI_API_KEY\") _api_base = os.environ.get(\"OPENAI_API_BASE\") _model_name = os.environ.get(\"OPENAI_MODEL_NAME\")  from pydantic import BaseModel, Field from bridgic.core.automa import GraphAutoma, worker, Snapshot from bridgic.core.automa.args import From from bridgic.core.automa.interaction import Event, Feedback, FeedbackSender, InteractionFeedback, InteractionException from bridgic.core.model.types import Message, Role from bridgic.core.model.protocols import PydanticModel from bridgic.llms.openai import OpenAILlm In\u00a0[25]: Copied! <pre># Set the LLM\nllm = OpenAILlm(api_base=_api_base, api_key=_api_key, timeout=10)\n\nclass CodeBlock(BaseModel):\n    code: str = Field(description=\"The code to be executed.\")\n\nclass CodeAssistant(GraphAutoma):\n    @worker(is_start=True)\n    async def generate_code(self, user_requirement: str):\n        response = await llm.astructured_output(\n            model=_model_name,\n            messages=[\n                Message.from_text(text=f\"You are a programming assistant. Please generate code according to the user's requirements.\", role=Role.SYSTEM),\n                Message.from_text(text=user_requirement, role=Role.USER),\n            ],\n            constraint=PydanticModel(model=CodeBlock)\n        )\n        return response.code\n\n    @worker(dependencies=[\"generate_code\"])\n    async def ask_to_run_code(self, code: str):\n        event = Event(event_type=\"can_run_code\", data=code)\n        feedback = await self.request_feedback_async(event)\n        return feedback.data\n        \n    @worker(dependencies=[\"ask_to_run_code\"])\n    async def output_result(self, feedback: str, code: str = From(\"generate_code\")):\n        code = code.strip(\"```python\").strip(\"```\")\n        if feedback == \"yes\":\n            print(f\"- - - - - - Result - - - - - -\")\n            exec(code)\n            print(f\"- - - - - - End - - - - - -\")\n        else:\n            print(f\"This code was rejected for execution. In response to the requirements, I have generated the following code:\\n```python\\n{code}\\n```\")\n</pre> # Set the LLM llm = OpenAILlm(api_base=_api_base, api_key=_api_key, timeout=10)  class CodeBlock(BaseModel):     code: str = Field(description=\"The code to be executed.\")  class CodeAssistant(GraphAutoma):     @worker(is_start=True)     async def generate_code(self, user_requirement: str):         response = await llm.astructured_output(             model=_model_name,             messages=[                 Message.from_text(text=f\"You are a programming assistant. Please generate code according to the user's requirements.\", role=Role.SYSTEM),                 Message.from_text(text=user_requirement, role=Role.USER),             ],             constraint=PydanticModel(model=CodeBlock)         )         return response.code      @worker(dependencies=[\"generate_code\"])     async def ask_to_run_code(self, code: str):         event = Event(event_type=\"can_run_code\", data=code)         feedback = await self.request_feedback_async(event)         return feedback.data              @worker(dependencies=[\"ask_to_run_code\"])     async def output_result(self, feedback: str, code: str = From(\"generate_code\")):         code = code.strip(\"```python\").strip(\"```\")         if feedback == \"yes\":             print(f\"- - - - - - Result - - - - - -\")             exec(code)             print(f\"- - - - - - End - - - - - -\")         else:             print(f\"This code was rejected for execution. In response to the requirements, I have generated the following code:\\n```python\\n{code}\\n```\") <p>In the <code>ask_to_run_code()</code> method of <code>CodeAssistant</code>, we use <code>request_feedback_async()</code> to send an Event to the human user and expect to receive a feedback. To handle this Event, the corresponding logic needs to be registered with the automa, like this:</p> In\u00a0[26]: Copied! <pre># Handle can_run_code event\ndef can_run_code_handler(event: Event, feedback_sender: FeedbackSender):\n    print(f\"Can I run this code now to verify if it's correct?\")\n    print(f\"```python\\n{event.data}\\n```\")\n    res = input(\"Please input your answer (yes/no): \")\n    if res in [\"yes\", \"no\"]:\n        feedback_sender.send(Feedback(data=res))\n    else:\n        print(\"Invalid input. Please input yes or no.\")\n        feedback_sender.send(Feedback(data=\"no\"))\n\n# register can_run_code event handler to `CodeAssistant` automa\ncode_assistant = CodeAssistant()\ncode_assistant.register_event_handler(\"can_run_code\", can_run_code_handler)\n</pre> # Handle can_run_code event def can_run_code_handler(event: Event, feedback_sender: FeedbackSender):     print(f\"Can I run this code now to verify if it's correct?\")     print(f\"```python\\n{event.data}\\n```\")     res = input(\"Please input your answer (yes/no): \")     if res in [\"yes\", \"no\"]:         feedback_sender.send(Feedback(data=res))     else:         print(\"Invalid input. Please input yes or no.\")         feedback_sender.send(Feedback(data=\"no\"))  # register can_run_code event handler to `CodeAssistant` automa code_assistant = CodeAssistant() code_assistant.register_event_handler(\"can_run_code\", can_run_code_handler) <p>Now let's use it!</p> In\u00a0[27]: Copied! <pre>await code_assistant.arun(user_requirement=\"Please write a function to print 'Hello, World!' and run it.\")\n</pre> await code_assistant.arun(user_requirement=\"Please write a function to print 'Hello, World!' and run it.\") <pre>Can I run this code now to verify if it's correct?\n```python\ndef greet():\n    print('Hello, World!')\n\ngreet()\n```\n- - - - - - Result - - - - - -\nHello, World!\n- - - - - - End - - - - - -\n</pre> <p>In the above example, Bridgic wrap the message sent to the human user in an <code>Event</code> and he message received from the user in a <code>FeedBack</code>.</p> <ul> <li><code>Event</code> contains three fields:<ul> <li><code>event_type</code>: A string. The event type is used to identify the registered event handler.</li> <li><code>timestamp</code>: A Python datetime object. The timestamp of the event. The default is <code>datetime.now()</code>.</li> <li><code>data</code>: The data attached to the event.</li> </ul> </li> <li><code>FeedBack</code> contains one field:<ul> <li><code>data</code>: The data attached to the feedback.</li> </ul> </li> </ul> <p><code>request_feedback_async()</code> send an event to the user and request for a feedback. This method call will block the caller until the feedback is received. However, thanks to Python\u2019s asynchronous event loop mechanism, other automas running on the same main thread will not be blocked.</p> <p>The registered event handler must be defined as type of <code>EventHandlerType</code>.  Here it should be a function that takes an <code>Event</code> and a <code>FeedbackSender</code> as arguments.</p> In\u00a0[\u00a0]: Copied! <pre>class MessageNotifier(GraphAutoma):\n    @worker(is_start=True)\n    async def notify(self, user_input: int, notify_int: int):\n        print(f\"Loop from 1 to {user_input}\")\n        for i in range(1, user_input + 1):\n            if i == notify_int:\n                event = Event(event_type=\"message_notification\", data=f\"Loop {i} times\")\n                self.post_event(event)\n\ndef message_notification_handler(event: Event):\n    print(f'!! Now count to {event.data}. !!')\n\nmessage_notifier = MessageNotifier()\nmessage_notifier.register_event_handler(\"message_notification\", message_notification_handler)\nawait message_notifier.arun(user_input=10, notify_int=5)\n        \n</pre> class MessageNotifier(GraphAutoma):     @worker(is_start=True)     async def notify(self, user_input: int, notify_int: int):         print(f\"Loop from 1 to {user_input}\")         for i in range(1, user_input + 1):             if i == notify_int:                 event = Event(event_type=\"message_notification\", data=f\"Loop {i} times\")                 self.post_event(event)  def message_notification_handler(event: Event):     print(f'!! Now count to {event.data}. !!')  message_notifier = MessageNotifier() message_notifier.register_event_handler(\"message_notification\", message_notification_handler) await message_notifier.arun(user_input=10, notify_int=5)          <pre>Loop from 1 to 10\n!! Now count to Loop 5 times. !!\n</pre> In\u00a0[28]: Copied! <pre>import os\nimport tempfile\nfrom httpx import delete\nfrom pydantic import BaseModel\nfrom datetime import datetime\nfrom bridgic.core.automa import GraphAutoma, worker, Snapshot\nfrom bridgic.core.automa.args import From\nfrom bridgic.core.automa.interaction import Event, InteractionFeedback, InteractionException\n\nclass ReimbursementRecord(BaseModel):\n    request_id: int\n    employee_id: int\n    employee_name: str\n    reimbursement_month: str\n    reimbursement_amount: float\n    description: str\n    created_at: datetime\n    updated_at: datetime\n\nclass AuditResult(BaseModel):\n    request_id: int\n    passed: bool\n    audit_reason: str\n\nclass ReimbursementWorkflow(GraphAutoma):\n    @worker(is_start=True)\n    async def load_record(self, request_id: int):\n        \"\"\"\n        The reimbursement workflow can be triggered by the OA system \u2014 for instance, when an employee submits a new reimbursement request. Each request is uniquely identified by a `request_id`, which is then used to retrieve the corresponding reimbursement record from the database. \n        \"\"\"\n        # Load the data from database.\n        return await self.load_record_from_database(request_id)\n    \n    @worker(dependencies=[\"load_record\"])\n    async def audit_by_rules(self, record: ReimbursementRecord):\n        \"\"\"\n        This method simulates the logic for automatically determining whether a reimbursement request complies with business rules.  \n\n        Typical reasons for a reimbursement request failing the audit include:\n\n        - Unusually large individual amounts\n        - Excessive total amounts within a month\n        - Expenses that do not meet reimbursement policies\n        - Missing or invalid supporting documents\n        - Duplicate submissions\n        - Other non-compliant cases\n        \"\"\"\n        if record.reimbursement_amount &gt; 2500:\n            return AuditResult(\n                request_id=record.request_id,\n                passed=False,\n                audit_reason=\"The reimbursement amount {record.reimbursement_amount} exceeds the limit of 2500.\"\n            )\n        # TODO: Add more audit rules here.\n        ...\n\n        return AuditResult(\n            request_id=record.request_id,\n            passed=True,\n            audit_reason=\"The reimbursement request passed the audit.\"\n        )\n    \n    @worker(dependencies=[\"audit_by_rules\"])\n    async def execute_payment(self, result: AuditResult, record: ReimbursementRecord = From(\"load_record\")):\n        if not result.passed:\n            print(f\"The reimbursement request {record.request_id} failed the audit. Reason: {result.audit_reason}\")\n            return False\n        \n        # The reimbursement request {record.request_id} has passed the audit rules. Requesting approval from the manager...\n        # human-in-the-loop: request approval from the manager.\n        event = Event(\n            event_type=\"request_approval\",\n            data={\n                \"reimbursement_record\": record,\n                \"audit_result\": result\n            }        \n        )\n        feedback: InteractionFeedback = self.interact_with_human(event)\n        if feedback.data == \"yes\":\n            await self.lanuch_payment_transaction(record.request_id)\n            print(f\"The reimbursement request {record.request_id} has been approved. Payment transaction launched.\")\n            return True\n\n        print(f\"!!!The reimbursement request {record.request_id} has been rejected. Payment transaction not launched.\\nRejection info:\\n {feedback.data}\")\n        return False\n\n    async def load_record_from_database(self, request_id: int):\n        # Simulate a database query...\n        return ReimbursementRecord(\n            request_id=request_id,\n            employee_id=888888,\n            employee_name=\"John Doe\",\n            reimbursement_month=\"2025-10\",\n            reimbursement_amount=1024.00,\n            description=\"Hotel expenses for a business trip\",\n            created_at=datetime(2025, 10, 11, 10, 0, 0),\n            updated_at=datetime(2025, 10, 11, 10, 0, 0)\n        )\n    async def lanuch_payment_transaction(self, request_id: int):\n        # Simulate a payment execution...\n        ...\n</pre> import os import tempfile from httpx import delete from pydantic import BaseModel from datetime import datetime from bridgic.core.automa import GraphAutoma, worker, Snapshot from bridgic.core.automa.args import From from bridgic.core.automa.interaction import Event, InteractionFeedback, InteractionException  class ReimbursementRecord(BaseModel):     request_id: int     employee_id: int     employee_name: str     reimbursement_month: str     reimbursement_amount: float     description: str     created_at: datetime     updated_at: datetime  class AuditResult(BaseModel):     request_id: int     passed: bool     audit_reason: str  class ReimbursementWorkflow(GraphAutoma):     @worker(is_start=True)     async def load_record(self, request_id: int):         \"\"\"         The reimbursement workflow can be triggered by the OA system \u2014 for instance, when an employee submits a new reimbursement request. Each request is uniquely identified by a `request_id`, which is then used to retrieve the corresponding reimbursement record from the database.          \"\"\"         # Load the data from database.         return await self.load_record_from_database(request_id)          @worker(dependencies=[\"load_record\"])     async def audit_by_rules(self, record: ReimbursementRecord):         \"\"\"         This method simulates the logic for automatically determining whether a reimbursement request complies with business rules.            Typical reasons for a reimbursement request failing the audit include:          - Unusually large individual amounts         - Excessive total amounts within a month         - Expenses that do not meet reimbursement policies         - Missing or invalid supporting documents         - Duplicate submissions         - Other non-compliant cases         \"\"\"         if record.reimbursement_amount &gt; 2500:             return AuditResult(                 request_id=record.request_id,                 passed=False,                 audit_reason=\"The reimbursement amount {record.reimbursement_amount} exceeds the limit of 2500.\"             )         # TODO: Add more audit rules here.         ...          return AuditResult(             request_id=record.request_id,             passed=True,             audit_reason=\"The reimbursement request passed the audit.\"         )          @worker(dependencies=[\"audit_by_rules\"])     async def execute_payment(self, result: AuditResult, record: ReimbursementRecord = From(\"load_record\")):         if not result.passed:             print(f\"The reimbursement request {record.request_id} failed the audit. Reason: {result.audit_reason}\")             return False                  # The reimbursement request {record.request_id} has passed the audit rules. Requesting approval from the manager...         # human-in-the-loop: request approval from the manager.         event = Event(             event_type=\"request_approval\",             data={                 \"reimbursement_record\": record,                 \"audit_result\": result             }                 )         feedback: InteractionFeedback = self.interact_with_human(event)         if feedback.data == \"yes\":             await self.lanuch_payment_transaction(record.request_id)             print(f\"The reimbursement request {record.request_id} has been approved. Payment transaction launched.\")             return True          print(f\"!!!The reimbursement request {record.request_id} has been rejected. Payment transaction not launched.\\nRejection info:\\n {feedback.data}\")         return False      async def load_record_from_database(self, request_id: int):         # Simulate a database query...         return ReimbursementRecord(             request_id=request_id,             employee_id=888888,             employee_name=\"John Doe\",             reimbursement_month=\"2025-10\",             reimbursement_amount=1024.00,             description=\"Hotel expenses for a business trip\",             created_at=datetime(2025, 10, 11, 10, 0, 0),             updated_at=datetime(2025, 10, 11, 10, 0, 0)         )     async def lanuch_payment_transaction(self, request_id: int):         # Simulate a payment execution...         ... <p>This workflow, <code>ReimbursementWorkflow</code>, consists of three steps:</p> <ul> <li><code>load_record</code>: Loading the reimbursement record identified by a <code>request_id</code> from database.</li> <li><code>audit_by_rules</code>: Automatically audit the reimbursement request by predefined rules.</li> <li><code>execute_payment</code>: Requesting approval from the manager (a human user) after the reimbursement request has passed the audit rules.</li> </ul> <p>In the third step (<code>execute_payment</code>), calling <code>interact_with_human()</code> posts an event and pauses the workflow execution.</p> In\u00a0[29]: Copied! <pre>async def save_snapshot_to_database(snapshot: Snapshot):\n    # Simulate a database storage using temporary files.\n    temp_dir = tempfile.TemporaryDirectory()\n    bytes_file = os.path.join(temp_dir.name, \"reimbursement_workflow.bytes\")\n    version_file = os.path.join(temp_dir.name, \"reimbursement_workflow.version\")\n    with open(bytes_file, \"wb\") as f:\n        f.write(snapshot.serialized_bytes)\n    with open(version_file, \"w\") as f:\n        f.write(snapshot.serialization_version)\n\n    return {\n        \"bytes_file\": bytes_file,\n        \"version_file\": version_file,\n        \"temp_dir\": temp_dir,\n    }\n\n\nreimbursement_workflow = ReimbursementWorkflow()\ntry:\n    await reimbursement_workflow.arun(request_id=123456)\nexcept InteractionException as e:\n    # The `ReimbursementWorkflow` instance has been paused and serialized to a snapshot.\n    interaction_id = e.interactions[0].interaction_id\n    record = e.interactions[0].event.data[\"reimbursement_record\"]\n    # Save the snapshot to the database.\n    db_context = await save_snapshot_to_database(e.snapshot)\n    print(\"The `ReimbursementWorkflow` instance has been paused and serialized to a snapshot.\")\n    print(\"The snapshot has been persisted to database.\")\n</pre> async def save_snapshot_to_database(snapshot: Snapshot):     # Simulate a database storage using temporary files.     temp_dir = tempfile.TemporaryDirectory()     bytes_file = os.path.join(temp_dir.name, \"reimbursement_workflow.bytes\")     version_file = os.path.join(temp_dir.name, \"reimbursement_workflow.version\")     with open(bytes_file, \"wb\") as f:         f.write(snapshot.serialized_bytes)     with open(version_file, \"w\") as f:         f.write(snapshot.serialization_version)      return {         \"bytes_file\": bytes_file,         \"version_file\": version_file,         \"temp_dir\": temp_dir,     }   reimbursement_workflow = ReimbursementWorkflow() try:     await reimbursement_workflow.arun(request_id=123456) except InteractionException as e:     # The `ReimbursementWorkflow` instance has been paused and serialized to a snapshot.     interaction_id = e.interactions[0].interaction_id     record = e.interactions[0].event.data[\"reimbursement_record\"]     # Save the snapshot to the database.     db_context = await save_snapshot_to_database(e.snapshot)     print(\"The `ReimbursementWorkflow` instance has been paused and serialized to a snapshot.\")     print(\"The snapshot has been persisted to database.\") <pre>The `ReimbursementWorkflow` instance has been paused and serialized to a snapshot.\nThe snapshot has been persisted to database.\n</pre> <p>When the <code>arun</code> method of the automa instance is called, an <code>InteractionException</code> will be raised as a result of invoking <code>interact_with_human()</code>.</p> <p>An <code>InteractionException</code> contains two fields:</p> <ul> <li><code>interactions</code>: A list of <code>Interaction</code>s, each <code>Interaction</code> containing an <code>interaction_id</code> and an <code>event</code>.</li> <li><code>snapshot</code>: a <code>Snapshot</code> instance, representing the Automa's current state serialized in bytes.</li> </ul> <p>Then the workflow, <code>ReimbursementWorkflow</code>, pauses, and the snapshot corresponding to the interaction is persisted in the database for later recovery.</p> In\u00a0[\u00a0]: Copied! <pre>async def load_snapshot_from_database(db_context):\n    # Simulate a database query using temporary files.\n    bytes_file = db_context[\"bytes_file\"]\n    version_file = db_context[\"version_file\"]\n    temp_dir = db_context[\"temp_dir\"]\n\n    with open(bytes_file, \"rb\") as f:\n        serialized_bytes = f.read()\n    with open(version_file, \"r\") as f:\n        serialization_version = f.read()\n    snapshot = Snapshot(\n        serialized_bytes=serialized_bytes, \n        serialization_version=serialization_version\n    )\n    return snapshot\n\nprint(\"Waiting for the manager's approval (It may take long time) ...\")\nhuman_feedback = input(\n    \"\\n\"\n    \"---------- Message to User ------------\\n\"\n    \"A reimbursement request has been submitted and audited by the system.\\n\"\n    \"Please check the details and give your approval or rejection.\\n\"\n\n    \"Reimbursement Request Details:\\n\"\n    f\"\\n{record.model_dump_json(indent=4)}\\n\"\n    \"If you approve the request, please input 'yes'.\\n\"\n    \"Otherwise, please input 'no' or the reason for rejection.\\n\"\n    \"Your input: \"\n    )\n\n# Load the snapshot from the database.\nsnapshot = await load_snapshot_from_database(db_context)\n# Deserialize the `ReimbursementWorkflow` instance from the snapshot.\nreimbursement_workflow = ReimbursementWorkflow.load_from_snapshot(snapshot)\nprint(\"-------------------------------------\\n\")\nprint(\"The `ReimbursementWorkflow` instance has been deserialized and loaded from the snapshot. It will resume to run immediately...\")\nfeedback = InteractionFeedback(\n    interaction_id=interaction_id,\n    data=human_feedback\n)\nawait reimbursement_workflow.arun(feedback_data=feedback)\n</pre> async def load_snapshot_from_database(db_context):     # Simulate a database query using temporary files.     bytes_file = db_context[\"bytes_file\"]     version_file = db_context[\"version_file\"]     temp_dir = db_context[\"temp_dir\"]      with open(bytes_file, \"rb\") as f:         serialized_bytes = f.read()     with open(version_file, \"r\") as f:         serialization_version = f.read()     snapshot = Snapshot(         serialized_bytes=serialized_bytes,          serialization_version=serialization_version     )     return snapshot  print(\"Waiting for the manager's approval (It may take long time) ...\") human_feedback = input(     \"\\n\"     \"---------- Message to User ------------\\n\"     \"A reimbursement request has been submitted and audited by the system.\\n\"     \"Please check the details and give your approval or rejection.\\n\"      \"Reimbursement Request Details:\\n\"     f\"\\n{record.model_dump_json(indent=4)}\\n\"     \"If you approve the request, please input 'yes'.\\n\"     \"Otherwise, please input 'no' or the reason for rejection.\\n\"     \"Your input: \"     )  # Load the snapshot from the database. snapshot = await load_snapshot_from_database(db_context) # Deserialize the `ReimbursementWorkflow` instance from the snapshot. reimbursement_workflow = ReimbursementWorkflow.load_from_snapshot(snapshot) print(\"-------------------------------------\\n\") print(\"The `ReimbursementWorkflow` instance has been deserialized and loaded from the snapshot. It will resume to run immediately...\") feedback = InteractionFeedback(     interaction_id=interaction_id,     data=human_feedback ) await reimbursement_workflow.arun(feedback_data=feedback) <pre>Waiting for the manager's approval (It may take long time) ...\n-------------------------------------\n\nThe `ReimbursementWorkflow` instance has been deserialized and loaded from the snapshot. It will resume to run immediately...\nThe reimbursement request 123456 has been approved. Payment transaction launched.\n</pre> <p>After an extended period, the user may complete the approval interaction for the reimbursement workflow. The system then retrieves the serialized snapshot from the database and deserializes it into an instance of the <code>ReimbursementWorkflow</code> class. Subsequently, the user\u2019s decision \u2014 either approval or rejection \u2014 is wrapped into an <code>InteractionFeedback</code> object, and the arun method of the automa is invoked again to resume the workflow execution from the previously paused state.</p> <p>When facing a situation that requires feedback but the waiting time is uncertain, this mechanism saves the current state and re-enters when the right moment comes in the future. This not only enables the system to release resources that would otherwise be occupied for a long time, but also allows it to be awakened at an appropriate time.</p>"},{"location":"tutorials/items/core_mechanism/human_in_the_loop/#human-in-the-loop","title":"Human-in-the-loop\u00b6","text":"<p>When building workflows or agents with Bridgic, developers can seamlessly integrate human-in-the-loop interactions into the execution flow. At any point, the system can pause its automated process to request human input \u2014 such as approval, verification, or additional instructions \u2014 and wait for a response. Once the human feedback is provided, the workflow or agent resumes execution from the point of interruption, adapting its behavior based on the new input. Bridgic ensures that the entire process, including paused and resumed states, can be reliably serialized and deserialized for persistence and recovery.</p>"},{"location":"tutorials/items/core_mechanism/human_in_the_loop/#interaction-scenarios","title":"Interaction Scenarios\u00b6","text":"<p>Let's go through a few simple examples to understand this process. Before that, let's set up the running environment.</p>"},{"location":"tutorials/items/core_mechanism/human_in_the_loop/#programming-assistant","title":"Programming assistant\u00b6","text":"<p>During the development of a programming assistant, it can be designed to automatically execute and verify the code it generates. However, since program execution consumes system resources, the user must decide whether to grant permission for the assistant to run the code.</p> <p>Let's achieve it with Bridgic. The source code can be downloaded in GitHub. The steps are as follows:</p> <ol> <li>Generate code based on user requirements.</li> <li>Ask the user if it is allowed to execute the generated code.</li> <li>Output result.</li> </ol>"},{"location":"tutorials/items/core_mechanism/human_in_the_loop/#counting-notifier","title":"Counting notifier\u00b6","text":"<p>Sometimes, it may be necessary to post an event without expecting any feedback, for example, message notifications or progress updates. At this point, we call the <code>post_event()</code> method and register a event handler of type <code>EventHandlerType</code> to process the event. Here the event handler should be a function that takes only an <code>Event</code> as an argument</p> <p>For example, a counting notifier is implemented to count from 1 up to the number specified by the <code>user_input</code> argument. The user can also specify which number (<code>notify_int</code>) should trigger a reminder.</p>"},{"location":"tutorials/items/core_mechanism/human_in_the_loop/#reimbursement-workflow","title":"Reimbursement Workflow\u00b6","text":"<p>In certain scenarios, it may be necessary to wait for feedback for a long time after triggering an event. However, keeping the system in a waiting state would result in unnecessary resource consumption.</p> <p>Bridgic provides a powerful <code>interact_with_human</code> mechanism for interruption recovery in this situation. This allows the workflow or agent to pause and persist its current execution state when such an event occur, wait for feedback, and then resume execution.</p> <p>Let's implement a reimbursement workflow that is automatically triggered by the enterprise's OA system and requires approval before the reimbursement can be completed. The source code can be downloaded in GitHub.</p>"},{"location":"tutorials/items/core_mechanism/human_in_the_loop/#what-have-we-learnt","title":"What have we learnt?\u00b6","text":"<p>Bridgic provides flexible support for any form of human-in-the-loop interaction:</p> <ul> <li><code>request_feedback_async</code>: Used when the event must return feedback before the program can proceed. The program remains blocked until feedback is received.</li> <li><code>post_event</code>: Used when you just want to notify or trigger an event without expecting any feedback. The main program never blocks.</li> <li><code>interact_with_human</code>: Used when feedback is required but may arrive much later. The program is suspended and persisted, and resumes immediately when feedback becomes available.</li> </ul>"},{"location":"tutorials/items/core_mechanism/modularity/","title":"Modularity","text":"In\u00a0[\u00a0]: Copied! <pre>import os\n\n# Set the API base and key.\n_api_key = os.environ.get(\"OPENAI_API_KEY\")\n_api_base = os.environ.get(\"OPENAI_API_BASE\")\n_model_name = os.environ.get(\"OPENAI_MODEL_NAME\")\n\n# Import the necessary modules.\nfrom typing import List\nfrom pydantic import BaseModel, Field\nfrom bridgic.core.automa import GraphAutoma, worker\nfrom bridgic.core.model.types import Message, Role\nfrom bridgic.core.model.protocols import PydanticModel\nfrom bridgic.llms.openai import OpenAILlm\n\nllm = OpenAILlm(api_base=_api_base, api_key=_api_key, timeout=30)\n</pre> import os  # Set the API base and key. _api_key = os.environ.get(\"OPENAI_API_KEY\") _api_base = os.environ.get(\"OPENAI_API_BASE\") _model_name = os.environ.get(\"OPENAI_MODEL_NAME\")  # Import the necessary modules. from typing import List from pydantic import BaseModel, Field from bridgic.core.automa import GraphAutoma, worker from bridgic.core.model.types import Message, Role from bridgic.core.model.protocols import PydanticModel from bridgic.llms.openai import OpenAILlm  llm = OpenAILlm(api_base=_api_base, api_key=_api_key, timeout=30) In\u00a0[\u00a0]: Copied! <pre>class Outline(BaseModel):\n    outline: List[str] = Field(description=\"The outline of the use input to write report\")\n\nclass OutlineWriter(GraphAutoma):\n    @worker(is_start=True)\n    async def pre_query(self, user_input: str):  # Receive the user's input and preprocess query\n        return user_input\n\n    @worker(dependencies=[\"pre_query\"], is_output=True)\n    async def topic_split(self, query: str):\n        response: Outline = await llm.astructured_output(\n            model=_model_name,\n            messages=[\n                Message.from_text(text=\"Write a sample report outline within 3 topics based on the user's input\", role=Role.SYSTEM),\n                Message.from_text(text=query, role=Role.USER,),\n            ],\n            constraint=PydanticModel(model=Outline)\n        )\n        return response\n</pre> class Outline(BaseModel):     outline: List[str] = Field(description=\"The outline of the use input to write report\")  class OutlineWriter(GraphAutoma):     @worker(is_start=True)     async def pre_query(self, user_input: str):  # Receive the user's input and preprocess query         return user_input      @worker(dependencies=[\"pre_query\"], is_output=True)     async def topic_split(self, query: str):         response: Outline = await llm.astructured_output(             model=_model_name,             messages=[                 Message.from_text(text=\"Write a sample report outline within 3 topics based on the user's input\", role=Role.SYSTEM),                 Message.from_text(text=query, role=Role.USER,),             ],             constraint=PydanticModel(model=Outline)         )         return response In\u00a0[\u00a0]: Copied! <pre>class ReportWriter(GraphAutoma):\n    @worker(dependencies=[\"outline_writer\"])\n    async def write_report(self, outline: Outline):\n        outline_str = \"\\n\".join(outline.outline)\n        print(f'- - - - - Outline - - - - -')\n        print(outline_str)\n        print(f'- - - - - End - - - - -\\n')\n        \n        response = await llm.achat(\n            model=_model_name,\n            messages=[\n                Message.from_text(text=\"write a sample report based on the user's input and strictly follow the outline\", role=Role.SYSTEM),\n                Message.from_text(text=f\"{outline_str}.\", role=Role.USER),\n            ],\n        )\n        print(f'- - - - - Report - - - - -')\n        print(response.message.content)\n        print(f'- - - - - End - - - - -\\n')\n        return response.message.content\n\nreport_writer = ReportWriter()\nreport_writer.add_worker(\n    key=\"outline_writer\",\n    worker=OutlineWriter(),\n    is_start=True\n)\n</pre> class ReportWriter(GraphAutoma):     @worker(dependencies=[\"outline_writer\"])     async def write_report(self, outline: Outline):         outline_str = \"\\n\".join(outline.outline)         print(f'- - - - - Outline - - - - -')         print(outline_str)         print(f'- - - - - End - - - - -\\n')                  response = await llm.achat(             model=_model_name,             messages=[                 Message.from_text(text=\"write a sample report based on the user's input and strictly follow the outline\", role=Role.SYSTEM),                 Message.from_text(text=f\"{outline_str}.\", role=Role.USER),             ],         )         print(f'- - - - - Report - - - - -')         print(response.message.content)         print(f'- - - - - End - - - - -\\n')         return response.message.content  report_writer = ReportWriter() report_writer.add_worker(     key=\"outline_writer\",     worker=OutlineWriter(),     is_start=True ) <p>Now, let's run it!</p> In\u00a0[19]: Copied! <pre>await report_writer.arun(user_input=\"What is an agent?\")\n</pre> await report_writer.arun(user_input=\"What is an agent?\") <pre>- - - - - Outline - - - - -\nDefinition of an Agent\nTypes of Agents in Different Contexts\nRole and Function of Agents in AI and Automation\n- - - - - End - - - - -\n\n- - - - - Report - - - - -\n**Report: Definition of an Agent, Types of Agents in Different Contexts, and Role and Function of Agents in AI and Automation**\n\n---\n\n**1. Definition of an Agent**\n\nAn *agent* is an autonomous entity that perceives its environment through sensors and acts upon that environment using actuators to achieve specific goals. In a broad sense, an agent is capable of making decisions based on its internal state and external inputs, adapting its behavior over time in response to changing conditions. The concept of an agent is foundational in artificial intelligence (AI), robotics, and automated systems. It operates independently or in coordination with other agents, following predefined rules, learning from experience, or using reasoning mechanisms to perform tasks efficiently.\n\nIn computational terms, an agent typically consists of:\n- **Perception**: Sensing and interpreting environmental inputs.\n- **Reasoning/Decision-making**: Processing information to determine the best course of action.\n- **Action**: Executing decisions through physical or digital means.\n\nAgents can be simple (e.g., a thermostat adjusting temperature) or complex (e.g., AI-driven trading bots in financial markets).\n\n---\n\n**2. Types of Agents in Different Contexts**\n\nAgents vary significantly depending on the domain or context in which they operate. Below are key types categorized by application areas:\n\n**a. In Artificial Intelligence (AI):**\n- **Simple Reflex Agents**: Make decisions based solely on current percept. Example: A thermostat that turns on the heater when the room temperature drops below a threshold.\n- **Model-Based Reflex Agents**: Use an internal model of the world to make decisions. Example: A self-driving car that predicts traffic patterns based on historical data.\n- **Goal-Based Agents**: Act to achieve specific goals. Example: A virtual assistant like Siri or Alexa that performs tasks such as setting reminders or sending messages.\n- **Utility-Based Agents**: Choose actions that maximize expected utility. Example: An AI recommending a product based on user preferences and potential profit.\n- **Learning Agents**: Improve performance over time through experience. Example: A recommendation engine that learns user behavior and adjusts suggestions.\n\n**b. In Robotics:**\n- **Autonomous Robots**: Operate independently in dynamic environments (e.g., drones, warehouse robots).\n- **Human-Robot Collaborative Agents**: Work alongside humans (e.g., robotic arms in manufacturing).\n- **Mobile Agents**: Move through physical environments to perform tasks (e.g., delivery robots).\n\n**c. In Software and Business Automation:**\n- **Business Process Agents**: Automate routine tasks like invoice processing or customer service responses.\n- **Workflow Agents**: Coordinate multiple tasks within a system (e.g., workflow management in CRM software).\n- **Customer Service Agents**: Chatbots that respond to user queries in real time.\n\n**d. In Distributed Systems:**\n- **Multi-Agent Systems (MAS)**: Groups of agents that interact to solve complex problems (e.g., traffic management, supply chain optimization).\n- **Swarm Agents**: Simulate collective behavior (e.g., ant colonies, drone swarms).\n\n---\n\n**3. Role and Function of Agents in AI and Automation**\n\nAgents play a pivotal role in advancing AI and automation by enabling intelligent, adaptive, and scalable systems. Their key functions include:\n\n- **Autonomy and Decision-Making**: Agents operate independently, making real-time decisions without constant human intervention, which is essential in dynamic environments such as autonomous vehicles or smart homes.\n\n- **Adaptability and Learning**: Through machine learning and reinforcement learning, agents evolve their strategies based on feedback, improving accuracy and efficiency over time.\n\n- **Efficiency and Scalability**: Agents can perform repetitive or time-consuming tasks quickly and consistently, reducing human workload and operational costs. For example, AI agents in customer service reduce response times and increase service availability.\n\n- **Interoperability and Integration**: Agents can communicate and collaborate across different platforms and systems, enabling seamless integration in enterprise environments (e.g., connecting ERP, CRM, and supply chain systems).\n\n- **Problem Solving and Optimization**: In complex domains like logistics or energy management, agents use algorithms to find optimal solutions, such as route planning or load balancing.\n\n- **Human-AI Collaboration**: Agents serve as intelligent interfaces between humans and machines, enhancing user experience by providing personalized, context-aware assistance.\n\nIn summary, agents are central to the development of intelligent systems that can perceive, reason, act, and learn\u2014making them indispensable in modern AI and automation ecosystems.\n\n---\n\n**Conclusion**\n\nThe concept of an agent bridges the gap between passive systems and intelligent, responsive automation. From simple rule-based tools to sophisticated AI-driven entities, agents are transforming how systems interact with their environments. As AI and automation continue to evolve, the design, deployment, and coordination of agents will remain a critical focus area across industries, driving innovation, efficiency, and human-machine collaboration.\n\n---  \n*End of Report*\n- - - - - End - - - - -\n\n</pre> <p>Great! We successfully reused <code>OutlineWriter</code> in <code>ReportWriter</code>.</p>"},{"location":"tutorials/items/core_mechanism/modularity/#modularity","title":"Modularity\u00b6","text":"<p>During the development process, we may have already had some individual automas. Now, we want to combine them to create a more powerful automa. An automa can also be added as a Worker to another Automa to achieve reuse.</p> <p>Let's understand it with a sample case.</p>"},{"location":"tutorials/items/core_mechanism/modularity/#report-writer","title":"Report Writer\u00b6","text":"<p>The typical process of report writing usually involves drafting an outline based on user input and then writing the report according to the outline. Now, let's first write an Automa for generating an outline, and then nest it within the Automa for writing the report.</p>"},{"location":"tutorials/items/core_mechanism/modularity/#1-initialize","title":"1. Initialize\u00b6","text":"<p>Initialize the runtime environment.</p>"},{"location":"tutorials/items/core_mechanism/modularity/#2-outline-automa","title":"2. Outline Automa\u00b6","text":"<p>Outline Automa drafts an outline based on user input. Output an <code>Outline</code> object for subsequent processing.</p>"},{"location":"tutorials/items/core_mechanism/modularity/#3-write-automa","title":"3. Write Automa\u00b6","text":"<p>We can reuse the <code>OutlineWriter</code> in <code>ReportWriter</code>.</p>"},{"location":"tutorials/items/core_mechanism/modularity/#what-have-we-learnt","title":"What have we learnt?\u00b6","text":"<p>we added an automa as a <code>Worker</code> to another <code>Automa</code> to achieve nested reuse. There are several points that need special attention during the reuse process:</p> <ol> <li>Worker Dependency: A <code>Worker</code> in an <code>Automa</code> can only depend on other workers within the same <code>Automa</code>, and cannot depend across different <code>Automa</code>.</li> <li>Routing: A <code>Worker</code> in an <code>Automa</code> can only route to other workers within the same <code>Automa</code>, and cannot route across different <code>Automa</code>.</li> <li>Human-in-the-loop: When a worker inside throws an event, it hands over this event to the automa agent for handling. At this point, the event handling functions of nested automa need to be registered to the outermost Automa.</li> </ol>"},{"location":"tutorials/items/core_mechanism/parameter_resolving/","title":"Parameter Resolving","text":"In\u00a0[\u00a0]: Copied! <pre>from typing import Tuple\nfrom bridgic.core.automa import GraphAutoma, worker\nfrom bridgic.core.automa.args import InOrder, ResultDispatchingRule\n</pre> from typing import Tuple from bridgic.core.automa import GraphAutoma, worker from bridgic.core.automa.args import InOrder, ResultDispatchingRule In\u00a0[2]: Copied! <pre>class ParallelProcessing(GraphAutoma):\n    @worker(is_start=True)\n    async def process_query_1(self, user_input: int) -&gt; int:\n        print(f\"Processing query 1 with input: {user_input}\")\n        return user_input * 2  # 2\n    \n    @worker(is_start=True)\n    async def process_query_2(self, user_input: int) -&gt; int:\n        print(f\"Processing query 2 with input: {user_input}\")\n        return user_input * 3  # 6\n    \n    @worker(dependencies=[\"process_query_1\", \"process_query_2\"], is_output=True)\n    async def aggregate_results(self, result1: int, result2: int) -&gt; int:\n        print(f\"Aggregating: {result1} + {result2}\")\n        return result1 + result2  # 8\n</pre> class ParallelProcessing(GraphAutoma):     @worker(is_start=True)     async def process_query_1(self, user_input: int) -&gt; int:         print(f\"Processing query 1 with input: {user_input}\")         return user_input * 2  # 2          @worker(is_start=True)     async def process_query_2(self, user_input: int) -&gt; int:         print(f\"Processing query 2 with input: {user_input}\")         return user_input * 3  # 6          @worker(dependencies=[\"process_query_1\", \"process_query_2\"], is_output=True)     async def aggregate_results(self, result1: int, result2: int) -&gt; int:         print(f\"Aggregating: {result1} + {result2}\")         return result1 + result2  # 8  <p>Now let's run it with <code>InOrder</code> to distribute different values to the two start workers:</p> In\u00a0[\u00a0]: Copied! <pre>automa = ParallelProcessing()\nres = await automa.arun(user_input=InOrder([1, 2]))\nprint(res)\n</pre> automa = ParallelProcessing() res = await automa.arun(user_input=InOrder([1, 2])) print(res) <pre>Processing query 1 with input: 1\nProcessing query 2 with input: 2\nAggregating: 2 + 6\n8\n</pre> <p>Great! As you can see, <code>InOrder([1, 2])</code> distributed the values <code>1</code> and <code>2</code> to <code>process_query_1</code> and <code>process_query_2</code> respectively, based on the order of the declaration of start workers.</p> <p>Note: The length of the list/tuple in <code>InOrder()</code> must match the number of start workers that accept the corresponding parameter. Otherwise, an error will be raised.</p> In\u00a0[8]: Copied! <pre>class ResultDispatchingExample(GraphAutoma):\n    @worker(is_start=True, result_dispatching_rule=ResultDispatchingRule.AS_IS)\n    async def generate_data(self, user_input: int) -&gt; int:\n        return user_input\n    \n    @worker(dependencies=[\"generate_data\"], result_dispatching_rule=ResultDispatchingRule.IN_ORDER)\n    async def process_and_split(self, data: int) -&gt; Tuple[int, int]:\n        print(f\"Processing data: {data}\")\n        # Return a tuple that will be distributed to downstream workers\n        return data + 1, data + 2  # (2, 3)\n    \n    @worker(dependencies=[\"process_and_split\"])\n    async def worker_a(self, value: int) -&gt; int:\n        print(f\"Worker A received: {value}\")\n        return value * 10  # 20\n    \n    @worker(dependencies=[\"process_and_split\"])\n    async def worker_b(self, value: int) -&gt; int:\n        print(f\"Worker B received: {value}\")\n        return value * 20  # 60\n    \n    @worker(dependencies=[\"worker_a\", \"worker_b\"], is_output=True)\n    async def combine_results(self, result_a: int, result_b: int) -&gt; int:\n        print(f\"Combining: {result_a} + {result_b}\")\n        return result_a + result_b  # 80\n</pre> class ResultDispatchingExample(GraphAutoma):     @worker(is_start=True, result_dispatching_rule=ResultDispatchingRule.AS_IS)     async def generate_data(self, user_input: int) -&gt; int:         return user_input          @worker(dependencies=[\"generate_data\"], result_dispatching_rule=ResultDispatchingRule.IN_ORDER)     async def process_and_split(self, data: int) -&gt; Tuple[int, int]:         print(f\"Processing data: {data}\")         # Return a tuple that will be distributed to downstream workers         return data + 1, data + 2  # (2, 3)          @worker(dependencies=[\"process_and_split\"])     async def worker_a(self, value: int) -&gt; int:         print(f\"Worker A received: {value}\")         return value * 10  # 20          @worker(dependencies=[\"process_and_split\"])     async def worker_b(self, value: int) -&gt; int:         print(f\"Worker B received: {value}\")         return value * 20  # 60          @worker(dependencies=[\"worker_a\", \"worker_b\"], is_output=True)     async def combine_results(self, result_a: int, result_b: int) -&gt; int:         print(f\"Combining: {result_a} + {result_b}\")         return result_a + result_b  # 80  <p>When <code>process_and_split</code> returns <code>(2, 3)</code>, we want to distribute these elements to <code>worker_a</code> and <code>worker_b</code> respectively. Note that <code>result_dispatching_rule</code> should be set on the worker that produces the results, not on the workers that receive them.</p> <p>Now let's run it:</p> In\u00a0[9]: Copied! <pre>automa = ResultDispatchingExample()\nres = await automa.arun(user_input=1)\nprint(res)\n</pre> automa = ResultDispatchingExample() res = await automa.arun(user_input=1) print(res) <pre>Processing data: 1\nWorker A received: 2\nWorker B received: 3\nCombining: 20 + 60\n80\n</pre> <p>Perfect! As you can see:</p> <ol> <li><code>process_and_split</code> returned <code>(2, 3)</code> and had <code>result_dispatching_rule=ResultDispatchingRule.IN_ORDER</code>.</li> <li>The first value <code>2</code> was distributed to <code>worker_a</code> (the first downstream worker).</li> <li>The second value <code>3</code> was distributed to <code>worker_b</code> (the second downstream worker).</li> <li>Both workers processed their values and the results were combined.</li> </ol> <p>Important Notes:</p> <ul> <li>The worker that sets <code>result_dispatching_rule=ResultDispatchingRule.IN_ORDER</code> must return an iterable (tuple or list).</li> <li>The length of the returned iterable must match the number of downstream workers that directly depend on this worker.</li> <li>The distribution order follows the order in which the downstream workers are declared in the graph.</li> </ul> <p>There are two mode of <code>ResultDispatchingRule</code>:</p> <ul> <li>AS_IS: The result of the worker will be sent as a whole to each downstream worker that depends on it. This is the default behavior.</li> <li>IN_ORDER: The workers will distribute the current results in sequence to the corresponding workers one by one according to the order in which the downstream workers are declared or added.</li> </ul> In\u00a0[\u00a0]: Copied! <pre># Get the environment variables.\nimport os\n\n_api_key = os.environ.get(\"OPENAI_API_KEY\")\n_api_base = os.environ.get(\"OPENAI_API_BASE\")\n_model_name = os.environ.get(\"OPENAI_MODEL_NAME\")\n\n# Import the necessary packages.\nfrom pydantic import BaseModel, Field\nfrom typing import List, Dict, Tuple\nfrom bridgic.core.automa import GraphAutoma, worker\nfrom bridgic.core.automa.args import ArgsMappingRule\nfrom bridgic.core.model.types import Message, Role\nfrom bridgic.core.model.protocols import PydanticModel\nfrom bridgic.llms.openai import OpenAILlm, OpenAIConfiguration\n\nllm = OpenAILlm(  # the llm instance\n    api_base=_api_base,\n    api_key=_api_key,\n    timeout=5,\n    configuration=OpenAIConfiguration(model=_model_name),\n)\n</pre> # Get the environment variables. import os  _api_key = os.environ.get(\"OPENAI_API_KEY\") _api_base = os.environ.get(\"OPENAI_API_BASE\") _model_name = os.environ.get(\"OPENAI_MODEL_NAME\")  # Import the necessary packages. from pydantic import BaseModel, Field from typing import List, Dict, Tuple from bridgic.core.automa import GraphAutoma, worker from bridgic.core.automa.args import ArgsMappingRule from bridgic.core.model.types import Message, Role from bridgic.core.model.protocols import PydanticModel from bridgic.llms.openai import OpenAILlm, OpenAIConfiguration  llm = OpenAILlm(  # the llm instance     api_base=_api_base,     api_key=_api_key,     timeout=5,     configuration=OpenAIConfiguration(model=_model_name), ) <p>Now let's implement this query expansion. We assume that the user query we receive is in JSON format. It contains three keys:</p> <ol> <li><code>id</code>: A string that indicates who inputs the query.</li> <li><code>query</code>: A string in the form of <code>Q: user_query</code> representing the question input by the user.</li> <li><code>date</code>: The time when the user entered the query.</li> </ol> In\u00a0[\u00a0]: Copied! <pre>query_obj = {\n    \"id\": \"user_1\",\n    \"query\": \"Q: What new developments have there been in RAG in the past year?\",\n    \"date\": \"2025-09-30\"\n}\n</pre> query_obj = {     \"id\": \"user_1\",     \"query\": \"Q: What new developments have there been in RAG in the past year?\",     \"date\": \"2025-09-30\" } <p>Furthermore, we define that when the model completes entity extraction and query expansion, it returns the result in a Pydantic data structure.</p> In\u00a0[5]: Copied! <pre>class EntityList(BaseModel):  # The expected format of the model output in the extract_entity worker\n    entities: List[str] = Field(description=\"All entities in the input.\")\n\nclass QueryList(BaseModel):  # The expected format of the model output in the expand_query worker\n    queries: List[str] = Field(description=\"All queries in the input.\")\n</pre> class EntityList(BaseModel):  # The expected format of the model output in the extract_entity worker     entities: List[str] = Field(description=\"All entities in the input.\")  class QueryList(BaseModel):  # The expected format of the model output in the expand_query worker     queries: List[str] = Field(description=\"All queries in the input.\") <p>Next, let's complete the three steps of query expansion to achieve our goal:</p> In\u00a0[\u00a0]: Copied! <pre>class QueryExpansion(GraphAutoma):\n    @worker(is_start=True)\n    async def pre_query(self, query_obj: Dict):  # Receive the user's input and preprocess query\n        query = query_obj[\"query\"].split(\":\")[1].strip()  \n        return query\n\n    @worker(is_start=True)\n    async def pre_date(self, query_obj: Dict):  # Receive the user's input and preprocess date\n        date = query_obj[\"date\"]\n        return date\n\n    @worker(dependencies=[\"pre_query\", \"pre_date\"], args_mapping_rule=ArgsMappingRule.AS_IS)\n    async def extract_entity(self, query: str, date: str):  # Extract the entity information from the question, get entity information.\n        response: EntityList = await llm.astructured_output(  \n            constraint=PydanticModel(model=EntityList),\n            messages=[\n                Message.from_text(text=\"extract the entity information from the given query\", role=Role.SYSTEM),\n                Message.from_text(text=query, role=Role.USER,),\n            ]\n        )\n        return query, response.entities, date\n\n    @worker(dependencies=[\"extract_entity\"], is_output=True, args_mapping_rule=ArgsMappingRule.AS_IS)\n    async def expand_query(self, query_meta: Tuple[str, List[str]]):  # Expand and obtain multiple queries.\n        query, entities, date = query_meta\n        task_input = f\"Query: {query}\\nEntities: {entities}\"\n        response: QueryList = await llm.astructured_output(\n            constraint=PydanticModel(model=QueryList),\n            messages=[\n                Message.from_text(text=f\"Centered around the given entities and given date {date}, expand the query to obtain multiple queries\", role=Role.SYSTEM),\n                Message.from_text(text=task_input, role=Role.USER,),\n            ]\n        )\n        return response.queries\n</pre> class QueryExpansion(GraphAutoma):     @worker(is_start=True)     async def pre_query(self, query_obj: Dict):  # Receive the user's input and preprocess query         query = query_obj[\"query\"].split(\":\")[1].strip()           return query      @worker(is_start=True)     async def pre_date(self, query_obj: Dict):  # Receive the user's input and preprocess date         date = query_obj[\"date\"]         return date      @worker(dependencies=[\"pre_query\", \"pre_date\"], args_mapping_rule=ArgsMappingRule.AS_IS)     async def extract_entity(self, query: str, date: str):  # Extract the entity information from the question, get entity information.         response: EntityList = await llm.astructured_output(               constraint=PydanticModel(model=EntityList),             messages=[                 Message.from_text(text=\"extract the entity information from the given query\", role=Role.SYSTEM),                 Message.from_text(text=query, role=Role.USER,),             ]         )         return query, response.entities, date      @worker(dependencies=[\"extract_entity\"], is_output=True, args_mapping_rule=ArgsMappingRule.AS_IS)     async def expand_query(self, query_meta: Tuple[str, List[str]]):  # Expand and obtain multiple queries.         query, entities, date = query_meta         task_input = f\"Query: {query}\\nEntities: {entities}\"         response: QueryList = await llm.astructured_output(             constraint=PydanticModel(model=QueryList),             messages=[                 Message.from_text(text=f\"Centered around the given entities and given date {date}, expand the query to obtain multiple queries\", role=Role.SYSTEM),                 Message.from_text(text=task_input, role=Role.USER,),             ]         )         return response.queries <p>Let's run it!</p> In\u00a0[7]: Copied! <pre>query_expansion = QueryExpansion()\nawait query_expansion.arun(query_obj)\n</pre> query_expansion = QueryExpansion() await query_expansion.arun(query_obj) Out[7]: <pre>['What are the latest advancements in Retrieval-Augmented Generation (RAG) technology as of 2025?',\n 'What new developments have emerged in RAG systems over the past 12 months?',\n 'How have RAG implementations evolved in the last year?',\n 'What innovations in RAG have been introduced between 2024 and 2025?',\n 'What are the key breakthroughs in RAG technology in 2025?',\n 'What new features or improvements have been added to RAG models in the past year?',\n 'How has the performance of RAG systems improved in the last 12 months?',\n 'What are the most recent trends and developments in RAG research and deployment?',\n 'What new techniques have been introduced in RAG to improve accuracy and efficiency in 2025?',\n 'What are the major updates in RAG frameworks and tools from 2024 to 2025?']</pre> <p>Great! We have successfully completed the small module for query expansion.</p> In\u00a0[\u00a0]: Copied! <pre>@worker(dependencies=[\"extract_entity\"], is_output=True, args_mapping_rule=ArgsMappingRule.AS_IS)\nasync def expand_query(self, query_meta: Tuple[str, List[str]]):  # Expand and obtain multiple queries.\n    query, entities, date = query_meta\n    ...\n</pre> @worker(dependencies=[\"extract_entity\"], is_output=True, args_mapping_rule=ArgsMappingRule.AS_IS) async def expand_query(self, query_meta: Tuple[str, List[str]]):  # Expand and obtain multiple queries.     query, entities, date = query_meta     ... <p>This operation requires knowing what the parameters of <code>query_meta</code> as a whole contain, which might seem inconvenient. Could we complete the unpacking operation and fill in the corresponding parameters when returning? At this point, the <code>UNPACK</code> mode comes in handy.</p> <p>Let's modify the <code>expand_query</code> in the above example and add some print messages.</p> In\u00a0[\u00a0]: Copied! <pre>class QueryExpansion(GraphAutoma):\n    @worker(is_start=True)\n    async def pre_query(self, query_obj: Dict):  # Receive the user's input and preprocess query\n        query = query_obj[\"query\"].split(\":\")[1].strip()  \n        return query\n\n    @worker(is_start=True)\n    async def pre_date(self, query_obj: Dict):  # Receive the user's input and preprocess date\n        date = query_obj[\"date\"]\n        return date\n\n    @worker(dependencies=[\"pre_query\", \"pre_date\"], args_mapping_rule=ArgsMappingRule.AS_IS)\n    async def extract_entity(self, query: str, date: str):  # Extract the entity information from the question, get entity information.\n        response: EntityList = await llm.astructured_output(  \n            constraint=PydanticModel(model=EntityList),\n            messages=[\n                Message.from_text(text=\"extract the entity information from the given query\", role=Role.SYSTEM),\n                Message.from_text(text=query, role=Role.USER,),\n            ]\n        )\n        return query, response.entities, date\n\n    @worker(dependencies=[\"extract_entity\"], is_output=True, args_mapping_rule=ArgsMappingRule.UNPACK)\n    async def expand_query(self, query: str, entities: List[str], date: str):  # Expand and obtain multiple queries.\n        print(f\"query: {query}, entities: {entities}, date: {date}\")\n        task_input = f\"Query: {query}\\nEntities: {entities}\"\n        response: QueryList = await llm.astructured_output(\n            constraint=PydanticModel(model=QueryList),\n            messages=[\n                Message.from_text(text=f\"Centered around the given entities and given date {date}, expand the query to obtain multiple queries\", role=Role.SYSTEM),\n                Message.from_text(text=task_input, role=Role.USER,),\n            ]\n        )\n        return response.queries\n</pre> class QueryExpansion(GraphAutoma):     @worker(is_start=True)     async def pre_query(self, query_obj: Dict):  # Receive the user's input and preprocess query         query = query_obj[\"query\"].split(\":\")[1].strip()           return query      @worker(is_start=True)     async def pre_date(self, query_obj: Dict):  # Receive the user's input and preprocess date         date = query_obj[\"date\"]         return date      @worker(dependencies=[\"pre_query\", \"pre_date\"], args_mapping_rule=ArgsMappingRule.AS_IS)     async def extract_entity(self, query: str, date: str):  # Extract the entity information from the question, get entity information.         response: EntityList = await llm.astructured_output(               constraint=PydanticModel(model=EntityList),             messages=[                 Message.from_text(text=\"extract the entity information from the given query\", role=Role.SYSTEM),                 Message.from_text(text=query, role=Role.USER,),             ]         )         return query, response.entities, date      @worker(dependencies=[\"extract_entity\"], is_output=True, args_mapping_rule=ArgsMappingRule.UNPACK)     async def expand_query(self, query: str, entities: List[str], date: str):  # Expand and obtain multiple queries.         print(f\"query: {query}, entities: {entities}, date: {date}\")         task_input = f\"Query: {query}\\nEntities: {entities}\"         response: QueryList = await llm.astructured_output(             constraint=PydanticModel(model=QueryList),             messages=[                 Message.from_text(text=f\"Centered around the given entities and given date {date}, expand the query to obtain multiple queries\", role=Role.SYSTEM),                 Message.from_text(text=task_input, role=Role.USER,),             ]         )         return response.queries <p>Let's run it!</p> In\u00a0[9]: Copied! <pre>query_expansion = QueryExpansion()\nawait query_expansion.arun(query_obj)\n</pre> query_expansion = QueryExpansion() await query_expansion.arun(query_obj) <pre>query: What new developments have there been in RAG in the past year?, entities: ['RAG', 'new developments', 'past year'], date: 2025-09-30\n</pre> Out[9]: <pre>['What are the latest advancements in Retrieval-Augmented Generation (RAG) technologies as of 2025?',\n 'What new developments have emerged in RAG systems over the past 12 months?',\n 'How has RAG evolved in the last year with regard to accuracy, efficiency, and scalability?',\n 'What are the key innovations in RAG that have been introduced between 2024 and 2025?',\n 'What new tools and frameworks have been launched for RAG implementation in the past year?',\n 'What recent breakthroughs in RAG have improved context handling and retrieval precision?',\n 'How have large language models integrated with RAG in the past year to enhance performance?',\n 'What are the most significant updates in RAG-based applications from 2024 to 2025?',\n 'What new techniques in RAG have been proposed to reduce hallucinations and improve factual consistency?',\n 'How have RAG solutions adapted to real-time data retrieval and dynamic content updates in the past year?']</pre> <p>Great! All the parameters were unpacked and accepted. It can be seen that the <code>unpack</code> mode makes our task flow clearer!</p> <p>However, it should be noted that the UNPACK mechanism requires that the current worker can only directly depend on one worker; otherwise, the results of multiple workers will be confused when unpacking!</p> In\u00a0[\u00a0]: Copied! <pre>class QueryExpansion(GraphAutoma):\n    @worker(is_start=True)\n    async def pre_query(self, query_obj: Dict):  # Receive the user's input and preprocess query\n        query = query_obj[\"query\"].split(\":\")[1].strip()  \n        return query\n\n    @worker(is_start=True)\n    async def pre_date(self, query_obj: Dict):  # Receive the user's input and preprocess date\n        date = query_obj[\"date\"]\n        return date\n\n    @worker(dependencies=[\"pre_query\", \"pre_date\"], args_mapping_rule=ArgsMappingRule.MERGE)\n    async def extract_entity(self, query_meta: Tuple[str, str]):  # Extract the entity information from the question, get entity information.\n        print(f\"query_meta: {query_meta}\")\n        query, date = query_meta\n        response: EntityList = await llm.astructured_output(  \n            constraint=PydanticModel(model=EntityList),\n            messages=[\n                Message.from_text(text=\"extract the entity information from the given query\", role=Role.SYSTEM),\n                Message.from_text(text=query, role=Role.USER,),\n            ]\n        )\n        return query, response.entities, date\n\n    @worker(dependencies=[\"extract_entity\"], is_output=True, args_mapping_rule=ArgsMappingRule.UNPACK)\n    async def expand_query(self, query: str, entities: List[str], date: str):  # Expand and obtain multiple queries.\n        task_input = f\"Query: {query}\\nEntities: {entities}\"\n        response: QueryList = await llm.astructured_output(\n            constraint=PydanticModel(model=QueryList),\n            messages=[\n                Message.from_text(text=f\"Centered around the given entities and given date {date}, expand the query to obtain multiple queries\", role=Role.SYSTEM),\n                Message.from_text(text=task_input, role=Role.USER,),\n            ]\n        )\n        return response.queries\n</pre> class QueryExpansion(GraphAutoma):     @worker(is_start=True)     async def pre_query(self, query_obj: Dict):  # Receive the user's input and preprocess query         query = query_obj[\"query\"].split(\":\")[1].strip()           return query      @worker(is_start=True)     async def pre_date(self, query_obj: Dict):  # Receive the user's input and preprocess date         date = query_obj[\"date\"]         return date      @worker(dependencies=[\"pre_query\", \"pre_date\"], args_mapping_rule=ArgsMappingRule.MERGE)     async def extract_entity(self, query_meta: Tuple[str, str]):  # Extract the entity information from the question, get entity information.         print(f\"query_meta: {query_meta}\")         query, date = query_meta         response: EntityList = await llm.astructured_output(               constraint=PydanticModel(model=EntityList),             messages=[                 Message.from_text(text=\"extract the entity information from the given query\", role=Role.SYSTEM),                 Message.from_text(text=query, role=Role.USER,),             ]         )         return query, response.entities, date      @worker(dependencies=[\"extract_entity\"], is_output=True, args_mapping_rule=ArgsMappingRule.UNPACK)     async def expand_query(self, query: str, entities: List[str], date: str):  # Expand and obtain multiple queries.         task_input = f\"Query: {query}\\nEntities: {entities}\"         response: QueryList = await llm.astructured_output(             constraint=PydanticModel(model=QueryList),             messages=[                 Message.from_text(text=f\"Centered around the given entities and given date {date}, expand the query to obtain multiple queries\", role=Role.SYSTEM),                 Message.from_text(text=task_input, role=Role.USER,),             ]         )         return response.queries <p>Let's run it!</p> In\u00a0[11]: Copied! <pre>query_expansion = QueryExpansion()\nawait query_expansion.arun(query_obj)\n</pre> query_expansion = QueryExpansion() await query_expansion.arun(query_obj) <pre>query_meta: ['What new developments have there been in RAG in the past year?', '2025-09-30']\n</pre> Out[11]: <pre>['What are the latest advancements in Retrieval-Augmented Generation (RAG) technology as of 2025?',\n 'What new developments have emerged in RAG systems over the past 12 months?',\n 'How has RAG evolved in the last year with recent innovations in AI and NLP?',\n 'What are the key updates and breakthroughs in RAG models from 2024 to 2025?',\n 'What new features or improvements have been introduced in RAG implementations in the past year?',\n 'What are the most significant RAG developments reported in 2025?',\n 'How have retrieval and generation components in RAG been improved in the last year?',\n 'What are the recent trends and new developments in RAG applications from 2024 to 2025?',\n 'What innovations in RAG have been introduced by leading AI companies in the past year?',\n 'What new challenges and solutions have emerged in RAG research over the last 12 months?']</pre> <p>Great! The results that <code>extract_entity</code> depends on from the workers have all been collected into a list and passed to its parameters.</p> In\u00a0[\u00a0]: Copied! <pre># import the From marker\nfrom bridgic.core.automa import From\n\n@worker(dependencies=[\"extract_entity\"], is_output=True, args_mapping_rule=ArgsMappingRule.UNPACK)\nasync def expand_query(self, query_meta: Tuple[str, str], date: str = From(\"pre_date\", \"2025-01-01\")):  # Expand and obtain multiple queries.\n    ...\n</pre> # import the From marker from bridgic.core.automa import From  @worker(dependencies=[\"extract_entity\"], is_output=True, args_mapping_rule=ArgsMappingRule.UNPACK) async def expand_query(self, query_meta: Tuple[str, str], date: str = From(\"pre_date\", \"2025-01-01\")):  # Expand and obtain multiple queries.     ... <p><code>date: str = From(\"pre_date\", \"2025-01-01\")</code> indicates that the value of <code>date</code> will be assigned based on the result of the <code>pre_date</code> worker. If the result from this worker has not yet been produced, the default value will be used instead.</p> <p>If the pre_date worker does not exist, or if the pre_date worker has not yet produced a result, and there is no default value, an error will be reported: AutomaDataInjectionError.</p> <p>Let's modify the above example and add some print messages.</p> In\u00a0[\u00a0]: Copied! <pre>class QueryExpansion(GraphAutoma):\n    @worker(is_start=True)\n    async def pre_query(self, query_obj: Dict):  # Receive the user's input and preprocess query\n        query = query_obj[\"query\"].split(\":\")[1].strip()  \n        return query\n\n    @worker(is_start=True)\n    async def pre_date(self, query_obj: Dict):  # Receive the user's input and preprocess date\n        date = query_obj[\"date\"]\n        return date\n\n    @worker(dependencies=[\"pre_query\"], args_mapping_rule=ArgsMappingRule.AS_IS)\n    async def extract_entity(self, query: str):  # Extract the entity information from the question, get entity information.\n        response: EntityList = await llm.astructured_output(  \n            constraint=PydanticModel(model=EntityList),\n            messages=[\n                Message.from_text(text=\"extract the entity information from the given query\", role=Role.SYSTEM),\n                Message.from_text(text=query, role=Role.USER,),\n            ]\n        )\n        return query, response.entities\n\n    @worker(dependencies=[\"extract_entity\"], is_output=True, args_mapping_rule=ArgsMappingRule.UNPACK)\n    async def expand_query(self, query: str, entities: List[str], date: str = From(\"pre_date\", \"2025-01-01\")):  # Expand and obtain multiple queries.\n        print(f\"query: {query}, entities: {entities}, date: {date}\")\n        task_input = f\"Query: {query}\\nEntities: {entities}\"\n        response: QueryList = await llm.astructured_output(\n            constraint=PydanticModel(model=QueryList),\n            messages=[\n                Message.from_text(text=f\"Centered around the given entities and given date {date}, expand the query to obtain multiple queries\", role=Role.SYSTEM),\n                Message.from_text(text=task_input, role=Role.USER,),\n            ]\n        )\n        return response.queries\n</pre> class QueryExpansion(GraphAutoma):     @worker(is_start=True)     async def pre_query(self, query_obj: Dict):  # Receive the user's input and preprocess query         query = query_obj[\"query\"].split(\":\")[1].strip()           return query      @worker(is_start=True)     async def pre_date(self, query_obj: Dict):  # Receive the user's input and preprocess date         date = query_obj[\"date\"]         return date      @worker(dependencies=[\"pre_query\"], args_mapping_rule=ArgsMappingRule.AS_IS)     async def extract_entity(self, query: str):  # Extract the entity information from the question, get entity information.         response: EntityList = await llm.astructured_output(               constraint=PydanticModel(model=EntityList),             messages=[                 Message.from_text(text=\"extract the entity information from the given query\", role=Role.SYSTEM),                 Message.from_text(text=query, role=Role.USER,),             ]         )         return query, response.entities      @worker(dependencies=[\"extract_entity\"], is_output=True, args_mapping_rule=ArgsMappingRule.UNPACK)     async def expand_query(self, query: str, entities: List[str], date: str = From(\"pre_date\", \"2025-01-01\")):  # Expand and obtain multiple queries.         print(f\"query: {query}, entities: {entities}, date: {date}\")         task_input = f\"Query: {query}\\nEntities: {entities}\"         response: QueryList = await llm.astructured_output(             constraint=PydanticModel(model=QueryList),             messages=[                 Message.from_text(text=f\"Centered around the given entities and given date {date}, expand the query to obtain multiple queries\", role=Role.SYSTEM),                 Message.from_text(text=task_input, role=Role.USER,),             ]         )         return response.queries <p>Let's run it!</p> In\u00a0[14]: Copied! <pre>query_expansion = QueryExpansion()\nawait query_expansion.arun(query_obj)\n</pre> query_expansion = QueryExpansion() await query_expansion.arun(query_obj) <pre>query: What new developments have there been in RAG in the past year?, entities: ['RAG', 'new developments', 'past year'], date: 2025-09-30\n</pre> Out[14]: <pre>['What are the latest advancements in Retrieval-Augmented Generation (RAG) technology as of 2025?',\n 'What new developments have emerged in RAG systems over the past 12 months?',\n 'How have RAG implementations evolved in the last year in terms of performance and scalability?',\n 'What innovations in RAG have been introduced in 2025 that improve accuracy and context handling?',\n 'What are the key breakthroughs in RAG research and deployment from 2024 to 2025?',\n 'What new tools and frameworks have been released for RAG in the past year?',\n 'How have privacy and security features improved in RAG systems over the last year?',\n 'What are the most notable RAG developments in enterprise AI applications from 2024 to 2025?',\n \"What recent improvements have been made to RAG's ability to handle long-context inputs?\",\n 'How has the integration of RAG with large language models evolved in the past year?']</pre> <p>I have modified <code>extract_entity</code>, and now it only accepts <code>query</code>, making its functionality more pure. Also, in <code>expand_query</code>, I have correctly obtained the <code>date</code>.</p> <pre>@worker(dependencies=[\"extract_entity\"], is_output=True args_mapping_rule=ArgsMappingRule.UNPACK)\nasync def expand_query(\n    self, \n    query: str, \n    entities: List[str], \n+    query_obj: Dict,  # The input of the entire Automa\n    date: str = From(\"pre_date\", \"2025-01-01\"), \n):  # Expand and obtain multiple queries.\n    ...\n</pre> <p>Let's modify the above example and add some print messages.</p> In\u00a0[\u00a0]: Copied! <pre>class QueryExpansion(GraphAutoma):\n    @worker(is_start=True)\n    async def pre_query(self, query_obj: Dict):  # Receive the user's input and preprocess query\n        query = query_obj[\"query\"].split(\":\")[1].strip()  \n        return query\n\n    @worker(is_start=True)\n    async def pre_date(self, query_obj: Dict):  # Receive the user's input and preprocess date\n        date = query_obj[\"date\"]\n        return date\n\n    @worker(dependencies=[\"pre_query\"], args_mapping_rule=ArgsMappingRule.AS_IS)\n    async def extract_entity(self, query: str):  # Extract the entity information from the question, get entity information.\n        response: EntityList = await llm.astructured_output(  \n            constraint=PydanticModel(model=EntityList),\n            messages=[\n                Message.from_text(text=\"extract the entity information from the given query\", role=Role.SYSTEM),\n                Message.from_text(text=query, role=Role.USER,),\n            ]\n        )\n        return query, response.entities\n\n    @worker(dependencies=[\"extract_entity\"], is_output=True, args_mapping_rule=ArgsMappingRule.UNPACK)\n    async def expand_query(self, query: str, entities: List[str], query_obj: Dict, date: str = From(\"pre_date\", \"2025-01-01\")):  # Expand and obtain multiple queries.\n        print(f\"query: {query}, entities: {entities}, date: {date}, query_obj: {query_obj}\")\n        task_input = f\"Query: {query}\\nEntities: {entities}\"\n        response: QueryList = await llm.astructured_output(\n            constraint=PydanticModel(model=QueryList),\n            messages=[\n                Message.from_text(text=f\"Centered around the given entities and given date {date}, expand the query to obtain multiple queries\", role=Role.SYSTEM),\n                Message.from_text(text=task_input, role=Role.USER,),\n            ]\n        )\n        return {\"id\": query_obj[\"id\"], \"queries\": response.queries}\n</pre> class QueryExpansion(GraphAutoma):     @worker(is_start=True)     async def pre_query(self, query_obj: Dict):  # Receive the user's input and preprocess query         query = query_obj[\"query\"].split(\":\")[1].strip()           return query      @worker(is_start=True)     async def pre_date(self, query_obj: Dict):  # Receive the user's input and preprocess date         date = query_obj[\"date\"]         return date      @worker(dependencies=[\"pre_query\"], args_mapping_rule=ArgsMappingRule.AS_IS)     async def extract_entity(self, query: str):  # Extract the entity information from the question, get entity information.         response: EntityList = await llm.astructured_output(               constraint=PydanticModel(model=EntityList),             messages=[                 Message.from_text(text=\"extract the entity information from the given query\", role=Role.SYSTEM),                 Message.from_text(text=query, role=Role.USER,),             ]         )         return query, response.entities      @worker(dependencies=[\"extract_entity\"], is_output=True, args_mapping_rule=ArgsMappingRule.UNPACK)     async def expand_query(self, query: str, entities: List[str], query_obj: Dict, date: str = From(\"pre_date\", \"2025-01-01\")):  # Expand and obtain multiple queries.         print(f\"query: {query}, entities: {entities}, date: {date}, query_obj: {query_obj}\")         task_input = f\"Query: {query}\\nEntities: {entities}\"         response: QueryList = await llm.astructured_output(             constraint=PydanticModel(model=QueryList),             messages=[                 Message.from_text(text=f\"Centered around the given entities and given date {date}, expand the query to obtain multiple queries\", role=Role.SYSTEM),                 Message.from_text(text=task_input, role=Role.USER,),             ]         )         return {\"id\": query_obj[\"id\"], \"queries\": response.queries} <p>Let's run it! When using the Inputs Propagation, the startup parameters must be passed in the form of keywords at startup.</p> In\u00a0[\u00a0]: Copied! <pre>query_expansion = QueryExpansion()\nawait query_expansion.arun(query_obj=query_obj)  # using keyword parameter passing\n</pre> query_expansion = QueryExpansion() await query_expansion.arun(query_obj=query_obj)  # using keyword parameter passing <pre>query: What new developments have there been in RAG in the past year?, entities: ['RAG', 'new developments', 'past year'], date: 2025-09-30, query_obj: {'id': 'user_1', 'query': 'Q: What new developments have there been in RAG in the past year?', 'date': '2025-09-30'}\n</pre> Out[\u00a0]: <pre>{'id': 'user_1',\n 'queries': ['What are the latest advancements in Retrieval-Augmented Generation (RAG) technologies as of 2025?',\n  'What new developments have emerged in RAG systems over the past 12 months?',\n  'How have RAG implementations evolved in the last year in terms of performance and scalability?',\n  'What are the key innovations in RAG models reported between 2024 and 2025?',\n  'What new techniques have been introduced in RAG to improve accuracy and context retention in the past year?',\n  'What recent breakthroughs in RAG have been highlighted in 2025?',\n  'How have industry leaders advanced RAG technology in the last year?',\n  'What are the most significant updates in RAG frameworks and tools from 2024 to 2025?',\n  'What new challenges and solutions have been proposed in RAG research over the past year?',\n  'What developments in RAG have improved real-time retrieval and generation performance in 2025?']}</pre> <p>Among all the ways of parameter passing mentioned above, the priority order is: arguments mapping positional parameters &gt; arguments injection &gt; propagation &gt; arguments mapping keyword parameters.</p>"},{"location":"tutorials/items/core_mechanism/parameter_resolving/#parameter-resolving","title":"Parameter Resolving\u00b6","text":"<p>In Bridgic, an execution unit (worker), can not only control how its results are passed to the workers that directly depend on it through the Result Dispatching mechanism, but also control how it receives the results from its preceding workers through the Parameter Binding mechanism. In addition, these mechanisms also determine how an automa\u2019s input arguments are distributed to its internal workers. These two mechanisms are collectively referred to in Bridgic as Parameter Resolving.</p>"},{"location":"tutorials/items/core_mechanism/parameter_resolving/#result-dispatching","title":"Result Dispatching\u00b6","text":"<p>Result Dispatching is a mechanism used for distributing data from a worker to multiple workers that directly depend on it, or from an automa\u2019s inputs to its start workers. Let's understand this feature through a practical example.</p> <p>Suppose we need to process multiple user queries in parallel. Each query needs to go through preprocessing, analysis, and then be aggregated. We can use the Result Dispatching mechanism to efficiently handle this scenario.</p>"},{"location":"tutorials/items/core_mechanism/parameter_resolving/#1-initialize","title":"1. Initialize\u00b6","text":"<p>Let's start by importing the necessary packages.</p>"},{"location":"tutorials/items/core_mechanism/parameter_resolving/#2-process-every-inputs","title":"2. Process Every Inputs\u00b6","text":"<p>First, let's understand how to use <code>InOrder</code> when calling <code>arun()</code>. When you have multiple start workers and want to distribute different input values to each of them, you can wrap the input data in <code>InOrder()</code>.</p>"},{"location":"tutorials/items/core_mechanism/parameter_resolving/#3-process-every-item-in-result-dispatching","title":"3. Process Every Item in Result Dispatching\u00b6","text":"<p>Now let's understand how to use <code>result_dispatching_rule=ResultDispatchingRule.IN_ORDER</code> to distribute a worker's output to multiple downstream workers.</p> <p>When a worker sets <code>result_dispatching_rule=ResultDispatchingRule.IN_ORDER</code>, its return value must be an iterable (tuple or list). Each element in the return value will be distributed to the downstream workers that directly depend on this worker, in the order they are declared or added.</p>"},{"location":"tutorials/items/core_mechanism/parameter_resolving/#parameter-binding","title":"Parameter Binding\u00b6","text":"<p>Parameter Binding is a mechanism that specifies how a worker receives arguments from its direct predecessors, its non-direct predecessors, or from the input arguments of the automa it belongs to. There are three ways to accomplish the Parameter Binding process, including Arguments Mapping, Arguments Injection, and Inputs Propagation. Now let's understand them through a sample example.</p>"},{"location":"tutorials/items/core_mechanism/parameter_resolving/#query-expansion","title":"Query expansion\u00b6","text":"<p>Query expansion is a common step in RAG and can enhance the quality of RAG. To enhance the quality of query expansion, developers often first extract the entity information from the query and use it to assist the model in expanding the original query.</p> <p>Now let's implement this. The user inputs the original query, and then we expand the query to obtain more queries. There are three steps to complete the query expansion:</p> <ol> <li>Receive the user's input and perform preprocessing to get the original query.</li> <li>Extract the entity information from the query to get the entity information.</li> <li>Expand and obtain multiple queries.</li> </ol>"},{"location":"tutorials/items/core_mechanism/parameter_resolving/#1-initialize","title":"1. Initialize\u00b6","text":"<p>Before we start, let's prepare the running environment. In this tutorial, we will use the OpenAI model integration (not OpenAI-like) that supports the <code>StructuredOutput</code> feature. Run the following <code>pip</code> command to make sure this integration is available.</p> <pre>pip install bridgic-llms-openai\n</pre>"},{"location":"tutorials/items/core_mechanism/parameter_resolving/#2-complete-query-expansion","title":"2. Complete Query Expansion\u00b6","text":""},{"location":"tutorials/items/core_mechanism/parameter_resolving/#what-have-we-learnt","title":"What have we learnt?\u00b6","text":""},{"location":"tutorials/items/core_mechanism/parameter_resolving/#result-dispatching","title":"Result Dispatching\u00b6","text":"<p>Result Dispatching provides powerful mechanisms for parallel / concurrent data processing:</p> <ul> <li><code>Distribute</code> in <code>arun()</code>: Distributes input values to multiple start workers element-wise</li> <li><code>ResultDispatchRule</code>: Allows a worker to distribute its output (must be iterable) to multiple downstream workers<ul> <li>AS_IS: Default mode to send the result of current worker as a whole</li> <li>Distribute: Set to send the result of current worker according to the order in which the downstream workers are declared or added</li> </ul> </li> </ul> <p>These features enable efficient parallel / concurrent processing and data flow management in complex workflows.</p>"},{"location":"tutorials/items/core_mechanism/parameter_resolving/#arguments-mapping","title":"Arguments Mapping\u00b6","text":"<p>Reviewing the code, we find that each <code>@worker</code> decorator has an <code>args_mapping_rule</code> parameter.</p> <p>The <code>args_mapping_rule</code> defines the way data is passed between directly dependent workers, that is, how the result of the previous worker is mapped to the parameter of the next worker. Its value can only be specified through the properties of <code>ArgsMappingRule</code>.</p>"},{"location":"tutorials/items/core_mechanism/parameter_resolving/#as_is-mode-default","title":"AS_IS mode (default)\u00b6","text":"<p>In the AS_IS mode, a worker will receive the output of all its directly dependent workers as input parameters in the order declared by the dependencies.</p> <p>In the above example, <code>extract_entity</code> declares dependencies: <code>dependencies=[\"pre_query\", \"pre_date\"]</code>, so the results of the two preceding workers will be mapped to the first and second parameters of <code>extract_entity</code> in the order specified by the dependencies declaration, the result of <code>pre_query</code> is mapped to <code>query</code> parameter and the result of <code>pre_date</code> is mapped to <code>date</code> parameter.</p> <p>Note: The declaration order in dependencies only affects the order of parameter mapping, but does not influence the execution order of the dependent workers.</p> <p>Additionally, if the previous worker returns a result with multiple values, such as <code>return x, y</code>, then all the results will be passed as a tuple result. So in the above example, the parameter <code>query_meta</code> of <code>expand_query</code> received all the result values from <code>extract_entity</code>.</p>"},{"location":"tutorials/items/core_mechanism/parameter_resolving/#unpack-mode","title":"UNPACK mode\u00b6","text":"<p>Let's go back to the previous example. In the <code>expand_query</code>, we receive the parameters from the previous worker in the <code>AS_IS</code> mode and manually unpack them as a whole, like this:</p>"},{"location":"tutorials/items/core_mechanism/parameter_resolving/#merge-mode","title":"MERGE mode\u00b6","text":"<p>At the same time, conversely, since there is an UNPACK mechanism, is there also a mechanism that can aggregate multiple results for receiving? This is particularly useful when a worker collects the results of multiple dependent workers. At this point, the <code>MERGE</code> mode comes in handy.</p> <p>Still referring to the example above, <code>extract_entity</code> actually received the results from two workers. Now let's try to make <code>extract_entity</code> receive all these results in a single parameter for use, instead of receiving two parameters.</p> <p>Let's modify the <code>extract_entity</code> in the above example and add some print messages.</p>"},{"location":"tutorials/items/core_mechanism/parameter_resolving/#arguments-injection","title":"Arguments Injection\u00b6","text":"<p>Looking back at the example above, we actually find that the <code>date</code> information is passed through <code>pre_date</code>, <code>extract_entity</code>, and finally reaches <code>expand_query</code>. However, in reality, <code>extract_entity</code> doesn't use this information at all. Thus, passing <code>date</code> here seems redundant. And The use of <code>date</code> in <code>expand_query</code> essentially only means that the data depends on it, but whether it is executed or not, this control dependency does not directly rely on it.</p> <p>Bridgic emphasizes the separation of data dependency and control dependency. This is beneficial for the future construction of complex graphs, as it allows for decoupling and avoids the need to adjust the entire graph due to changes in data dependency.</p> <p>In Bridgic, we can use Arguments Injection to make it. We can indicate which worker's result to take by using the <code>From</code> marker when declaring parameters, and at the same time set the default value if no result is obtained. For example:</p>"},{"location":"tutorials/items/core_mechanism/parameter_resolving/#inputs-propagation","title":"Inputs Propagation\u00b6","text":"<p>Looking back at the example above again, our program did not process the <code>id</code> field in the input at all. Eventually, we only returned a list of generalized problems, which might cause the external call to be unable to associate which \"id\" corresponds to the result. However, this ID neither requires preprocessing nor is it needed for entity extraction.</p> <p>We can use Inputs Propagation to resolve it. This can be achieved by adding the name of the startup parameter to the worker when declaring the parameters.</p>"},{"location":"tutorials/items/core_mechanism/worker_callback/","title":"Worker Callback","text":"In\u00a0[19]: Copied! <pre>from typing import Any, Dict, Optional\nfrom bridgic.core.automa import GraphAutoma\nfrom bridgic.core.automa.worker import WorkerCallback\n\nclass LoggingCallback(WorkerCallback):\n    \"\"\"Log worker lifecycle events.\"\"\"\n\n    def __init__(self, tag: str = None):\n        self._tag = tag or \"\"\n\n    async def on_worker_start(\n        self,\n        key: str,\n        is_top_level: bool = False,\n        parent: Optional[GraphAutoma] = None,\n        arguments: Dict[str, Any] = None,\n    ) -&gt; None:\n        print(self._tag + f\"[START] {key} args={arguments}\")\n\n    async def on_worker_end(\n        self,\n        key: str,\n        is_top_level: bool = False,\n        parent: Optional[GraphAutoma] = None,\n        arguments: Dict[str, Any] = None,\n        result: Any = None,\n    ) -&gt; None:\n        print(self._tag + f\"[END] {key} result={result}\")\n\n    async def on_worker_error(\n        self,\n        key: str,\n        is_top_level: bool = False,\n        parent: Optional[GraphAutoma] = None,\n        arguments: Dict[str, Any] = None,\n        error: Exception = None,\n    ) -&gt; bool:\n        print(self._tag + f\"[ERROR] {key} -&gt; {error}\")\n        return False  # Returning False means don't suppress the exception.\n</pre> from typing import Any, Dict, Optional from bridgic.core.automa import GraphAutoma from bridgic.core.automa.worker import WorkerCallback  class LoggingCallback(WorkerCallback):     \"\"\"Log worker lifecycle events.\"\"\"      def __init__(self, tag: str = None):         self._tag = tag or \"\"      async def on_worker_start(         self,         key: str,         is_top_level: bool = False,         parent: Optional[GraphAutoma] = None,         arguments: Dict[str, Any] = None,     ) -&gt; None:         print(self._tag + f\"[START] {key} args={arguments}\")      async def on_worker_end(         self,         key: str,         is_top_level: bool = False,         parent: Optional[GraphAutoma] = None,         arguments: Dict[str, Any] = None,         result: Any = None,     ) -&gt; None:         print(self._tag + f\"[END] {key} result={result}\")      async def on_worker_error(         self,         key: str,         is_top_level: bool = False,         parent: Optional[GraphAutoma] = None,         arguments: Dict[str, Any] = None,         error: Exception = None,     ) -&gt; bool:         print(self._tag + f\"[ERROR] {key} -&gt; {error}\")         return False  # Returning False means don't suppress the exception. In\u00a0[20]: Copied! <pre>from bridgic.core.automa.worker import WorkerCallbackBuilder\n\n# Build in shared instance mode.\nshared_builder = WorkerCallbackBuilder(LoggingCallback)\nisolated_builder = WorkerCallbackBuilder(LoggingCallback, is_shared=False)\n\n# Build in independent instance mode.\nshared_builder_with_args = WorkerCallbackBuilder(LoggingCallback, init_kwargs={\"tag\": \"-\"})\nisolated_builder_with_args = WorkerCallbackBuilder(LoggingCallback, init_kwargs={\"tag\": \"-\"}, is_shared=False)\n</pre> from bridgic.core.automa.worker import WorkerCallbackBuilder  # Build in shared instance mode. shared_builder = WorkerCallbackBuilder(LoggingCallback) isolated_builder = WorkerCallbackBuilder(LoggingCallback, is_shared=False)  # Build in independent instance mode. shared_builder_with_args = WorkerCallbackBuilder(LoggingCallback, init_kwargs={\"tag\": \"-\"}) isolated_builder_with_args = WorkerCallbackBuilder(LoggingCallback, init_kwargs={\"tag\": \"-\"}, is_shared=False) <p>Global-Level Configuration</p> <p>The following example shows how to configure callbacks at the global level using <code>GlobalSetting</code>, which applies the callback to all workers across all Automa instances in your application.</p> In\u00a0[21]: Copied! <pre>from bridgic.core.automa import GraphAutoma, worker\nfrom bridgic.core.automa.worker import WorkerCallbackBuilder\nfrom bridgic.core.config import GlobalSetting\n\nGlobalSetting.set(callback_builders=[WorkerCallbackBuilder(LoggingCallback)])\n\nclass MyAutoma(GraphAutoma):\n    @worker(is_start=True)\n    async def step1(self, x: int) -&gt; int:\n        return x + 1\n\nautoma = MyAutoma(name=\"test-automa\")  # Will log for all workers inside.\nawait automa.arun(x=10)\n</pre> from bridgic.core.automa import GraphAutoma, worker from bridgic.core.automa.worker import WorkerCallbackBuilder from bridgic.core.config import GlobalSetting  GlobalSetting.set(callback_builders=[WorkerCallbackBuilder(LoggingCallback)])  class MyAutoma(GraphAutoma):     @worker(is_start=True)     async def step1(self, x: int) -&gt; int:         return x + 1  automa = MyAutoma(name=\"test-automa\")  # Will log for all workers inside. await automa.arun(x=10) <pre>[START] test-automa args={'args': (), 'kwargs': {'x': 10}, 'feedback_data': None}\n[START] step1 args={'args': (), 'kwargs': {'x': 10}}\n[END] step1 result=11\n[END] test-automa result=None\n</pre> <p>Automa-Level Configuration</p> <p>The following example shows how to configure callbacks at the automa level using <code>RunningOptions</code>, which applies the callback to all workers within a specific Automa instance.</p> In\u00a0[22]: Copied! <pre>from bridgic.core.automa import GraphAutoma, RunningOptions, worker\nfrom bridgic.core.automa.worker import WorkerCallbackBuilder\n\n# Because this example is under jupyter-notebook environment, GlobalSetting needs to be reset.\n# In real deveopment, this line is not necessary to have a default global setting.\nGlobalSetting.set(callback_builders=[])\n\nclass MyAutoma(GraphAutoma):\n    @worker(is_start=True)\n    async def step1(self, x: int) -&gt; int:\n        return x + 1\n\nrunning_options = RunningOptions(callback_builders=[WorkerCallbackBuilder(LoggingCallback)])\nautoma = MyAutoma(name=\"test-automa\", running_options=running_options)  # Will log for all workers inside.\nawait automa.arun(x=10)\n</pre> from bridgic.core.automa import GraphAutoma, RunningOptions, worker from bridgic.core.automa.worker import WorkerCallbackBuilder  # Because this example is under jupyter-notebook environment, GlobalSetting needs to be reset. # In real deveopment, this line is not necessary to have a default global setting. GlobalSetting.set(callback_builders=[])  class MyAutoma(GraphAutoma):     @worker(is_start=True)     async def step1(self, x: int) -&gt; int:         return x + 1  running_options = RunningOptions(callback_builders=[WorkerCallbackBuilder(LoggingCallback)]) automa = MyAutoma(name=\"test-automa\", running_options=running_options)  # Will log for all workers inside. await automa.arun(x=10) <pre>[START] test-automa args={'args': (), 'kwargs': {'x': 10}, 'feedback_data': None}\n[START] step1 args={'args': (), 'kwargs': {'x': 10}}\n[END] step1 result=11\n[END] test-automa result=None\n</pre> <p>Worker-Level Configuration</p> <p>The following example shows how to configure callbacks at the worker level by passing <code>callback_builders</code> to the <code>@worker</code> decorator, which applies the callback only to the specific worker.</p> In\u00a0[23]: Copied! <pre>from bridgic.core.automa import GraphAutoma, worker\nfrom bridgic.core.automa.worker import WorkerCallbackBuilder\n\n# Because this example is under jupyter-notebook environment, GlobalSetting needs to be reset.\n# In real deveopment, this line is not necessary to have a default global setting.\nGlobalSetting.set(callback_builders=[])\n\nclass MyAutoma(GraphAutoma):\n    @worker(\n        is_start=True,\n        callback_builders=[WorkerCallbackBuilder(LoggingCallback)],\n    )\n    async def step1(self, x: int) -&gt; int:\n        return x + 1\n\nautoma = MyAutoma(name=\"test-automa\")  # Will only log for \"step1\" worker.\nawait automa.arun(x=10)\n</pre> from bridgic.core.automa import GraphAutoma, worker from bridgic.core.automa.worker import WorkerCallbackBuilder  # Because this example is under jupyter-notebook environment, GlobalSetting needs to be reset. # In real deveopment, this line is not necessary to have a default global setting. GlobalSetting.set(callback_builders=[])  class MyAutoma(GraphAutoma):     @worker(         is_start=True,         callback_builders=[WorkerCallbackBuilder(LoggingCallback)],     )     async def step1(self, x: int) -&gt; int:         return x + 1  automa = MyAutoma(name=\"test-automa\")  # Will only log for \"step1\" worker. await automa.arun(x=10)  <pre>[START] step1 args={'args': (), 'kwargs': {'x': 10}}\n[END] step1 result=11\n</pre> In\u00a0[24]: Copied! <pre>from bridgic.core.automa import GraphAutoma, RunningOptions, worker\nfrom bridgic.core.automa.worker import WorkerCallback, WorkerCallbackBuilder\nfrom bridgic.core.config import GlobalSetting\n\n# Top-level automa\nclass TopAutoma(GraphAutoma):\n    @worker(is_start=True)\n    async def top_worker(self, x: int) -&gt; int:\n        return x + 1\n\n# Inner automa (will be used as a nested worker)\nclass InnerAutoma(GraphAutoma):\n    @worker(is_start=True)\n    async def inner_worker(self, x: int) -&gt; int:\n        return x * 2\n\n# Configure callback at global setting, with &lt;Global&gt; tag.\nGlobalSetting.set(\n    callback_builders=[\n        WorkerCallbackBuilder(LoggingCallback, init_kwargs={\"tag\": \"&lt;Global&gt;\"}),\n    ]\n)\n\n# Configure callback at top-level automa, with &lt;Automa&gt; tag.\nrunning_options = RunningOptions(\n    callback_builders=[\n        WorkerCallbackBuilder(LoggingCallback, init_kwargs={\"tag\": \"&lt;Automa&gt;\"})\n    ]\n)\nautoma = TopAutoma(name=\"top-automa\", running_options=running_options)\n\n# Add a instance of InnerAutoma as a worker.\nautoma.add_worker(\"nested_automa_as_worker\", InnerAutoma(name=\"inner-automa\"), dependencies=[\"top_worker\"])\n\n# When executed:\n# - Callbacks that from GlobalSetting will be propagated to all workers application-wide.\n# - Callbacks that from RunningOptions will be propagated to all workers inside the \"top-level\" automa.\nawait automa.arun(x=10)\n</pre> from bridgic.core.automa import GraphAutoma, RunningOptions, worker from bridgic.core.automa.worker import WorkerCallback, WorkerCallbackBuilder from bridgic.core.config import GlobalSetting  # Top-level automa class TopAutoma(GraphAutoma):     @worker(is_start=True)     async def top_worker(self, x: int) -&gt; int:         return x + 1  # Inner automa (will be used as a nested worker) class InnerAutoma(GraphAutoma):     @worker(is_start=True)     async def inner_worker(self, x: int) -&gt; int:         return x * 2  # Configure callback at global setting, with  tag. GlobalSetting.set(     callback_builders=[         WorkerCallbackBuilder(LoggingCallback, init_kwargs={\"tag\": \"\"}),     ] )  # Configure callback at top-level automa, with  tag. running_options = RunningOptions(     callback_builders=[         WorkerCallbackBuilder(LoggingCallback, init_kwargs={\"tag\": \"\"})     ] ) automa = TopAutoma(name=\"top-automa\", running_options=running_options)  # Add a instance of InnerAutoma as a worker. automa.add_worker(\"nested_automa_as_worker\", InnerAutoma(name=\"inner-automa\"), dependencies=[\"top_worker\"])  # When executed: # - Callbacks that from GlobalSetting will be propagated to all workers application-wide. # - Callbacks that from RunningOptions will be propagated to all workers inside the \"top-level\" automa. await automa.arun(x=10)  <pre>&lt;Global&gt;[START] top-automa args={'args': (), 'kwargs': {'x': 10}, 'feedback_data': None}\n&lt;Automa&gt;[START] top-automa args={'args': (), 'kwargs': {'x': 10}, 'feedback_data': None}\n&lt;Global&gt;[START] top_worker args={'args': (), 'kwargs': {'x': 10}}\n&lt;Automa&gt;[START] top_worker args={'args': (), 'kwargs': {'x': 10}}\n&lt;Global&gt;[END] top_worker result=11\n&lt;Automa&gt;[END] top_worker result=11\n&lt;Global&gt;[START] nested_automa_as_worker args={'args': (11,), 'kwargs': {'feedback_data': None, 'x': 10}}\n&lt;Automa&gt;[START] nested_automa_as_worker args={'args': (11,), 'kwargs': {'feedback_data': None, 'x': 10}}\n&lt;Global&gt;[START] inner_worker args={'args': (11,), 'kwargs': {}}\n&lt;Automa&gt;[START] inner_worker args={'args': (11,), 'kwargs': {}}\n&lt;Global&gt;[END] inner_worker result=22\n&lt;Automa&gt;[END] inner_worker result=22\n&lt;Global&gt;[END] nested_automa_as_worker result=None\n&lt;Automa&gt;[END] nested_automa_as_worker result=None\n&lt;Global&gt;[END] top-automa result=None\n&lt;Automa&gt;[END] top-automa result=None\n</pre> In\u00a0[25]: Copied! <pre>import warnings\n\nfrom typing import Union\nfrom bridgic.core.automa.worker import WorkerCallback\n\nclass ValueErrorHandler(WorkerCallback):\n    async def on_worker_error(\n        self,\n        key: str,\n        is_top_level: bool = False,\n        parent: Optional[GraphAutoma] = None,\n        arguments: Dict[str, Any] = None,\n        error: ValueError = None\n    ) -&gt; bool:\n        warnings.warn(\"ValueError in %s: %s\", key, error)\n        return True  # Swallow ValueError.\n\nclass MultipleErrorHandler(WorkerCallback):\n    async def on_worker_error(\n        self,\n        key: str,\n        is_top_level: bool = False,\n        parent: Optional[GraphAutoma] = None,\n        arguments: Dict[str, Any] = None,\n        error: Union[KeyError, TypeError] = None\n    ) -&gt; bool:\n        warnings.warn(\"Recoverable issue: %s\", error)\n        return False  # Re-raise it.\n</pre> import warnings  from typing import Union from bridgic.core.automa.worker import WorkerCallback  class ValueErrorHandler(WorkerCallback):     async def on_worker_error(         self,         key: str,         is_top_level: bool = False,         parent: Optional[GraphAutoma] = None,         arguments: Dict[str, Any] = None,         error: ValueError = None     ) -&gt; bool:         warnings.warn(\"ValueError in %s: %s\", key, error)         return True  # Swallow ValueError.  class MultipleErrorHandler(WorkerCallback):     async def on_worker_error(         self,         key: str,         is_top_level: bool = False,         parent: Optional[GraphAutoma] = None,         arguments: Dict[str, Any] = None,         error: Union[KeyError, TypeError] = None     ) -&gt; bool:         warnings.warn(\"Recoverable issue: %s\", error)         return False  # Re-raise it. <p>The framework calls every matching callback, so you can compose specialized handlers with broader \"catch-all\" callbacks.</p>"},{"location":"tutorials/items/core_mechanism/worker_callback/#worker-callback","title":"Worker Callback\u00b6","text":""},{"location":"tutorials/items/core_mechanism/worker_callback/#introduction","title":"Introduction\u00b6","text":"<p>The Worker Callback mechanism will invoke pre-defined hook methods at key points during worker execution, allowing you to add cross-cutting concerns such as logging, validation, monitoring, and error handling without modifying your business logic. This tutorial will guide you through understanding and using the Worker Callback mechanism.</p> <p><code>WorkerCallback</code> is the base class of all callback instances that are invokded around a worker. You can implement the following three methods to subclass <code>WorkerCallback</code>:</p> Method Description <code>on_worker_start()</code> Called before worker execution. Use for input validation, logging, or monitoring <code>on_worker_end()</code> Called after successful execution. Use for result logging, event publishing, or metrics <code>on_worker_error()</code> Called when an exception is raised. Use for error handling, logging, or suppression"},{"location":"tutorials/items/core_mechanism/worker_callback/#creating-a-custom-callback","title":"Creating a Custom Callback\u00b6","text":""},{"location":"tutorials/items/core_mechanism/worker_callback/#step-1-define-a-class","title":"Step 1: Define a Class\u00b6","text":"<p>To create a custom callback, simply subclass <code>WorkerCallback</code> and implement the methods above. You don't need to implement all three methods, but only implement the ones that you need. The base <code>WorkerCallback</code> class provides default implementations that do nothing. Here we define a <code>LoggingCallback</code> class that implements all of the three hook methods.</p>"},{"location":"tutorials/items/core_mechanism/worker_callback/#step-2-choose-building-mode","title":"Step 2: Choose Building Mode\u00b6","text":"<p>Callbacks are instantiated through <code>WorkerCallbackBuilder</code>, which delays construction until the worker is created and lets you control its sharing mode.</p> <ul> <li>Shared instance mode (<code>is_shared=True</code>, default): all workers within the declaration scope reuse the same callback instance. This ideal for stateful integrations (e.g., a tracing client).</li> <li>Independent instance mode (<code>is_shared=False</code>): every worker receives its own callback instance. This is useful when you need isolated state or thread safety.</li> </ul>"},{"location":"tutorials/items/core_mechanism/worker_callback/#step-3-decide-the-scope","title":"Step 3: Decide the Scope\u00b6","text":"<p>Choose where the builder should take effect. Bridgic merges builders from the widest scope to the narrowest one:</p> Level Registration Method Scope of Effect Global-level <code>GlobalSetting</code> Applies to all workers across all Automa instances Automa-level <code>RunningOptions</code> Applies to every worker inside one Automa instance Worker-level <code>@worker</code> or <code>add_worker()</code> Applies only to the targeted worker"},{"location":"tutorials/items/core_mechanism/worker_callback/#features-you-need-to-know","title":"Features you Need to Know\u00b6","text":""},{"location":"tutorials/items/core_mechanism/worker_callback/#callback-propagation","title":"Callback Propagation\u00b6","text":"<p>Nested automa will inherit callbacks from their parent (even higher ancestor) scope by reading the initialized running options. This ensures instrumentation remains consistent across multi levels. When you set a callback in the <code>RunningOptions</code> during automa initialization, all workers in any nested automata will automatically inherit that callback:</p> <p>In the following example, the <code>LoggingCallback</code> configured at the top-level automa propagates to:</p> <ul> <li>All workers directly in <code>TopAutoma</code> (<code>top_worker</code>, <code>nested_automa_worker</code>)</li> <li>All workers inside the nested <code>InnerAutoma</code> (<code>inner_worker</code>)</li> </ul>"},{"location":"tutorials/items/core_mechanism/worker_callback/#dynamical-topology-support","title":"Dynamical Topology Support\u00b6","text":"<p><code>GraphAutoma.add_worker()</code> and related APIs allow you to modify the topology at runtime. When a new worker is added, Bridgic automatically builds its callback list using the current global builders, the builders from its ancestors' running options, and the builders passed with the <code>add_worker()</code> call. As a result:</p> <ul> <li>Dynamically added workers receive the same instrumentation guarantees as statically declared ones.</li> <li>Nested automa inserted later as a new worker inherits callbacks from its ancestors scopes.</li> </ul> <p>This design keeps long-running agentic systems observable even as they grow or reconfigure themselves during execution.</p>"},{"location":"tutorials/items/core_mechanism/worker_callback/#exception-handling","title":"Exception Handling\u00b6","text":""},{"location":"tutorials/items/core_mechanism/worker_callback/#exception-type-matching","title":"Exception Type Matching\u00b6","text":"<p>The <code>on_worker_error()</code> method allows for fine-grained and flexible error handling by inspecting the type annotation of its <code>error</code> parameter. You can indicate exactly which exception types you want your handler to respond to, by annotating the <code>error</code> parameter with a specific exception type. At runtime, the Bridgic framework will automatically match and invoke your callback only for those exceptions that match the annotation.</p> <p>Below is a simple example comparison table:</p> Type annotation of <code>error</code> The matched exception type(s) to trigger <code>on_worker_error</code> <code>Exception</code> All exceptions <code>ValueError</code> <code>ValueError</code> and its subclasses (e.g., <code>UnicodeDecodeError</code>) <code>Union[Type1, Type2, ...]</code> <code>Type1</code> and <code>Type2</code> will be considered to be matched"},{"location":"tutorials/items/core_mechanism/worker_callback/#exception-suppression","title":"Exception Suppression\u00b6","text":"<p>The return value of <code>on_worker_error</code> will determine whether to suppress the captured exception:</p> Value Behavior <code>True</code> Means to suppress the exception; the worker result becomes <code>None</code>. <code>False</code> Means to observe only; the framework re-raises after all callbacks finish. <p>Specially, to ensure human-in-the-loop flows stay intact, <code>InteractionException</code> should never be suppressed.</p>"},{"location":"tutorials/items/core_mechanism/worker_callback/#best-practices","title":"Best Practices\u00b6","text":"<ol> <li><p>Keep callbacks lightweight: Callback methods are called for each worker that they are responsible for, so keep them fast and avoid blocking operations.</p> </li> <li><p>Use appropriate scope: Use global-level for application-wide concerns, automa-level for specific instance, and worker-level for fine-grained control.</p> </li> <li><p>Handle exceptions carefully: Be thoughtful about which exceptions to suppress. Suppressing exceptions can hide bugs and make debugging difficult.</p> </li> <li><p>Use shared instances wisely: Shared instances are great for maintaining connections or state, but be aware of thread-safety concerns.</p> </li> <li><p>Leverage the parent parameter: The <code>parent</code> parameter gives you access to the automa's context, allowing you to post events, request feedback, or interact with the automa's state.</p> </li> </ol>"},{"location":"tutorials/items/core_mechanism/worker_callback/#next-steps","title":"Next Steps\u00b6","text":"<ul> <li>Learn about Observability to see how callbacks enable system transparency</li> <li>Explore Callback Integrations for ready-to-use callback implementations</li> </ul>"},{"location":"tutorials/items/model_integration/","title":"Model","text":""},{"location":"tutorials/items/model_integration/#our-philosophy","title":"Our Philosophy","text":"<ul> <li>Model Neutrality</li> <li>Protocol Driven Design</li> </ul> <p>Bridgic is designed as a model-neutral framework that treats all LLM providers - whether they are commercial vendors (OpenAI, etc.) or inference engines (vLLM, etc.) - as equals. This architectural decision will reduce the extra effort developers need to put in when switching models.</p> <p></p>"},{"location":"tutorials/items/model_integration/#model-neutrality","title":"Model Neutrality","text":"<p>Bridgic\u2019s API is fundamentally model-neutral. When designing our interfaces, we deliberately avoid assumptions about any particular model provider or technology. Instead, we focus on delivering:</p> <ul> <li>Functional Simplicity: The APIs are intuitive and easy to use, allowing you to interact with any supported model provider through clear and minimal calls, with no provider-specific complexity.</li> <li>Consistent Experience: Regardless of which underlying LLM or model engine you use, Bridgic ensures a uniform invocation model, standardized error handling, and predictable behavior.</li> </ul> <p>This model-agnostic approach allows you to swap, extend, or combine providers with minimal friction, promoting genuine flexibility and future-proof integration in your agent pipelines.</p>"},{"location":"tutorials/items/model_integration/#protocol-driven-design","title":"Protocol Driven Design","text":"<p>At the heart of Bridgic's model integration lies the Protocol Pattern, which defines clear behavioral contracts without imposing implementation details. This design enables:</p> <ol> <li>Extensibility: New model providers can be integrated by implementing well-defined protocols (<code>StructuredOutput</code>, <code>ToolSelection</code>, etc.)</li> <li>Capability Declaration: Each provider explicitly declares its capabilities through protocol implementation, making it clear what features are available</li> <li>Flexibility: Providers can implement only the protocols they support, avoiding forced compatibility with unsupported features</li> <li>Type Safety: Protocols provide compile-time type checking and IDE support, improving developer experience</li> </ol> <p> </p> <p>This architecture means that when you work with a model provider in Bridgic, you explicitly know what capabilities it offers, and the framework enforces these contracts at both development and runtime.</p>"},{"location":"tutorials/items/model_integration/llm_integration/","title":"LLM Integration","text":"In\u00a0[2]: Copied! <pre>import os\nfrom dotenv import load_dotenv\nfrom bridgic.llms.openai import OpenAILlm, OpenAIConfiguration\n\nload_dotenv()\n\n_api_key = os.environ.get(\"OPENAI_API_KEY\")\n\nllm = OpenAILlm(\n    api_key=_api_key,\n)\n\nconfig = OpenAIConfiguration(\n    model=\"gpt-4o\",\n    temperature=0.7,\n    max_tokens=2000,\n)\n\nllm = OpenAILlm(\n    api_key=_api_key,\n    configuration=config,\n    timeout=30.0,\n)\n</pre> import os from dotenv import load_dotenv from bridgic.llms.openai import OpenAILlm, OpenAIConfiguration  load_dotenv()  _api_key = os.environ.get(\"OPENAI_API_KEY\")  llm = OpenAILlm(     api_key=_api_key, )  config = OpenAIConfiguration(     model=\"gpt-4o\",     temperature=0.7,     max_tokens=2000, )  llm = OpenAILlm(     api_key=_api_key,     configuration=config,     timeout=30.0, ) In\u00a0[6]: Copied! <pre>from bridgic.core.model.types import Message, Role\n\n# Create messages\nmessages = [\n    Message.from_text(\"You are a helpful assistant.\", role=Role.SYSTEM),\n    Message.from_text(\"What is the capital of France?\", role=Role.USER),\n]\n\n# Get response\nresponse = llm.chat(\n    messages=messages,\n    model=\"gpt-4o\",\n    temperature=0.7,\n)\n\nprint(response.message.content)\n</pre> from bridgic.core.model.types import Message, Role  # Create messages messages = [     Message.from_text(\"You are a helpful assistant.\", role=Role.SYSTEM),     Message.from_text(\"What is the capital of France?\", role=Role.USER), ]  # Get response response = llm.chat(     messages=messages,     model=\"gpt-4o\",     temperature=0.7, )  print(response.message.content)  <pre>The capital of France is Paris.\n</pre> In\u00a0[8]: Copied! <pre># Stream response chunks\nfor chunk in llm.stream(messages=messages, model=\"gpt-4o\"):\n    print(chunk.delta, end=\"|\", flush=True)  # Print each chunk as it arrives\n</pre> # Stream response chunks for chunk in llm.stream(messages=messages, model=\"gpt-4o\"):     print(chunk.delta, end=\"|\", flush=True)  # Print each chunk as it arrives <pre>The| capital| of| France| is| Paris|.|</pre> In\u00a0[14]: Copied! <pre>from pydantic import BaseModel, Field\nfrom bridgic.core.model.protocols import PydanticModel, JsonSchema\n\n# Option 1: Using Pydantic Models\nclass MathProblemSolution(BaseModel):\n    \"\"\"Solution to a math problem with reasoning\"\"\"\n    reasoning: str = Field(description=\"Step-by-step reasoning\")\n    answer: int = Field(description=\"Final numerical answer\")\n\nmessages = [\n    Message.from_text(\"What is 15 * 23?\", role=Role.USER)\n]\n\n# Get structured output\nsolution = llm.structured_output(\n    messages=messages,\n    constraint=PydanticModel(model=MathProblemSolution),\n    model=\"gpt-4o\",\n)\n\nprint(f\"REASONING:\\n\\n{solution.reasoning}\\n\")\nprint(f\"ANSWER: {solution.answer}\\n\")\n</pre> from pydantic import BaseModel, Field from bridgic.core.model.protocols import PydanticModel, JsonSchema  # Option 1: Using Pydantic Models class MathProblemSolution(BaseModel):     \"\"\"Solution to a math problem with reasoning\"\"\"     reasoning: str = Field(description=\"Step-by-step reasoning\")     answer: int = Field(description=\"Final numerical answer\")  messages = [     Message.from_text(\"What is 15 * 23?\", role=Role.USER) ]  # Get structured output solution = llm.structured_output(     messages=messages,     constraint=PydanticModel(model=MathProblemSolution),     model=\"gpt-4o\", )  print(f\"REASONING:\\n\\n{solution.reasoning}\\n\") print(f\"ANSWER: {solution.answer}\\n\") <pre>REASONING:\n\n15 multiplied by 23 can be broken down into smaller, more manageable calculations using the distributive property of multiplication. Here's how:\n\n1. **Break down 23:**\n   - 23 can be expressed as 20 + 3.\n\n2. **Apply the distributive property:**\n   - 15 * 23 = 15 * (20 + 3)\n   - According to the distributive property, this can be expanded to:\n     - 15 * 20 + 15 * 3\n\n3. **Calculate the individual products:**\n   - **15 * 20**\n     - 15 * 2 = 30\n     - Append a zero (since you are multiplying by 20, which is 10 times 2):\n     - 15 * 20 = 300\n   \n   - **15 * 3**\n     - 15 * 3 = 45\n\n4. **Add the two results together:**\n   - 300 + 45 = 345\n\nThus, using the distributive property and breaking down the numbers into simpler parts, we find that 15 multiplied by 23 equals 345.\n\nANSWER: 345\n\n</pre> In\u00a0[15]: Copied! <pre>from bridgic.core.model.types import Tool\n\n# Define available tools\ntools = [\n    Tool(\n        name=\"get_weather\",\n        description=\"Get the current weather for a location\",\n        parameters={\n            \"type\": \"object\",\n            \"properties\": {\n                \"location\": {\n                    \"type\": \"string\",\n                    \"description\": \"City name, e.g., 'San Francisco, CA'\"\n                },\n                \"unit\": {\n                    \"type\": \"string\",\n                    \"enum\": [\"celsius\", \"fahrenheit\"],\n                    \"description\": \"Temperature unit\"\n                }\n            },\n            \"required\": [\"location\"]\n        }\n    ),\n    Tool(\n        name=\"calculate\",\n        description=\"Perform mathematical calculations\",\n        parameters={\n            \"type\": \"object\",\n            \"properties\": {\n                \"expression\": {\n                    \"type\": \"string\",\n                    \"description\": \"Mathematical expression to evaluate\"\n                }\n            },\n            \"required\": [\"expression\"]\n        }\n    )\n]\n\n# Model selects appropriate tool\nmessages = [\n    Message.from_text(\"What's the weather like in Paris?\", role=Role.USER)\n]\n\ntool_calls, content = llm.select_tool(\n    messages=messages,\n    tools=tools,\n    model=\"gpt-4o\",\n    tool_choice=\"auto\",\n)\n\n# Process tool calls\nfor tool_call in tool_calls:\n    print(f\"Tool: {tool_call.name}\")\n    print(f\"Arguments: {tool_call.arguments}\")\n    print(f\"Call ID: {tool_call.id}\")\n</pre> from bridgic.core.model.types import Tool  # Define available tools tools = [     Tool(         name=\"get_weather\",         description=\"Get the current weather for a location\",         parameters={             \"type\": \"object\",             \"properties\": {                 \"location\": {                     \"type\": \"string\",                     \"description\": \"City name, e.g., 'San Francisco, CA'\"                 },                 \"unit\": {                     \"type\": \"string\",                     \"enum\": [\"celsius\", \"fahrenheit\"],                     \"description\": \"Temperature unit\"                 }             },             \"required\": [\"location\"]         }     ),     Tool(         name=\"calculate\",         description=\"Perform mathematical calculations\",         parameters={             \"type\": \"object\",             \"properties\": {                 \"expression\": {                     \"type\": \"string\",                     \"description\": \"Mathematical expression to evaluate\"                 }             },             \"required\": [\"expression\"]         }     ) ]  # Model selects appropriate tool messages = [     Message.from_text(\"What's the weather like in Paris?\", role=Role.USER) ]  tool_calls, content = llm.select_tool(     messages=messages,     tools=tools,     model=\"gpt-4o\",     tool_choice=\"auto\", )  # Process tool calls for tool_call in tool_calls:     print(f\"Tool: {tool_call.name}\")     print(f\"Arguments: {tool_call.arguments}\")     print(f\"Call ID: {tool_call.id}\")  <pre>Tool: get_weather\nArguments: {'location': 'Paris'}\nCall ID: call_aLv7xon4zhsNVMcnLmxsGJ3v\n</pre>"},{"location":"tutorials/items/model_integration/llm_integration/#llm-integration","title":"LLM Integration\u00b6","text":""},{"location":"tutorials/items/model_integration/llm_integration/#installation","title":"Installation\u00b6","text":"<p>Bridgic uses a modular installation strategy\u2014install only the components you require.</p> <p>Each model integration is available as a separate package, so you can minimize dependencies and keep your environment streamlined.</p> <pre># For general OpenAI-compatible APIs, only support basic chat interfaces (Groq, Together AI, etc.)\npip install bridgic-llms-openai-like\n\n# For OpenAI models (GPT-4, GPT-3.5, etc.)\npip install bridgic-llms-openai\n\n# For vLLM server deployments\npip install bridgic-llms-vllm\n</pre> Package <code>BaseLlm</code> <code>StructuredOutput</code> <code>ToolSelection</code> <code>bridgic-llms-openai-like</code> \u2705 \u274c \u274c <code>bridgic-llms-openai</code> \u2705 \u2705 \u2705 <code>bridgic-llms-vllm</code> \u2705 \u2705 \u2705"},{"location":"tutorials/items/model_integration/llm_integration/#llm-usage","title":"LLM Usage\u00b6","text":"<p>This section demonstrates the complete lifecycle of working with models in Bridgic, from initialization to advanced features.</p>"},{"location":"tutorials/items/model_integration/llm_integration/#1-initialization","title":"1. Initialization\u00b6","text":"<p>Model initialization is straightforward and follows a consistent pattern across all providers.</p>"},{"location":"tutorials/items/model_integration/llm_integration/#2-basic-interfaces","title":"2. Basic Interfaces\u00b6","text":"<p>All LLM providers in Bridgic implement the <code>BaseLlm</code> abstract class, which defines the fundamental <code>chat</code>/<code>stream</code> interfaces and their asynchronous variant.</p>"},{"location":"tutorials/items/model_integration/llm_integration/#21-chat","title":"2.1 Chat\u00b6","text":"<p>The most basic interface for getting a complete response from the model:</p>"},{"location":"tutorials/items/model_integration/llm_integration/#22-streaming","title":"2.2 Streaming\u00b6","text":"<p>For real-time response generation:</p>"},{"location":"tutorials/items/model_integration/llm_integration/#3-advanced-protocols","title":"3. Advanced Protocols\u00b6","text":"<p>Advanced interfaces are provided through optional protocols that providers can implement based on their capabilities.</p>"},{"location":"tutorials/items/model_integration/llm_integration/#31-structured-output-structuredoutput-protocol","title":"3.1 Structured Output (<code>StructuredOutput</code> Protocol)\u00b6","text":"<p>Generate outputs that conform to specific schemas or formats:</p>"},{"location":"tutorials/items/model_integration/llm_integration/#32-tool-selection-toolselection-protocol","title":"3.2 Tool Selection (<code>ToolSelection</code> Protocol)\u00b6","text":"<p>Enable models to select and use tools (function calling):</p>"},{"location":"tutorials/items/observability/","title":"Observability","text":""},{"location":"tutorials/items/observability/#why-observability-matters","title":"Why Observability Matters","text":"<p>Developing and managing agentic systems comes with its own challenges. Unlike traditional software where execution flows are predictable, agentic systems involve dynamic decision-making, complex workflows, and interactions with external services. Without proper observability, these systems can become \"black boxes\"\u2014you know inputs and outputs, but have little insight into what happens in between.</p> <p>Effective observability provides several critical benefits:</p> <ul> <li> <p>Understanding Runtime Behavior: See what's actually happening as your system executes, including which tasks are running, their execution order, and how they interact with each other.</p> </li> <li> <p>Task Level Visibility: Track inputs and outputs at the granularity of individual task units, enabling you to understand data flow, identify where transformations occur, and debug issues at the right level of detail.</p> </li> <li> <p>Performance Optimization: Help measure execution times and identify performance bottlenecks of your agentic system built with Bridgic. This visibility is essential for continuously optimizing agentic systems.</p> </li> </ul>"},{"location":"tutorials/items/observability/#how-observability-works","title":"How Observability Works","text":""},{"location":"tutorials/items/observability/#worker-execution","title":"Worker Execution","text":"<p>In the Bridgic framework, each task execution happens at the Worker granularity. Worker is the basic unit that represents a discrete task which can be executed. This worker-centric architecture provides natural boundaries for observability instrumentation, regardless of complexity. Whether you're building a simple sequential workflow or a complex graph-based agentic system (like ReAct), every operation runs as a worker.</p>"},{"location":"tutorials/items/observability/#callback-mechanism","title":"Callback Mechanism","text":"<p>Bridgic provides a flexible callback mechanism that hooks into worker execution at key lifecycle points. This callback system is designed to be non-intrusive\u2014callbacks observe and instrument execution without modifying your core business logic. Refer to Worker Callback Mechanism for more about how this mechanism works in detail.</p>"},{"location":"tutorials/items/observability/#available-third-party-integrations","title":"Available Third-Party Integrations","text":"<p>Bridgic currently provides integrations with the following observability platforms:</p> <ul> <li>Opik Integration</li> <li>LangWatch Integration</li> </ul>"},{"location":"tutorials/items/observability/lang_watch_integration/","title":"LangWatch Integration","text":""},{"location":"tutorials/items/observability/lang_watch_integration/#overview","title":"Overview","text":"<p>LangWatch is a comprehensive observability platform designed for LLM applications. The <code>bridgic-traces-langwatch</code> package enables seamless integration of LangWatch into your Bridgic-based agentic workflows.</p> <p>This integration is primarily supported by <code>LangWatchTraceCallback</code>, a WorkerCallback implementation that automatically instruments the worker execution with LangWatch tracing, which provides comprehensive observability by:</p> <ul> <li>Worker Execution Traces Tracking: Record the execution of each worker as a span in LangWatch, allowing to visualize start/end time, duration.</li> <li>Worker Execution Data Reporting: Capture the input, output and other necessary information and then log to the LangWatch platform.</li> <li>Hierarchical Trace Structure: Organize execution traces in a hierarchy that reflects the nesting between automa layers, making it straightforward to see how top-level automa is composed of the execution of multiple nested workers.</li> </ul>"},{"location":"tutorials/items/observability/lang_watch_integration/#prerequisites","title":"Prerequisites","text":"<p>LangWatch provides a hosted version of the platform, or you can run the platform locally.</p> <ul> <li>To use the hosted version, you need to create a LangWatch account and grab your API Keyfrom the dashboard.</li> <li>To run LangWatch locally, see the self-hosting guide for more information.</li> </ul>"},{"location":"tutorials/items/observability/lang_watch_integration/#using-langwatch-in-bridgic","title":"Using LangWatch in Bridgic","text":""},{"location":"tutorials/items/observability/lang_watch_integration/#step-1-install-package","title":"Step 1: Install package","text":"<pre><code># Install the LangWatch tracing package\npip install bridgic-traces-langwatch\n</code></pre>"},{"location":"tutorials/items/observability/lang_watch_integration/#step-2-configure-langwatch","title":"Step 2: Configure LangWatch","text":"<p>Using Environment Variables</p> <p>Set the following environment variables:</p> <pre><code>export LANGWATCH_API_KEY=\"your-api-key-here\"\nexport LANGWATCH_ENDPOINT=\"https://app.langwatch.ai\"  # Optional, defaults to https://app.langwatch.ai\n</code></pre>"},{"location":"tutorials/items/observability/lang_watch_integration/#step-3-register-the-callback","title":"Step 3: Register the callback","text":"<p>You can enable LangWatch tracing globally with a single helper call (recommended), or wire the callback manually when you need custom behavior. Bridgic also lets you scope tracing to a single automa via <code>RunningOptions</code>.</p>"},{"location":"tutorials/items/observability/lang_watch_integration/#method-1-application-wide-registration-helper-or-manual","title":"Method 1: Application-wide registration (helper or manual)","text":"<p>Choose whichever snippet fits your setup\u2014they produce the same effect.</p> start_langwatch_traceGlobalSetting <pre><code>from bridgic.traces.langwatch import start_langwatch_trace\n\nstart_langwatch_trace(\n    api_key=None,            # defaults to LANGWATCH_API_KEY env var\n    endpoint_url=None,       # defaults to LANGWATCH_ENDPOINT or https://app.langwatch.ai\n    base_attributes=None     # optional: shared attributes applied to every trace\n)\n</code></pre> <pre><code>from bridgic.core.automa.worker import WorkerCallbackBuilder\nfrom bridgic.core.config import GlobalSetting\nfrom bridgic.traces.langwatch import LangWatchTraceCallback\n\nGlobalSetting.set(callback_builders=[WorkerCallbackBuilder(\n    LangWatchTraceCallback,\n    init_kwargs={\n        \"api_key\": None,\n        \"endpoint_url\": None,\n        \"base_attributes\": None\n    }\n)])\n</code></pre> <pre><code>from bridgic.core.automa import GraphAutoma, worker\n\nclass DataAnalysisAutoma(GraphAutoma):\n    @worker(is_start=True)\n    async def collect_data(self, topic: str) -&gt; dict:\n        \"\"\"Collect data for the given topic.\"\"\"\n        # Simulate data collection\n        await asyncio.sleep(1)\n        return {\n            \"topic\": topic,\n            \"data_points\": [\"point1\", \"point2\", \"point3\"],\n            \"timestamp\": \"2024-01-01\"\n        }\n\n    @worker(dependencies=[\"collect_data\"])\n    async def analyze_trends(self, data: dict) -&gt; dict:\n        \"\"\"Analyze trends in the collected data.\"\"\"\n        # Simulate trend analysis\n        await asyncio.sleep(1)\n        return {\n            \"trends\": [\"trend1\", \"trend2\"],\n            \"confidence\": 0.85,\n            \"source_data\": data\n        }\n\n    @worker(dependencies=[\"analyze_trends\"], is_output=True)\n    async def generate_report(self, analysis: dict) -&gt; str:\n        \"\"\"Generate a final report.\"\"\"\n        await asyncio.sleep(1)\n        return f\"Report: Found {len(analysis['trends'])} trends with {analysis['confidence']} confidence.\"\n\nasync def automa_arun():\n    # Call start_langwatch_trace(...) or configure GlobalSetting(...) once at startup\n    from bridgic.traces.langwatch import start_langwatch_trace\n\n    start_langwatch_trace(\n        api_key=None,\n        endpoint_url=None,\n        base_attributes=None\n    )\n\n    automa = DataAnalysisAutoma()\n    result = await automa.arun(topic=\"market analysis\")\n    print(result)\n\nif __name__ == \"__main__\":\n    import asyncio\n    asyncio.run(automa_arun())\n</code></pre>"},{"location":"tutorials/items/observability/lang_watch_integration/#method-2-per-automa-scope-with-runningoptions","title":"Method 2: Per-automa scope with <code>RunningOptions</code>","text":"<p>When only a specific automa needs LangWatch tracing, configure the callback through <code>RunningOptions</code>.</p> <pre><code>from bridgic.core.automa import GraphAutoma, RunningOptions, worker\nfrom bridgic.core.automa.worker import WorkerCallbackBuilder\nfrom bridgic.traces.langwatch import LangWatchTraceCallback\n\nclass DataAnalysisAutoma(GraphAutoma):\n    @worker(is_start=True)\n    async def collect_data(self, topic: str) -&gt; dict:\n        \"\"\"Collect data for the given topic.\"\"\"\n        # Simulate data collection\n        await asyncio.sleep(1)\n        return {\n            \"topic\": topic,\n            \"data_points\": [\"point1\", \"point2\", \"point3\"],\n            \"timestamp\": \"2024-01-01\"\n        }\n\n    @worker(dependencies=[\"collect_data\"])\n    async def analyze_trends(self, data: dict) -&gt; dict:\n        \"\"\"Analyze trends in the collected data.\"\"\"\n        # Simulate trend analysis\n        await asyncio.sleep(1)\n        return {\n            \"trends\": [\"trend1\", \"trend2\"],\n            \"confidence\": 0.85,\n            \"source_data\": data\n        }\n\n    @worker(dependencies=[\"analyze_trends\"], is_output=True)\n    async def generate_report(self, analysis: dict) -&gt; str:\n        \"\"\"Generate a final report.\"\"\"\n        await asyncio.sleep(1)\n        return f\"Report: Found {len(analysis['trends'])} trends with {analysis['confidence']} confidence.\"\n\nasync def automa_arun():\n    builder = WorkerCallbackBuilder(LangWatchTraceCallback, init_kwargs={\n        \"api_key\": None,\n        \"endpoint_url\": None,\n        \"base_attributes\": None\n    })\n    running_options = RunningOptions(callback_builders=[builder])\n    automa = DataAnalysisAutoma(running_options=running_options)\n    result = await automa.arun(topic=\"market analysis\")\n    print(result)\n\nif __name__ == \"__main__\":\n    import asyncio\n    asyncio.run(automa_arun())\n</code></pre> <p>Once your Bridgic application has finished running, traces will be automatically sent to LangWatch. You can view them in the LangWatch dashboard to explore rich visual insights and detailed traces of your workflow.</p>"},{"location":"tutorials/items/observability/opik_integration/","title":"Opik Integration","text":""},{"location":"tutorials/items/observability/opik_integration/#overview","title":"Overview","text":"<p>Comet Opik is a comprehensive observability platform designed for agentic systems. The <code>bridgic-traces-opik</code> package enables seamless integration of Opik into your Bridgic-based agentic workflows.</p> <p>This integration is primarily supported by <code>OpikTraceCallback</code>, a WorkerCallback implementation that automatically instruments the worker execution with Opik tracing, which provides comprehensive observability by:</p> <ul> <li>Worker Execution Traces Tracking: Record the execution of each worker as a span in Opik, allowing to visualize start/end time, duration.</li> <li>Worker Execution Data Reporting: Capture the input, output and other necessary information and then log to the opik platform.</li> <li>Hierarchical Trace Structure: Organize execution traces in a hierarchy that reflects the nesting between automa layers, making it straightforward to see how top-level automa is composed of the execution of multiple nested workers.</li> </ul>"},{"location":"tutorials/items/observability/opik_integration/#prerequisites","title":"Prerequisites","text":"<p>Comet provides a hosted version of the Opik platform, or you can run the platform locally.</p> <ul> <li>To use the hosted version, you need to create a Comet account and grab your API Key.</li> <li>To run the Opik platform locally, see the installation guide for more information.</li> </ul>"},{"location":"tutorials/items/observability/opik_integration/#using-opik-in-bridgic","title":"Using Opik in Bridgic","text":""},{"location":"tutorials/items/observability/opik_integration/#step-1-install-package","title":"Step 1: Install package","text":"<pre><code># Automatically install the Opik package\npip install bridgic-traces-opik\n</code></pre>"},{"location":"tutorials/items/observability/opik_integration/#step-2-configure-opik","title":"Step 2: Configure Opik","text":"<p>The recommended approach to configuring the Python SDK is to use the opik configure command. This will prompt you to set up your API key and Opik instance URL (if applicable) to ensure proper routing and authentication. All details will be saved to a configuration file.</p> Opik CloudSelf-hosting <p>If you are using the Cloud version of the platform, you can configure the SDK by running:</p> <pre><code>import opik\n\nopik.configure(use_local=False)\n</code></pre> <p>You can also configure the SDK by calling <code>configure</code> from the Command line:</p> <pre><code>opik configure\n</code></pre> <p>If you are self-hosting the platform, you can configure the SDK by running:</p> <pre><code>import opik\n\nopik.configure(use_local=True)\n</code></pre> <p>or from the Command line:</p> <pre><code>opik configure --use_local\n</code></pre> <p>The <code>configure</code> methods will prompt you for the necessary information and save it to a configuration file (<code>~/.opik.config</code>). When using the command line version, you can use the <code>-y</code> or <code>--yes</code> flag to automatically approve any confirmation prompts:</p> <pre><code>opik configure --yes\n</code></pre>"},{"location":"tutorials/items/observability/opik_integration/#step-3-register-the-callback","title":"Step 3: Register the callback","text":"<p>You can register Opik tracing at the scope that best fits your application. <code>start_opik_trace</code> is the fastest path (a single line that configures global tracing via <code>GlobalSetting</code>). When you want to customize the same global setup or target only a specific automa, Bridgic exposes direct hooks for both use cases.</p>"},{"location":"tutorials/items/observability/opik_integration/#method-1-application-wide-registration-helper-or-manual","title":"Method 1: Application-wide registration (helper or manual)","text":"<p>Pick one of the two options below\u2014they produce the exact same runtime behavior:</p> start_opik_traceGlobalSetting <pre><code>from bridgic.traces.opik import start_opik_trace\nstart_opik_trace(project_name=\"bridgic-integration-demo\")\n</code></pre> <pre><code>from bridgic.core.automa.worker import WorkerCallbackBuilder\nfrom bridgic.core.config import GlobalSetting\nfrom bridgic.traces.opik import OpikTraceCallback\n\nGlobalSetting.set(callback_builders=[WorkerCallbackBuilder(\n    OpikTraceCallback,\n    init_kwargs={\"project_name\": \"bridgic-integration-demo\"}\n)])\n</code></pre> <pre><code>from bridgic.core.automa import GraphAutoma, worker\nfrom bridgic.traces.opik import start_opik_trace\n\nclass DataAnalysisAutoma(GraphAutoma):\n    @worker(is_start=True)\n    async def collect_data(self, topic: str) -&gt; dict:\n        \"\"\"Collect data for the given topic.\"\"\"\n        # Simulate data collection\n        await asyncio.sleep(1)\n        return {\n            \"topic\": topic,\n            \"data_points\": [\"point1\", \"point2\", \"point3\"],\n            \"timestamp\": \"2024-01-01\"\n        }\n\n    @worker(dependencies=[\"collect_data\"])\n    async def analyze_trends(self, data: dict) -&gt; dict:\n        \"\"\"Analyze trends in the collected data.\"\"\"\n        # Simulate trend analysis\n        await asyncio.sleep(1)\n        return {\n            \"trends\": [\"trend1\", \"trend2\"],\n            \"confidence\": 0.85,\n            \"source_data\": data\n        }\n\n    @worker(dependencies=[\"analyze_trends\"], is_output=True)\n    async def generate_report(self, analysis: dict) -&gt; str:\n        \"\"\"Generate a final report.\"\"\"\n        await asyncio.sleep(1)\n        return f\"Report: Found {len(analysis['trends'])} trends with {analysis['confidence']} confidence.\"\n\nasync def automa_arun():\n    # Call either start_opik_trace(...) or GlobalSetting.set(...) once at startup\n    start_opik_trace(project_name=\"bridgic-integration-demo\")\n    automa = DataAnalysisAutoma()\n    result = await automa.arun(topic=\"market analysis\")\n    print(result)\n\nif __name__ == \"__main__\":\n    import asyncio\n    asyncio.run(automa_arun())\n</code></pre>"},{"location":"tutorials/items/observability/opik_integration/#method-2-per-automa-scope-with-runningoptions","title":"Method 2: Per-automa scope with <code>RunningOptions</code>","text":"<p>When only a specific automa needs tracing, configure the callback through <code>RunningOptions</code>. Each automa gets its own callback instance, leaving other automa untouched.</p> <pre><code>from bridgic.core.automa import GraphAutoma, RunningOptions, worker\nfrom bridgic.core.automa.worker import WorkerCallbackBuilder\nfrom bridgic.traces.opik import OpikTraceCallback\n\nclass DataAnalysisAutoma(GraphAutoma):\n    @worker(is_start=True)\n    async def collect_data(self, topic: str) -&gt; dict:\n        \"\"\"Collect data for the given topic.\"\"\"\n        # Simulate data collection\n        await asyncio.sleep(1)\n        return {\n            \"topic\": topic,\n            \"data_points\": [\"point1\", \"point2\", \"point3\"],\n            \"timestamp\": \"2024-01-01\"\n        }\n\n    @worker(dependencies=[\"collect_data\"])\n    async def analyze_trends(self, data: dict) -&gt; dict:\n        \"\"\"Analyze trends in the collected data.\"\"\"\n        # Simulate trend analysis\n        await asyncio.sleep(1)\n        return {\n            \"trends\": [\"trend1\", \"trend2\"],\n            \"confidence\": 0.85,\n            \"source_data\": data\n        }\n\n    @worker(dependencies=[\"analyze_trends\"], is_output=True)\n    async def generate_report(self, analysis: dict) -&gt; str:\n        \"\"\"Generate a final report.\"\"\"\n        await asyncio.sleep(1)\n        return f\"Report: Found {len(analysis['trends'])} trends with {analysis['confidence']} confidence.\"\n\nasync def automa_arun():\n    builder = WorkerCallbackBuilder(OpikTraceCallback, init_kwargs={\"project_name\": \"bridgic-integration-demo\"})\n    running_options = RunningOptions(callback_builders=[builder])\n    automa = DataAnalysisAutoma(running_options=running_options)\n    result = await automa.arun(topic=\"market analysis\")\n    print(result)\n\nif __name__ == \"__main__\":\n    import asyncio\n    asyncio.run(automa_arun())\n</code></pre> <p>Once your Bridgic application has finished running, your terminal might display the following message:</p> <pre><code>$ python bridgic-demo/demo.py \nOPIK: Started logging traces to the \"bridgic-integration-demo\" project at http://localhost:5173/api/v1/session/redirect/projects/?trace_id=019a9709-e437-7b30-861e-76006b75e969&amp;path=aHR0cDovL2xvY2FsaG9zdDo1MTczL2FwaS8=\nReport: Found 2 trends with 0.85 confidence.\n</code></pre> <p>You can dive into the Opik app to explore rich visual insights and detailed traces of your workflow.</p>"},{"location":"tutorials/items/quick_start/quick_start/","title":"Quick Start","text":"<p>Execute the following commands in the shell:</p> <pre>export OPENAI_API_KEY=\"&lt;your_openai_api_key&gt;\"\nexport OPENAI_MODEL_NAME=\"&lt;the_model_name&gt;\"\n</pre> In\u00a0[\u00a0]: Copied! <pre># Get the environment variables.\nimport os\n\n# Import the necessary packages.\nfrom bridgic.core.automa import GraphAutoma, worker\nfrom bridgic.core.model.types import Message, Role\n\n# Here we use OpenAILikeLlm because the package `bridgic-llms-openai-like` is installed automatically \n# when you install Bridgic. This makes sure the OpenAI-like model integration works out of the box.\nfrom bridgic.llms.openai_like import OpenAILikeLlm, OpenAILikeConfiguration\n\n\n# In this tutorial, we use OpenAI as an example. \n# You can freely replace these model settings to use any LLM provider you like.\n_api_key = os.environ.get(\"OPENAI_API_KEY\")\n_api_base = os.environ.get(\"OPENAI_API_BASE\")\n_model_name = os.environ.get(\"OPENAI_MODEL_NAME\")\n\nllm = OpenAILikeLlm(\n    api_key=_api_key,\n    api_base=_api_base,\n    configuration=OpenAILikeConfiguration(model=_model_name),\n    timeout=20,\n)\n</pre> # Get the environment variables. import os  # Import the necessary packages. from bridgic.core.automa import GraphAutoma, worker from bridgic.core.model.types import Message, Role  # Here we use OpenAILikeLlm because the package `bridgic-llms-openai-like` is installed automatically  # when you install Bridgic. This makes sure the OpenAI-like model integration works out of the box. from bridgic.llms.openai_like import OpenAILikeLlm, OpenAILikeConfiguration   # In this tutorial, we use OpenAI as an example.  # You can freely replace these model settings to use any LLM provider you like. _api_key = os.environ.get(\"OPENAI_API_KEY\") _api_base = os.environ.get(\"OPENAI_API_BASE\") _model_name = os.environ.get(\"OPENAI_MODEL_NAME\")  llm = OpenAILikeLlm(     api_key=_api_key,     api_base=_api_base,     configuration=OpenAILikeConfiguration(model=_model_name),     timeout=20, ) In\u00a0[19]: Copied! <pre>class WordLearningAssistant(GraphAutoma):\n    @worker(is_start=True)\n    async def generate_derivatives(self, word: str):\n        print(f\"------Generating derivatives for {word}------\")\n        response = await llm.achat(\n            model=_model_name,\n            messages=[\n                Message.from_text(text=\"You are a word learning assistant. Generate derivatives of the input word in a list.\", role=Role.SYSTEM),\n                Message.from_text(text=word, role=Role.USER),\n            ]\n        )\n        print(response.message.content)\n        print(f\"------End of generating derivatives------\\n\")\n        return response.message.content\n\n    @worker(dependencies=[\"generate_derivatives\"], is_output=True)\n    async def make_sentences(self, derivatives):\n        print(f\"------Making sentences with------\")\n        response = await llm.achat(\n            model=_model_name,\n            messages=[\n                Message.from_text(text=\"You are a word learning assistant. Make sentences with the input derivatives in a list.\", role=Role.SYSTEM),\n                Message.from_text(text=derivatives, role=Role.USER),\n            ]\n        )\n        print(response.message.content)\n        print(f\"------End of making sentences------\\n\")\n        return response.message.content\n\nword_learning_assistant = WordLearningAssistant()\n</pre> class WordLearningAssistant(GraphAutoma):     @worker(is_start=True)     async def generate_derivatives(self, word: str):         print(f\"------Generating derivatives for {word}------\")         response = await llm.achat(             model=_model_name,             messages=[                 Message.from_text(text=\"You are a word learning assistant. Generate derivatives of the input word in a list.\", role=Role.SYSTEM),                 Message.from_text(text=word, role=Role.USER),             ]         )         print(response.message.content)         print(f\"------End of generating derivatives------\\n\")         return response.message.content      @worker(dependencies=[\"generate_derivatives\"], is_output=True)     async def make_sentences(self, derivatives):         print(f\"------Making sentences with------\")         response = await llm.achat(             model=_model_name,             messages=[                 Message.from_text(text=\"You are a word learning assistant. Make sentences with the input derivatives in a list.\", role=Role.SYSTEM),                 Message.from_text(text=derivatives, role=Role.USER),             ]         )         print(response.message.content)         print(f\"------End of making sentences------\\n\")         return response.message.content  word_learning_assistant = WordLearningAssistant() In\u00a0[20]: Copied! <pre>res = await word_learning_assistant.arun(word=\"happy\")\n</pre> res = await word_learning_assistant.arun(word=\"happy\") <pre>------Generating derivatives for happy------\nHere are some derivatives of the word \"happy\":\n\n1. Happiness\n2. Happily\n3. Happier\n4. Happiest\n5. Unhappy\n6. Unhappiness\n7. Happinesses (plural form)\n8. Happifying (gerund form)\n9. Happify (verb form)\n\nFeel free to ask for derivatives of another word!\n------End of generating derivatives------\n\n------Making sentences with------\nSure! Here are sentences using each of the derivatives of the word \"happy\":\n\n1. **Happiness**: The pursuit of happiness is a common goal for many people.\n2. **Happily**: She smiled happily as she opened her birthday gifts.\n3. **Happier**: After taking a vacation, I felt much happier than I had in months.\n4. **Happiest**: That day was the happiest moment of my life when my daughter graduated.\n5. **Unhappy**: He seemed unhappy at the party and left early.\n6. **Unhappiness**: Her unhappiness was evident in her quiet demeanor.\n7. **Happinesses**: Different people find happinesses in various aspects of life, like family, work, and hobbies.\n8. **Happifying**: The act of volunteering can be a happifying experience for both the giver and the receiver.\n9. **Happify**: Listening to uplifting music can help to happify your day.\n\nLet me know if you need sentences for another word!\n------End of making sentences------\n\n</pre> <p>Congratulations! We have successfully completed the word learning assistant, which performed the task exactly according to our requirements.</p> In\u00a0[\u00a0]: Copied! <pre>class MyAutoma(GraphAutoma):\n    @worker(is_start=True)\n    async def start(self, x: int):\n        return x    \n</pre> class MyAutoma(GraphAutoma):     @worker(is_start=True)     async def start(self, x: int):         return x     <p>Or, you can also use it like this:</p> In\u00a0[\u00a0]: Copied! <pre>class MyAutoma(GraphAutoma): ...\nmy_automa = MyAutoma()\n\n# Add the function as a worker with worker decorator in the instance of the automa\n@my_automa.worker(is_start=True)\nasync def start(x: int):\n    return x\n</pre> class MyAutoma(GraphAutoma): ... my_automa = MyAutoma()  # Add the function as a worker with worker decorator in the instance of the automa @my_automa.worker(is_start=True) async def start(x: int):     return x  <p>Another API <code>add_func_as_worker()</code> can also be used to add workers into a <code>GraphAutoma</code>.</p> In\u00a0[\u00a0]: Copied! <pre>async def start(x: int):\n    return x\n\nclass MyAutoma(GraphAutoma): ...\nmy_automa = MyAutoma()\n\n# Add the function as a worker\nmy_automa.add_func_as_worker(\n    key=\"start\",\n    func=start,\n    is_start=True,\n)\n</pre> async def start(x: int):     return x  class MyAutoma(GraphAutoma): ... my_automa = MyAutoma()  # Add the function as a worker my_automa.add_func_as_worker(     key=\"start\",     func=start,     is_start=True, )  <p>In addition to functions being convertible to workers, subclasses that inherit from <code>Worker</code> and override either <code>run()</code> or <code>arun()</code> can also be used directly as workers, whose instances can be added into a <code>GraphAutoma</code> by the <code>add_worker()</code> API.</p> In\u00a0[\u00a0]: Copied! <pre>from bridgic.core.automa.worker import Worker\n\nclass MyWorker(Worker):\n    async def arun(self, x: int):\n        return x\n\nmy_worker = MyWorker()\n\n# Add the worker to the automa\nclass MyAutoma(GraphAutoma): ...\nmy_automa = MyAutoma()\nmy_automa.add_worker(\n    key=\"my_worker\",\n    worker=my_worker,\n    is_start=True,\n)\n\n# Run the worker\nres = await my_automa.arun(x=1)\nprint(res)\n</pre> from bridgic.core.automa.worker import Worker  class MyWorker(Worker):     async def arun(self, x: int):         return x  my_worker = MyWorker()  # Add the worker to the automa class MyAutoma(GraphAutoma): ... my_automa = MyAutoma() my_automa.add_worker(     key=\"my_worker\",     worker=my_worker,     is_start=True, )  # Run the worker res = await my_automa.arun(x=1) print(res) <p>Note:</p> <ol> <li>A specific worker that inherits from <code>Worker</code> must override either the <code>run()</code> or <code>arun()</code> method.</li> <li>Bridgic is a framework primarily designed for asynchronous execution, if both <code>run()</code> and <code>arun()</code> of a worker are overridden, <code>arun()</code> will take precedence. Refer to <code>Worker</code> for details.</li> </ol> <p>In any of these ways the workers can be correctly added into <code>MyAutoma</code>.</p> <p>Whether using decorator syntax or the corresponding API, there are usually some parameters:</p> <ol> <li><code>key</code>: A string used as the worker key. As the unique identifier of a worker in the current automa, it must be ensured that there are no duplicate keys within the same automa. Function or class names are used by default.</li> <li><code>func</code>(in <code>add_func_as_worker()</code>) or <code>worker</code>(in <code>add_worker()</code>): The actual callable object. The decorator syntax does not need this parameter.</li> <li><code>is_start</code>: <code>True</code> or <code>False</code>. Marking the worker as the start worker of the automa. It can be set for multiple workers.</li> <li><code>dependencies</code>: A list of worker keys. Marking the preceding workers that the worker depends on.</li> <li><code>is_output</code>: <code>True</code> or <code>False</code>. Marking the worker as the output worker of the automa. Only one output worker can be set per execution branch.</li> <li><code>args_mapping_rule</code>: The arguments mapping rule. For detailed information on the parameter binding between workers, please refer to the tutorial: Parameter Binding</li> </ol> <p>Note: In Bridgic, a worker must be added to an automa before it can be scheduled and executed. In another word, you shouldn\u2019t directly call <code>worker.arun()</code> or <code>worker.run()</code> to run a worker.</p> In\u00a0[3]: Copied! <pre># Write workers in MyAutoma\nclass MyAutoma(GraphAutoma):\n    @worker(is_start=True)\n    async def worker_0(self, a, b, x, y):\n        print(f\"worker_0: a={a}, b={b}, x={x}, y={y}\")\n\n    @worker(is_start=True)\n    async def worker_1(self, x, y):\n        print(f\"worker_1: x={x}, y={y}\")\n</pre> # Write workers in MyAutoma class MyAutoma(GraphAutoma):     @worker(is_start=True)     async def worker_0(self, a, b, x, y):         print(f\"worker_0: a={a}, b={b}, x={x}, y={y}\")      @worker(is_start=True)     async def worker_1(self, x, y):         print(f\"worker_1: x={x}, y={y}\") <p>After all the required workers are defined in an automa, the automa can be called with <code>await automa_obj.arun(*args, **kwargs)</code> to start the entire scheduling process.</p> <p>Bridgic is a framework built on asynchronous programming. Thus <code>Graphautoma</code> must be started using arun(). However, workers may execute in concurrency mode when needed.</p> <p>At startup, the arguments of <code>automa_obj.arun(*args, **kwargs)</code> will be distributed to the worker with <code>is_start=True</code> according to positional parameters and keyword parameters.</p> <ul> <li>positional parameters: The positional arguments passed to <code>arun</code> are mapped to the parameters of the workers marked with <code>is_start=True</code>, following the order in which they are provided. An error will be raised if the parameter list of some worker is shorter than the number of positional arguments passed to <code>arun()</code>.</li> <li>keyword parameters: The keyword arguments passed to <code>arun</code> are mapped to the corresponding parameters of the workers marked with <code>is_start=True</code>.</li> <li>priority: Positional arguments take precedence over keyword arguments..</li> </ul> <p>For example: we pass positional arguments <code>1</code> and <code>2</code>, and keyword arguments <code>x=3</code>, <code>y=4</code>.</p> In\u00a0[4]: Copied! <pre>my_automa = MyAutoma()\nawait my_automa.arun(1, 2, x=3, y=4)\n</pre> my_automa = MyAutoma() await my_automa.arun(1, 2, x=3, y=4) <pre>worker_0: a=1, b=2, x=3, y=4\nworker_1: x=1, y=2\n</pre> <p><code>1</code> and <code>2</code> were received in order by the first and second parameters of <code>worker_0</code> and <code>worker_1</code> respectively. Because positional arguments take precedence over keyword arguments, even if the parameter names of <code>worker_1</code> are the same as the input keyword parameters, they will still preferentially receive positional arguments.</p> <p>An error will be raised if the parameter list of some worker is shorter than the number of positional arguments passed to <code>arun</code>.</p> In\u00a0[\u00a0]: Copied! <pre>my_automa = MyAutoma()\nawait my_automa.arun(1, 2, 3, y=4)  # worker_1 raises an error\n</pre> my_automa = MyAutoma() await my_automa.arun(1, 2, 3, y=4)  # worker_1 raises an error  <p>If all arguments are passed in keyword format, each worker with <code>is_start=True</code> can receive the corresponding values.</p> In\u00a0[10]: Copied! <pre>my_automa = MyAutoma()\nawait my_automa.arun(a=1, b=2, x=3, y=4)\n</pre> my_automa = MyAutoma() await my_automa.arun(a=1, b=2, x=3, y=4) <pre>worker_0: a=1, b=2, x=3, y=4\nworker_1: x=3, y=4\n</pre> <p>Now, we can start building our Bridgic project!</p>"},{"location":"tutorials/items/quick_start/quick_start/#quick-start","title":"Quick Start\u00b6","text":"<p>In this tutorial, we assume that Bridgic is already installed on your system. If that\u2019s not the case, see Installation.</p> <p>Let's start by building a simple word learning assistant. You provide a word, and the assistant will generate its derived forms and create sentences with them. This example will also show how to use Bridgic in practice.</p>"},{"location":"tutorials/items/quick_start/quick_start/#word-learning-assistant","title":"Word learning assistant\u00b6","text":""},{"location":"tutorials/items/quick_start/quick_start/#1-model-initialization","title":"1. Model Initialization\u00b6","text":"<p>Before getting started, let's set up our environment. In this quick start, we'll use the integration out of the box. For an in-depth explanation of model integration, see: LLM Integration.</p>"},{"location":"tutorials/items/quick_start/quick_start/#2-automa-orchestration","title":"2. Automa Orchestration\u00b6","text":"<p>There are two steps to complete the word learning assistant:</p> <ol> <li>Generate derivatives of the input word.</li> <li>Make sentences with derivatives.</li> </ol>"},{"location":"tutorials/items/quick_start/quick_start/#3-agent-running","title":"3. Agent Running\u00b6","text":"<p>Let's run this assistant, via <code>arun</code> method:</p>"},{"location":"tutorials/items/quick_start/quick_start/#what-have-we-learnt","title":"What have we learnt?\u00b6","text":"<p>The above example idemonstrates a typical way to write an agent application with Bridgic. Let's now explore some of its components.</p>"},{"location":"tutorials/items/quick_start/quick_start/#worker","title":"Worker\u00b6","text":"<p>Any callable object (such as functions, methods, etc.) can be converted into a worker object which serve as the basic execution unit in Bridgic for scheduling and orchestration.</p> <p>Just as in the example of the word learning assistant, we can use a decorator syntax <code>@worker</code> to wrap functions and methods into a worker object.</p>"},{"location":"tutorials/items/quick_start/quick_start/#graphautoma","title":"GraphAutoma\u00b6","text":"<p>An automa is an entity that manages and orchestrates a group of workers, serving as the scheduling engine . In the example of the word learning assistant above, we used the subclass of <code>Automa</code>, i.e. <code>GraphAutoma</code>, which performs the scheduling according to the topological sorting among workers.</p> <p>You should subclass <code>GraphAutoma</code> and declare methods as workers with <code>@worker</code>.</p>"}]}